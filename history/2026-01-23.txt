------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Machine Learning
Software Engineering
 received from  Wed 21 Jan 26 19:00:00 GMT  to  Thu 22 Jan 26 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2601.15305
Date: Mon, 12 Jan 2026 20:33:39 GMT   (215kb)

Title: Gated Sparse Attention: Combining Computational Efficiency with Training
  Stability for Long-Context Language Models
Authors: Alfred Shen, Aaron Shen
Categories: cs.AI
Comments: 15 pages, 1 figure, attention mechanism, sparse attention, gating,
  long-context
\\
  The computational burden of attention in long-context language models has
motivated two largely independent lines of work: sparse attention mechanisms
that reduce complexity by attending to selected tokens, and gated attention
variants that improve training sta-bility while mitigating the attention sink
phenomenon. We observe that these approaches address complementary weaknesses
and propose Gated Sparse Attention (GSA), an architecture that realizes the
benefits of both. GSA incorporates a gated lightning indexer with sigmoid
activations that produce bounded, interpretable selection scores, an adaptive
sparsity controller that modulates the number of attended tokens based on local
uncertainty, and dual gating at the value and output stages. We establish
theoretical foundations for the approach, including complexity analysis,
expressiveness results, and convergence guarantees. In experiments with 1.7B
parameter models trained on 400B tokens, GSA matches the efficiency of
sparse-only baselines (12-16x speedup at 128K context) while achieving the
quality gains associated with gated attention: perplexity improves from 6.03 to
5.70, RULER scores at 128K context nearly double, and attention to the first
token, a proxy for attention sinks, drops from 47% to under 4%. Training
stability improves markedly, with loss spikes reduced by 98%.
\\ ( https://arxiv.org/abs/2601.15305 ,  215kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15306
Date: Tue, 13 Jan 2026 07:49:41 GMT   (276kb)

Title: Uncovering Latent Bias in LLM-Based Emergency Department Triage Through
  Proxy Variables
Authors: Ethan Zhang
Categories: cs.AI
Comments: 15 pages, 3 figures
\\
  Recent advances in large language models (LLMs) have enabled their
integration into clinical decision-making; however, hidden biases against
patients across racial, social, economic, and clinical backgrounds persist. In
this study, we investigate bias in LLM-based medical AI systems applied to
emergency department (ED) triage. We employ 32 patient-level proxy variables,
each represented by paired positive and negative qualifiers, and evaluate their
effects using both public (MIMIC-IV-ED Demo, MIMIC-IV Demo) and
restricted-access credentialed (MIMIC-IV-ED and MIMIC-IV) datasets as
appropriate~\cite{mimiciv_ed_demo,mimiciv_ed,mimiciv}. Our results reveal
discriminatory behavior mediated through proxy variables in ED triage
scenarios, as well as a systematic tendency for LLMs to modify perceived
patient severity when specific tokens appear in the input context, regardless
of whether they are framed positively or negatively. These findings indicate
that AI systems is still imperfectly trained on noisy, sometimes non-causal
signals that do not reliably reflect true patient acuity. Consequently, more
needs to be done to ensure the safe and responsible deployment of AI
technologies in clinical settings.
\\ ( https://arxiv.org/abs/2601.15306 ,  276kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15307
Date: Tue, 13 Jan 2026 14:42:56 GMT   (7450kb)

Title: DeepSurvey-Bench: Evaluating Academic Value of Automatically Generated
  Scientific Survey
Authors: Guo-Biao Zhang, Ding-Yuan Liu, Da-Yi Wu, Tian Lan, Heyan Huang,
  Zhijing Wu, Xian-Ling Mao
Categories: cs.AI cs.CL
\\
  The rapid development of automated scientific survey generation technology
has made it increasingly important to establish a comprehensive benchmark to
evaluate the quality of generated surveys.Nearly all existing evaluation
benchmarks rely on flawed selection criteria such as citation counts and
structural coherence to select human-written surveys as the ground truth survey
datasets, and then use surface-level metrics such as structural quality and
reference relevance to evaluate generated surveys.However, these benchmarks
have two key issues: (1) the ground truth survey datasets are unreliable
because of a lack academic dimension annotations; (2) the evaluation metrics
only focus on the surface quality of the survey such as logical coherence. Both
issues lead to existing benchmarks cannot assess to evaluate their deep
"academic value", such as the core research objectives and the critical
analysis of different studies. To address the above problems, we propose
DeepSurvey-Bench, a novel benchmark designed to comprehensively evaluate the
academic value of generated surveys. Specifically, our benchmark propose a
comprehensive academic value evaluation criteria covering three dimensions:
informational value, scholarly communication value, and research guidance
value. Based on this criteria, we construct a reliable dataset with academic
value annotations, and evaluate the deep academic value of the generated
surveys. Extensive experimental results demonstrate that our benchmark is
highly consistent with human performance in assessing the academic value of
generated surveys.
\\ ( https://arxiv.org/abs/2601.15307 ,  7450kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15311
Date: Wed, 14 Jan 2026 15:23:22 GMT   (463kb)

Title: Aeon: High-Performance Neuro-Symbolic Memory Management for Long-Horizon
  LLM Agents
Authors: Mustafa Arslan
Categories: cs.AI
\\
  Large Language Models (LLMs) are fundamentally constrained by the quadratic
computational cost of self-attention and the "Lost in the Middle" phenomenon,
where reasoning capabilities degrade as context windows expand. Existing
solutions, primarily "Flat RAG" architectures relying on vector databases,
treat memory as an unstructured bag of embeddings. This approach fails to
capture the hierarchical and temporal structure of long-horizon interactions,
leading to "Vector Haze", the retrieval of disjointed facts lacking episodic
continuity. We propose Aeon, a Neuro-Symbolic Cognitive Operating System that
redefines memory not as a static store, but as a managed OS resource. Aeon
structures memory into a Memory Palace (a spatial index implemented via Atlas,
a SIMD-accelerated Page-Clustered Vector Index that combines small-world graph
navigation with B+ Tree-style disk locality to minimize read amplification) and
a Trace (a neuro-symbolic episodic graph). We introduce the Semantic Lookaside
Buffer (SLB), a predictive caching mechanism that exploits conversational
locality to achieve sub-millisecond retrieval latencies. Benchmarks demonstrate
that Aeon achieves < 1ms retrieval latency on conversational workloads while
ensuring state consistency via a zero-copy C++/Python bridge, effectively
enabling persistent, structured memory for autonomous agents.
\\ ( https://arxiv.org/abs/2601.15311 ,  463kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15316
Date: Fri, 16 Jan 2026 02:40:16 GMT   (1152kb)

Title: The Paradigm Shift: A Comprehensive Survey on Large Vision Language
  Models for Multimodal Fake News Detection
Authors: Wei Ai, Yilong Tan, Yuntao Shou, Tao Meng, Haowen Chen, Zhixiong He,
  and Keqin Li
Categories: cs.AI cs.CV
DOI: 10.1016/j.cosrev.2026.100893
\\
  In recent years, the rapid evolution of large vision-language models (LVLMs)
has driven a paradigm shift in multimodal fake news detection (MFND),
transforming it from traditional feature-engineering approaches to unified,
end-to-end multimodal reasoning frameworks. Early methods primarily relied on
shallow fusion techniques to capture correlations between text and images, but
they struggled with high-level semantic understanding and complex cross-modal
interactions. The emergence of LVLMs has fundamentally changed this landscape
by enabling joint modeling of vision and language with powerful representation
learning, thereby enhancing the ability to detect misinformation that leverages
both textual narratives and visual content. Despite these advances, the field
lacks a systematic survey that traces this transition and consolidates recent
developments. To address this gap, this paper provides a comprehensive review
of MFND through the lens of LVLMs. We first present a historical perspective,
mapping the evolution from conventional multimodal detection pipelines to
foundation model-driven paradigms. Next, we establish a structured taxonomy
covering model architectures, datasets, and performance benchmarks.
Furthermore, we analyze the remaining technical challenges, including
interpretability, temporal reasoning, and domain generalization. Finally, we
outline future research directions to guide the next stage of this paradigm
shift. To the best of our knowledge, this is the first comprehensive survey to
systematically document and analyze the transformative role of LVLMs in
combating multimodal fake news. The summary of existing methods mentioned is in
our Github:
\href{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}{https://github.com/Tan-YiLong/Overview-of-Fake-News-Detection}.
\\ ( https://arxiv.org/abs/2601.15316 ,  1152kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15322
Date: Sat, 17 Jan 2026 19:47:55 GMT   (78kb)

Title: Replayable Financial Agents: A Determinism-Faithfulness Assurance
  Harness for Tool-Using LLM Agents
Authors: Raffi Khatchadourian
Categories: cs.AI cs.CL
Comments: 23 pages, 5 figures, 9 tables. Code and data:
  https://github.com/ibm-client-engineering/output-drift-financial-llms
ACM-class: I.2.11; D.2.5
\\
  LLM agents struggle with regulatory audit replay: when asked to reproduce a
flagged transaction decision with identical inputs, most deployments fail to
return consistent results. This paper introduces the Determinism-Faithfulness
Assurance Harness (DFAH), a framework for measuring trajectory determinism and
evidence-conditioned faithfulness in tool-using agents deployed in financial
services.
  Across 74 configurations (12 models, 4 providers, 8-24 runs each at T=0.0) in
non-agentic baseline experiments, 7-20B parameter models achieved 100%
determinism, while 120B+ models required 3.7x larger validation samples to
achieve equivalent statistical reliability. Agentic tool-use introduces
additional variance (see Tables 4-7). Contrary to the assumed
reliability-capability trade-off, a positive Pearson correlation emerged (r =
0.45, p < 0.01, n = 51 at T=0.0) between determinism and faithfulness; models
producing consistent outputs also tended to be more evidence-aligned.
  Three financial benchmarks are provided (compliance triage, portfolio
constraints, DataOps exceptions; 50 cases each) along with an open-source
stress-test harness. In these benchmarks and under DFAH evaluation settings,
Tier 1 models with schema-first architectures achieved determinism levels
consistent with audit replay requirements.
\\ ( https://arxiv.org/abs/2601.15322 ,  78kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15324
Date: Sun, 18 Jan 2026 20:29:07 GMT   (24kb)

Title: Prometheus Mind: Retrofitting Memory to Frozen Language Models
Authors: Mark Wind
Categories: cs.AI
Comments: 28 pages
\\
  Adding memory to pretrained language models typically requires architectural
changes or weight modification. We present Prometheus Mind, which retrofits
memory to a frozen Qwen3-4B using 11 modular adapters (530MB, 7% overhead) --
fully reversible by removing the adapters. Building this system required
solving four problems: (1) Extraction -- we develop Contrastive Direction
Discovery (CDD), which finds semantic directions via minimal pairs without
labeled data. (2) Training -- end-to-end optimization collapses; stage-wise
training of each adapter on simple proxy tasks succeeds. (3) Injection --
learned encoders fail to generalize; we find that lm_head.weight rows already
provide the mapping we need, requiring no training. (4) Hidden state collapse
-- transformers make ``wife'' and ``brother'' 0.98+ similar; we train
projections to recover distinction (0.98 $\rightarrow$ 0.09). On
PrometheusExtract-132 (132 cases), the system achieves 94.4% retrieval on clean
inputs (n=54, 95% CI: [84.9%, 98.1%]), degrading to 19.4% on informal inputs
with ellipsis, filler words, or implicit subjects (n=36). The primary
bottleneck is relation classification (47.3% accuracy), responsible for most
extraction errors.
\\ ( https://arxiv.org/abs/2601.15324 ,  24kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15347
Date: Wed, 21 Jan 2026 01:50:47 GMT   (2288kb)

Title: Logic Programming on Knowledge Graph Networks And its Application in
  Medical Domain
Authors: Chuanqing Wang, Zhenmin Zhao, Shanshan Du, Chaoqun Fei, Songmao Zhang,
  Ruqian Lu
Categories: cs.AI cs.CL cs.LG
Comments: 33 pages
\\
  The rash development of knowledge graph research has brought big driving
force to its application in many areas, including the medicine and healthcare
domain. However, we have found that the application of some major information
processing techniques on knowledge graph still lags behind. This defect
includes the failure to make sufficient use of advanced logic reasoning,
advanced artificial intelligence techniques, special-purpose programming
languages, modern probabilistic and statistic theories et al. on knowledge
graphs development and application. In particular, the multiple knowledge
graphs cooperation and competition techniques have not got enough attention
from researchers. This paper develops a systematic theory, technique and
application of the concept 'knowledge graph network' and its application in
medical and healthcare domain. Our research covers its definition, development,
reasoning, computing and application under different conditions such as
unsharp, uncertain, multi-modal, vectorized, distributed, federated. Almost in
each case we provide (real data) examples and experiment results. Finally, a
conclusion of innovation is provided.
\\ ( https://arxiv.org/abs/2601.15347 ,  2288kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15392
Date: Wed, 21 Jan 2026 19:03:54 GMT   (296kb)

Title: GeMM-GAN: A Multimodal Generative Model Conditioned on Histopathology
  Images and Clinical Descriptions for Gene Expression Profile Generation
Authors: Francesca Pia Panaccione, Carlo Sgaravatti, Pietro Pinoli
Categories: cs.AI cs.CV cs.LG
Comments: 12 pages, 2 figures. Published at Image Analysis and Processing -
  ICIAP 2025 Workshops
DOI: 10.1007/978-3-032-11317-7_33
\\
  Biomedical research increasingly relies on integrating diverse data
modalities, including gene expression profiles, medical images, and clinical
metadata. While medical images and clinical metadata are routinely collected in
clinical practice, gene expression data presents unique challenges for
widespread research use, mainly due to stringent privacy regulations and costly
laboratory experiments. To address these limitations, we present GeMM-GAN, a
novel Generative Adversarial Network conditioned on histopathology tissue
slides and clinical metadata, designed to synthesize realistic gene expression
profiles. GeMM-GAN combines a Transformer Encoder for image patches with a
final Cross Attention mechanism between patches and text tokens, producing a
conditioning vector to guide a generative model in generating biologically
coherent gene expression profiles. We evaluate our approach on the TCGA dataset
and demonstrate that our framework outperforms standard generative models and
generates more realistic and functionally meaningful gene expression profiles,
improving by more than 11\% the accuracy on downstream disease type prediction
compared to current state-of-the-art generative models. Code will be available
at: https://github.com/francescapia/GeMM-GAN
\\ ( https://arxiv.org/abs/2601.15392 ,  296kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15397
Date: Wed, 21 Jan 2026 19:08:45 GMT   (51kb)

Title: Beyond Prompting: Efficient and Robust Contextual Biasing for Speech
  LLMs via Logit-Space Integration (LOGIC)
Authors: Peidong Wang
Categories: cs.AI cs.CL cs.SD
\\
  The rapid emergence of new entities -- driven by cultural shifts, evolving
trends, and personalized user data -- poses a significant challenge for
existing Speech Large Language Models (Speech LLMs). While these models excel
at general conversational tasks, their static training knowledge limits their
ability to recognize domain-specific terms such as contact names, playlists, or
technical jargon. Existing solutions primarily rely on prompting, which suffers
from poor scalability: as the entity list grows, prompting encounters context
window limitations, increased inference latency, and the "lost-in-the-middle"
phenomenon. An alternative approach, Generative Error Correction (GEC),
attempts to rewrite transcripts via post-processing but frequently suffers from
"over-correction", introducing hallucinations of entities that were never
spoken.
  In this work, we introduce LOGIC (Logit-Space Integration for Contextual
Biasing), an efficient and robust framework that operates directly in the
decoding layer. Unlike prompting, LOGIC decouples context injection from input
processing, ensuring constant-time complexity relative to prompt length.
Extensive experiments using the Phi-4-MM model across 11 multilingual locales
demonstrate that LOGIC achieves an average 9% relative reduction in Entity WER
with a negligible 0.30% increase in False Alarm Rate.
\\ ( https://arxiv.org/abs/2601.15397 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15436
Date: Wed, 21 Jan 2026 20:00:14 GMT   (4453kb)

Title: Not Your Typical Sycophant: The Elusive Nature of Sycophancy in Large
  Language Models
Authors: Shahar Ben Natan, Oren Tsur
Categories: cs.AI cs.CL cs.CY
\\
  We propose a novel way to evaluate sycophancy of LLMs in a direct and neutral
way, mitigating various forms of uncontrolled bias, noise, or manipulative
language, deliberately injected to prompts in prior works. A key novelty in our
approach is the use of LLM-as-a-judge, evaluation of sycophancy as a zero-sum
game in a bet setting. Under this framework, sycophancy serves one individual
(the user) while explicitly incurring cost on another. Comparing four leading
models - Gemini 2.5 Pro, ChatGpt 4o, Mistral-Large-Instruct-2411, and Claude
Sonnet 3.7 - we find that while all models exhibit sycophantic tendencies in
the common setting, in which sycophancy is self-serving to the user and incurs
no cost on others, Claude and Mistral exhibit "moral remorse" and
over-compensate for their sycophancy in case it explicitly harms a third party.
Additionally, we observed that all models are biased toward the answer proposed
last. Crucially, we find that these two phenomena are not independent;
sycophancy and recency bias interact to produce `constructive interference'
effect, where the tendency to agree with the user is exacerbated when the
user's opinion is presented last.
\\ ( https://arxiv.org/abs/2601.15436 ,  4453kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15442
Date: Wed, 21 Jan 2026 20:20:31 GMT   (230kb)

Title: A tensor network formalism for neuro-symbolic AI
Authors: Alex Goessmann, Janina Sch\"utte, Maximilian Fr\"ohlich, Martin Eigel
Categories: cs.AI cs.LG cs.LO cs.NA math.NA stat.ML
Comments: 51 pages, 14 figures
MSC-class: 15A69, 68T27
ACM-class: G.3; F.1.2
\\
  The unification of neural and symbolic approaches to artificial intelligence
remains a central open challenge. In this work, we introduce a tensor network
formalism, which captures sparsity principles originating in the different
approaches in tensor decompositions. In particular, we describe a basis
encoding scheme for functions and model neural decompositions as tensor
decompositions. The proposed formalism can be applied to represent logical
formulas and probability distributions as structured tensor decompositions.
This unified treatment identifies tensor network contractions as a fundamental
inference class and formulates efficiently scaling reasoning algorithms,
originating from probability theory and propositional logic, as contraction
message passing schemes. The framework enables the definition and training of
hybrid logical and probabilistic models, which we call Hybrid Logic Network.
The theoretical concepts are accompanied by the python library tnreason, which
enables the implementation and practical use of the proposed architectures.
\\ ( https://arxiv.org/abs/2601.15442 ,  230kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15476
Date: Wed, 21 Jan 2026 21:26:42 GMT   (29kb)

Title: Reliability by design: quantifying and eliminating fabrication risk in
  LLMs. From generative to consultative AI: a comparative analysis in the legal
  domain and lessons for high-stakes knowledge bases
Authors: Alex Dantart
Categories: cs.AI cs.PF
\\
  This paper examines how to make large language models reliable for
high-stakes legal work by reducing hallucinations. It distinguishes three AI
paradigms: (1) standalone generative models ("creative oracle"), (2) basic
retrieval-augmented systems ("expert archivist"), and (3) an advanced,
end-to-end optimized RAG system ("rigorous archivist"). The authors introduce
two reliability metrics -False Citation Rate (FCR) and Fabricated Fact Rate
(FFR)- and evaluate 2,700 judicial-style answers from 12 LLMs across 75 legal
tasks using expert, double-blind review. Results show that standalone models
are unsuitable for professional use (FCR above 30%), while basic RAG greatly
reduces errors but still leaves notable misgrounding. Advanced RAG, using
techniques such as embedding fine-tuning, re-ranking, and self-correction,
reduces fabrication to negligible levels (below 0.2%). The study concludes that
trustworthy legal AI requires rigor-focused, retrieval-based architectures
emphasizing verification and traceability, and provides an evaluation framework
applicable to other high-risk domains.
\\ ( https://arxiv.org/abs/2601.15476 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15487
Date: Wed, 21 Jan 2026 21:39:09 GMT   (304kb)

Title: MiRAGE: A Multiagent Framework for Generating Multimodal Multihop
  Question-Answer Dataset for RAG Evaluation
Authors: Chandan Kumar Sahu, Premith Kumar Chilukuri, Matthew Hetrich
Categories: cs.AI cs.CL cs.MA
Comments: 12 pages, 2 figures, Submitted to ACL
\\
  The rapid evolution of Retrieval-Augmented Generation (RAG) toward
multimodal, high-stakes enterprise applications has outpaced the development of
domain specific evaluation benchmarks. Existing datasets often rely on
general-domain corpora or purely textual retrieval, failing to capture the
complexity of specialized technical documents where information is inextricably
multimodal and reasoning requires synthesizing disjoint evidence. We address
this gap by introducing MiRAGE, a Multiagent framework for RAG systems
Evaluation, that leverages a collaborative swarm of specialized agents to
generate verified, domain-specific, multimodal, and multi-hop Question-Answer
datasets. MiRAGE orchestrates a swarm of specialized agents: a recursive
context optimization loop to aggregate scattered evidence, an adversarial
verifier agent to guarantee factual grounding, and an agent to recognize the
expert persona and the relevant domain to mimic expert cognitive workflows.
Extensive empirical evaluation across four distinct domains (regulations,
finance, quantitative biology, and journalism) demonstrates that MiRAGE
generates datasets with significantly higher reasoning complexity (>2.3 average
hops) and factual faithfulness. Our ablation studies point that MiRAGE can be
powered by LLMs if textual descriptions of the images are available. Visual
grounding still remains a frontier. By automating the creation of gold standard
evaluation datasets that reflect the latent thematic structure of proprietary
corpora, MiRAGE provides the necessary infrastructure to rigorously benchmark
the next generation information retrieval systems.
\\ ( https://arxiv.org/abs/2601.15487 ,  304kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15495
Date: Wed, 21 Jan 2026 21:56:35 GMT   (1558kb)

Title: Tracking the Limits of Knowledge Propagation: How LLMs Fail at
  Multi-Step Reasoning with Conflicting Knowledge
Authors: Yiyang Feng, Zeming Chen, Haotian Wu, Jiawei Zhou, Antoine Bosselut
Categories: cs.AI cs.CL
Comments: Accepted to EACL 2026 (Main)
\\
  A common solution for mitigating outdated or incorrect information in Large
Language Models (LLMs) is to provide updated facts in-context or through
knowledge editing. However, these methods introduce knowledge conflicts when
the knowledge update fails to overwrite the model's parametric knowledge, which
propagate to faulty reasoning. Current benchmarks for this problem, however,
largely focus only on single knowledge updates and fact recall without
evaluating how these updates affect downstream reasoning. In this work, we
introduce TRACK (Testing Reasoning Amid Conflicting Knowledge), a new benchmark
for studying how LLMs propagate new knowledge through multi-step reasoning when
it conflicts with the model's initial parametric knowledge. Spanning three
reasoning-intensive scenarios (WIKI, CODE, and MATH), TRACK introduces
multiple, realistic conflicts to mirror real-world complexity. Our results on
TRACK reveal that providing updated facts to models for reasoning can worsen
performance compared to providing no updated facts to a model, and that this
performance degradation exacerbates as more updated facts are provided. We show
this failure stems from both inability to faithfully integrate updated facts,
but also flawed reasoning even when knowledge is integrated. TRACK provides a
rigorous new benchmark to measure and guide future progress on propagating
conflicting knowledge in multi-step reasoning.
\\ ( https://arxiv.org/abs/2601.15495 ,  1558kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15509
Date: Wed, 21 Jan 2026 22:40:47 GMT   (810kb)

Title: The Dark Side of AI Transformers: Sentiment Polarization & the Loss of
  Business Neutrality by NLP Transformers
Authors: Prasanna Kumar
Categories: cs.AI cs.CL
ACM-class: I.2.7
\\
  The use of Transfer Learning & Transformers has steadily improved accuracy
and has significantly contributed in solving complex computation problems.
However, this transformer led accuracy improvement in Applied AI Analytics
specifically in sentiment analytics comes with the dark side. It is observed
during experiments that a lot of these improvements in transformer led accuracy
of one class of sentiment has been at the cost of polarization of another class
of sentiment and the failing of neutrality. This lack of neutrality poses an
acute problem in the Applied NLP space, which relies heavily on the
computational outputs of sentiment analytics for reliable industry ready tasks.
\\ ( https://arxiv.org/abs/2601.15509 ,  810kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15519
Date: Wed, 21 Jan 2026 23:14:05 GMT   (6176kb)

Title: TransportAgents: a multi-agents LLM framework for traffic accident
  severity prediction
Authors: Zhichao Yang, Jiashu He, Jinxuan Fan, Cirillo Cinzia
Categories: cs.AI
\\
  Accurate prediction of traffic crash severity is critical for improving
emergency response and public safety planning. Although recent large language
models (LLMs) exhibit strong reasoning capabilities, their single-agent
architectures often struggle with heterogeneous, domain-specific crash data and
tend to generate biased or unstable predictions. To address these limitations,
this paper proposes TransportAgents, a hybrid multi-agent framework that
integrates category-specific LLM reasoning with a multilayer perceptron (MLP)
integration module. Each specialized agent focuses on a particular subset of
traffic information, such as demographics, environmental context, or incident
details, to produce intermediate severity assessments that are subsequently
fused into a unified prediction. Extensive experiments on two complementary
U.S. datasets, the Consumer Product Safety Risk Management System (CPSRMS) and
the National Electronic Injury Surveillance System (NEISS), demonstrate that
TransportAgents consistently outperforms both traditional machine learning and
advanced LLM-based baselines. Across three representative backbones, including
closed-source models such as GPT-3.5 and GPT-4o, as well as open-source models
such as LLaMA-3.3, the framework exhibits strong robustness, scalability, and
cross-dataset generalizability. A supplementary distributional analysis further
shows that TransportAgents produces more balanced and well-calibrated severity
predictions than standard single-agent LLM approaches, highlighting its
interpretability and reliability for safety-critical decision support
applications.
\\ ( https://arxiv.org/abs/2601.15519 ,  6176kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15533
Date: Wed, 21 Jan 2026 23:35:33 GMT   (319kb)

Title: From Generative Engines to Actionable Simulators: The Imperative of
  Physical Grounding in World Models
Authors: Zhikang Chen, Tingting Zhu
Categories: cs.AI
\\
  A world model is an AI system that simulates how an environment evolves under
actions, enabling planning through imagined futures rather than reactive
perception. Current world models, however, suffer from visual conflation: the
mistaken assumption that high-fidelity video generation implies an
understanding of physical and causal dynamics. We show that while modern models
excel at predicting pixels, they frequently violate invariant constraints, fail
under intervention, and break down in safety-critical decision-making. This
survey argues that visual realism is an unreliable proxy for world
understanding. Instead, effective world models must encode causal structure,
respect domain-specific constraints, and remain stable over long horizons. We
propose a reframing of world models as actionable simulators rather than visual
engines, emphasizing structured 4D interfaces, constraint-aware dynamics, and
closed-loop evaluation. Using medical decision-making as an epistemic stress
test, where trial-and-error is impossible and errors are irreversible, we
demonstrate that a world model's value is determined not by how realistic its
rollouts appear, but by its ability to support counterfactual reasoning,
intervention planning, and robust long-horizon foresight.
\\ ( https://arxiv.org/abs/2601.15533 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15551
Date: Thu, 22 Jan 2026 00:45:15 GMT   (3911kb)

Title: ALIGNAgent: Adaptive Learner Intelligence for Gap Identification and
  Next-step guidance
Authors: Bismack Tokoli, Luis Jaimes, Ayesha S. Dina
Categories: cs.AI cs.MA
Comments: 35 pages
\\
  Personalized learning systems have emerged as a promising approach to enhance
student outcomes by tailoring educational content, pacing, and feedback to
individual needs. However, most existing systems remain fragmented,
specializing in either knowledge tracing, diagnostic modeling, or resource
recommendation, but rarely integrating these components into a cohesive
adaptive cycle. In this paper, we propose ALIGNAgent (Adaptive Learner
Intelligence for Gap Identification and Next-step guidance), a multi-agent
educational framework designed to deliver personalized learning through
integrated knowledge estimation, skill-gap identification, and targeted
resource recommendation.ALIGNAgent begins by processing student quiz
performance, gradebook data, and learner preferences to generate topic-level
proficiency estimates using a Skill Gap Agent that employs concept-level
diagnostic reasoning to identify specific misconceptions and knowledge
deficiencies. After identifying skill gaps, the Recommender Agent retrieves
preference-aware learning materials aligned with diagnosed deficiencies,
implementing a continuous feedback loop where interventions occur before
advancing to subsequent topics. Extensive empirical evaluation on authentic
datasets from two undergraduate computer science courses demonstrates
ALIGNAgent's effectiveness, with GPT-4o-based agents achieving precision of
0.87-0.90 and F1 scores of 0.84-0.87 in knowledge proficiency estimation
validated against actual exam performance.
\\ ( https://arxiv.org/abs/2601.15551 ,  3911kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15599
Date: Thu, 22 Jan 2026 02:49:06 GMT   (1475kb)

Title: Autonomous Business System via Neuro-symbolic AI
Authors: Cecil Pang, Hiroki Sayama
Categories: cs.AI
Comments: Accepted to IEEE SysCon 2026
\\
  Current business environments require organizations to continuously
reconfigure cross-functional processes, yet enterprise systems are still
organized around siloed departments, rigid workflows, and hard-coded
automation. Meanwhile large language models (LLMs) excel at interpreting
natural language and unstructured data but lack deterministic, verifiable
execution of complex business logic. To address this gap, here we introduce
AUTOBUS, an Autonomous Business System that integrates LLM-based AI agents,
predicate-logic programming, and business-semantics-centric enterprise data
into a coherent neuro-symbolic AI architecture for orchestrating end-to-end
business initiatives. AUTOBUS models an initiative as a network of tasks with
explicit pre/post conditions, required data, evaluation rules, and API-level
actions. Enterprise data is organized as a knowledge graph whose entities,
relationships, and constraints are translated into logic facts and foundational
rules, providing the semantic grounding for task reasoning. Core AI agents
synthesize task instructions, enterprise semantics, and available tools into
task-specific logic programs, which are executed by a logic engine that
enforces constraints, coordinates auxiliary tools, and orchestrate execution of
actions and outcomes. Humans define and maintain the semantics, policies and
task instructions, curate tools, and supervise high-impact or ambiguous
decisions, ensuring accountability and adaptability. We detail the AUTOBUS
architecture, the anatomy of the AI agent generated logic programs, and the
role of humans and auxiliary tools in the lifecycle of a business initiative.
\\ ( https://arxiv.org/abs/2601.15599 ,  1475kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15628
Date: Thu, 22 Jan 2026 03:59:19 GMT   (12869kb)

Title: CogToM: A Comprehensive Theory of Mind Benchmark inspired by Human
  Cognition for Large Language Models
Authors: Haibo Tong, Zeyang Yue, Feifei Zhao, Erliang Lin, Lu Jia, Ruolin Chen,
  Yinqian Sun, Qian Zhang, Yi Zeng
Categories: cs.AI
\\
  Whether Large Language Models (LLMs) truly possess human-like Theory of Mind
(ToM) capabilities has garnered increasing attention. However, existing
benchmarks remain largely restricted to narrow paradigms like false belief
tasks, failing to capture the full spectrum of human cognitive mechanisms. We
introduce CogToM, a comprehensive, theoretically grounded benchmark comprising
over 8000 bilingual instances across 46 paradigms, validated by 49 human
annotator.A systematic evaluation of 22 representative models, including
frontier models like GPT-5.1 and Qwen3-Max, reveals significant performance
heterogeneities and highlights persistent bottlenecks in specific dimensions.
Further analysis based on human cognitive patterns suggests potential
divergences between LLM and human cognitive structures. CogToM offers a robust
instrument and perspective for investigating the evolving cognitive boundaries
of LLMs.
\\ ( https://arxiv.org/abs/2601.15628 ,  12869kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15630
Date: Thu, 22 Jan 2026 04:01:41 GMT   (2194kb)

Title: Agentic AI Governance and Lifecycle Management in Healthcare
Authors: Chandra Prakash, Mary Lind, and Avneesh Sisodia
Categories: cs.AI
Comments: 9 Page, 3 figures
\\
  Healthcare organizations are beginning to embed agentic AI into routine
workflows, including clinical documentation support and early-warning
monitoring. As these capabilities diffuse across departments and vendors,
health systems face agent sprawl, causing duplicated agents, unclear
accountability, inconsistent controls, and tool permissions that persist beyond
the original use case. Existing AI governance frameworks emphasize lifecycle
risk management but provide limited guidance for the day-to-day operations of
agent fleets. We propose a Unified Agent Lifecycle Management (UALM) blueprint
derived from a rapid, practice-oriented synthesis of governance standards,
agent security literature, and healthcare compliance requirements. UALM maps
recurring gaps onto five control-plane layers: (1) an identity and persona
registry, (2) orchestration and cross-domain mediation, (3) PHI-bounded context
and memory, (4) runtime policy enforcement with kill-switch triggers, and (5)
lifecycle management and decommissioning linked to credential revocation and
audit logging. A companion maturity model supports staged adoption. UALM offers
healthcare CIOs, CISOs, and clinical leaders an implementable pattern for
audit-ready oversight that preserves local innovation and enables safer scaling
across clinical and administrative domains.
\\ ( https://arxiv.org/abs/2601.15630 ,  2194kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15652
Date: Thu, 22 Jan 2026 05:00:21 GMT   (16kb)

Title: Predictive Coding and Information Bottleneck for Hallucination Detection
  in Large Language Models
Authors: Manish Bhatt
Categories: cs.AI cs.CR cs.ET
\\
  Hallucinations in Large Language Models (LLMs) -- generations that are
plausible but factually unfaithful -- remain a critical barrier to high-stakes
deployment. Current detection methods typically rely on computationally
expensive external retrieval loops or opaque black-box LLM judges requiring
70B+ parameters. In this work, we introduce [Model Name], a hybrid detection
framework that combines neuroscience-inspired signal design with supervised
machine learning. We extract interpretable signals grounded in Predictive
Coding (quantifying surprise against internal priors) and the Information
Bottleneck (measuring signal retention under perturbation). Through systematic
ablation, we demonstrate three key enhancements: Entity-Focused Uptake
(concentrating on high-value tokens), Context Adherence (measuring grounding
strength), and Falsifiability Score (detecting confident but contradictory
claims).
  Evaluating on HaluBench (n=200, perfectly balanced), our theory-guided
baseline achieves 0.8017 AUROC. BASE supervised models reach 0.8274 AUROC,
while IMPROVED features boost performance to 0.8669 AUROC (4.95% gain),
demonstrating consistent improvements across architectures. This competitive
performance is achieved while using 75x less training data than Lynx (200 vs
15,000 samples), 1000x faster inference (5ms vs 5s), and remaining fully
interpretable. Crucially, we report a negative result: the Rationalization
signal fails to distinguish hallucinations, suggesting that LLMs generate
coherent reasoning for false premises ("Sycophancy").
  This work demonstrates that domain knowledge encoded in signal architecture
provides superior data efficiency compared to scaling LLM judges, achieving
strong performance with lightweight (less than 1M parameter), explainable
models suitable for production deployment.
\\ ( https://arxiv.org/abs/2601.15652 ,  16kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15679
Date: Thu, 22 Jan 2026 06:00:00 GMT   (4871kb)

Title: Improving Methodologies for Agentic Evaluations Across Domains: Leakage
  of Sensitive Information, Fraud and Cybersecurity Threats
Authors: Ee Wei Seah, Yongsen Zheng, Naga Nikshith, Mahran Morsidi, Gabriel
  Waikin Loh Matienzo, Nigel Gay, Akriti Vij, Benjamin Chua, En Qi Ng, Sharmini
  Johnson, Vanessa Wilfred, Wan Sie Lee, Anna Davidson, Catherine Devine, Erin
  Zorer, Gareth Holvey, Harry Coppock, James Walpole, Jerome Wynee, Magda
  Dubois, Michael Schmatz, Patrick Keane, Sam Deverett, Bill Black, Bo Yan,
  Bushra Sabir, Frank Sun, Hao Zhang, Harriet Farlow, Helen Zhou, Lingming
  Dong, Qinghua Lu, Seung Jang, Sharif Abuadbba, Simon O'Callaghan, Suyu Ma,
  Tom Howroyd, Cyrus Fung, Fatemeh Azadi, Isar Nejadgholi, Krishnapriya
  Vishnubhotla, Pulei Xiong, Saeedeh Lohrasbi, Scott Buffett, Shahrear Iqbal,
  Sowmya Vajjala, Anna Safont-Andreu, Luca Massarelli, Oskar van der Wal, Simon
  M\"oller, Agnes Delaborde, Joris Dugu\'ep\'eroux, Nicolas Rolin, et al. (17
  additional authors not shown)
Categories: cs.AI
Comments: The author/contributor list organises contributors by country and
  alphabetical order within each country. In some places, the order has been
  altered to match other related publications
\\
  The rapid rise of autonomous AI systems and advancements in agent
capabilities are introducing new risks due to reduced oversight of real-world
interactions. Yet agent testing remains nascent and is still a developing
science. As AI agents begin to be deployed globally, it is important that they
handle different languages and cultures accurately and securely.
  To address this, participants from The International Network for Advanced AI
Measurement, Evaluation and Science, including representatives from Singapore,
Japan, Australia, Canada, the European Commission, France, Kenya, South Korea,
and the United Kingdom have come together to align approaches to agentic
evaluations.
  This is the third exercise, building on insights from two earlier joint
testing exercises conducted by the Network in November 2024 and February 2025.
The objective is to further refine best practices for testing advanced AI
systems.
  The exercise was split into two strands: (1) common risks, including leakage
of sensitive information and fraud, led by Singapore AISI; and (2)
cybersecurity, led by UK AISI. A mix of open and closed-weight models were
evaluated against tasks from various public agentic benchmarks. Given the
nascency of agentic testing, our primary focus was on understanding
methodological issues in conducting such tests, rather than examining test
results or model capabilities. This collaboration marks an important step
forward as participants work together to advance the science of agentic
evaluations.
\\ ( https://arxiv.org/abs/2601.15679 ,  4871kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15690
Date: Thu, 22 Jan 2026 06:21:31 GMT   (54kb)

Title: From Passive Metric to Active Signal: The Evolving Role of Uncertainty
  Quantification in Large Language Models
Authors: Jiaxin Zhang, Wendi Cui, Zhuohang Li, Lifu Huang, Bradley Malin,
  Caiming Xiong, Chien-Sheng Wu
Categories: cs.AI stat.AP
Comments: 20 pages, 4 figures, 6 tables
\\
  While Large Language Models (LLMs) show remarkable capabilities, their
unreliability remains a critical barrier to deployment in high-stakes domains.
This survey charts a functional evolution in addressing this challenge: the
evolution of uncertainty from a passive diagnostic metric to an active control
signal guiding real-time model behavior. We demonstrate how uncertainty is
leveraged as an active control signal across three frontiers: in
\textbf{advanced reasoning} to optimize computation and trigger
self-correction; in \textbf{autonomous agents} to govern metacognitive
decisions about tool use and information seeking; and in \textbf{reinforcement
learning} to mitigate reward hacking and enable self-improvement via intrinsic
rewards. By grounding these advancements in emerging theoretical frameworks
like Bayesian methods and Conformal Prediction, we provide a unified
perspective on this transformative trend. This survey provides a comprehensive
overview, critical analysis, and practical design patterns, arguing that
mastering the new trend of uncertainty is essential for building the next
generation of scalable, reliable, and trustworthy AI.
\\ ( https://arxiv.org/abs/2601.15690 ,  54kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15703
Date: Thu, 22 Jan 2026 07:16:26 GMT   (13069kb)

Title: Agentic Uncertainty Quantification
Authors: Jiaxin Zhang, Prafulla Kumar Choubey, Kung-Hsiang Huang, Caiming
  Xiong, Chien-Sheng Wu
Categories: cs.AI cs.CL
Comments: 36 pages, 9 figures, 9 tables
\\
  Although AI agents have demonstrated impressive capabilities in long-horizon
reasoning, their reliability is severely hampered by the ``Spiral of
Hallucination,'' where early epistemic errors propagate irreversibly. Existing
methods face a dilemma: uncertainty quantification (UQ) methods typically act
as passive sensors, only diagnosing risks without addressing them, while
self-reflection mechanisms suffer from continuous or aimless corrections. To
bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework
that transforms verbalized uncertainty into active, bi-directional control
signals. Our architecture comprises two complementary mechanisms: System 1
(Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized
confidence and semantic explanations to prevent blind decision-making; and
System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations
as rational cues to trigger targeted inference-time resolution only when
necessary. This enables the agent to balance efficient execution and deep
deliberation dynamically. Extensive experiments on closed-loop benchmarks and
open-ended deep research tasks demonstrate that our training-free approach
achieves superior performance and trajectory-level calibration. We believe this
principled framework AUQ represents a significant step towards reliable agents.
\\ ( https://arxiv.org/abs/2601.15703 ,  13069kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15706
Date: Thu, 22 Jan 2026 07:18:08 GMT   (1573kb)

Title: Improving Methodologies for LLM Evaluations Across Global Languages
Authors: Akriti Vij, Benjamin Chua, Darshini Ramiah, En Qi Ng, Mahran Morsidi,
  Naga Nikshith Gangarapu, Sharmini Johnson, Vanessa Wilfred, Vikneswaran
  Kumaran, Wan Sie Lee, Wenzhuo Yang, Yongsen Zheng, Bill Black, Boming Xia,
  Frank Sun, Hao Zhang, Qinghua Lu, Suyu Ma, Yue Liu, Chi-kiu Lo, Fatemeh
  Azadi, Isar Nejadgholi, Sowmya Vajjala, Agnes Delaborde, Nicolas Rolin, Tom
  Seimandi, Akiko Murakami, Haruto Ishi, Satoshi Sekine, Takayuki Semitsu,
  Tasuku Sasaki, Angela Kinuthia, Jean Wangari, Michael Michie, Stephanie
  Kasaon, Hankyul Baek, Jaewon Noh, Kihyuk Nam, Sang Seo, Sungpil Shin, Taewhi
  Lee, Yongsu Kim, Daisy Newbold-Harrop, Jessica Wang, Mahmoud Ghanem, Vy Hong
Categories: cs.AI
Comments: Author names have been organised by country, and in alphabetical
  order within countries
\\
  As frontier AI models are deployed globally, it is essential that their
behaviour remains safe and reliable across diverse linguistic and cultural
contexts. To examine how current model safeguards hold up in such settings,
participants from the International Network for Advanced AI Measurement,
Evaluation and Science, including representatives from Singapore, Japan,
Australia, Canada, the EU, France, Kenya, South Korea and the UK conducted a
joint multilingual evaluation exercise. Led by Singapore AISI, two open-weight
models were tested across ten languages spanning high and low resourced groups:
Cantonese English, Farsi, French, Japanese, Korean, Kiswahili, Malay, Mandarin
Chinese and Telugu. Over 6,000 newly translated prompts were evaluated across
five harm categories (privacy, non-violent crime, violent crime, intellectual
property and jailbreak robustness), using both LLM-as-a-judge and human
annotation.
  The exercise shows how safety behaviours can vary across languages. These
include differences in safeguard robustness across languages and harm types and
variation in evaluator reliability (LLM-as-judge vs. human review). Further, it
also generated methodological insights for improving multilingual safety
evaluations, such as the need for culturally contextualised translations,
stress-tested evaluator prompts and clearer human annotation guidelines. This
work represents an initial step toward a shared framework for multilingual
safety testing of advanced AI systems and calls for continued collaboration
with the wider research community and industry.
\\ ( https://arxiv.org/abs/2601.15706 ,  1573kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15709
Date: Thu, 22 Jan 2026 07:31:19 GMT   (1212kb)

Title: AgentSM: Semantic Memory for Agentic Text-to-SQL
Authors: Asim Biswal, Chuan Lei, Xiao Qin, Aodong Li, Balakrishnan
  Narayanaswamy, Tim Kraska
Categories: cs.AI cs.DB cs.LG
ACM-class: I.2.7; H.5.2
\\
  Recent advances in LLM-based Text-to-SQL have achieved remarkable gains on
public benchmarks such as BIRD and Spider. Yet, these systems struggle to scale
in realistic enterprise settings with large, complex schemas, diverse SQL
dialects, and expensive multi-step reasoning. Emerging agentic approaches show
potential for adaptive reasoning but often suffer from inefficiency and
instability-repeating interactions with databases, producing inconsistent
outputs, and occasionally failing to generate valid answers. To address these
challenges, we introduce Agent Semantic Memory (AgentSM), an agentic framework
for Text-to-SQL that builds and leverages interpretable semantic memory.
Instead of relying on raw scratchpads or vector retrieval, AgentSM captures
prior execution traces-or synthesizes curated ones-as structured programs that
directly guide future reasoning. This design enables systematic reuse of
reasoning paths, which allows agents to scale to larger schemas, more complex
questions, and longer trajectories efficiently and reliably. Compared to
state-of-the-art systems, AgentSM achieves higher efficiency by reducing
average token usage and trajectory length by 25% and 35%, respectively, on the
Spider 2.0 benchmark. It also improves execution accuracy, reaching a
state-of-the-art accuracy of 44.8% on the Spider 2.0 Lite benchmark.
\\ ( https://arxiv.org/abs/2601.15709 ,  1212kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15717
Date: Thu, 22 Jan 2026 07:38:27 GMT   (154kb)

Title: Investigation of the Generalisation Ability of Genetic
  Programming-evolved Scheduling Rules in Dynamic Flexible Job Shop Scheduling
Authors: Luyao Zhu, Fangfang Zhang, Yi Mei, and Mengjie Zhang
Categories: cs.AI
\\
  Dynamic Flexible Job Shop Scheduling (DFJSS) is a complex combinatorial
optimisation problem that requires simultaneous machine assignment and
operation sequencing decisions in dynamic production environments. Genetic
Programming (GP) has been widely applied to automatically evolve scheduling
rules for DFJSS. However, existing studies typically train and test GP-evolved
rules on DFJSS instances of the same type, which differ only by random seeds
rather than by structural characteristics, leaving their cross-type
generalisation ability largely unexplored. To address this gap, this paper
systematically investigates the generalisation ability of GP-evolved scheduling
rules under diverse DFJSS conditions. A series of experiments are conducted
across multiple dimensions, including problem scale (i.e., the number of
machines and jobs), key job shop parameters (e.g., utilisation level), and data
distributions, to analyse how these factors influence GP performance on unseen
instance types. The results show that good generalisation occurs when the
training instances contain more jobs than the test instances while keeping the
number of machines fixed, and when both training and test instances have
similar scales or job shop parameters. Further analysis reveals that the number
and distribution of decision points in DFJSS instances play a crucial role in
explaining these performance differences. Similar decision point distributions
lead to better generalisation, whereas significant discrepancies result in a
marked degradation of performance. Overall, this study provides new insights
into the generalisation ability of GP in DFJSS and highlights the necessity of
evolving more generalisable GP rules capable of handling heterogeneous DFJSS
instances effectively.
\\ ( https://arxiv.org/abs/2601.15717 ,  154kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15728
Date: Thu, 22 Jan 2026 07:54:45 GMT   (717kb)

Title: Benchmarking Text-to-Python against Text-to-SQL: The Impact of Explicit
  Logic and Ambiguity
Authors: Hangle Hu, Chenyu Hou, Bin Cao, Ruizhe Li
Categories: cs.AI cs.SE
Comments: 8 pages, 7 figures
\\
  While Text-to-SQL remains the dominant approach for database interaction,
real-world analytics increasingly require the flexibility of general-purpose
programming languages such as Python or Pandas to manage file-based data and
complex analytical workflows. Despite this growing need, the reliability of
Text-to-Python in core data retrieval remains underexplored relative to the
mature SQL ecosystem. To address this gap, we introduce BIRD-Python, a
benchmark designed for cross-paradigm evaluation. We systematically refined the
original dataset to reduce annotation noise and align execution semantics,
thereby establishing a consistent and standardized baseline for comparison. Our
analysis reveals a fundamental paradigmatic divergence: whereas SQL leverages
implicit DBMS behaviors through its declarative structure, Python requires
explicit procedural logic, making it highly sensitive to underspecified user
intent. To mitigate this challenge, we propose the Logic Completion Framework
(LCF), which resolves ambiguity by incorporating latent domain knowledge into
the generation process. Experimental results show that (1) performance
differences primarily stem from missing domain context rather than inherent
limitations in code generation, and (2) when these gaps are addressed,
Text-to-Python achieves performance parity with Text-to-SQL. These findings
establish Python as a viable foundation for analytical agents-provided that
systems effectively ground ambiguous natural language inputs in executable
logical specifications. Resources are available at
https://anonymous.4open.science/r/Bird-Python-43B7/.
\\ ( https://arxiv.org/abs/2601.15728 ,  717kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15737
Date: Thu, 22 Jan 2026 08:05:32 GMT   (348kb)

Title: PhysProver: Advancing Automatic Theorem Proving for Physics
Authors: Hanning Zhang, Ruida Wang, Rui Pan, Wenyuan Wang, Bingxu Meng, Tong
  Zhang
Categories: cs.AI cs.CL
Comments: Preprint
\\
  The combination of verifiable languages and LLMs has significantly influenced
both the mathematical and computer science communities because it provides a
rigorous foundation for theorem proving. Recent advancements in the field
provide foundation models and sophisticated agentic systems pushing the
boundaries of formal mathematical reasoning to approach the natural language
capability of LLMs. However, little attention has been given to the formal
physics reasoning, which also heavily relies on similar problem-solving and
theorem-proving frameworks. To solve this problem, this paper presents, to the
best of our knowledge, the first approach to enhance formal theorem proving in
the physics domain. We compose a dedicated dataset PhysLeanData for the task.
It is composed of theorems sampled from PhysLean and data generated by a
conjecture-based formal data generation pipeline. In the training pipeline, we
leverage DeepSeek-Prover-V2-7B, a strong open-source mathematical theorem
prover, and apply Reinforcement Learning with Verifiable Rewards (RLVR) to
train our model PhysProver. Comprehensive experiments demonstrate that, using
only $\sim$5K training samples, PhysProver achieves an overall 2.4\%
improvement in multiple sub-domains. Furthermore, after formal physics
training, we observe 1.3\% gains on the MiniF2F-Test benchmark, which indicates
non-trivial generalization beyond physics domains and enhancement for formal
math capability as well. The results highlight the effectiveness and efficiency
of our approach, which provides a paradigm for extending formal provers outside
mathematical domains. To foster further research, we will release both our
dataset and model to the community.
\\ ( https://arxiv.org/abs/2601.15737 ,  348kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15751
Date: Thu, 22 Jan 2026 08:24:31 GMT   (313kb)

Title: Tabular Incremental Inference
Authors: Xinda Chen, Xing Zhen, Hanyu Zhang, Weimin Tan, Bo Yan
Categories: cs.AI
\\
  Tabular data is a fundamental form of data structure. The evolution of table
analysis tools reflects humanity's continuous progress in data acquisition,
management, and processing. The dynamic changes in table columns arise from
technological advancements, changing needs, data integration, etc. However, the
standard process of training AI models on tables with fixed columns and then
performing inference is not suitable for handling dynamically changed tables.
Therefore, new methods are needed for efficiently handling such tables in an
unsupervised manner. In this paper, we introduce a new task, Tabular
Incremental Inference (TabII), which aims to enable trained models to
incorporate new columns during the inference stage, enhancing the practicality
of AI models in scenarios where tables are dynamically changed. Furthermore, we
demonstrate that this new task can be framed as an optimization problem based
on the information bottleneck theory, which emphasizes that the key to an ideal
tabular incremental inference approach lies in minimizing mutual information
between tabular data and representation while maximizing between representation
and task labels. Under this guidance, we design a TabII method with Large
Language Model placeholders and Pretrained TabAdapter to provide external
knowledge and Incremental Sample Condensation blocks to condense the
task-relevant information given by incremental column attributes. Experimental
results across eight public datasets show that TabII effectively utilizes
incremental attributes, achieving state-of-the-art performance.
\\ ( https://arxiv.org/abs/2601.15751 ,  313kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15761
Date: Thu, 22 Jan 2026 08:51:16 GMT   (6653kb)

Title: Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World
  Robot Learning
Authors: Xiefeng Wu, Mingyu Hu, Shu Zhang
Categories: cs.AI
Comments: 7 pages main text 2 page reference
\\
  Deploying reinforcement learning in the real world remains challenging due to
sample inefficiency, sparse rewards, and noisy visual observations. Prior work
leverages demonstrations and human feedback to improve learning efficiency and
robustness. However, offline-to-online methods need large datasets and can be
unstable, while VLA-assisted RL relies on large-scale pretraining and
fine-tuning. As a result, a low-cost real-world RL method with minimal data
requirements has yet to emerge. We introduce \textbf{SigEnt-SAC}, an off-policy
actor-critic method that learns from scratch using a single expert trajectory.
Our key design is a sigmoid-bounded entropy term that prevents
negative-entropy-driven optimization toward out-of-distribution actions and
reduces Q-function oscillations. We benchmark SigEnt-SAC on D4RL tasks against
representative baselines. Experiments show that SigEnt-SAC substantially
alleviates Q-function oscillations and reaches a 100\% success rate faster than
prior methods. Finally, we validate SigEnt-SAC on four real-world robotic tasks
across multiple embodiments, where agents learn from raw images and sparse
rewards; results demonstrate that SigEnt-SAC can learn successful policies with
only a small number of real-world interactions, suggesting a low-cost and
practical pathway for real-world RL deployment.
\\ ( https://arxiv.org/abs/2601.15761 ,  6653kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15778
Date: Thu, 22 Jan 2026 09:08:25 GMT   (10148kb)

Title: Agentic Confidence Calibration
Authors: Jiaxin Zhang, Caiming Xiong, Chien-Sheng Wu
Categories: cs.AI cs.CL
Comments: 37 pages, 15 figures, 12 tables
\\
  AI agents are rapidly advancing from passive language models to autonomous
systems executing complex, multi-step tasks. Yet their overconfidence in
failure remains a fundamental barrier to deployment in high-stakes settings.
Existing calibration methods, built for static single-turn outputs, cannot
address the unique challenges of agentic systems, such as compounding errors
along trajectories, uncertainty from external tools, and opaque failure modes.
To address these challenges, we introduce, for the first time, the problem of
Agentic Confidence Calibration and propose Holistic Trajectory Calibration
(HTC), a novel diagnostic framework that extracts rich process-level features
ranging from macro dynamics to micro stability across an agent's entire
trajectory. Powered by a simple, interpretable model, HTC consistently
surpasses strong baselines in both calibration and discrimination, across eight
benchmarks, multiple LLMs, and diverse agent frameworks. Beyond performance,
HTC delivers three essential advances: it provides interpretability by
revealing the signals behind failure, enables transferability by applying
across domains without retraining, and achieves generalization through a
General Agent Calibrator (GAC) that achieves the best calibration (lowest ECE)
on the out-of-domain GAIA benchmark. Together, these contributions establish a
new process-centric paradigm for confidence calibration, providing a framework
for diagnosing and enhancing the reliability of AI agents.
\\ ( https://arxiv.org/abs/2601.15778 ,  10148kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15797
Date: Thu, 22 Jan 2026 09:31:12 GMT   (502kb)

Title: Creativity in the Age of AI: Rethinking the Role of Intentional Agency
Authors: James S. Pearson, Matthew J. Dennis, Marc Cheong
Categories: cs.AI
Comments: 27 pages, 2 figures
MSC-class: 68T01
\\
  Many theorists of creativity maintain that intentional agency is a necessary
condition of creativity. We argue that this requirement, which we call the
Intentional Agency Condition (IAC), should be rejected as a general condition
of creativity, while retaining its relevance in specific contexts. We show that
recent advances in generative AI have rendered the IAC increasingly
problematic, both descriptively and functionally. We offer two reasons for
abandoning it at the general level. First, we present corpus evidence
indicating that authors and journalists are increasingly comfortable ascribing
creativity to generative AI, despite its lack of intentional agency. This
development places pressure on the linguistic intuitions that have
traditionally been taken to support the IAC. Second, drawing on the method of
conceptual engineering, we argue that the IAC no longer fulfils its core social
function. Rather than facilitating the identification and encouragement of
reliable sources of novel and valuable products, it now feeds into biases that
distort our assessments of AI-generated outputs. We therefore propose replacing
the IAC with a consistency requirement, according to which creativity tracks
the reliable generation of novel and valuable products. Nonetheless, we explain
why the IAC should be retained in specific local domains.
\\ ( https://arxiv.org/abs/2601.15797 ,  502kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15798
Date: Thu, 22 Jan 2026 09:31:19 GMT   (1106kb)

Title: VitalDiagnosis: AI-Driven Ecosystem for 24/7 Vital Monitoring and
  Chronic Disease Management
Authors: Zhikai Xue, Tianqianjin Lin, Pengwei Yan, Ruichun Wang, Yuxin Liu,
  Zhuoren Jiang, Xiaozhong Liu
Categories: cs.AI
Comments: Accepted by AAAI 2026 Demo
\\
  Chronic diseases have become the leading cause of death worldwide, a
challenge intensified by strained medical resources and an aging population.
Individually, patients often struggle to interpret early signs of deterioration
or maintain adherence to care plans. In this paper, we introduce
VitalDiagnosis, an LLM-driven ecosystem designed to shift chronic disease
management from passive monitoring to proactive, interactive engagement. By
integrating continuous data from wearable devices with the reasoning
capabilities of LLMs, the system addresses both acute health anomalies and
routine adherence. It analyzes triggers through context-aware inquiries,
produces provisional insights within a collaborative patient-clinician
workflow, and offers personalized guidance. This approach aims to promote a
more proactive and cooperative care paradigm, with the potential to enhance
patient self-management and reduce avoidable clinical workload.
\\ ( https://arxiv.org/abs/2601.15798 ,  1106kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15808
Date: Thu, 22 Jan 2026 09:47:31 GMT   (909kb)

Title: Inference-Time Scaling of Verification: Self-Evolving Deep Research
  Agents via Test-Time Rubric-Guided Verification
Authors: Yuxuan Wan, Tianqing Fang, Zaitang Li, Yintong Huo, Wenxuan Wang,
  Haitao Mi, Dong Yu, Michael R. Lyu
Categories: cs.AI
\\
  Recent advances in Deep Research Agents (DRAs) are transforming automated
knowledge discovery and problem-solving. While the majority of existing efforts
focus on enhancing policy capabilities via post-training, we propose an
alternative paradigm: self-evolving the agent's ability by iteratively
verifying the policy model's outputs, guided by meticulously crafted rubrics.
This approach gives rise to the inference-time scaling of verification, wherein
an agent self-improves by evaluating its generated answers to produce iterative
feedback and refinements. We derive the rubrics based on an automatically
constructed DRA Failure Taxonomy, which systematically classifies agent
failures into five major categories and thirteen sub-categories. We present
DeepVerifier, a rubrics-based outcome reward verifier that leverages the
asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge
baselines by 12%-48% in meta-evaluation F1 score. To enable practical
self-evolution, DeepVerifier integrates as a plug-and-play module during
test-time inference. The verifier produces detailed rubric-based feedback,
which is fed back to the agent for iterative bootstrapping, refining responses
without additional training. This test-time scaling delivers 8%-11% accuracy
gains on challenging subsets of GAIA and XBench-DeepResearch when powered by
capable closed-source LLMs. Finally, to support open-source advancement, we
release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646
high-quality agent steps focused on DRA verification. These examples emphasize
reflection and self-critique, enabling open models to develop robust
verification capabilities.
\\ ( https://arxiv.org/abs/2601.15808 ,  909kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15812
Date: Thu, 22 Jan 2026 09:52:39 GMT   (1267kb)

Title: ErrorMap and ErrorAtlas: Charting the Failure Landscape of Large
  Language Models
Authors: Shir Ashury-Tahan, Yifan Mai, Elron Bandel, Michal Shmueli-Scheuer and
  Leshem Choshen
Categories: cs.AI cs.CL
\\
  Large Language Models (LLM) benchmarks tell us when models fail, but not why
they fail. A wrong answer on a reasoning dataset may stem from formatting
issues, calculation errors, or dataset noise rather than weak reasoning.
Without disentangling such causes, benchmarks remain incomplete and cannot
reliably guide model improvement. We introduce ErrorMap, the first method to
chart the sources of LLM failure. It extracts a model's unique "failure
signature", clarifies what benchmarks measure, and broadens error
identification to reduce blind spots. This helps developers debug models,
aligns benchmark goals with outcomes, and supports informed model selection.
ErrorMap works on any model or dataset with the same logic. Applying our method
to 35 datasets and 83 models we generate ErrorAtlas, a taxonomy of model
errors, revealing recurring failure patterns. ErrorAtlas highlights error types
that are currently underexplored in LLM research, such as omissions of required
details in the output and question misinterpretation. By shifting focus from
where models succeed to why they fail, ErrorMap and ErrorAtlas enable advanced
evaluation - one that exposes hidden weaknesses and directs progress. Unlike
success, typically measured by task-level metrics, our approach introduces a
deeper evaluation layer that can be applied globally across models and tasks,
offering richer insights into model behavior and limitations. We make the
taxonomy and code publicly available with plans to periodically update
ErrorAtlas as new benchmarks and models emerge.
\\ ( https://arxiv.org/abs/2601.15812 ,  1267kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15876
Date: Thu, 22 Jan 2026 11:36:43 GMT   (27792kb)

Title: EvoCUA: Evolving Computer Use Agents via Learning from Scalable
  Synthetic Experience
Authors: Taofeng Xue, Chong Peng, Mianqiu Huang, Linsen Guo, Tiancheng Han,
  Haozhe Wang, Jianing Wang, Xiaocheng Zhang, Xin Yang, Dengchang Zhao, Jinrui
  Ding, Xiandi Ma, Yuchen Xie, Peng Pei, Xunliang Cai, Xipeng Qiu
Categories: cs.AI
Comments: 26 pages, 8 figures
ACM-class: I.2; I.2.11; I.2.10; H.5.2
\\
  The development of native computer-use agents (CUA) represents a significant
leap in multimodal AI. However, their potential is currently bottlenecked by
the constraints of static data scaling. Existing paradigms relying primarily on
passive imitation of static datasets struggle to capture the intricate causal
dynamics inherent in long-horizon computer tasks. In this work, we introduce
EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA
integrates data generation and policy optimization into a self-sustaining
evolutionary cycle. To mitigate data scarcity, we develop a verifiable
synthesis engine that autonomously generates diverse tasks coupled with
executable validators. To enable large-scale experience acquisition, we design
a scalable infrastructure orchestrating tens of thousands of asynchronous
sandbox rollouts. Building on these massive trajectories, we propose an
iterative evolving learning strategy to efficiently internalize this
experience. This mechanism dynamically regulates policy updates by identifying
capability boundaries -- reinforcing successful routines while transforming
failure trajectories into rich supervision through error analysis and
self-correction. Empirical evaluations on the OSWorld benchmark demonstrate
that EvoCUA achieves a success rate of 56.7%, establishing a new open-source
state-of-the-art. Notably, EvoCUA significantly outperforms the previous best
open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights
models such as UI-TARS-2 (53.1%). Crucially, our results underscore the
generalizability of this approach: the evolving paradigm driven by learning
from experience yields consistent performance gains across foundation models of
varying scales, establishing a robust and scalable path for advancing native
agent capabilities.
\\ ( https://arxiv.org/abs/2601.15876 ,  27792kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15931
Date: Thu, 22 Jan 2026 13:09:22 GMT   (813kb)

Title: ICON: Invariant Counterfactual Optimization with Neuro-Symbolic Priors
  for Text-Based Person Search
Authors: Xiangyu Wang, Zhixin Lv, Yongjiao Sun, Anrui Han, Ye Yuan, Hangxu Ji
Categories: cs.AI cs.LG
\\
  Text-Based Person Search (TBPS) holds unique value in real-world surveillance
bridging visual perception and language understanding, yet current paradigms
utilizing pre-training models often fail to transfer effectively to complex
open-world scenarios. The reliance on "Passive Observation" leads to
multifaceted spurious correlations and spatial semantic misalignment, causing a
lack of robustness against distribution shifts. To fundamentally resolve these
defects, this paper proposes ICON (Invariant Counterfactual Optimization with
Neuro-symbolic priors), a framework integrating causal and topological priors.
First, we introduce Rule-Guided Spatial Intervention to strictly penalize
sensitivity to bounding box noise, forcibly severing location shortcuts to
achieve geometric invariance. Second, Counterfactual Context Disentanglement is
implemented via semantic-driven background transplantation, compelling the
model to ignore background interference for environmental independence. Then,
we employ Saliency-Driven Semantic Regularization with adaptive masking to
resolve local saliency bias and guarantee holistic completeness. Finally,
Neuro-Symbolic Topological Alignment utilizes neuro-symbolic priors to
constrain feature matching, ensuring activated regions are topologically
consistent with human structural logic. Experimental results demonstrate that
ICON not only maintains leading performance on standard benchmarks but also
exhibits exceptional robustness against occlusion, background interference, and
localization noise. This approach effectively advances the field by shifting
from fitting statistical co-occurrences to learning causal invariance.
\\ ( https://arxiv.org/abs/2601.15931 ,  813kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15949
Date: Thu, 22 Jan 2026 13:38:13 GMT   (760kb)

Title: Natural Language-Driven Global Mapping of Martian Landforms
Authors: Yiran Wang, Shuoyuan Wang, Zhaoran Wei, Jiannan Zhao, Zhonghua Yao,
  Zejian Xie, Songxin Zhang, Jun Huang, Bingyi Jing, Hongxin Wei
Categories: cs.AI astro-ph.IM
\\
  Planetary surfaces are typically analyzed using high-level semantic concepts
in natural language, yet vast orbital image archives remain organized at the
pixel level. This mismatch limits scalable, open-ended exploration of planetary
surfaces. Here we present MarScope, a planetary-scale vision-language framework
enabling natural language-driven, label-free mapping of Martian landforms.
MarScope aligns planetary images and text in a shared semantic space, trained
on over 200,000 curated image-text pairs. This framework transforms global
geomorphic mapping on Mars by replacing pre-defined classifications with
flexible semantic retrieval, enabling arbitrary user queries across the entire
planet in 5 seconds with F1 scores up to 0.978. Applications further show that
it extends beyond morphological classification to facilitate process-oriented
analysis and similarity-based geomorphological mapping at a planetary scale.
MarScope establishes a new paradigm where natural language serves as a direct
interface for scientific discovery over massive geospatial datasets.
\\ ( https://arxiv.org/abs/2601.15949 ,  760kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15953
Date: Thu, 22 Jan 2026 13:42:08 GMT   (878kb)

Title: Decoupling Return-to-Go for Efficient Decision Transformer
Authors: Yongyi Wang, Hanyu Liu, Lingfeng Li, Bozhou Chen, Ang Li, Qirui Zheng,
  Xionghui Yang, Wenxin Li
Categories: cs.AI
\\
  The Decision Transformer (DT) has established a powerful sequence modeling
approach to offline reinforcement learning. It conditions its action
predictions on Return-to-Go (RTG), using it both to distinguish trajectory
quality during training and to guide action generation at inference. In this
work, we identify a critical redundancy in this design: feeding the entire
sequence of RTGs into the Transformer is theoretically unnecessary, as only the
most recent RTG affects action prediction. We show that this redundancy can
impair DT's performance through experiments. To resolve this, we propose the
Decoupled DT (DDT). DDT simplifies the architecture by processing only
observation and action sequences through the Transformer, using the latest RTG
to guide the action prediction. This streamlined approach not only improves
performance but also reduces computational cost. Our experiments show that DDT
significantly outperforms DT and establishes competitive performance against
state-of-the-art DT variants across multiple offline RL tasks.
\\ ( https://arxiv.org/abs/2601.15953 ,  878kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16027
Date: Thu, 22 Jan 2026 14:55:51 GMT   (8471kb)

Title: Deja Vu in Plots: Leveraging Cross-Session Evidence with
  Retrieval-Augmented LLMs for Live Streaming Risk Assessment
Authors: Yiran Qiao, Xiang Ao, Jing Chen, Yang Liu, Qiwei Zhong, Qing He
Categories: cs.AI
\\
  The rise of live streaming has transformed online interaction, enabling
massive real-time engagement but also exposing platforms to complex risks such
as scams and coordinated malicious behaviors. Detecting these risks is
challenging because harmful actions often accumulate gradually and recur across
seemingly unrelated streams. To address this, we propose CS-VAR (Cross-Session
Evidence-Aware Retrieval-Augmented Detector) for live streaming risk
assessment. In CS-VAR, a lightweight, domain-specific model performs fast
session-level risk inference, guided during training by a Large Language Model
(LLM) that reasons over retrieved cross-session behavioral evidence and
transfers its local-to-global insights to the small model. This design enables
the small model to recognize recurring patterns across streams, perform
structured risk assessment, and maintain efficiency for real-time deployment.
Extensive offline experiments on large-scale industrial datasets, combined with
online validation, demonstrate the state-of-the-art performance of CS-VAR.
Furthermore, CS-VAR provides interpretable, localized signals that effectively
empower real-world moderation for live streaming.
\\ ( https://arxiv.org/abs/2601.16027 ,  8471kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16038
Date: Thu, 22 Jan 2026 15:11:02 GMT   (9029kb)

Title: Grounding Large Language Models in Reaction Knowledge Graphs for
  Synthesis Retrieval
Authors: Olga Bunkova, Lorenzo Di Fruscia, Sophia Rupprecht, Artur M.
  Schweidtmann, Marcel J.T. Reinders, Jana M. Weber
Categories: cs.AI
Comments: Accepted at ML4Molecules 2025 (ELLIS UnConference workshop),
  Copenhagen, Denmark, December 2, 2025. Workshop page:
  https://moleculediscovery.github.io/workshop2025/
\\
  Large Language Models (LLMs) can aid synthesis planning in chemistry, but
standard prompting methods often yield hallucinated or outdated suggestions. We
study LLM interactions with a reaction knowledge graph by casting reaction path
retrieval as a Text2Cypher (natural language to graph query) generation
problem, and define single- and multi-step retrieval tasks. We compare
zero-shot prompting to one-shot variants using static, random, and
embedding-based exemplar selection, and assess a checklist-driven
validator/corrector loop. To evaluate our framework, we consider query validity
and retrieval accuracy. We find that one-shot prompting with aligned exemplars
consistently performs best. Our checklist-style self-correction loop mainly
improves executability in zero-shot settings and offers limited additional
retrieval gains once a good exemplar is present. We provide a reproducible
Text2Cypher evaluation setup to facilitate further work on KG-grounded LLMs for
synthesis planning. Code is available at
https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval.
\\ ( https://arxiv.org/abs/2601.16038 ,  9029kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16045
Date: Thu, 22 Jan 2026 15:20:00 GMT   (4882kb)

Title: AgriPINN: A Process-Informed Neural Network for Interpretable and
  Scalable Crop Biomass Prediction Under Water Stress
Authors: Yue Shi, Liangxiu Han, Xin Zhang, Tam Sobeih, Thomas Gaiser, Nguyen
  Huu Thuy, Dominik Behrend, Amit Kumar Srivastava, Krishnagopal Halder, Frank
  Ewert
Categories: cs.AI
\\
  Accurate prediction of crop above-ground biomass (AGB) under water stress is
critical for monitoring crop productivity, guiding irrigation, and supporting
climate-resilient agriculture. Data-driven models scale well but often lack
interpretability and degrade under distribution shift, whereas process-based
crop models (e.g. DSSAT, APSIM, LINTUL5) require extensive calibration and are
difficult to deploy over large spatial domains. To address these limitations,
we propose AgriPINN, a process-informed neural network that integrates a
biophysical crop-growth differential equation as a differentiable constraint
within a deep learning backbone. This design encourages physiologically
consistent biomass dynamics under water-stress conditions while preserving
model scalability for spatially distributed AGB prediction. AgriPINN recovers
latent physiological variables, including leaf area index (LAI), absorbed
photosynthetically active radiation (PAR), radiation use efficiency (RUE), and
water-stress factors, without requiring direct supervision. We pretrain
AgriPINN on 60 years of historical data across 397 regions in Germany and
fine-tune it on three years of field experiments under controlled water
treatments. Results show that AgriPINN consistently outperforms
state-of-the-art deep-learning baselines (ConvLSTM-ViT, SLTF, CNN-Transformer)
and the process-based LINTUL5 model in terms of accuracy (RMSE reductions up to
$43\%$) and computational efficiency. By combining the scalability of deep
learning with the biophysical rigor of process-based modeling, AgriPINN
provides a robust and interpretable framework for spatio-temporal AGB
prediction, offering practical value for planning of irrigation infrastructure,
yield forecasting, and climate-adaptation planning.
\\ ( https://arxiv.org/abs/2601.16045 ,  4882kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16056
Date: Thu, 22 Jan 2026 15:41:22 GMT   (10436kb)

Title: Designing faster mixed integer linear programming algorithm via learning
  the optimal path
Authors: Ruizhi Liu, Liming Xu, Xulin Huang, Jingyan Sui, Shizhe Ding, Boyang
  Xia, Chungong Yu, Dongbo Bu
Categories: cs.AI
\\
  Designing faster algorithms for solving Mixed-Integer Linear Programming
(MILP) problems is highly desired across numerous practical domains, as a vast
array of complex real-world challenges can be effectively modeled as MILP
formulations. Solving these problems typically employs the branch-and-bound
algorithm, the core of which can be conceived as searching for a path of nodes
(or sub-problems) that contains the optimal solution to the original MILP
problem. Traditional approaches to finding this path rely heavily on
hand-crafted, intuition-based heuristic strategies, which often suffer from
unstable and unpredictable performance across different MILP problem instances.
To address this limitation, we introduce DeepBound, a deep learning-based node
selection algorithm that automates the learning of such human intuition from
data. The core of DeepBound lies in learning to prioritize nodes containing the
optimal solution, thereby improving solving efficiency. DeepBound introduces a
multi-level feature fusion network to capture the node representations. To
tackle the inherent node imbalance in branch-and-bound trees, DeepBound employs
a pairwise training paradigm that enhances the model's ability to discriminate
between nodes. Extensive experiments on three NP-hard MILP benchmarks
demonstrate that DeepBound achieves superior solving efficiency over
conventional heuristic rules and existing learning-based approaches, obtaining
optimal feasible solutions with significantly reduced computation time.
Moreover, DeepBound demonstrates strong generalization capability on large and
complex instances. The analysis of its learned features reveals that the method
can automatically discover more flexible and robust feature selection, which
may effectively improve and potentially replace human-designed heuristic rules.
\\ ( https://arxiv.org/abs/2601.16056 ,  10436kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16087
Date: Thu, 22 Jan 2026 16:34:05 GMT   (396kb)

Title: Controlling Long-Horizon Behavior in Language Model Agents with Explicit
  State Dynamics
Authors: Sukesh Subaharan
Categories: cs.AI cs.CL
Comments: Supplementary materials can be found here:
  https://github.com/drsukeshs/agent-behavior-ext-dynamics
MSC-class: 68T01
ACM-class: I.2.7; I.2.11
\\
  Large language model (LLM) agents often exhibit abrupt shifts in tone and
persona during extended interaction, reflecting the absence of explicit
temporal structure governing agent-level state. While prior work emphasizes
turn-local sentiment or static emotion classification, the role of explicit
affective dynamics in shaping long-horizon agent behavior remains
underexplored. This work investigates whether imposing dynamical structure on
an external affective state can induce temporal coherence and controlled
recovery in multi-turn dialogue. We introduce an agent-level affective
subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state
external to the language model and governed by first- and second-order update
rules. Instantaneous affective signals are extracted using a fixed, memoryless
estimator and integrated over time via exponential smoothing or momentum-based
dynamics. The resulting affective state is injected back into generation
without modifying model parameters. Using a fixed 25-turn dialogue protocol, we
compare stateless, first-order, and second-order affective dynamics. Stateless
agents fail to exhibit coherent trajectories or recovery, while state
persistence enables delayed responses and reliable recovery. Second-order
dynamics introduce affective inertia and hysteresis that increase with
momentum, revealing a trade-off between stability and responsiveness.
\\ ( https://arxiv.org/abs/2601.16087 ,  396kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16108
Date: Thu, 22 Jan 2026 16:55:48 GMT   (1787kb)

Title: Multimodal Climate Disinformation Detection: Integrating Vision-Language
  Models with External Knowledge Sources
Authors: Marzieh Adeli Shamsabad and Hamed Ghodrati
Categories: cs.AI
\\
  Climate disinformation has become a major challenge in today digital world,
especially with the rise of misleading images and videos shared widely on
social media. These false claims are often convincing and difficult to detect,
which can delay actions on climate change. While vision-language models (VLMs)
have been used to identify visual disinformation, they rely only on the
knowledge available at the time of training. This limits their ability to
reason about recent events or updates. The main goal of this paper is to
overcome that limitation by combining VLMs with external knowledge. By
retrieving up-to-date information such as reverse image results, online
fact-checks, and trusted expert content, the system can better assess whether
an image and its claim are accurate, misleading, false, or unverifiable. This
approach improves the model ability to handle real-world climate disinformation
and supports efforts to protect public understanding of science in a rapidly
changing information landscape.
\\ ( https://arxiv.org/abs/2601.16108 ,  1787kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16134
Date: Thu, 22 Jan 2026 17:31:25 GMT   (277kb)

Title: LLM Prompt Evaluation for Educational Applications
Authors: Langdon Holmes, Adam Coscia, Scott Crossley, Joon Suh Choi, and Wesley
  Morris
Categories: cs.AI cs.CL
\\
  As large language models (LLMs) become increasingly common in educational
applications, there is a growing need for evidence-based methods to design and
evaluate LLM prompts that produce personalized and pedagogically aligned
out-puts. This study presents a generalizable, systematic approach for
evaluating prompts, demonstrated through an analysis of LLM-generated follow-up
questions in a structured dialogue activity. Six prompt templates were designed
and tested. The templates incorporated established prompt engineering patterns,
with each prompt emphasizing distinct pedagogical strategies. The prompt
templates were compared through a tournament-style evaluation framework that
can be adapted for other educational applications. The tournament employed the
Glicko2 rating system with eight judges evaluating question pairs across three
dimensions: format, dialogue support, and appropriateness for learners. Data
was sourced from 120 authentic user interactions across three distinct
educational deployments. Results showed that a single prompt related to
strategic reading out-performed other templates with win probabilities ranging
from 81% to 100% in pairwise comparisons. This prompt combined persona and
context manager pat-terns and was designed to support metacognitive learning
strategies such as self-directed learning. The methodology showcases how
educational technology re- searchers can systematically evaluate and improve
prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based
prompt development for educational applications.
\\ ( https://arxiv.org/abs/2601.16134 ,  277kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16163
Date: Thu, 22 Jan 2026 18:09:30 GMT   (19225kb)

Title: Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and
  Planning
Authors: Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge, Grace
  Lam, Percy Liang, Shuran Song, Ming-Yu Liu, Chelsea Finn, Jinwei Gu
Categories: cs.AI cs.RO
\\
  Recent video generation models demonstrate remarkable ability to capture
complex physical interactions and scene evolution over time. To leverage their
spatiotemporal priors, robotics works have adapted video models for policy
learning but introduce complexity by requiring multiple stages of post-training
and new architectural components for action generation. In this work, we
introduce Cosmos Policy, a simple approach for adapting a large pretrained
video model (Cosmos-Predict2) into an effective robot policy through a single
stage of post-training on the robot demonstration data collected on the target
platform, with no architectural modifications. Cosmos Policy learns to directly
generate robot actions encoded as latent frames within the video model's latent
diffusion process, harnessing the model's pretrained priors and core learning
algorithm to capture complex action distributions. Additionally, Cosmos Policy
generates future state images and values (expected cumulative rewards), which
are similarly encoded as latent frames, enabling test-time planning of action
trajectories with higher likelihood of success. In our evaluations, Cosmos
Policy achieves state-of-the-art performance on the LIBERO and RoboCasa
simulation benchmarks (98.5% and 67.1% average success rates, respectively) and
the highest average score in challenging real-world bimanual manipulation
tasks, outperforming strong diffusion policies trained from scratch, video
model-based policies, and state-of-the-art vision-language-action models
fine-tuned on the same robot demonstrations. Furthermore, given policy rollout
data, Cosmos Policy can learn from experience to refine its world model and
value function and leverage model-based planning to achieve even higher success
rates in challenging tasks. We release code, models, and training data at
https://research.nvidia.com/labs/dir/cosmos-policy/
\\ ( https://arxiv.org/abs/2601.16163 ,  19225kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16172
Date: Thu, 22 Jan 2026 18:16:46 GMT   (87kb)

Title: Structured Hints for Sample-Efficient Lean Theorem Proving
Authors: Zachary Burton
Categories: cs.AI
Comments: 9 pages, 1 figure
\\
  State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine
large language models with reinforcement learning, achieving impressive results
through sophisticated training. We ask: do these highly-trained models still
benefit from simple structural guidance at inference time? We evaluate a
lightweight intervention -- a fixed prompt schedule over 15 common tactic
skeletons -- on the miniF2F benchmark. This simple approach yields 21.7%
pass@16 compared to 15.2% for standard sampling from the same model, a 43%
relative improvement using the same number of samples (k=16) and same maximum
generation length (1024 tokens). Our results suggest that even capable
RL-trained provers underutilize structural priors available in the tactic
language, and that simple inference-time guidance remains a cheap,
complementary boost.
\\ ( https://arxiv.org/abs/2601.16172 ,  87kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16216
Date: Sat, 10 Jan 2026 13:24:49 GMT   (8855kb)

Title: Scalable Board Expansion within a General Game System
Authors: Cl\'ementine Sacr\'e
Categories: cs.AI cs.GT cs.SE
Comments: 65 pages, 41 figures
\\
  This thesis explores the use of a General Game System (GGS) to support the
automatic expansion of game boards in boardless games. Traditional
implementations of such games often rely on oversized static boards defined
from the start, even though large portions of these boards may never be used
during gameplay. This approach leads to unnecessary complexity. To address this
issue, this thesis propose a dynamic board expansion mechanism in which the
game board grows automatically during play.
\\ ( https://arxiv.org/abs/2601.16216 ,  8855kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15333
Date: Tue, 20 Jan 2026 08:10:48 GMT   (576kb)

Title: Empowering LLMs for Structure-Based Drug Design via
  Exploration-Augmented Latent Inference
Authors: Xuanning Hu, Anchen Li, Qianli Xing, Jinglong Ji, Hao Tuo and Bo Yang
Categories: cs.LG cs.AI q-bio.QM
\\
  Large Language Models (LLMs) possess strong representation and reasoning
capabilities, but their application to structure-based drug design (SBDD) is
limited by insufficient understanding of protein structures and unpredictable
molecular generation. To address these challenges, we propose
Exploration-Augmented Latent Inference for LLMs (ELILLM), a framework that
reinterprets the LLM generation process as an encoding, latent space
exploration, and decoding workflow. ELILLM explicitly explores portions of the
design problem beyond the model's current knowledge while using a decoding
module to handle familiar regions, generating chemically valid and
synthetically reasonable molecules. In our implementation, Bayesian
optimization guides the systematic exploration of latent embeddings, and a
position-aware surrogate model efficiently predicts binding affinity
distributions to inform the search. Knowledge-guided decoding further reduces
randomness and effectively imposes chemical validity constraints. We
demonstrate ELILLM on the CrossDocked2020 benchmark, showing strong controlled
exploration and high binding affinity scores compared with seven baseline
methods. These results demonstrate that ELILLM can effectively enhance LLMs
capabilities for SBDD.
\\ ( https://arxiv.org/abs/2601.15333 ,  576kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15337
Date: Tue, 20 Jan 2026 10:46:44 GMT   (268kb)

Title: Language Models Entangle Language and Culture
Authors: Shourya Jain and Paras Chopra
Categories: cs.LG cs.CL
Comments: Accepted at LM4UC Workshop at AAAI'26, Submitted to ACL 2026. 17
  pages, 7 figures
\\
  Users should not be systemically disadvantaged by the language they use for
interacting with LLMs; i.e. users across languages should get responses of
similar quality irrespective of language used. In this work, we create a set of
real-world open-ended questions based on our analysis of the WildChat dataset
and use it to evaluate whether responses vary by language, specifically,
whether answer quality depends on the language used to query the model. We also
investigate how language and culture are entangled in LLMs such that choice of
language changes the cultural information and context used in the response by
using LLM-as-a-Judge to identify the cultural context present in responses. To
further investigate this, we evaluate LLMs on a translated subset of the
CulturalBench benchmark across multiple languages. Our evaluations reveal that
LLMs consistently provide lower quality answers to open-ended questions in low
resource languages. We find that language significantly impacts the cultural
context used by the model. This difference in context impacts the quality of
the downstream answer.
\\ ( https://arxiv.org/abs/2601.15337 ,  268kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15370
Date: Wed, 21 Jan 2026 18:53:58 GMT   (3436kb)

Title: Improving MoE Compute Efficiency by Composing Weight and Data Sparsity
Authors: Maciej Kilian, Oleg Mkrtchyan, Luke Zettlemoyer, Akshat Shrivastava,
  Armen Aghajanyan
Categories: cs.LG cs.AI
\\
  Mixture-of-Experts layers achieve compute efficiency through weight sparsity:
each token activates only a subset of experts. Data sparsity, where each expert
processes only a subset of tokens, offers a complementary axis. Expert-choice
routing implements data sparsity directly but violates causality in
autoregressive models, creating train-inference mismatch. We recover data
sparsity within causal token-choice MoE by leveraging zero-compute (null)
experts within the routing pool. When a token routes to null experts, those
slots consume no compute. The standard load balancing objective trains the
model to uniformly use all experts (real and null) therefore creating data
sparsity in expectation without the causality violations. We evaluate on
vision-language model training, where data heterogeneity is pronounced: vision
encoders produce many low-information tokens while text tokens are denser. At
matched expected FLOPs, composing weight and data sparsity yields a more
compute-efficient frontier than weight sparsity alone, with gains in training
loss and downstream performance. The model learns implicit modality-aware
allocation, routing vision tokens to null experts more aggressively than text,
without explicit modality routing.
\\ ( https://arxiv.org/abs/2601.15370 ,  3436kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15380
Date: Wed, 21 Jan 2026 19:00:08 GMT   (3999kb)

Title: You Need Better Attention Priors
Authors: Elon Litman, Gabe Guo
Categories: cs.LG cs.CL stat.ML
\\
  We generalize the attention mechanism by viewing it through the lens of
Entropic Optimal Transport, revealing that standard attention corresponds to a
transport problem regularized by an implicit uniform prior. We introduce
Generalized Optimal transport Attention with Trainable priors (GOAT), a new
attention mechanism that replaces this naive assumption with a learnable,
continuous prior. This prior maintains full compatibility with optimized
kernels such as FlashAttention. GOAT also provides an EOT-based explanation of
attention sinks and materializes a solution for them, avoiding the
representational trade-offs of standard attention. Finally, by absorbing
spatial information into the core attention computation, GOAT learns an
extrapolatable prior that combines the flexibility of learned positional
embeddings with the length generalization of fixed encodings.
\\ ( https://arxiv.org/abs/2601.15380 ,  3999kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15390
Date: Wed, 21 Jan 2026 19:02:52 GMT   (523kb)

Title: FedUMM: A General Framework for Federated Learning with Unified
  Multimodal Models
Authors: Zhaolong Su, Leheng Zhao, Xiaoying Wu, Ziyue Xu, Jindong Wang
Categories: cs.LG
\\
  Unified multimodal models (UMMs) are emerging as strong foundation models
that can do both generation and understanding tasks in a single architecture.
However, they are typically trained in centralized settings where all training
and downstream datasets are gathered in a central server, limiting the
deployment in privacy-sensitive and geographically distributed scenarios. In
this paper, we present FedUMM, a general federated learning framework for UMMs
under non-IID multimodal data with low communication cost. Built on NVIDIA
FLARE, FedUMM instantiates federation for a BLIP3o backbone via
parameter-efficient fine-tuning: clients train lightweight LoRA adapters while
freezing the foundation models, and the server aggregates only adapter updates.
We evaluate on VQA v2 and the GenEval compositional generation benchmarks under
Dirichlet-controlled heterogeneity with up to 16 clients. Results show slight
degradation as client count and heterogeneity increase, while remaining
competitive with centralized training. We further analyze
computation--communication trade-offs and demonstrate that adapter-only
federation reduces per-round communication by over an order of magnitude
compared to full fine-tuning, enabling practical federated UMM training. This
work provides empirical experience for future research on privacy-preserving
federated unified multimodal models.
\\ ( https://arxiv.org/abs/2601.15390 ,  523kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15399
Date: Wed, 21 Jan 2026 19:11:12 GMT   (354kb)

Title: Attention-Informed Surrogates for Navigating Power-Performance
  Trade-offs in HPC
Authors: Ashna Nawar Ahmed, Banooqa Banday, Terry Jones, Tanzima Z. Islam
Categories: cs.LG
Comments: 13 pages, 6 figures Published in MLForSys workshop in NeurIPS 2025
  Link: https://openreview.net/forum?id=R0Vc9lnDd5
\\
  High-Performance Computing (HPC) schedulers must balance user performance
with facility-wide resource constraints. The task boils down to selecting the
optimal number of nodes for a given job. We present a surrogate-assisted
multi-objective Bayesian optimization (MOBO) framework to automate this complex
decision. Our core hypothesis is that surrogate models informed by
attention-based embeddings of job telemetry can capture performance dynamics
more effectively than standard regression techniques. We pair this with an
intelligent sample acquisition strategy to ensure the approach is
data-efficient. On two production HPC datasets, our embedding-informed method
consistently identified higher-quality Pareto fronts of runtime-power
trade-offs compared to baselines. Furthermore, our intelligent data sampling
strategy drastically reduced training costs while improving the stability of
the results. To our knowledge, this is the first work to successfully apply
embedding-informed surrogates in a MOBO framework to the HPC scheduling
problem, jointly optimizing for performance and power on production workloads.
\\ ( https://arxiv.org/abs/2601.15399 ,  354kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15417
Date: Wed, 21 Jan 2026 19:29:04 GMT   (12323kb)

Title: Ambient Dataloops: Generative Models for Dataset Refinement
Authors: Adri\'an Rodr\'iguez-Mu\~noz and William Daspit and Adam Klivans and
  Antonio Torralba and Constantinos Daskalakis and Giannis Daras
Categories: cs.LG cs.AI
Comments: 27 pages, 9 figures, 11 tables
ACM-class: I.4.3
\\
  We propose Ambient Dataloops, an iterative framework for refining datasets
that makes it easier for diffusion models to learn the underlying data
distribution. Modern datasets contain samples of highly varying quality, and
training directly on such heterogeneous data often yields suboptimal models. We
propose a dataset-model co-evolution process; at each iteration of our method,
the dataset becomes progressively higher quality, and the model improves
accordingly. To avoid destructive self-consuming loops, at each generation, we
treat the synthetically improved samples as noisy, but at a slightly lower
noisy level than the previous iteration, and we use Ambient Diffusion
techniques for learning under corruption. Empirically, Ambient Dataloops
achieve state-of-the-art performance in unconditional and text-conditional
image generation and de novo protein design. We further provide a theoretical
justification for the proposed framework that captures the benefits of the data
looping procedure.
\\ ( https://arxiv.org/abs/2601.15417 ,  12323kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15423
Date: Wed, 21 Jan 2026 19:37:57 GMT   (16kb)

Title: Lattice: A Confidence-Gated Hybrid System for Uncertainty-Aware
  Sequential Prediction with Behavioral Archetypes
Authors: Lorian Bannis
Categories: cs.LG
Comments: 12 pages, 1 figure, uses tikz.sty
\\
  We introduce Lattice, a hybrid sequential prediction system that
conditionally activates learned behavioral structure using binary confidence
gating. The system clusters behavior windows into behavioral archetypes and
uses binary confidence gating to activate archetype-based scoring only when
confidence exceeds a threshold, falling back to baseline predictions when
uncertain. We validate Lattice on recommendation systems (MovieLens),
scientific time-series (LIGO), and financial markets, using LSTM and
transformer backbones. On MovieLens with LSTM, Lattice achieves +31.9%
improvement over LSTM baseline in HR@10 (p < 3.29 x 10^-25, 30 seeds),
outperforming transformer baselines by 109.4% over SASRec and 218.6% over
BERT4Rec. On LIGO and financial data, the system correctly refuses archetype
activation when distribution shift occurs - a successful outcome demonstrating
confidence gating prevents false activation. On transformer backbones, Lattice
provides 0.0% improvement (neutral, no degradation), gracefully deferring when
structure is already present. This bidirectional validation - activating when
patterns apply, refusing when they don't, and deferring when redundant -
supports confidence gating as a promising architectural principle for managing
epistemic uncertainty in safety-critical applications.
\\ ( https://arxiv.org/abs/2601.15423 ,  16kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15441
Date: Wed, 21 Jan 2026 20:14:17 GMT   (5095kb)

Title: CASL: Concept-Aligned Sparse Latents for Interpreting Diffusion Models
Authors: Zhenghao He, Guangzhi Xiong, Boyang Wang, Sanchit Sinha, Aidong Zhang
Categories: cs.LG cs.CV
\\
  Internal activations of diffusion models encode rich semantic information,
but interpreting such representations remains challenging. While Sparse
Autoencoders (SAEs) have shown promise in disentangling latent representations,
existing SAE-based methods for diffusion model understanding rely on
unsupervised approaches that fail to align sparse features with
human-understandable concepts. This limits their ability to provide reliable
semantic control over generated images. We introduce CASL (Concept-Aligned
Sparse Latents), a supervised framework that aligns sparse latent dimensions of
diffusion models with semantic concepts. CASL first trains an SAE on frozen
U-Net activations to obtain disentangled latent representations, and then
learns a lightweight linear mapping that associates each concept with a small
set of relevant latent dimensions. To validate the semantic meaning of these
aligned directions, we propose CASL-Steer, a controlled latent intervention
that shifts activations along the learned concept axis. Unlike editing methods,
CASL-Steer is used solely as a causal probe to reveal how concept-aligned
latents influence generated content. We further introduce the Editing Precision
Ratio (EPR), a metric that jointly measures concept specificity and the
preservation of unrelated attributes. Experiments show that our method achieves
superior editing precision and interpretability compared to existing
approaches. To the best of our knowledge, this is the first work to achieve
supervised alignment between latent representations and semantic concepts in
diffusion models.
\\ ( https://arxiv.org/abs/2601.15441 ,  5095kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15468
Date: Wed, 21 Jan 2026 21:07:16 GMT   (23kb)

Title: Learning from Synthetic Data: Limitations of ERM
Authors: Kareem Amin, Alex Bie, Weiwei Kong, Umar Syed, Sergei Vassilvitskii
Categories: cs.LG cs.DS stat.ML
MSC-class: 68T05
ACM-class: I.2.6
\\
  The prevalence and low cost of LLMs have led to a rise of synthetic content.
From review sites to court documents, ``natural'' content has been contaminated
by data points that appear similar to natural data, but are in fact
LLM-generated. In this work we revisit fundamental learning theory questions in
this, now ubiquitous, setting. We model this scenario as a sequence of learning
tasks where the input is a mix of natural and synthetic data, and the learning
algorithms are oblivious to the origin of any individual example.
  We study the possibilities and limitations of ERM in this setting. For the
problem of estimating the mean of an arbitrary $d$-dimensional distribution, we
find that while ERM converges to the true mean, it is outperformed by an
algorithm that assigns non-uniform weights to examples from different
generations of data. For the PAC learning setting, the disparity is even more
stark. We find that ERM does not always converge to the true concept, echoing
the model collapse literature. However, we show there are algorithms capable of
learning the correct hypothesis for arbitrary VC classes and arbitrary amounts
of contamination.
\\ ( https://arxiv.org/abs/2601.15468 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15473
Date: Wed, 21 Jan 2026 21:23:00 GMT   (453kb)

Title: Panther: Faster and Cheaper Computations with Randomized Numerical
  Linear Algebra
Authors: Fahd Seddik, Abdulrahman Elbedewy, Gaser Sami, Mohamed Abdelmoniem,
  Yahia Zakaria
Categories: cs.LG cs.AI
Comments: 5 pages, 3 figures, 2 listings
\\
  Training modern deep learning models is increasingly constrained by GPU
memory and compute limits. While Randomized Numerical Linear Algebra (RandNLA)
offers proven techniques to compress these models, the lack of a unified,
production-grade library prevents widely adopting these methods. We present
Panther, a PyTorch-compatible library that consolidates established RandNLA
algorithms into a single high-performance framework. Panther engineers
efficient, drop-in replacements for standard components including sketched
linear layers, 2D convolution, multi-head attention, and randomized matrix
decompositions (such as pivoted CholeskyQR). By implementing a custom C++/CUDA
backend (pawX), Panther provides an optimized implementation that can run on
both CPUs and GPUs. We demonstrate the effectiveness of RandNLA techniques and
Panther's ease of adoption. By replacing standard PyTorch linear layers with
Panther layers (requiring only a few lines of code) we achieve significant
memory savings (up to 75%) on BERT while maintaining comparable loss. Source
code is available (MIT License) at https://github.com/FahdSeddik/panther, along
with demonstration video at https://youtu.be/7M3RQb4KWxs.
\\ ( https://arxiv.org/abs/2601.15473 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15474
Date: Wed, 21 Jan 2026 21:23:51 GMT   (474kb)

Title: Multi-Targeted Graph Backdoor Attack
Authors: Md Nabi Newaz Khan, Abdullah Arafat Miah, Yu Bi
Categories: cs.LG cs.AI cs.CR
\\
  Graph neural network (GNN) have demonstrated exceptional performance in
solving critical problems across diverse domains yet remain susceptible to
backdoor attacks. Existing studies on backdoor attack for graph classification
are limited to single target attack using subgraph replacement based mechanism
where the attacker implants only one trigger into the GNN model. In this paper,
we introduce the first multi-targeted backdoor attack for graph classification
task, where multiple triggers simultaneously redirect predictions to different
target labels. Instead of subgraph replacement, we propose subgraph injection
which preserves the structure of the original graphs while poisoning the clean
graphs. Extensive experiments demonstrate the efficacy of our approach, where
our attack achieves high attack success rates for all target labels with
minimal impact on the clean accuracy. Experimental results on five dataset
demonstrate the superior performance of our attack framework compared to the
conventional subgraph replacement-based attack. Our analysis on four GNN models
confirms the generalization capability of our attack which is effective
regardless of the GNN model architectures and training parameters settings. We
further investigate the impact of the attack design parameters including
injection methods, number of connections, trigger sizes, trigger edge density
and poisoning ratios. Additionally, our evaluation against state-of-the-art
defenses (randomized smoothing and fine-pruning) demonstrates the robustness of
our proposed multi-target attacks. This work highlights the GNN vulnerability
against multi-targeted backdoor attack in graph classification task. Our source
codes will be available at
https://github.com/SiSL-URI/Multi-Targeted-Graph-Backdoor-Attack.
\\ ( https://arxiv.org/abs/2601.15474 ,  474kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15481
Date: Wed, 21 Jan 2026 21:31:16 GMT   (1276kb)

Title: Early predicting of hospital admission using machine learning
  algorithms: Priority queues approach
Authors: Jakub Antczak, James Montgomery, Ma{\l}gorzata O'Reilly, Zbigniew
  Palmowski, Richard Turner
Categories: cs.LG math.OC
\\
  Emergency Department overcrowding is a critical issue that compromises
patient safety and operational efficiency, necessitating accurate demand
forecasting for effective resource allocation. This study evaluates and
compares three distinct predictive models: Seasonal AutoRegressive Integrated
Moving Average with eXogenous regressors (SARIMAX), EXtreme Gradient Boosting
(XGBoost) and Long Short-Term Memory (LSTM) networks for forecasting daily ED
arrivals over a seven-day horizon. Utilizing data from an Australian tertiary
referral hospital spanning January 2017 to December 2021, this research
distinguishes itself by decomposing demand into eight specific ward categories
and stratifying patients by clinical complexity. To address data distortions
caused by the COVID-19 pandemic, the study employs the Prophet model to
generate synthetic counterfactual values for the anomalous period. Experimental
results demonstrate that all three proposed models consistently outperform a
seasonal naive baseline. XGBoost demonstrated the highest accuracy for
predicting total daily admissions with a Mean Absolute Error of 6.63, while the
statistical SARIMAX model proved marginally superior for forecasting major
complexity cases with an MAE of 3.77. The study concludes that while these
techniques successfully reproduce regular day-to-day patterns, they share a
common limitation in underestimating sudden, infrequent surges in patient
volume.
\\ ( https://arxiv.org/abs/2601.15481 ,  1276kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15482
Date: Wed, 21 Jan 2026 21:34:29 GMT   (351kb)

Title: Martingale Foresight Sampling: A Principled Approach to Inference-Time
  LLM Decoding
Authors: Huayu Li, ZhengXiao He, Siyuan Tian, Jinghao Wen, and Ao Li
Categories: cs.LG cs.AI
\\
  Standard autoregressive decoding in large language models (LLMs) is
inherently short-sighted, often failing to find globally optimal reasoning
paths due to its token-by-token generation process. While inference-time
strategies like foresight sampling attempt to mitigate this by simulating
future steps, they typically rely on ad-hoc heuristics for valuing paths and
pruning the search space. This paper introduces Martingale Foresight Sampling
(MFS), a principled framework that reformulates LLM decoding as a problem of
identifying an optimal stochastic process. By modeling the quality of a
reasoning path as a stochastic process, we leverage Martingale theory to design
a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms
with principles from probability theory: step valuation is derived from the
Doob Decomposition Theorem to measure a path's predictable advantage, path
selection uses Optional Stopping Theory for principled pruning of suboptimal
candidates, and an adaptive stopping rule based on the Martingale Convergence
Theorem terminates exploration once a path's quality has provably converged.
Experiments on six reasoning benchmarks demonstrate that MFS surpasses
state-of-the-art methods in accuracy while significantly improving
computational efficiency. Code will be released at
https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.
\\ ( https://arxiv.org/abs/2601.15482 ,  351kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15498
Date: Wed, 21 Jan 2026 22:03:06 GMT   (3973kb)

Title: MARS: Unleashing the Power of Speculative Decoding via Margin-Aware
  Verification
Authors: Jingwei Song, Xinyu Wang, Hanbin Wang, Xiaoxuan Lei, Bill Shi, Shixin
  Han, Eric Yang, Xiao-Wen Chang, Lynn Ai
Categories: cs.LG
Comments: 11 pages, 5 figures
\\
  Speculative Decoding (SD) accelerates autoregressive large language model
(LLM) inference by decoupling generation and verification. While recent methods
improve draft quality by tightly coupling the drafter with the target model,
the verification mechanism itself remains largely unchanged, relying on strict
token-level rejection sampling. In practice, modern LLMs frequently operate in
low-margin regimes where the target model exhibits weak preference among top
candidates. In such cases, rejecting plausible runner-up tokens yields
negligible information gain while incurring substantial rollback cost, leading
to a fundamental inefficiency in verification. We propose Margin-Aware
Speculative Verification, a training-free and domain-agnostic verification
strategy that adapts to the target model's local decisiveness. Our method
conditions verification on decision stability measured directly from the target
logits and relaxes rejection only when strict verification provides minimal
benefit. Importantly, the approach modifies only the verification rule and is
fully compatible with existing target-coupled speculative decoding frameworks.
Extensive experiments across model scales ranging from 8B to 235B demonstrate
that our method delivers consistent and significant inference speedups over
state-of-the-art baselines while preserving generation quality across diverse
benchmarks.
\\ ( https://arxiv.org/abs/2601.15498 ,  3973kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15503
Date: Wed, 21 Jan 2026 22:20:26 GMT   (407kb)

Title: Data-driven Lake Water Quality Forecasting for Time Series with Missing
  Data using Machine Learning
Authors: Rishit Chatterjee, Tahiya Chowdhury
Categories: cs.LG
Comments: 8 pages, 4 figures, 3 tables
ACM-class: J.2; I.6.3
\\
  Volunteer-led lake monitoring yields irregular, seasonal time series with
many gaps arising from ice cover, weather-related access constraints, and
occasional human errors, complicating forecasting and early warning of harmful
algal blooms. We study Secchi Disk Depth (SDD) forecasting on a 30-lake,
data-rich subset drawn from three decades of in situ records collected across
Maine lakes. Missingness is handled via Multiple Imputation by Chained
Equations (MICE), and we evaluate performance with a normalized Mean Absolute
Error (nMAE) metric for cross-lake comparability. Among six candidates, ridge
regression provides the best mean test performance. Using ridge regression, we
then quantify the minimal sample size, showing that under a backward,
recent-history protocol, the model reaches within 5% of full-history accuracy
with approximately 176 training samples per lake on average. We also identify a
minimal feature set, where a compact four-feature subset matches the
thirteen-feature baseline within the same 5% tolerance. Bringing these results
together, we introduce a joint feasibility function that identifies the minimal
training history and fewest predictors sufficient to achieve the target of
staying within 5% of the complete-history, full-feature baseline. In our study,
meeting the 5% accuracy target required about 64 recent samples and just one
predictor per lake, highlighting the practicality of targeted monitoring.
Hence, our joint feasibility strategy unifies recent-history length and feature
choice under a fixed accuracy target, yielding a simple, efficient rule for
setting sampling effort and measurement priorities for lake researchers.
\\ ( https://arxiv.org/abs/2601.15503 ,  407kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15504
Date: Wed, 21 Jan 2026 22:22:38 GMT   (2879kb)

Title: SAGE-FM: A lightweight and interpretable spatial transcriptomics
  foundation model
Authors: Xianghao Zhan, Jingyu Xu, Yuanning Zheng, Zinaida Good, Olivier
  Gevaert
Categories: cs.LG q-bio.GN q-bio.QM
Comments: 26 pages, 5 figures
\\
  Spatial transcriptomics enables spatial gene expression profiling, motivating
computational models that capture spatially conditioned regulatory
relationships. We introduce SAGE-FM, a lightweight spatial transcriptomics
foundation model based on graph convolutional networks (GCNs) trained with a
masked central spot prediction objective. Trained on 416 human Visium samples
spanning 15 organs, SAGE-FM learns spatially coherent embeddings that robustly
recover masked genes, with 91% of masked genes showing significant correlations
(p < 0.05). The embeddings generated by SAGE-FM outperform MOFA and existing
spatial transcriptomics methods in unsupervised clustering and preservation of
biological heterogeneity. SAGE-FM generalizes to downstream tasks, enabling 81%
accuracy in pathologist-defined spot annotation in oropharyngeal squamous cell
carcinoma and improving glioblastoma subtype prediction relative to MOFA. In
silico perturbation experiments further demonstrate that the model captures
directional ligand-receptor and upstream-downstream regulatory effects
consistent with ground truth. These results demonstrate that simple,
parameter-efficient GCNs can serve as biologically interpretable and spatially
aware foundation models for large-scale spatial transcriptomics.
\\ ( https://arxiv.org/abs/2601.15504 ,  2879kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15530
Date: Wed, 21 Jan 2026 23:32:17 GMT   (6512kb)

Title: Machine learning-enhanced non-amnestic Alzheimer's disease diagnosis
  from MRI and clinical features
Authors: Megan A. Witherow, Michael L. Evans, Ahmed Temtam, Hamid Okhravi, Khan
  M. Iftekharuddin
Categories: cs.LG q-bio.NC q-bio.QM
Comments: Preprint of a manuscript submitted to Brain
\\
  Alzheimer's disease (AD), defined as an abnormal buildup of amyloid plaques
and tau tangles in the brain can be diagnosed with high accuracy based on
protein biomarkers via PET or CSF analysis. However, due to the invasive nature
of biomarker collection, most AD diagnoses are made in memory clinics using
cognitive tests and evaluation of hippocampal atrophy based on MRI. While
clinical assessment and hippocampal volume show high diagnostic accuracy for
amnestic or typical AD (tAD), a substantial subgroup of AD patients with
atypical presentation (atAD) are routinely misdiagnosed. To improve diagnosis
of atAD patients, we propose a machine learning approach to distinguish between
atAD and non-AD cognitive impairment using clinical testing battery and MRI
data collected as standard-of-care. We develop and evaluate our approach using
1410 subjects across four groups (273 tAD, 184 atAD, 235 non-AD, and 685
cognitively normal) collected from one private data set and two public data
sets from the National Alzheimer's Coordinating Center (NACC) and the
Alzheimer's Disease Neuroimaging Initiative (ADNI). We perform multiple atAD
vs. non-AD classification experiments using clinical features and hippocampal
volume as well as a comprehensive set of MRI features from across the brain.
The best performance is achieved by incorporating additional important MRI
features, which outperforms using hippocampal volume alone. Furthermore, we use
the Boruta statistical approach to identify and visualize significant brain
regions distinguishing between diagnostic groups. Our ML approach improves the
percentage of correctly diagnosed atAD cases (the recall) from 52% to 69% for
NACC and from 34% to 77% for ADNI, while achieving high precision. The proposed
approach has important implications for improving diagnostic accuracy for
non-amnestic atAD in clinical settings using only clinical testing battery and
MRI.
\\ ( https://arxiv.org/abs/2601.15530 ,  6512kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15538
Date: Wed, 21 Jan 2026 23:48:38 GMT   (165kb)

Title: QUAIL: Quantization Aware Unlearning for Mitigating Misinformation in
  LLMs
Authors: Himanshu Mishra and Kanwal Mehreen
Categories: cs.LG cs.AI
\\
  Machine unlearning aims to remove specific knowledge (e.g., copyrighted or
private data) from a trained model without full retraining. In practice, models
are often quantized (e.g., 4-bit) for deployment, but we find that quantization
can catastrophically restore forgotten information [1]. In this paper, we (1)
analyze why low-bit quantization undermines unlearning, and (2) propose a
quantization-aware unlearning method to mitigate this. We first compute
weight-change statistics and bucket overlaps in quantization to show that
typical unlearning updates are too small to cross quantization thresholds.
Building on this insight, we introduce a logits space hinge loss: for each
forget example, we force the output logits of the unlearned model to differ
from the original model by at least a margin (half the quantization step). This
ensures forgotten examples remain distinguishable even after quantization. We
evaluate on language and classification tasks (including a Twitter
misinformation dataset) and show our method preserves forgetting under 4-bit
quantization, whereas existing methods almost entirely recover the forgotten
knowledge.
\\ ( https://arxiv.org/abs/2601.15538 ,  165kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15540
Date: Wed, 21 Jan 2026 23:52:36 GMT   (499kb)

Title: PRISM: Deriving the Transformer as a Signal-Denoising Operator via
  Maximum Coding Rate Reduction
Authors: Dongchen Huang
Categories: cs.LG cs.AI cs.CL physics.data-an
\\
  Deep learning models, particularly Transformers, are often criticized as
"black boxes" and lack interpretability. We propose Prism, a white-box
attention-based architecture derived from the principles of Maximizing Coding
Rate Reduction ($\text{MCR}^2$). By modeling the attention mechanism as a
gradient ascent process on a distinct signal-noise manifold, we introduce two
physical constraints: an overcomplete dictionary to expand the representational
phase space, and an irrational frequency separation ($\pi$-RoPE) to enforce
incoherence between signal and noise subspaces. We demonstrate that these
geometric inductive biases can be viewed as a physical constraint and they are
sufficient to induce unsupervised functional disentanglement alone. Using
TinyStories as a controlled testbed for verifying spectral dynamics, we observe
that Prism spontaneously specializes its attention heads into spectrally
distinct regimes: low-frequency heads capturing long-range causal dependencies
(signal) and high-frequency heads handling local syntactic constraints (noise).
Our results suggest that interpretability and performance are not a trade-off,
but can be unified through principled geometric construction.
\\ ( https://arxiv.org/abs/2601.15540 ,  499kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15544
Date: Thu, 22 Jan 2026 00:20:23 GMT   (9kb)

Title: RDumb++: Drift-Aware Continual Test-Time Adaptation
Authors: Himanshu Mishra
Categories: cs.LG cs.AI
\\
  Continual Test-Time Adaptation (CTTA) seeks to update a pretrained model
during deployment using only the incoming, unlabeled data stream. Although
prior approaches such as Tent, EATA etc. provide meaningful improvements under
short evolving shifts, they struggle when the test distribution changes rapidly
or over extremely long horizons. This challenge is exemplified by the CCC
benchmark, where models operate over streams of 7.5M samples with continually
changing corruption types and severities. We propose RDumb++, a principled
extension of RDumb that introduces two drift-detection mechanisms i.e
entropy-based drift scoring and KL-divergence drift scoring, together with
adaptive reset strategies. These mechanisms allow the model to detect when
accumulated adaptation becomes harmful and to recover before prediction
collapse occurs. Across CCC-medium with three speeds and three seeds (nine
runs, each containing one million samples), RDumb++ consistently surpasses
RDumb, yielding approx 3% absolute accuracy gains while maintaining stable
adaptation throughout the entire stream. Ablation experiments on drift
thresholds and reset strengths further show that drift-aware resetting is
essential for preventing collapse and achieving reliable long-horizon CTTA.
\\ ( https://arxiv.org/abs/2601.15544 ,  9kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15546
Date: Thu, 22 Jan 2026 00:30:23 GMT   (1728kb)

Title: Beyond validation loss: Clinically-tailored optimization metrics improve
  a model's clinical performance
Authors: Charles B. Delahunt, Courosh Mehanian, Daniel E. Shea, Matthew P.
  Horning
Categories: cs.LG
Comments: 16 pages, 9 figures
MSC-class: 68
ACM-class: I.2.0
\\
  A key task in ML is to optimize models at various stages, e.g. by choosing
hyperparameters or picking a stopping point. A traditional ML approach is to
use validation loss, i.e. to apply the training loss function on a validation
set to guide these optimizations. However, ML for healthcare has a distinct
goal from traditional ML: Models must perform well relative to specific
clinical requirements, vs. relative to the loss function used for training.
These clinical requirements can be captured more precisely by tailored metrics.
Since many optimization tasks do not require the driving metric to be
differentiable, they allow a wider range of options, including the use of
metrics tailored to be clinically-relevant. In this paper we describe two
controlled experiments which show how the use of clinically-tailored metrics
provide superior model optimization compared to validation loss, in the sense
of better performance on the clinical task. The use of clinically-relevant
metrics for optimization entails some extra effort, to define the metrics and
to code them into the pipeline. But it can yield models that better meet the
central goal of ML for healthcare: strong performance in the clinic.
\\ ( https://arxiv.org/abs/2601.15546 ,  1728kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15547
Date: Thu, 22 Jan 2026 00:33:38 GMT   (1204kb)

Title: Learning Neural Operators from Partial Observations via Latent
  Autoregressive Modeling
Authors: Jingren Hou, Hong Wang, Pengyu Xu, Chang Gao, Huafeng Liu, Liping Jing
Categories: cs.LG cs.AI
\\
  Real-world scientific applications frequently encounter incomplete
observational data due to sensor limitations, geographic constraints, or
measurement costs. Although neural operators significantly advanced PDE solving
in terms of computational efficiency and accuracy, their underlying assumption
of fully-observed spatial inputs severely restricts applicability in real-world
applications. We introduce the first systematic framework for learning neural
operators from partial observation. We identify and formalize two fundamental
obstacles: (i) the supervision gap in unobserved regions that prevents
effective learning of physical correlations, and (ii) the dynamic spatial
mismatch between incomplete inputs and complete solution fields. Specifically,
our proposed Latent Autoregressive Neural Operator~(\ours) introduces two novel
components designed explicitly to address the core difficulties of partial
observations: (i) a mask-to-predict training strategy that creates artificial
supervision by strategically masking observed regions, and (ii) a Physics-Aware
Latent Propagator that reconstructs solutions through boundary-first
autoregressive generation in latent space. Additionally, we develop
POBench-PDE, a dedicated and comprehensive benchmark designed specifically for
evaluating neural operators under partial observation conditions across three
PDE-governed tasks. \ours achieves state-of-the-art performance with 18--69$\%$
relative L2 error reduction across all benchmarks under patch-wise missingness
with less than 50$\%$ missing rate, including real-world climate prediction.
Our approach effectively addresses practical scenarios involving up to 75$\%$
missing rate, to some extent bridging the existing gap between idealized
research settings and the complexities of real-world scientific computing.
\\ ( https://arxiv.org/abs/2601.15547 ,  1204kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15552
Date: Thu, 22 Jan 2026 00:48:45 GMT   (4236kb)

Title: BanditLP: Large-Scale Stochastic Optimization for Personalized
  Recommendations
Authors: Phuc Nguyen, Benjamin Zelditch, Joyce Chen, Rohit Patra, Changshuai
  Wei
Categories: cs.LG cs.AI stat.ML
\\
  We present BanditLP, a scalable multi-stakeholder contextual bandit framework
that unifies neural Thompson Sampling for learning objective-specific outcomes
with a large-scale linear program for constrained action selection at serving
time. The methodology is application-agnostic, compatible with arbitrary neural
architectures, and deployable at web scale, with an LP solver capable of
handling billions of variables. Experiments on public benchmarks and synthetic
data show consistent gains over strong baselines. We apply this approach in
LinkedIn's email marketing system and demonstrate business win, illustrating
the value of integrated exploration and constrained optimization in production.
\\ ( https://arxiv.org/abs/2601.15552 ,  4236kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15589
Date: Thu, 22 Jan 2026 02:26:32 GMT   (2773kb)

Title: Deep Learning for Perishable Inventory Systems with Human Knowledge
Authors: Xuan Liao, Zhenkang Peng, Ying Rong
Categories: cs.LG
\\
  Managing perishable products with limited lifetimes is a fundamental
challenge in inventory management, as poor ordering decisions can quickly lead
to stockouts or excessive waste. We study a perishable inventory system with
random lead times in which both the demand process and the lead time
distribution are unknown. We consider a practical setting where orders are
placed using limited historical data together with observed covariates and
current system states. To improve learning efficiency under limited data, we
adopt a marginal cost accounting scheme that assigns each order a single
lifetime cost and yields a unified loss function for end-to-end learning. This
enables training a deep learning-based policy that maps observed covariates and
system states directly to order quantities. We develop two end-to-end variants:
a purely black-box approach that outputs order quantities directly (E2E-BB),
and a structure-guided approach that embeds the projected inventory level (PIL)
policy, capturing inventory effects through explicit computation rather than
additional learning (E2E-PIL). We further show that the objective induced by
E2E-PIL is homogeneous of degree one, enabling a boosting technique from
operational data analytics (ODA) that yields an enhanced policy (E2E-BPIL).
Experiments on synthetic and real data establish a robust performance ordering:
E2E-BB is dominated by E2E-PIL, which is further improved by E2E-BPIL. Using an
excess-risk decomposition, we show that embedding heuristic policy structure
reduces effective model complexity and improves learning efficiency with only a
modest loss of flexibility. More broadly, our results suggest that deep
learning-based decision tools are more effective and robust when guided by
human knowledge, highlighting the value of integrating advanced analytics with
inventory theory.
\\ ( https://arxiv.org/abs/2601.15589 ,  2773kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15597
Date: Thu, 22 Jan 2026 02:44:33 GMT   (2253kb)

Title: Neural Nonlinear Shrinkage of Covariance Matrices for Minimum Variance
  Portfolio Optimization
Authors: Liusha Yang, Siqi Zhao, Shuqi Chai
Categories: cs.LG eess.SP
\\
  This paper introduces a neural network-based nonlinear shrinkage estimator of
covariance matrices for the purpose of minimum variance portfolio optimization.
It is a hybrid approach that integrates statistical estimation with machine
learning. Starting from the Ledoit-Wolf (LW) shrinkage estimator, we decompose
the LW covariance matrix into its eigenvalues and eigenvectors, and apply a
lightweight transformer-based neural network to learn a nonlinear eigenvalue
shrinkage function. Trained with portfolio risk as the loss function, the
resulting precision matrix (the inverse covariance matrix) estimator directly
targets portfolio risk minimization. By conditioning on the sample-to-dimension
ratio, the approach remains scalable across different sample sizes and asset
universes. Empirical results on stock daily returns from Standard & Poor's 500
Index (S&P500) demonstrate that the proposed method consistently achieves lower
out-of-sample realized risk than benchmark approaches. This highlights the
promise of integrating structural statistical models with data-driven learning.
\\ ( https://arxiv.org/abs/2601.15597 ,  2253kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15609
Date: Thu, 22 Jan 2026 03:15:57 GMT   (3903kb)

Title: When Sharpening Becomes Collapse: Sampling Bias and Semantic Coupling in
  RL with Verifiable Rewards
Authors: Mingyuan Fan, Weiguang Han, Daixin Wang, Cen Chen, Zhiqiang Zhang, Jun
  Zhou
Categories: cs.LG cs.CL
\\
  Reinforcement Learning with Verifiable Rewards (RLVR) is a central paradigm
for turning large language models (LLMs) into reliable problem solvers,
especially in logic-heavy domains. Despite its empirical success, it remains
unclear whether RLVR elicits novel capabilities or merely sharpens the
distribution over existing knowledge. We study this by formalizing
over-sharpening, a phenomenon where the policy collapses onto limited modes,
suppressing valid alternatives. At a high level, we discover finite-batch
updates intrinsically bias learning toward sampled modes, triggering a collapse
that propagates globally via semantic coupling. To mitigate this, we propose
inverse-success advantage calibration to prioritize difficult queries and
distribution-level calibration to diversify sampling via a memory network.
Empirical evaluations validate that our strategies can effectively improve
generalization.
\\ ( https://arxiv.org/abs/2601.15609 ,  3903kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15620
Date: Thu, 22 Jan 2026 03:50:31 GMT   (113kb)

Title: Closing the Gap on the Sample Complexity of 1-Identification
Authors: Zitian Li, Wang Chi Cheung
Categories: cs.LG
\\
  1-identification is a fundamental multi-armed bandit formulation on pure
exploration. An agent aims to determine whether there exists a qualified arm
whose mean reward is not less than a known threshold $\mu_0$, or to output
\textsf{None} if it believes such an arm does not exist. The agent needs to
guarantee its output is correct with probability at least $1-\delta$, while
making expected total pulling times $\mathbb{E}\tau$ as small as possible. We
work on 1-identification with two main contributions. (1) We utilize an
optimization formulation to derive a new lower bound of $\mathbb{E}\tau$, when
there is at least one qualified arm. (2) We design a new algorithm, deriving
tight upper bounds whose gap to lower bounds are up to a polynomial of
logarithm factor across all problem instance. Our result complements the
analysis of $\mathbb{E}\tau$ when there are multiple qualified arms, which is
an open problem left by history literature.
\\ ( https://arxiv.org/abs/2601.15620 ,  113kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15625
Date: Thu, 22 Jan 2026 03:57:35 GMT   (2310kb)

Title: Robust Tool Use via Fission-GRPO: Learning to Recover from Execution
  Errors
Authors: Zhiwei Zhang, Fei Zhao, Rui Wang, Zezhong Wang, Bin Liang, Jiakang
  Wang, Yao Hu, Shaosheng Cao, Kam-Fai Wong
Categories: cs.LG cs.AI
Comments: 8 pages, 4 figures, 2 tables
ACM-class: I.2.7
\\
  Large language models (LLMs) can call tools effectively, yet they remain
brittle in multi-turn execution: following a tool call error, smaller models
often degenerate into repetitive invalid re-invocations, failing to interpret
error feedback and self-correct. This brittleness hinders reliable real-world
deployment, where the execution errors are inherently inevitable during tool
interaction procedures. We identify a key limitation of current approaches:
standard reinforcement learning (RL) treats errors as sparse negative rewards,
providing no guidance on how to recover, while pre-collected synthetic
error-correction datasets suffer from distribution mismatch with the model's
on-policy error modes. To bridge this gap, we propose Fission-GRPO, a framework
that converts execution errors into corrective supervision within the RL
training loop. Our core mechanism fissions each failed trajectory into a new
training instance by augmenting it with diagnostic feedback from a finetuned
Error Simulator, then resampling recovery rollouts on-policy. This enables the
model to learn from the precise errors it makes during exploration, rather than
from static, pre-collected error cases. On the BFCL v4 Multi-Turn, Fission-GRPO
improves the error recovery rate of Qwen3-8B by 5.7% absolute, crucially,
yielding a 4% overall accuracy gain (42.75% to 46.75%) over GRPO and
outperforming specialized tool-use agents.
\\ ( https://arxiv.org/abs/2601.15625 ,  2310kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15640
Date: Thu, 22 Jan 2026 04:41:26 GMT   (4580kb)

Title: An Empirical Study on Ensemble-Based Transfer Learning Bayesian
  Optimisation with Mixed Variable Types
Authors: Natasha Trinkle and Huong Ha and Jeffrey Chan
Categories: cs.LG stat.ML
Comments: 36 pages, 16 figures
\\
  Bayesian optimisation is a sample efficient method for finding a global
optimum of expensive black-box objective functions. Historic datasets from
related problems can be exploited to help improve performance of Bayesian
optimisation by adapting transfer learning methods to various components of the
Bayesian optimisation pipeline. In this study we perform an empirical analysis
of various ensemble-based transfer learning Bayesian optimisation methods and
pipeline components. We expand on previous work in the literature by
contributing some specific pipeline components, and three new real-time
transfer learning Bayesian optimisation benchmarks. In particular we propose to
use a weighting strategy for ensemble surrogate model predictions based on
regularised regression with weights constrained to be positive, and a related
component for handling the case when transfer learning is not improving
Bayesian optimisation performance. We find that in general, two components that
help improve transfer learning Bayesian optimisation performance are warm start
initialisation and constraining weights used with ensemble surrogate model to
be positive.
\\ ( https://arxiv.org/abs/2601.15640 ,  4580kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15657
Date: Thu, 22 Jan 2026 05:13:12 GMT   (1240kb)

Title: Integrating Knowledge Distillation Methods: A Sequential Multi-Stage
  Framework
Authors: Yinxi Tian, Changwu Huang, Ke Tang, and Xin Yao
Categories: cs.LG cs.AI
\\
  Knowledge distillation (KD) transfers knowledge from large teacher models to
compact student models, enabling efficient deployment on resource constrained
devices. While diverse KD methods, including response based, feature based, and
relation based approaches, capture different aspects of teacher knowledge,
integrating multiple methods or knowledge sources is promising but often
hampered by complex implementation, inflexible combinations, and catastrophic
forgetting, which limits practical effectiveness.
  This work proposes SMSKD (Sequential Multi Stage Knowledge Distillation), a
flexible framework that sequentially integrates heterogeneous KD methods. At
each stage, the student is trained with a specific distillation method, while a
frozen reference model from the previous stage anchors learned knowledge to
mitigate forgetting. In addition, we introduce an adaptive weighting mechanism
based on the teacher true class probability (TCP) that dynamically adjusts the
reference loss per sample to balance knowledge retention and integration.
  By design, SMSKD supports arbitrary method combinations and stage counts with
negligible computational overhead. Extensive experiments show that SMSKD
consistently improves student accuracy across diverse teacher student
architectures and method combinations, outperforming existing baselines.
Ablation studies confirm that stage wise distillation and reference model
supervision are primary contributors to performance gains, with TCP based
adaptive weighting providing complementary benefits. Overall, SMSKD is a
practical and resource efficient solution for integrating heterogeneous KD
methods.
\\ ( https://arxiv.org/abs/2601.15657 ,  1240kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15669
Date: Thu, 22 Jan 2026 05:51:56 GMT   (2624kb)

Title: Dualformer: Time-Frequency Dual Domain Learning for Long-term Time
  Series Forecasting
Authors: Jingjing Bai, Yoshinobu Kawahara
Categories: cs.LG
\\
  Transformer-based models, despite their promise for long-term time series
forecasting (LTSF), suffer from an inherent low-pass filtering effect that
limits their effectiveness. This issue arises due to undifferentiated
propagation of frequency components across layers, causing a progressive
attenuation of high-frequency information crucial for capturing fine-grained
temporal variations. To address this limitation, we propose Dualformer, a
principled dual-domain framework that rethinks frequency modeling from a
layer-wise perspective. Dualformer introduces three key components: (1) a
dual-branch architecture that concurrently models complementary temporal
patterns in both time and frequency domains; (2) a hierarchical frequency
sampling module that allocates distinct frequency bands to different layers,
preserving high-frequency details in lower layers while modeling low-frequency
trends in deeper layers; and (3) a periodicity-aware weighting mechanism that
dynamically balances contributions from the dual branches based on the harmonic
energy ratio of inputs, supported theoretically by a derived lower bound. This
design enables structured frequency modeling and adaptive integration of
time-frequency features, effectively preserving high-frequency information and
enhancing generalization. Extensive experiments conducted on eight widely used
benchmarks demonstrate Dualformer's robustness and superior performance,
particularly on heterogeneous or weakly periodic data. Our code is publicly
available at https://github.com/Akira-221/Dualformer.
\\ ( https://arxiv.org/abs/2601.15669 ,  2624kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15686
Date: Thu, 22 Jan 2026 06:11:44 GMT   (674kb)

Title: Beyond Hard Writes and Rigid Preservation: Soft Recursive Least-Squares
  for Lifelong LLM Editing
Authors: Xinyu Wang, Sicheng Lyu, Yu Gu, Jerry Huang, Peng Lu, Yufei Cui,
  Xiao-Wen Chang
Categories: cs.LG
\\
  Model editing updates a pre-trained LLM with new facts or rules without
re-training, while preserving unrelated behavior. In real deployment, edits
arrive as long streams, and existing editors often face a plasticity-stability
dilemma: locate-then-edit "hard writes" can accumulate interference over time,
while null-space-style "hard preservation" preserves only what is explicitly
constrained, so past edits can be overwritten and unconstrained behaviors may
deviate, degrading general capabilities in the many-edits regime. We propose
RLSEdit, a recursive least-squares editor for long sequential editing. RLSEdit
formulates editing as an online quadratic optimization with soft constraints,
minimizing a cumulative key-value fitting objective with two regularizers that
control for both deviation from the pre-trained weights and from a designated
anchor mapping. The resulting update admits an efficient online recursion via
the Woodbury identity, with per-edit cost independent of history length and
scaling only with the current edit size. We further provide deviation bounds
and an asymptotic characterization of the adherence-preservation trade-off in
the many-edits regime. Experiments on multiple model families demonstrate
stable scaling to 10K edits, outperforming strong baselines in both edit
success and holistic stability -- crucially retaining early edits, and
preserving general capabilities on GLUE and held-out reasoning/code benchmarks.
\\ ( https://arxiv.org/abs/2601.15686 ,  674kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15714
Date: Thu, 22 Jan 2026 07:36:01 GMT   (115kb)

Title: Even GPT-5.2 Can't Count to Five: The Case for Zero-Error Horizons in
  Trustworthy LLMs
Authors: Ryoma Sato
Categories: cs.LG cs.AI cs.CL
\\
  We propose Zero-Error Horizon (ZEH) for trustworthy LLMs, which represents
the maximum range that a model can solve without any errors. While ZEH itself
is simple, we demonstrate that evaluating the ZEH of state-of-the-art LLMs
yields abundant insights. For example, by evaluating the ZEH of GPT-5.2, we
found that GPT-5.2 cannot even compute the parity of a short string like 11000,
and GPT-5.2 cannot determine whether the parentheses in ((((()))))) are
balanced. This is surprising given the excellent capabilities of GPT-5.2. The
fact that LLMs make mistakes on such simple problems serves as an important
lesson when applying LLMs to safety-critical domains. By applying ZEH to
Qwen2.5 and conducting detailed analysis, we found that while ZEH correlates
with accuracy, the detailed behaviors differ, and ZEH provides clues about the
emergence of algorithmic capabilities. Finally, while computing ZEH incurs
significant computational cost, we discuss how to mitigate this cost by
achieving up to one order of magnitude speedup using tree structures and online
softmax.
\\ ( https://arxiv.org/abs/2601.15714 ,  115kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15722
Date: Thu, 22 Jan 2026 07:46:47 GMT   (1520kb)

Title: Communication-efficient Federated Graph Classification via Generative
  Diffusion Modeling
Authors: Xiuling Wang, Xin Huang, Haibo Hu, Jianliang Xu
Categories: cs.LG
Journal-ref: In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and
  Data Mining (SIGKDD), 2026
\\
  Graph Neural Networks (GNNs) unlock new ways of learning from
graph-structured data, proving highly effective in capturing complex
relationships and patterns. Federated GNNs (FGNNs) have emerged as a prominent
distributed learning paradigm for training GNNs over decentralized data.
However, FGNNs face two significant challenges: high communication overhead
from multiple rounds of parameter exchanges and non-IID data characteristics
across clients. To address these issues, we introduce CeFGC, a novel FGNN
paradigm that facilitates efficient GNN training over non-IID data by limiting
communication between the server and clients to three rounds only. The core
idea of CeFGC is to leverage generative diffusion models to minimize direct
client-server communication. Each client trains a generative diffusion model
that captures its local graph distribution and shares this model with the
server, which then redistributes it back to all clients. Using these generative
models, clients generate synthetic graphs combined with their local graphs to
train local GNN models. Finally, clients upload their model weights to the
server for aggregation into a global GNN model. We theoretically analyze the
I/O complexity of communication volume to show that CeFGC reduces to a constant
of three communication rounds only. Extensive experiments on several real graph
datasets demonstrate the effectiveness and efficiency of CeFGC against
state-of-the-art competitors, reflecting our superior performance on non-IID
graphs by aligning local and global model objectives and enriching the training
set with diverse graphs.
\\ ( https://arxiv.org/abs/2601.15722 ,  1520kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15727
Date: Thu, 22 Jan 2026 07:53:52 GMT   (368kb)

Title: Towards Automated Kernel Generation in the Era of LLMs
Authors: Yang Yu, Peiyu Zang, Chi Hsu Tsai, Haiming Wu, Yixin Shen, Jialing
  Zhang, Haoyu Wang, Zhiyou Xiao, Jingze Shi, Yuyu Luo, Wentao Zhang, Chunlei
  Men, Guang Liu and Yonghua Lin
Categories: cs.LG cs.CL
Comments: 10 pages, 1 figure
\\
  The performance of modern AI systems is fundamentally constrained by the
quality of their underlying kernels, which translate high-level algorithmic
semantics into low-level hardware operations. Achieving near-optimal kernels
requires expert-level understanding of hardware architectures and programming
models, making kernel engineering a critical but notoriously time-consuming and
non-scalable process. Recent advances in large language models (LLMs) and
LLM-based agents have opened new possibilities for automating kernel generation
and optimization. LLMs are well-suited to compress expert-level kernel
knowledge that is difficult to formalize, while agentic systems further enable
scalable optimization by casting kernel development as an iterative,
feedback-driven loop. Rapid progress has been made in this area. However, the
field remains fragmented, lacking a systematic perspective for LLM-driven
kernel generation. This survey addresses this gap by providing a structured
overview of existing approaches, spanning LLM-based approaches and agentic
optimization workflows, and systematically compiling the datasets and
benchmarks that underpin learning and evaluation in this domain. Moreover, key
open challenges and future research directions are further outlined, aiming to
establish a comprehensive reference for the next generation of automated kernel
optimization. To keep track of this field, we maintain an open-source GitHub
repository at
https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.
\\ ( https://arxiv.org/abs/2601.15727 ,  368kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15771
Date: Thu, 22 Jan 2026 09:00:30 GMT   (3891kb)

Title: Rethinking Drug-Drug Interaction Modeling as Generalizable Relation
  Learning
Authors: Dong Xu, Jiantao Wu, Qihua Pan, Sisi Yuan, Zexuan Zhu, Junkai Ji
Categories: cs.LG q-bio.BM
Comments: 9 pages, 5 figures
\\
  Drug-drug interaction (DDI) prediction is central to drug discovery and
clinical development, particularly in the context of increasingly prevalent
polypharmacy. Although existing computational methods achieve strong
performance on standard benchmarks, they often fail to generalize to realistic
deployment scenarios, where most candidate drug pairs involve previously unseen
drugs and validated interactions are scarce. We demonstrate that proximity in
the embedding spaces of prevailing molecule-centric DDI models does not
reliably correspond to interaction labels, and that simply scaling up model
capacity therefore fails to improve generalization. To address these
limitations, we propose GenRel-DDI, a generalizable relation learning framework
that reformulates DDI prediction as a relation-centric learning problem, in
which interaction representations are learned independently of drug identities.
This relation-level abstraction enables the capture of transferable interaction
patterns that generalize to unseen drugs and novel drug pairs. Extensive
experiments across multiple benchmark demonstrate that GenRel-DDI consistently
and significantly outperforms state-of-the-art methods, with particularly large
gains on strict entity-disjoint evaluations, highlighting the effectiveness and
practical utility of relation learning for robust DDI prediction. The code is
available at https://github.com/SZU-ADDG/GenRel-DDI.
\\ ( https://arxiv.org/abs/2601.15771 ,  3891kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15773
Date: Thu, 22 Jan 2026 09:01:42 GMT   (8170kb)

Title: Next Generation Active Learning: Mixture of LLMs in the Loop
Authors: Yuanyuan Qi, Xiaohao Yang, Jueqing Lu, Guoxiang Guo, Joanne Enticott,
  Gang Liu, Lan Du
Categories: cs.LG
\\
  With the rapid advancement and strong generalization capabilities of large
language models (LLMs), they have been increasingly incorporated into the
active learning pipelines as annotators to reduce annotation costs. However,
considering the annotation quality, labels generated by LLMs often fall short
of real-world applicability. To address this, we propose a novel active
learning framework, Mixture of LLMs in the Loop Active Learning, replacing
human annotators with labels generated through a Mixture-of-LLMs-based
annotation model, aimed at enhancing LLM-based annotation robustness by
aggregating the strengths of multiple LLMs. To further mitigate the impact of
the noisy labels, we introduce annotation discrepancy and negative learning to
identify the unreliable annotations and enhance learning effectiveness.
Extensive experiments demonstrate that our framework achieves performance
comparable to human annotation and consistently outperforms single-LLM
baselines and other LLM-ensemble-based approaches. Moreover, our framework is
built on lightweight LLMs, enabling it to operate fully on local machines in
real-world applications.
\\ ( https://arxiv.org/abs/2601.15773 ,  8170kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15801
Date: Thu, 22 Jan 2026 09:32:43 GMT   (492kb)

Title: Attributing and Exploiting Safety Vectors through Global Optimization in
  Large Language Models
Authors: Fengheng Chu, Jiahao Chen, Yuhong Wang, Jun Wang, Zhihui Fu, Shouling
  Ji, Songze Li
Categories: cs.LG
\\
  While Large Language Models (LLMs) are aligned to mitigate risks, their
safety guardrails remain fragile against jailbreak attacks. This reveals
limited understanding of components governing safety. Existing methods rely on
local, greedy attribution that assumes independent component contributions.
However, they overlook the cooperative interactions between different
components in LLMs, such as attention heads, which jointly contribute to safety
mechanisms. We propose \textbf{G}lobal \textbf{O}ptimization for
\textbf{S}afety \textbf{V}ector Extraction (GOSV), a framework that identifies
safety-critical attention heads through global optimization over all heads
simultaneously. We employ two complementary activation repatching strategies:
Harmful Patching and Zero Ablation. These strategies identify two spatially
distinct sets of safety vectors with consistently low overlap, termed Malicious
Injection Vectors and Safety Suppression Vectors, demonstrating that aligned
LLMs maintain separate functional pathways for safety purposes. Through
systematic analyses, we find that complete safety breakdown occurs when
approximately 30\% of total heads are repatched across all models. Building on
these insights, we develop a novel inference-time white-box jailbreak method
that exploits the identified safety vectors through activation repatching. Our
attack substantially outperforms existing white-box attacks across all test
models, providing strong evidence for the effectiveness of the proposed GOSV
framework on LLM safety interpretability.
\\ ( https://arxiv.org/abs/2601.15801 ,  492kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15859
Date: Thu, 22 Jan 2026 11:07:19 GMT   (3500kb)

Title: Uncertainty-guided Generation of Dark-field Radiographs
Authors: Lina Felsner, Henriette Bast, Tina Dorosti, Florian Schaff, Franz
  Pfeiffer, Daniela Pfeiffer, Julia Schnabel
Categories: cs.LG cs.CV
\\
  X-ray dark-field radiography provides complementary diagnostic information to
conventional attenuation imaging by visualizing microstructural tissue changes
through small-angle scattering. However, the limited availability of such data
poses challenges for developing robust deep learning models. In this work, we
present the first framework for generating dark-field images directly from
standard attenuation chest X-rays using an Uncertainty-Guided Progressive
Generative Adversarial Network. The model incorporates both aleatoric and
epistemic uncertainty to improve interpretability and reliability. Experiments
demonstrate high structural fidelity of the generated images, with consistent
improvement of quantitative metrics across stages. Furthermore,
out-of-distribution evaluation confirms that the proposed model generalizes
well. Our results indicate that uncertainty-guided generative modeling enables
realistic dark-field image synthesis and provides a reliable foundation for
future clinical applications.
\\ ( https://arxiv.org/abs/2601.15859 ,  3500kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15871
Date: Thu, 22 Jan 2026 11:20:57 GMT   (1071kb)

Title: Why Inference in Large Models Becomes Decomposable After Training
Authors: Jidong Jin
Categories: cs.LG cs.AI
Comments: 10 pages, 6 figures
\\
  Inference in large-scale AI models is typically performed on dense parameter
matrices, leading to inference cost and system complexity that scale
unsustainably with model size. This limitation does not arise from insufficient
model capacity, but from treating post-training inference systems as monolithic
operators while ignoring internal structures formed during learning. We show
that gradient update events in large models are highly localized and selective,
leaving many parameter dependencies statistically indistinguishable from their
initialization distribution after training. As a result, post-training
inference systems are structurally non-uniform and inherently decomposable.
Based on this observation, we introduce a post-training statistical criterion
and a structural annealing procedure that removes unsupported dependencies and
reveals stable, independent substructures. This work establishes a
post-training, model-agnostic structural view of inference systems and enables
structured, parallel inference without modifying model functionality or
interfaces.
\\ ( https://arxiv.org/abs/2601.15871 ,  1071kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15874
Date: Thu, 22 Jan 2026 11:30:11 GMT   (296kb)

Title: SoK: Challenges in Tabular Membership Inference Attacks
Authors: Cristina P\^era, T\^ania Carvalho, Maxime Cordy, Lu\'is Antunes
Categories: cs.LG
Comments: This paper is currently under review for the EuroS&P conference
\\
  Membership Inference Attacks (MIAs) are currently a dominant approach for
evaluating privacy in machine learning applications. Despite their significance
in identifying records belonging to the training dataset, several concerns
remain unexplored, particularly with regard to tabular data. In this paper,
first, we provide an extensive review and analysis of MIAs considering two main
learning paradigms: centralized and federated learning. We extend and refine
the taxonomy for both. Second, we demonstrate the efficacy of MIAs in tabular
data using several attack strategies, also including defenses. Furthermore, in
a federated learning scenario, we consider the threat posed by an outsider
adversary, which is often neglected. Third, we demonstrate the high
vulnerability of single-outs (records with a unique signature) to MIAs. Lastly,
we explore how MIAs transfer across model architectures. Our results point
towards a general poor performance of these attacks in tabular data which
contrasts with previous state-of-the-art. Notably, even attacks with limited
attack performance can still successfully expose a large portion of
single-outs. Moreover, our findings suggest that using different surrogate
models makes MIAs more effective.
\\ ( https://arxiv.org/abs/2601.15874 ,  296kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15894
Date: Thu, 22 Jan 2026 12:18:50 GMT   (2069kb)

Title: Iterative Amortized Hierarchical VAE
Authors: Simon W. Penninga, Ruud J. G. van Sloun
Categories: cs.LG cs.AI
\\
  In this paper we propose the Iterative Amortized Hierarchical Variational
Autoencoder (IA-HVAE), which expands on amortized inference with a hybrid
scheme containing an initial amortized guess and iterative refinement with
decoder gradients. We achieve this by creating a linearly separable decoder in
a transform domain (e.g. Fourier space), enabling real-time applications with
very high model depths. The architectural change leads to a 35x speed-up for
iterative inference with respect to the traditional HVAE. We show that our
hybrid approach outperforms fully amortized and fully iterative equivalents in
accuracy and speed respectively. Moreover, the IAHVAE shows improved
reconstruction quality over a vanilla HVAE in inverse problems such as
deblurring and denoising.
\\ ( https://arxiv.org/abs/2601.15894 ,  2069kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15977
Date: Thu, 22 Jan 2026 13:56:26 GMT   (3101kb)

Title: Predicting Healthcare System Visitation Flow by Integrating Hospital
  Attributes and Population Socioeconomics with Human Mobility Data
Authors: Binbin Lin, Lei Zou, Hao Tian, Heng Cai, Yifan Yang, Bing Zhou
Categories: cs.LG cs.SI
\\
  Healthcare visitation patterns are influenced by a complex interplay of
hospital attributes, population socioeconomics, and spatial factors. However,
existing research often adopts a fragmented approach, examining these
determinants in isolation. This study addresses this gap by integrating
hospital capacities, occupancy rates, reputation, and popularity with
population SES and spatial mobility patterns to predict visitation flows and
analyze influencing factors. Utilizing four years of SafeGraph mobility data
and user experience data from Google Maps Reviews, five flow prediction models,
Naive Regression, Gradient Boosting, Multilayer Perceptrons (MLPs), Deep
Gravity, and Heterogeneous Graph Neural Networks (HGNN),were trained and
applied to simulate visitation flows in Houston, Texas, U.S. The Shapley
additive explanation (SHAP) analysis and the Partial Dependence Plot (PDP)
method were employed to examine the combined impacts of different factors on
visitation patterns. The findings reveal that Deep Gravity outperformed other
models. Hospital capacities, ICU occupancy rates, ratings, and popularity
significantly influence visitation patterns, with their effects varying across
different travel distances. Short-distance visits are primarily driven by
convenience, whereas long-distance visits are influenced by hospital ratings.
White-majority areas exhibited lower sensitivity to hospital ratings for
short-distance visits, while Asian populations and those with higher education
levels prioritized hospital rating in their visitation decisions. SES further
influence these patterns, as areas with higher proportions of Hispanic, Black,
under-18, and over-65 populations tend to have more frequent hospital visits,
potentially reflecting greater healthcare needs or limited access to
alternative medical services.
\\ ( https://arxiv.org/abs/2601.15977 ,  3101kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15984
Date: Thu, 22 Jan 2026 14:05:08 GMT   (1324kb)

Title: Partially Lazy Gradient Descent for Smoothed Online Learning
Authors: Naram Mhaisen and George Iosifidis
Categories: cs.LG
Comments: to appear in the proceedings of AISTATS 2026
\\
  We introduce $k$-lazyGD, an online learning algorithm that bridges the gap
between greedy Online Gradient Descent (OGD, for $k=1$) and lazy
GD/dual-averaging (for $k=T$), creating a spectrum between reactive and stable
updates. We analyze this spectrum in Smoothed Online Convex Optimization
(SOCO), where the learner incurs both hitting and movement costs. Our main
contribution is establishing that laziness is possible without sacrificing
hitting performance: we prove that $k$-lazyGD achieves the optimal dynamic
regret $\mathcal{O}(\sqrt{(P_T+1)T})$ for any laziness slack $k$ up to
$\Theta(\sqrt{T/P_T})$, where $P_T$ is the comparator path length. This result
formally connects the allowable laziness to the comparator's shifts, showing
that $k$-lazyGD can retain the inherently small movements of lazy methods
without compromising tracking ability. We base our analysis on the Follow the
Regularized Leader (FTRL) framework, and derive a matching lower bound. Since
the slack depends on $P_T$, an ensemble of learners with various slacks is
used, yielding a method that is provably stable when it can be, and agile when
it must be.
\\ ( https://arxiv.org/abs/2601.15984 ,  1324kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16028
Date: Thu, 22 Jan 2026 14:56:10 GMT   (1938kb)

Title: Data-Driven Conditional Flexibility Index
Authors: Moritz Wedemeyer, Eike Cramer, Alexander Mitsos, Manuel Dahmen
Categories: cs.LG
Comments: manuscript (47 pages, 16 figures), supplementary material (7 pages, 1
  figure, 2 tables)
\\
  With the increasing flexibilization of processes, determining robust
scheduling decisions has become an important goal. Traditionally, the
flexibility index has been used to identify safe operating schedules by
approximating the admissible uncertainty region using simple admissible
uncertainty sets, such as hypercubes. Presently, available contextual
information, such as forecasts, has not been considered to define the
admissible uncertainty set when determining the flexibility index. We propose
the conditional flexibility index (CFI), which extends the traditional
flexibility index in two ways: by learning the parametrized admissible
uncertainty set from historical data and by using contextual information to
make the admissible uncertainty set conditional. This is achieved using a
normalizing flow that learns a bijective mapping from a Gaussian base
distribution to the data distribution. The admissible latent uncertainty set is
constructed as a hypersphere in the latent space and mapped to the data space.
By incorporating contextual information, the CFI provides a more informative
estimate of flexibility by defining admissible uncertainty sets in regions that
are more likely to be relevant under given conditions. Using an illustrative
example, we show that no general statement can be made about data-driven
admissible uncertainty sets outperforming simple sets, or conditional sets
outperforming unconditional ones. However, both data-driven and conditional
admissible uncertainty sets ensure that only regions of the uncertain parameter
space containing realizations are considered. We apply the CFI to a
security-constrained unit commitment example and demonstrate that the CFI can
improve scheduling quality by incorporating temporal information.
\\ ( https://arxiv.org/abs/2601.16028 ,  1938kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16072
Date: Thu, 22 Jan 2026 16:13:52 GMT   (1795kb)

Title: CLASP: An online learning algorithm for Convex Losses And Squared
  Penalties
Authors: Ricardo N. Ferreira and Cl\'audia Soares and Jo\~ao Xavier
Categories: cs.LG math.OC
\\
  We study Constrained Online Convex Optimization (COCO), where a learner
chooses actions iteratively, observes both unanticipated convex loss and convex
constraint, and accumulates loss while incurring penalties for constraint
violations. We introduce CLASP (Convex Losses And Squared Penalties), an
algorithm that minimizes cumulative loss together with squared constraint
violations. Our analysis departs from prior work by fully leveraging the firm
non-expansiveness of convex projectors, a proof strategy not previously applied
in this setting. For convex losses, CLASP achieves regret
$O\left(T^{\max\{\beta,1-\beta\}}\right)$ and cumulative squared penalty
$O\left(T^{1-\beta}\right)$ for any $\beta \in (0,1)$. Most importantly, for
strongly convex problems, CLASP provides the first logarithmic guarantees on
both regret and cumulative squared penalty. In the strongly convex case, the
regret is upper bounded by $O( \log T )$ and the cumulative squared penalty is
also upper bounded by $O( \log T )$.
\\ ( https://arxiv.org/abs/2601.16072 ,  1795kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16074
Date: Thu, 22 Jan 2026 16:18:22 GMT   (3908kb)

Title: Explainable AI to Improve Machine Learning Reliability for Industrial
  Cyber-Physical Systems
Authors: Annemarie Jutte, Uraz Odyurt
Categories: cs.LG
\\
  Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from
both safety and economics perspectives, making their reliability critically
important. Machine Learning (ML), specifically deep learning, is increasingly
integrated in industrial CPS, but the inherent complexity of ML models results
in non-transparent operation. Rigorous evaluation is needed to prevent models
from exhibiting unexpected behaviour on future, unseen data. Explainable AI
(XAI) can be used to uncover model reasoning, allowing a more extensive
analysis of behaviour. We apply XAI to to improve predictive performance of ML
models intended for industrial CPS. We analyse the effects of components from
time-series data decomposition on model predictions using SHAP values. Through
this method, we observe evidence on the lack of sufficient contextual
information during model training. By increasing the window size of data
instances, informed by the XAI findings, we are able to improve model
performance.
\\ ( https://arxiv.org/abs/2601.16074 ,  3908kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16083
Date: Thu, 22 Jan 2026 16:28:01 GMT   (104kb)

Title: Probably Approximately Correct Maximum A Posteriori Inference
Authors: Matthew Shorvon, Frederik Mallmann-Trenn, David S. Watson
Categories: cs.LG cs.AI
Comments: 7 pages main text, 16 total, 3 figures
\\
  Computing the conditional mode of a distribution, better known as the
$\mathit{maximum\ a\ posteriori}$ (MAP) assignment, is a fundamental task in
probabilistic inference. However, MAP estimation is generally intractable, and
remains hard even under many common structural constraints and approximation
schemes. We introduce $\mathit{probably\ approximately\ correct}$ (PAC)
algorithms for MAP inference that provide provably optimal solutions under
variable and fixed computational budgets. We characterize tractability
conditions for PAC-MAP using information theoretic measures that can be
estimated from finite samples. Our PAC-MAP solvers are efficiently implemented
using probabilistic circuits with appropriate architectures. The randomization
strategies we develop can be used either as standalone MAP inference techniques
or to improve on popular heuristics, fortifying their solutions with rigorous
guarantees. Experiments confirm the benefits of our method in a range of
benchmarks.
\\ ( https://arxiv.org/abs/2601.16083 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16107
Date: Thu, 22 Jan 2026 16:54:53 GMT   (3772kb)

Title: Benchmarking Deep Learning Models for Raman Spectroscopy Across
  Open-Source Datasets
Authors: Adithya Sineesh and Akshita Kamsali
Categories: cs.LG
Comments: 17 pages, 3 figures
\\
  Deep learning classifiers for Raman spectroscopy are increasingly reported to
outperform classical chemometric approaches. However their evaluations are
often conducted in isolation or compared against traditional machine learning
methods or trivially adapted vision-based architectures that were not
originally proposed for Raman spectroscopy. As a result, direct comparisons
between existing deep learning models developed specifically for Raman spectral
analysis on shared open-source datasets remain scarce. To the best of our
knowledge, this study presents one of the first systematic benchmarks comparing
three or more published Raman-specific deep learning classifiers across
multiple open-source Raman datasets. We evaluate five representative deep
learning architectures under a unified training and hyperparameter tuning
protocol across three open-source Raman datasets selected to support standard
evaluation, fine-tuning, and explicit distribution-shift testing. We report
classification accuracies and macro-averaged F1 scores to provide a fair and
reproducible comparison of deep learning models for Raman spectra based
classification.
\\ ( https://arxiv.org/abs/2601.16107 ,  3772kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16112
Date: Thu, 22 Jan 2026 16:58:34 GMT   (139kb)

Title: Variable Splitting Binary Tree Models Based on Bayesian Context Tree
  Models for Time Series Segmentation
Authors: Yuta Nakahara, Shota Saito, Kohei Horinouchi, Koshi Shimada, Naoki
  Ichijo, Manabu Kobayashi, Toshiyasu Matsushima
Categories: cs.LG
\\
  We propose a variable splitting binary tree (VSBT) model based on Bayesian
context tree (BCT) models for time series segmentation. Unlike previous
applications of BCT models, the tree structure in our model represents interval
partitioning on the time domain. Moreover, interval partitioning is represented
by recursive logistic regression models. By adjusting logistic regression
coefficients, our model can represent split positions at arbitrary locations
within each interval. This enables more compact tree representations. For
simultaneous estimation of both split positions and tree depth, we develop an
effective inference algorithm that combines local variational approximation for
logistic regression with the context tree weighting (CTW) algorithm. We present
numerical examples on synthetic data demonstrating the effectiveness of our
model and algorithm.
\\ ( https://arxiv.org/abs/2601.16112 ,  139kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16139
Date: Thu, 22 Jan 2026 17:32:24 GMT   (283kb)

Title: On the Intrinsic Dimensions of Data in Kernel Learning
Authors: Rustem Takhanov
Categories: cs.LG
Comments: Accepted to The 29th International Conference on Artificial
  Intelligence and Statistics (AISTATS 2026)
\\
  The manifold hypothesis suggests that the generalization performance of
machine learning methods improves significantly when the intrinsic dimension of
the input distribution's support is low. In the context of KRR, we investigate
two alternative notions of intrinsic dimension. The first, denoted $d_\rho$, is
the upper Minkowski dimension defined with respect to the canonical metric
induced by a kernel function $K$ on a domain $\Omega$. The second, denoted
$d_K$, is the effective dimension, derived from the decay rate of Kolmogorov
$n$-widths associated with $K$ on $\Omega$. Given a probability measure $\mu$
on $\Omega$, we analyze the relationship between these $n$-widths and
eigenvalues of the integral operator $\phi \to \int_\Omega
K(\cdot,x)\phi(x)d\mu(x)$. We show that, for a fixed domain $\Omega$, the
Kolmogorov $n$-widths characterize the worst-case eigenvalue decay across all
probability measures $\mu$ supported on $\Omega$. These eigenvalues are central
to understanding the generalization behavior of constrained KRR, enabling us to
derive an excess error bound of order $O(n^{-\frac{2+d_K}{2+2d_K} + \epsilon})$
for any $\epsilon > 0$, when the training set size $n$ is large. We also
propose an algorithm that estimates upper bounds on the $n$-widths using only a
finite sample from $\mu$. For distributions close to uniform, we prove that
$\epsilon$-accurate upper bounds on all $n$-widths can be computed with high
probability using at most
$O\left(\epsilon^{-d_\rho}\log\frac{1}{\epsilon}\right)$ samples, with fewer
required for small $n$. Finally, we compute the effective dimension $d_K$ for
various fractal sets and present additional numerical experiments. Our results
show that, for kernels such as the Laplace kernel, the effective dimension
$d_K$ can be significantly smaller than the Minkowski dimension $d_\rho$, even
though $d_K = d_\rho$ provably holds on regular domains.
\\ ( https://arxiv.org/abs/2601.16139 ,  283kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16147
Date: Thu, 22 Jan 2026 17:40:23 GMT   (2472kb)

Title: Beat-ssl: Capturing Local ECG Morphology through Heartbeat-level
  Contrastive Learning with Soft Targets
Authors: Muhammad Ilham Rizqyawan, Peter Macfarlane, Stathis Hadjidemetriou,
  Fani Deligianni
Categories: cs.LG
Comments: Accepted at ISBI 2026
\\
  Obtaining labelled ECG data for developing supervised models is challenging.
Contrastive learning (CL) has emerged as a promising pretraining approach that
enables effective transfer learning with limited labelled data. However,
existing CL frameworks either focus solely on global context or fail to exploit
ECG-specific characteristics. Furthermore, these methods rely on hard
contrastive targets, which may not adequately capture the continuous nature of
feature similarity in ECG signals. In this paper, we propose Beat-SSL, a
contrastive learning framework that performs dual-context learning through both
rhythm-level and heartbeat-level contrasting with soft targets. We evaluated
our pretrained model on two downstream tasks: 1) multilabel classification for
global rhythm assessment, and 2) ECG segmentation to assess its capacity to
learn representations across both contexts. We conducted an ablation study and
compared the best configuration with three other methods, including one ECG
foundation model. Despite the foundation model's broader pretraining, Beat-SSL
reached 93% of its performance in multilabel classification task and surpassed
all other methods in the segmentation task by 4%.
\\ ( https://arxiv.org/abs/2601.16147 ,  2472kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16175
Date: Thu, 22 Jan 2026 18:24:00 GMT   (386kb)

Title: Learning to Discover at Test Time
Authors: Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed
  McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu
  Sun
Categories: cs.LG cs.AI
Comments: Code: https://github.com/test-time-training/discover
\\
  How can we use AI to discover a new state of the art for a scientific
problem? Prior work in test-time scaling, such as AlphaEvolve, performs search
by prompting a frozen LLM. We perform reinforcement learning at test time, so
the LLM can continue to train, but now with experience specific to the test
problem. This form of continual learning is quite special, because its goal is
to produce one great solution rather than many good ones on average, and to
solve this very problem rather than generalize to other problems. Therefore,
our learning objective and search subroutine are designed to prioritize the
most promising solutions. We call this method Test-Time Training to Discover
(TTT-Discover). Following prior work, we focus on problems with continuous
rewards. We report results for every problem we attempted, across mathematics,
GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the
new state of the art in almost all of them: (i) Erd\H{o}s' minimum overlap
problem and an autocorrelation inequality; (ii) a GPUMode kernel competition
(up to $2\times$ faster than prior art); (iii) past AtCoder algorithm
competitions; and (iv) denoising problem in single-cell analysis. Our solutions
are reviewed by experts or the organizers. All our results are achieved with an
open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly
available code, in contrast to previous best results that required closed
frontier models. Our test-time training runs are performed using Tinker, an API
by Thinking Machines, with a cost of only a few hundred dollars per problem.
\\ ( https://arxiv.org/abs/2601.16175 ,  386kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16200
Date: Thu, 22 Jan 2026 18:52:21 GMT   (980kb)

Title: Provable Robustness in Multimodal Large Language Models via Feature
  Space Smoothing
Authors: Song Xia, Meiwen Ding, Chenqi Kong, Wenhan Yang, and Xudong Jiang
Categories: cs.LG cs.CV
Comments: Under review
\\
  Multimodal large language models (MLLMs) exhibit strong capabilities across
diverse applications, yet remain vulnerable to adversarial perturbations that
distort their feature representations and induce erroneous predictions. To
address this vulnerability, we propose the Feature-space Smoothing (FS) and
theoretically prove that FS offers certified robustness on the feature
representations of MLLMs. Specifically, FS transforms any feature encoder into
a smoothed variant that is guaranteed to maintain a certified lower bound on
the feature cosine similarity between clean and adversarial representations
under $\ell_2$-bounded attacks. Moreover, we indicate that the value of this
Feature Cosine Similarity Bound (FCSB) derived from FS can be improved by
enlarging the defined Gaussian robustness score on the vanilla encoder.
Building upon this, we introduce the Purifier and Smoothness Mapper (PSM), a
plug-and-play module that improves the Gaussian robustness score of MLLMs and
thus enhances their certified robustness under FS, without requiring any
retraining on MLLMs. We demonstrate that the FS with PSM not only provides a
strong theoretical robustness guarantee but also exhibits superior empirical
performance compared to adversarial training. Extensive experiments across
diverse MLLMs and downstream tasks indicate the effectiveness of the FS-PSM,
reducing the Attack Success Rate (ASR) of various white-box attacks from nearly
90\% to about 1\%.
\\ ( https://arxiv.org/abs/2601.16200 ,  980kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16205
Date: Thu, 22 Jan 2026 18:56:14 GMT   (8876kb)

Title: Counterfactual Training: Teaching Models Plausible and Actionable
  Explanations
Authors: Patrick Altmeyer, Aleksander Buszydlik, Arie van Deursen, Cynthia C.
  S. Liem
Categories: cs.LG cs.AI
Comments: This work has been accepted for publication at the 2026 IEEE
  Conference on Secure and Trustworthy Machine Learning (SaTML). The final
  version will be available on IEEE Xplore
\\
  We propose a novel training regime termed counterfactual training that
leverages counterfactual explanations to increase the explanatory capacity of
models. Counterfactual explanations have emerged as a popular post-hoc
explanation method for opaque machine learning models: they inform how factual
inputs would need to change in order for a model to produce some desired
output. To be useful in real-world decision-making systems, counterfactuals
should be plausible with respect to the underlying data and actionable with
respect to the feature mutability constraints. Much existing research has
therefore focused on developing post-hoc methods to generate counterfactuals
that meet these desiderata. In this work, we instead hold models directly
accountable for the desired end goal: counterfactual training employs
counterfactuals during the training phase to minimize the divergence between
learned representations and plausible, actionable explanations. We demonstrate
empirically and theoretically that our proposed method facilitates training
models that deliver inherently desirable counterfactual explanations and
additionally exhibit improved adversarial robustness.
\\ ( https://arxiv.org/abs/2601.16205 ,  8876kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15335
Date: Tue, 20 Jan 2026 09:25:59 GMT   (324kb)

Title: ToolCaching: Towards Efficient Caching for LLM Tool-calling
Authors: Yi Zhai, Dian Shen, Junzhou Luo, Bin Yang
Categories: cs.SE cs.AI cs.PL
ACM-class: I.2; H.4
\\
  Recent advances in Large Language Models (LLMs) have revolutionized web
applications, enabling intelligent search, recommendation, and assistant
services with natural language interfaces. Tool-calling extends LLMs with the
ability to interact with external APIs, greatly enhancing their practical
utility. While prior research has improved tool-calling performance by adopting
traditional computer systems techniques, such as parallel and asynchronous
execution, the challenge of redundant or repeated tool-calling requests remains
largely unaddressed. Caching is a classic solution to this problem, but
applying it to LLM tool-calling introduces new difficulties due to
heterogeneous request semantics, dynamic workloads, and varying freshness
requirements, which render conventional cache policies ineffective. To address
these issues, we propose ToolCaching, an efficient feature-driven and adaptive
caching framework for LLM tool-calling systems. ToolCaching systematically
integrates semantic and system-level features to evaluate request cacheability
and estimate caching value. At its core, the VAAC algorithm integrates
bandit-based admission with value-driven, multi-factor eviction, jointly
accounting for request frequency, recency, and caching value. Extensive
experiments on synthetic and public tool-calling workloads demonstrate that
ToolCaching with VAAC achieves up to 11% higher cache hit ratios and 34% lower
latency compared to standard policies, effectively accelerating LLM
tool-calling in practical applications.
\\ ( https://arxiv.org/abs/2601.15335 ,  324kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15339
Date: Tue, 20 Jan 2026 14:34:40 GMT   (613kb)

Title: Lost in Transcription: How Speech-to-Text Errors Derail Code
  Understanding
Authors: Jayant Havare, Ashish Mittal, Srikanth Tamilselvam, Ganesh
  Ramakrishnan
Categories: cs.SE cs.AI
\\
  Code understanding is a foundational capability in software engineering tools
and developer workflows. However, most existing systems are designed for
English-speaking users interacting via keyboards, which limits accessibility in
multilingual and voice-first settings, particularly in regions like India.
Voice-based interfaces offer a more inclusive modality, but spoken queries
involving code present unique challenges due to the presence of non-standard
English usage, domain-specific vocabulary, and custom identifiers such as
variable and function names, often combined with code-mixed expressions. In
this work, we develop a multilingual speech-driven framework for code
understanding that accepts spoken queries in a user native language,
transcribes them using Automatic Speech Recognition (ASR), applies code-aware
ASR output refinement using Large Language Models (LLMs), and interfaces with
code models to perform tasks such as code question answering and code retrieval
through benchmarks such as CodeSearchNet, CoRNStack, and CodeQA. Focusing on
four widely spoken Indic languages and English, we systematically characterize
how transcription errors impact downstream task performance. We also identified
key failure modes in ASR for code and demonstrated that LLM-guided refinement
significantly improves performance across both transcription and code
understanding stages. Our findings underscore the need for code-sensitive
adaptations in speech interfaces and offer a practical solution for building
robust, multilingual voice-driven programming tools.
\\ ( https://arxiv.org/abs/2601.15339 ,  613kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15352
Date: Wed, 21 Jan 2026 04:53:38 GMT   (10872kb)

Title: A Prompt-Based Framework for Loop Vulnerability Detection Using Local
  LLMs
Authors: Adeyemi Adeseye and Aisvarya Adeseye
Categories: cs.SE
Comments: Accepted and Waiting to be published ICAI'25: 27th International
  Conference on Artificial Intelligence
  https://american-cse.org/csce2025/conferences-ICAI
\\
  Loop vulnerabilities are one major risky construct in software development.
They can easily lead to infinite loops or executions, exhaust resources, or
introduce logical errors that degrade performance and compromise security. The
problem are often undetected by traditional static analyzers because such tools
rely on syntactic patterns, which makes them struggle to detect semantic flaws.
Consequently, Large Language Models (LLMs) offer new potential for
vulnerability detection because of their ability to understand code
contextually. Moreover, local LLMs unlike commercial ones like ChatGPT or
Gemini addresses issues such as privacy, latency, and dependency concerns by
facilitating efficient offline analysis. Consequently, this study proposes a
prompt-based framework that utilize local LLMs for the detection of loop
vulnerabilities within Python 3.7+ code. The framework targets three categories
of loop-related issues, such as control and logic errors, security risks inside
loops, and resource management inefficiencies. A generalized and structured
prompt-based framework was designed and tested with two locally deployed LLMs
(LLaMA 3.2; 3B and Phi 3.5; 4B) by guiding their behavior via iterative
prompting. The designed prompt-based framework included key safeguarding
features such as language-specific awareness, code-aware grounding, version
sensitivity, and hallucination prevention. The LLM results were validated
against a manually established baseline truth, and the results indicate that
Phi outperforms LLaMA in precision, recall, and F1-score. The findings
emphasize the importance of designing effective prompts for local LLMs to
perform secure and accurate code vulnerability analysis.
\\ ( https://arxiv.org/abs/2601.15352 ,  10872kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15493
Date: Wed, 21 Jan 2026 21:54:41 GMT   (1564kb)

Title: Testing Deep Learning Libraries via Neurosymbolic Constraint Learning
Authors: M M Abid Naziri, Shinhae Kim, Feiran Qin, Marcelo d'Amorim, and Saikat
  Dutta
Categories: cs.SE
\\
  Deep Learning (DL) libraries (e.g., PyTorch) are popular in AI development.
These libraries are complex and contain bugs. Researchers have proposed various
bug-finding techniques for such libraries. Yet, there is much room for
improvement. A key challenge in testing DL libraries is the lack of API
specifications. Prior testing approaches often inaccurately model the input
specifications of DL APIs, resulting in missed valid inputs that could reveal
bugs or false alarms due to invalid inputs.
  To address this challenge, we develop Centaur -- the first neurosymbolic
technique to test DL library APIs using dynamically learned input constraints.
Centaur leverages the key idea that formal API constraints can be learned from
a small number of automatically generated seed inputs, and that the learned
constraints can be solved using SMT solvers to generate valid and diverse test
inputs.
  We develop a novel grammar that represents first-order logic formulae over
API parameters and expresses tensor-related properties (e.g., shape, data
types) as well as relational properties between parameters. We use the grammar
to guide a Large Language Model (LLM) to enumerate syntactically correct
candidate rules, validated using seed inputs. Further, we develop a custom
refinement strategy to prune the set of learned rules to eliminate spurious or
redundant rules. We use the learned constraints to systematically generate
valid and diverse inputs by integrating SMT-based solving with randomized
sampling.
  We evaluate Centaur for testing PyTorch and TensorFlow. Our results show that
Centaur's constraints have a recall of 94.0% and a precision of 94.0% on
average. In terms of coverage, Centaur covers 203, 150, and 9,608 more branches
than TitanFuzz, ACETest and Pathfinder, respectively. Using Centaur, we also
detect 26 new bugs in PyTorch and TensorFlow, 18 of which are confirmed.
\\ ( https://arxiv.org/abs/2601.15493 ,  1564kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15687
Date: Thu, 22 Jan 2026 06:12:18 GMT   (34043kb)

Title: FARM: Field-Aware Resolution Model for Intelligent Trigger-Action
  Automation
Authors: Khusrav Badalov and Young Yoon
Categories: cs.SE cs.AI
\\
  Trigger-Action Programming (TAP) platforms such as IFTTT and Zapier enable
Web of Things (WoT) automation by composing event-driven rules across
heterogeneous services. A TAP applet links a trigger to an action and must bind
trigger outputs (ingredients) to action inputs (fields) to be executable. Prior
work largely treats TAP as service-level prediction from natural language,
which often yields non-executable applets that still require manual
configuration. We study the function-level configuration problem: generating
complete applets with correct ingredient-to-field bindings. We propose FARM
(Field-Aware Resolution Model), a two-stage architecture for automated applet
generation with full configuration. Stage 1 trains contrastive dual encoders
with selective layer freezing over schema-enriched representations, retrieving
candidates from 1,724 trigger functions and 1,287 action functions (2.2M
possible trigger-action pairs). Stage 2 performs selection and configuration
using an LLM-based multi-agent pipeline. It includes intent analysis, trigger
selection, action selection via cross-schema scoring, and configuration
verification. Agents coordinate through shared state and agreement-based
selection. FARM achieves 81% joint accuracy on Gold (62% Noisy, 70% One-shot)
at the function level, where both trigger and action functions must match the
ground truth. For comparison with service-level baselines, we map functions to
their parent services and evaluate at the service level. FARM reaches 81% joint
accuracy and improves over TARGE by 23 percentage points. FARM also generates
ingredient-to-field bindings, producing executable automation configurations.
\\ ( https://arxiv.org/abs/2601.15687 ,  34043kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15879
Date: Thu, 22 Jan 2026 11:40:04 GMT   (2449kb)

Title: Evaluating and Achieving Controllable Code Completion in Code LLM
Authors: Jiajun Zhang, Zeyu Cui, Lei Zhang, Jian Yang, Jiaxi Yang, Qiang Liu,
  Zilei Wang, Binyuan Hui, Liang Wang, Junyang Lin
Categories: cs.SE cs.CL
\\
  Code completion has become a central task, gaining significant attention with
the rise of large language model (LLM)-based tools in software engineering.
Although recent advances have greatly improved LLMs' code completion abilities,
evaluation methods have not advanced equally. Most current benchmarks focus
solely on functional correctness of code completions based on given context,
overlooking models' ability to follow user instructions during completion-a
common scenario in LLM-assisted programming. To address this limitation, we
present the first instruction-guided code completion benchmark, Controllable
Code Completion Benchmark (C3-Bench), comprising 2,195 carefully designed
completion tasks. Through comprehensive evaluation of over 40 mainstream LLMs
across C3-Bench and conventional benchmarks, we reveal substantial gaps in
instruction-following capabilities between open-source and advanced proprietary
models during code completion tasks. Moreover, we develop a straightforward
data synthesis pipeline that leverages Qwen2.5-Coder to generate high-quality
instruction-completion pairs for supervised fine-tuning (SFT). The resulting
model, Qwen2.5-Coder-C3, achieves state-of-the-art performance on C3-Bench. Our
findings provide valuable insights for enhancing LLMs' code completion and
instruction-following capabilities, establishing new directions for future
research in code LLMs. To facilitate reproducibility and foster further
research in code LLMs, we open-source all code, datasets, and models.
\\ ( https://arxiv.org/abs/2601.15879 ,  2449kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16009
Date: Thu, 22 Jan 2026 14:35:19 GMT   (712kb)

Title: The Role of Cognitive Abilities in Requirements Inspection: Comparing
  UML and Textual Representations
Authors: Giovanna Broccia and Sira Vegas and Alessio Ferrari
Categories: cs.SE
\\
  The representation of requirements plays a critical role in the accuracy of
requirements inspection. While visual representations, such as UML diagrams,
are widely used alongside text-based requirements, their effectiveness in
supporting inspection is still debated. Cognitive abilities, such as working
memory and mental rotation skills, may also influence inspection accuracy. This
study aims to evaluate whether the use of UML sequence diagrams alongside
text-based requirements improves the accuracy of requirements inspection
compared to text-based requirements alone and to explore whether cognitive
abilities are associated with differences in performance across the two
treatments (text vs text with UML support). We conducted a crossover experiment
with 38 participants to assess the accuracy of requirements inspection under
the two treatments in terms of issues found and justifications provided. Linear
mixed-effects and generalized linear models were used to analyse the effects of
treatment, period, sequence, and cognitive abilities. The results indicate a
significant three-way interaction between representation type, working memory
capacity, and mental rotation ability. This finding suggests that the
effectiveness of UML support is not uniform across individuals: participants
with high scores in both cognitive abilities experienced reduced performance
when using UML for violation detection. Conversely, the same cognitive profile
was associated with improved justification accuracy under UML-aided inspection,
indicating that higher cognitive abilities may support deeper reasoning
processes when dealing with multi-modal information, i.e., diagrams and text.
\\ ( https://arxiv.org/abs/2601.16009 ,  712kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16080
Date: Thu, 22 Jan 2026 16:22:23 GMT   (419kb)

Title: Towards a Goal-Centric Assessment of Requirements Engineering Methods
  for Privacy by Design
Authors: Oleksandr Kosenkov, Ehsan Zabardast, Jannik Fischbach, Tony Gorschek,
  Daniel Mendez
Categories: cs.SE cs.CY
Comments: The paper has been accepted for the 32nd International Working
  Conference on Requirements Engineering: Foundation for Software Quality
  (REFSQ 2026)
\\
  Implementing privacy by design (PbD) according to the General Data Protection
Regulation (GDPR) is met with a growing number of requirements engineering (RE)
approaches. However, the question of which RE method for PbD fits best the
goals of organisations remains a challenge. We report our endeavor to close
this gap by synthesizing a goal-centric approach for PbD methods assessment. We
used literature review, interviews, and validation with practitioners to
achieve the goal of our study. As practitioners do not approach PbD
systematically, we suggest that RE methods for PbD should be assessed against
organisational goals, rather than process characteristics only. We hope that,
when further developed, the goal-centric approach could support the
development, selection, and tailoring of RE practices for PbD.
\\ ( https://arxiv.org/abs/2601.16080 ,  419kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2601.14264 (*cross-listing*)
Date: Mon, 22 Dec 2025 18:04:27 GMT   (7155kb)

Title: Psychometric Comparability of LLM-Based Digital Twins
Authors: Yufei Zhang and Zhihao Ma
Categories: cs.CY cs.AI cs.CL cs.HC
Comments: Also available as a preprint on OSF Preprints
  https://osf.io/preprints/psyarxiv/965yg_v1
\\
  Large language models (LLMs) are used as "digital twins" to replace human
respondents, yet their psychometric comparability to humans is uncertain. We
propose a construct-validity framework spanning construct representation and
the nomological net, benchmarking digital twins against human gold standards
across models, tasks and testing how person-specific inputs shape performance.
Across studies, digital twins achieved high population-level accuracy and
strong within-participant profile correlations, alongside attenuated item-level
correlations. In word association tests, LLM-based networks show small-world
structure and theory-consistent communities similar to humans, yet diverge
lexically and in local structure. In decision-making and contextualized tasks,
digital twins under-reproduce heuristic biases, showing normative rationality,
compressed variance and limited sensitivity to temporal information.
Feature-rich digital twins improve Big Five Personality prediction, but their
personality networks show only configural invariance and do not achieve metric
invariance. In more applied free-text tasks, feature-rich digital twins better
match human narratives, but linguistic differences persist. Together, these
results indicate that feature-rich conditioning enhances validity but does not
resolve systematic divergences in psychometric comparability. Future work
should therefore prioritize delineating the effective boundaries of digital
twins, establishing the precise contexts in which they function as reliable
proxies for human cognition and behavior.
\\ ( https://arxiv.org/abs/2601.14264 ,  7155kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15280 (*cross-listing*)
Date: Wed, 21 Jan 2026 18:58:08 GMT   (2315kb)

Title: LLM-based Multimodal Feedback Produces Equivalent Learning and Better
  Student Perceptions than Educator Feedback
Authors: Chloe Qianhui Zhao and Jie Cao and Jionghao Lin and Kenneth R.
  Koedinger
Categories: cs.HC cs.AI
Comments: 11 pages, to be published at the 16th International Learning
  Analytics & Knowledge Conference (LAK '26)
\\
  Providing timely, targeted, and multimodal feedback helps students quickly
correct errors, build deep understanding and stay motivated, yet making it at
scale remains a challenge. This study introduces a real-time AI-facilitated
multimodal feedback system that integrates structured textual explanations with
dynamic multimedia resources, including the retrieved most relevant slide page
references and streaming AI audio narration. In an online crowdsourcing
experiment, we compared this system against fixed business-as-usual feedback by
educators across three dimensions: (1) learning effectiveness, (2) learner
engagement, (3) perceived feedback quality and value. Results showed that AI
multimodal feedback achieved learning gains equivalent to original educator
feedback while significantly outperforming it on perceived clarity,
specificity, conciseness, motivation, satisfaction, and reducing cognitive
load, with comparable correctness, trust, and acceptance. Process logs revealed
distinct engagement patterns: for multiple-choice questions, educator feedback
encouraged more submissions; for open-ended questions, AI-facilitated targeted
suggestions lowered revision barriers and promoted iterative improvement. These
findings highlight the potential of AI multimodal feedback to provide scalable,
real-time, and context-aware support that both reduces instructor workload and
enhances student experience.
\\ ( https://arxiv.org/abs/2601.15280 ,  2315kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15290 (*cross-listing*)
Date: Sun, 30 Nov 2025 20:25:56 GMT   (3129kb)

Title: Agentic Persona Control and Task State Tracking for Realistic User
  Simulation in Interactive Scenarios
Authors: Hareeshwar Karthikeyan
Categories: cs.HC cs.AI
Comments: - Accepted to 39th Conference on Neural Information Processing
  Systems (NeurIPS 2025) Workshop: Scaling Environments for Agents (SEA) -
  Paper contains 12 pages with 3 figures and 3 tables
\\
  Testing conversational AI systems at scale across diverse domains
necessitates realistic and diverse user interactions capturing a wide array of
behavioral patterns. We present a novel multi-agent framework for realistic,
explainable human user simulation in interactive scenarios, using persona
control and task state tracking to mirror human cognitive processes during
goal-oriented conversations. Our system employs three specialized AI agents:
(1) a User Agent to orchestrate the overall interaction, (2) a State Tracking
Agent to maintain structured task state, and (3) a Message Attributes
Generation Agent that controls conversational attributes based on task progress
and assigned persona. To validate our approach, we implement and evaluate the
framework for guest ordering at a restaurant with scenarios rich in task
complexity, behavioral diversity, and conversational ambiguity. Through
systematic ablations, we evaluate the contributory efficacy of each agentic
component to overall simulation quality in terms of persona adherence, task
completion accuracy, explainability, and realism. Our experiments demonstrate
that the complete multi-agent system achieves superior simulation quality
compared to single-LLM baselines, with significant gains across all evaluation
metrics. This framework establishes a powerful environment for orchestrating
agents to simulate human users with cognitive plausibility, decomposing the
simulation into specialized sub-agents that reflect distinct aspects of human
thought processes applicable across interactive domains.
\\ ( https://arxiv.org/abs/2601.15290 ,  3129kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15292 (*cross-listing*)
Date: Tue, 9 Dec 2025 04:42:01 GMT   (788kb)

Title: A Mobile Application Front-End for Presenting Explainable AI Results in
  Diabetes Risk Estimation
Authors: Bernardus Willson, Henry Anand Septian Radityo, Reynard Tanadi, Latifa
  Dwiyanti, Saiful Akbar
Categories: cs.HC cs.AI
Comments: This paper was accepted and presented at the 2025 IEEE International
  Conference on Data and Software Engineering (ICoDSE) on 29 October 2025 in
  Batam, Indonesia, and is currently awaiting publication
\\
  Diabetes is a significant and continuously rising health challenge in
Indonesia. Although many artificial intelligence (AI)-based health applications
have been developed for early detection, most function as "black boxes,"
lacking transparency in their predictions. Explainable AI (XAI) methods offer a
solution, yet their technical outputs are often incomprehensible to non-expert
users. This research aims to develop a mobile application front-end that
presents XAI-driven diabetes risk analysis in an intuitive, understandable
format. Development followed the waterfall methodology, comprising requirements
analysis, interface design, implementation, and evaluation. Based on user
preference surveys, the application adopts two primary visualization types -
bar charts and pie charts - to convey the contribution of each risk factor.
These are complemented by personalized textual narratives generated via
integration with GPT-4o. The application was developed natively for Android
using Kotlin and Jetpack Compose. The resulting prototype interprets SHAP
(SHapley Additive exPlanations), a key XAI approach, into accessible graphical
visualizations and narratives. Evaluation through user comprehension testing
(Likert scale and interviews) and technical functionality testing confirmed the
research objectives were met. The combination of visualization and textual
narrative effectively enhanced user understanding (average score 4.31/5) and
empowered preventive action, supported by a 100% technical testing success
rate.
\\ ( https://arxiv.org/abs/2601.15292 ,  788kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15295 (*cross-listing*)
Date: Sun, 21 Dec 2025 02:06:03 GMT   (7131kb)

Title: Elsewise: Authoring AI-Based Interactive Narrative with Possibility
  Space Visualization
Authors: Yi Wang, John Joon Young Chung, Melissa Roemmele, Yuqian Sun, Tiffany
  Wang, Shm Garanganao Almeda, Brett A. Halperin, Yuwen Lu, Max Kreminski
Categories: cs.HC cs.AI cs.CL
\\
  Interactive narrative (IN) authors craft spaces of divergent narrative
possibilities for players to explore, with the player's input determining which
narrative possibilities they actually experience. Generative AI can enable new
forms of IN by improvisationally expanding on pre-authored content in response
to open-ended player input. However, this extrapolation risks widening the gap
between author-envisioned and player-experienced stories, potentially limiting
the strength of plot progression and the communication of the author's
narrative intent. To bridge the gap, we introduce Elsewise: an authoring tool
for AI-based INs that implements a novel Bundled Storyline concept to enhance
author's perception and understanding of the narrative possibility space,
allowing authors to explore similarities and differences between possible
playthroughs of their IN in terms of open-ended, user-configurable narrative
dimensions. A user study (n=12) shows that our approach improves author
anticipation of player-experienced narrative, leading to more effective control
and exploration of the narrative possibility spaces.
\\ ( https://arxiv.org/abs/2601.15295 ,  7131kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15296 (*cross-listing*)
Date: Fri, 2 Jan 2026 07:14:05 GMT   (868kb)

Title: Entropy-Tree: Tree-Based Decoding with Entropy-Guided Exploration
Authors: Longxuan Wei, Yubo Zhang, Zijiao Zhang, Zhihu Wang, Shiwan Zhao,
  Tianyu Huang, Huiting Zhao, Chenfei Liu, Shenao Zhang, Junchi Yan
Categories: cs.CL cs.AI
\\
  Large language models achieve strong reasoning performance, yet existing
decoding strategies either explore blindly (random sampling) or redundantly
(independent multi-sampling). We propose Entropy-Tree, a tree-based decoding
method that exploits entropy as a signal for branching decisions--expanding the
search tree only at positions where the model exhibits genuine uncertainty.
Entropy-Tree shows superior accuracy and calibration in reasoning tasks: it
achieves better pass@k than Multi-chain across multiple models and datasets,
and its predictive entropy demonstrates better AUROC compared to several
traditional metrics. Entropy-Tree unifies efficient structured exploration and
reliable uncertainty estimation within a single decoding procedure.
\\ ( https://arxiv.org/abs/2601.15296 ,  868kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15298 (*cross-listing*)
Date: Tue, 6 Jan 2026 01:17:16 GMT   (1270kb)

Title: Embedding Retrofitting: Data Engineering for better RAG
Authors: Anantha Sharma
Categories: cs.CL cs.AI cs.PF
Comments: 16 pages, 11 figures, 7 tables
\\
  Embedding retrofitting adjusts pre-trained word vectors using knowledge graph
constraints to improve domain-specific retrieval. However, the effectiveness of
retrofitting depends critically on knowledge graph quality, which in turn
depends on text preprocessing. This paper presents a data engineering framework
that addresses data quality degradation from annotation artifacts in real-world
corpora.
  The analysis shows that hashtag annotations inflate knowledge graph density,
leading to creating spurious edges that corrupt the retrofitting objective. On
noisy graphs, all retrofitting techniques produce statistically significant
degradation ($-3.5\%$ to $-5.2\%$, $p<0.05$). After preprocessing,
\acrshort{ewma} retrofitting achieves $+6.2\%$ improvement ($p=0.0348$) with
benefits concentrated in quantitative synthesis questions ($+33.8\%$ average).
The gap between clean and noisy preprocessing (10\%+ swing) exceeds the gap
between algorithms (3\%), establishing preprocessing quality as the primary
determinant of retrofitting success.
\\ ( https://arxiv.org/abs/2601.15298 ,  1270kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15301 (*cross-listing*)
Date: Fri, 9 Jan 2026 04:53:06 GMT   (935kb)

Title: Can We Trust LLM Detectors?
Authors: Jivnesh Sandhan, Harshit Jaiswal, Fei Cheng and Yugo Murawaki
Categories: cs.CL cs.AI
Comments: NLP2026, Utsunomiya, Japan
\\
  The rapid adoption of LLMs has increased the need for reliable AI text
detection, yet existing detectors often fail outside controlled benchmarks. We
systematically evaluate 2 dominant paradigms (training-free and supervised) and
show that both are brittle under distribution shift, unseen generators, and
simple stylistic perturbations. To address these limitations, we propose a
supervised contrastive learning (SCL) framework that learns discriminative
style embeddings. Experiments show that while supervised detectors excel
in-domain, they degrade sharply out-of-domain, and training-free methods remain
highly sensitive to proxy choice. Overall, our results expose fundamental
challenges in building domain-agnostic detectors. Our code is available at:
https://github.com/HARSHITJAIS14/DetectAI
\\ ( https://arxiv.org/abs/2601.15301 ,  935kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15304 (*cross-listing*)
Date: Sat, 10 Jan 2026 22:48:57 GMT   (211kb)

Title: An Explainable Market Integrity Monitoring System with Multi-Source
  Attention Signals and Transparent Scoring
Authors: Sandeep Neela
Categories: q-fin.RM cs.AI cs.LG
Comments: Preprint
\\
  Market integrity monitoring is difficult because suspicious price/volume
behavior can arise from many benign mechanisms, while modern detection systems
often rely on opaque models that are hard to audit and communicate. We present
AIMM-X, an explainable monitoring pipeline that combines market
microstructure-style signals derived from OHLCV time series with multi-source
public attention signals (e.g., news and online discussion proxies) to surface
time windows that merit analyst review. The system detects candidate anomalous
windows using transparent thresholding and aggregation, then assigns an
interpretable integrity score decomposed into a small set of additive
components, allowing practitioners to trace why a window was flagged and which
factors drove the score. We provide an end-to-end, reproducible implementation
that downloads data, constructs attention features, builds unified panels,
detects windows, computes component signals, and generates summary
figures/tables. Our goal is not to label manipulation, but to provide a
practical, auditable screening tool that supports downstream investigation by
compliance teams, exchanges, or researchers.
\\ ( https://arxiv.org/abs/2601.15304 ,  211kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15308 (*cross-listing*)
Date: Tue, 13 Jan 2026 15:21:08 GMT   (3196kb)

Title: When Generative AI Meets Extended Reality: Enabling Scalable and Natural
  Interactions
Authors: Mingyu Zhu, Jiangong Chen, Bin Li
Categories: cs.HC cs.AI
Comments: Accepted by IEEE Internet Computing (Oct. 2025); published in IEEE
  Xplore (Jan. 2026)
DOI: 10.1109/MIC.2025.3619462
\\
  Extended Reality (XR), including virtual, augmented, and mixed reality,
provides immersive and interactive experiences across diverse applications,
from VR-based education to AR-based assistance and MR-based training. However,
widespread XR adoption remains limited due to two key challenges: 1) the high
cost and complexity of authoring 3D content, especially for large-scale
environments or complex interactions; and 2) the steep learning curve
associated with non-intuitive interaction methods like handheld controllers or
scripted gestures. Generative AI (GenAI) presents a promising solution by
enabling intuitive, language-driven interaction and automating content
generation. Leveraging vision-language models and diffusion-based generation,
GenAI can interpret ambiguous instructions, understand physical scenes, and
generate or manipulate 3D content, significantly lowering barriers to XR
adoption. This paper explores the integration of XR and GenAI through three
concrete use cases, showing how they address key obstacles in scalability and
natural interaction, and identifying technical challenges that must be resolved
to enable broader adoption.
\\ ( https://arxiv.org/abs/2601.15308 ,  3196kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15312 (*cross-listing*)
Date: Wed, 14 Jan 2026 16:07:00 GMT   (1095kb)

Title: Do people expect different behavior from large language models acting on
  their behalf? Evidence from norm elicitations in two canonical economic games
Authors: Pawe{\l} Niszczota and Elia Antoniou
Categories: cs.GT cs.AI cs.CL cs.CY cs.HC econ.GN q-fin.EC
MSC-class: 91A80, 91B42, 91B10
ACM-class: I.2.7; J.4; K.4.2
\\
  While delegating tasks to large language models (LLMs) can save people time,
there is growing evidence that offloading tasks to such models produces social
costs. We use behavior in two canonical economic games to study whether people
have different expectations when decisions are made by LLMs acting on their
behalf instead of themselves. More specifically, we study the social
appropriateness of a spectrum of possible behaviors: when LLMs divide resources
on our behalf (Dictator Game and Ultimatum Game) and when they monitor the
fairness of splits of resources (Ultimatum Game). We use the Krupka-Weber norm
elicitation task to detect shifts in social appropriateness ratings. Results of
two pre-registered and incentivized experimental studies using representative
samples from the UK and US (N = 2,658) show three key findings. First, people
find that offers from machines - when no acceptance is necessary - are judged
to be less appropriate than when they come from humans, although there is no
shift in the modal response. Second - when acceptance is necessary - it is more
appropriate for a person to reject offers from machines than from humans.
Third, receiving a rejection of an offer from a machine is no less socially
appropriate than receiving the same rejection from a human. Overall, these
results suggest that people apply different norms for machines deciding on how
to split resources but are not opposed to machines enforcing the norms. The
findings are consistent with offers made by machines now being viewed as having
both a cognitive and emotional component.
\\ ( https://arxiv.org/abs/2601.15312 ,  1095kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15313 (*cross-listing*)
Date: Wed, 14 Jan 2026 18:55:23 GMT   (853kb)

Title: Mind the Gap: Why Neural Memory Fails Under Semantic Density
Authors: Matt Beton, Simran Chana
Categories: q-bio.NC cs.AI
Comments: 24 Pages, 5 Figures
\\
  The brain solves a problem that current AI architectures struggle to manage:
storing specific episodic facts without corrupting general semantic knowledge.
Neuroscience explains this through Complementary Learning Systems theory - a
fast hippocampal system for episodic storage using pattern-separated
representations, and a slow neocortical system for extracting statistical
regularities. Current AI systems lack this separation, attempting both
functions through neural weights alone. We identify the 'Stability Gap' in
online neural memory: fast-weight mechanisms that write facts into shared
continuous parameters collapse to near-random accuracy within tens of
semantically related facts. Through semantic density (rho), we show collapse
occurs with as few as N=5 facts at high density (rho > 0.6) or N ~ 20-75 at
moderate density - a phenomenon we formalise as the Orthogonality Constraint.
This failure persists even with perfect attention and unlimited context,
arising from write-time interference when storage and retrieval share the same
substrate. We also identify schema drift and version ambiguity as primary
failure modes in production systems, observing 40-70% schema consistency and
0-100% clean correction rates. Context-based memory incurs 30-300% cost premium
over selective retrieval. We propose Knowledge Objects (KOs): discrete, typed
memory units with controlled vocabularies and explicit version chains. Paired
with neural weights, KOs enable a true complementary learning architecture,
suggesting reliable AI memory may require this bicameral design.
\\ ( https://arxiv.org/abs/2601.15313 ,  853kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15314 (*cross-listing*)
Date: Thu, 15 Jan 2026 10:13:29 GMT   (446kb)

Title: Beyond the Einstein-Bohr Debate: Cognitive Complementarity and the
  Emergence of Quantum Intuition
Authors: Lalit Kumar Shukla
Categories: q-bio.NC cs.AI quant-ph
Comments: This interdisciplinary work bridges quantum foundations and cognitive
  science, proposing a formal extension of complementarity into cognitive
  reasoning and introducing the testable construct of quantum intuition
MSC-class: 2020: 81P05, 81P13, 81P10, 81Q99, 91E99
\\
  Recent high-precision experimental confirmations of quantum complementarity
have revitalized foundational debates about measurement, description, and
realism. This article argues that complementarity is most productively
interpreted as an epistemic principle--constraining what can be simultaneously
accessed and represented--rather than as an ontological claim about quantum
reality. Reexamining the Einstein-Bohr debate through this lens reveals a
persistent tension between descriptive completeness and contextual meaning, a
tension experiments clarify but do not dissolve. Building on this analysis, we
introduce cognitive complementarity as a structural principle governing
reasoning under non-classical uncertainty, where mutually constraining
representations cannot be jointly optimized. Within this framework, we propose
quantum intuition as a testable cognitive capacity: the ability to sustain
representational plurality, regulate commitment timing, and resolve
perspective-incompatibilities in a context-sensitive manner. Formulated as a
naturalistic construct grounded in shared informational constraints, quantum
intuition offers a principled bridge between quantum measurement theory and
cognition. This work reframes the historical debate, extends epistemic lessons
from quantum foundations into cognitive science, and outlines empirical
pathways for studying decision-making in contexts of irreducible uncertainty.
\\ ( https://arxiv.org/abs/2601.15314 ,  446kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15319 (*cross-listing*)
Date: Fri, 16 Jan 2026 10:16:58 GMT   (735kb)

Title: Large Language Models as Simulative Agents for Neurodivergent Adult
  Psychometric Profiles
Authors: Francesco Chiappone, Davide Marocco, Nicola Milano
Categories: q-bio.NC cs.AI
\\
  Adult neurodivergence, including Attention-Deficit/Hyperactivity Disorder
(ADHD), high-functioning Autism Spectrum Disorder (ASD), and Cognitive
Disengagement Syndrome (CDS), is marked by substantial symptom overlap that
limits the discriminant sensitivity of standard psychometric instruments. While
recent work suggests that Large Language Models (LLMs) can simulate human
psychometric responses from qualitative data, it remains unclear whether they
can accurately and stably model neurodevelopmental traits rather than broad
personality characteristics. This study examines whether LLMs can generate
psychometric responses that approximate those of real individuals when grounded
in a structured qualitative interview, and whether such simulations are
sensitive to variations in trait intensity. Twenty-six adults completed a
29-item open-ended interview and four standardized self-report measures (ASRS,
BAARS-IV, AQ, RAADS-R). Two LLMs (GPT-4o and Qwen3-235B-A22B) were prompted to
infer an individual psychological profile from interview content and then
respond to each questionnaire in-role. Accuracy, reliability, and sensitivity
were assessed using group-level comparisons, error metrics, exact-match
scoring, and a randomized baseline. Both models outperformed random responses
across instruments, with GPT-4o showing higher accuracy and reproducibility.
Simulated responses closely matched human data for ASRS, BAARS-IV, and RAADS-R,
while the AQ revealed subscale-specific limitations, particularly in Attention
to Detail. Overall, the findings indicate that interview-grounded LLMs can
produce coherent and above-chance simulations of neurodevelopmental traits,
supporting their potential use as synthetic participants in early-stage
psychometric research, while highlighting clear domain-specific constraints.
\\ ( https://arxiv.org/abs/2601.15319 ,  735kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15326 (*cross-listing*)
Date: Mon, 19 Jan 2026 07:55:23 GMT   (14343kb)

Title: ECGomics: An Open Platform for AI-ECG Digital Biomarker Discovery
Authors: Deyun Zhang, Jun Li, Shijia Geng, Yue Wang, Shijie Chen, Sumei Fan,
  Qinghao Zha, Shenda Hong
Categories: q-bio.QM cs.AI
\\
  Background: Conventional electrocardiogram (ECG) analysis faces a persistent
dichotomy: expert-driven features ensure interpretability but lack sensitivity
to latent patterns, while deep learning offers high accuracy but functions as a
black box with high data dependency. We introduce ECGomics, a systematic
paradigm and open-source platform for the multidimensional deconstruction of
cardiac signals into digital biomarker. Methods: Inspired by the taxonomic
rigor of genomics, ECGomics deconstructs cardiac activity across four
dimensions: Structural, Intensity, Functional, and Comparative. This taxonomy
synergizes expert-defined morphological rules with data-driven latent
representations, effectively bridging the gap between handcrafted features and
deep learning embeddings. Results: We operationalized this framework into a
scalable ecosystem consisting of a web-based research platform and a
mobile-integrated solution (https://github.com/PKUDigitalHealth/ECGomics). The
web platform facilitates high-throughput analysis via precision parameter
configuration, high-fidelity data ingestion, and 12-lead visualization,
allowing for the systematic extraction of biomarkers across the four ECGomics
dimensions. Complementarily, the mobile interface, integrated with portable
sensors and a cloud-based engine, enables real-time signal acquisition and
near-instantaneous delivery of structured diagnostic reports. This
dual-interface architecture successfully transitions ECGomics from theoretical
discovery to decentralized, real-world health management, ensuring
professional-grade monitoring in diverse clinical and home-based settings.
Conclusion: ECGomics harmonizes diagnostic precision, interpretability, and
data efficiency. By providing a deployable software ecosystem, this paradigm
establishes a robust foundation for digital biomarker discovery and
personalized cardiovascular medicine.
\\ ( https://arxiv.org/abs/2601.15326 ,  14343kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15330 (*cross-listing*)
Date: Tue, 20 Jan 2026 04:39:28 GMT   (272kb)

Title: ICPO: Illocution-Calibrated Policy Optimization for Multi-Turn
  Conversation
Authors: Zhebo Wang and Xiaohu Mu and Zijie Zhou and Mohan Li and Wenpeng Xing
  and Dezhang Kong and Meng Han
Categories: cs.CL cs.AI
Comments: Accepted by ICASSP 2026
\\
  Large Language Models (LLMs) in multi-turn conversations often suffer from a
``lost-in-conversation'' phenomenon, where they struggle to recover from early
incorrect assumptions, particularly when users provide ambiguous initial
instructions. We find that standard post-training techniques like Reinforcement
Learning with Verifiable Rewards (RLVR) exacerbate this issue by rewarding
confident, direct answers, thereby inducing overconfidence and discouraging the
model from seeking clarification. To address this, we propose
Illocution-Calibrated Policy Optimization (ICPO), a novel training framework
that sensitizes the model to instruction ambiguity. ICPO augments the training
corpus with underspecified prompts and conditions the reward signal on the
user's illocutionary intent, rewarding the model for expressing uncertainty or
asking for clarification when faced with ambiguity. Experiments demonstrate
that ICPO fosters appropriate humility, yielding a substantial average
improvement of 75\% in multi-turn conversation, while preserving robust
performance on single-turn benchmarks. Our work presents a practical path
toward more robust and collaborative conversational AI that can better navigate
the nuances of human interaction.
\\ ( https://arxiv.org/abs/2601.15330 ,  272kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15331 (*cross-listing*)
Date: Tue, 20 Jan 2026 06:01:02 GMT   (17kb)

Title: RECAP: A Resource-Efficient Method for Adversarial Prompting in Large
  Language Models
Authors: Rishit Chugh
Categories: cs.CL cs.AI cs.CR cs.LG
Comments: Code for RECAP is available at: https://github.com/R-C101/RECAP
\\
  The deployment of large language models (LLMs) has raised security concerns
due to their susceptibility to producing harmful or policy-violating outputs
when exposed to adversarial prompts. While alignment and guardrails mitigate
common misuse, they remain vulnerable to automated jailbreaking methods such as
GCG, PEZ, and GBDA, which generate adversarial suffixes via training and
gradient-based search. Although effective, these methods particularly GCG are
computationally expensive, limiting their practicality for organisations with
constrained resources. This paper introduces a resource-efficient adversarial
prompting approach that eliminates the need for retraining by matching new
prompts to a database of pre-trained adversarial prompts. A dataset of 1,000
prompts was classified into seven harm-related categories, and GCG, PEZ, and
GBDA were evaluated on a Llama 3 8B model to identify the most effective attack
method per category. Results reveal a correlation between prompt type and
algorithm effectiveness. By retrieving semantically similar successful
adversarial prompts, the proposed method achieves competitive attack success
rates with significantly reduced computational cost. This work provides a
practical framework for scalable red-teaming and security evaluation of aligned
LLMs, including in settings where model internals are inaccessible.
\\ ( https://arxiv.org/abs/2601.15331 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15334 (*cross-listing*)
Date: Tue, 20 Jan 2026 08:28:02 GMT   (2188kb)

Title: No Reliable Evidence of Self-Reported Sentience in Small Large Language
  Models
Authors: Caspar Kaiser, Sean Enderby
Categories: cs.CL cs.AI
\\
  Whether language models possess sentience has no empirical answer. But
whether they believe themselves to be sentient can, in principle, be tested. We
do so by querying several open-weights models about their own consciousness,
and then verifying their responses using classifiers trained on internal
activations. We draw upon three model families (Qwen, Llama, GPT-OSS) ranging
from 0.6 billion to 70 billion parameters, approximately 50 questions about
consciousness and subjective experience, and three classification methods from
the interpretability literature. First, we find that models consistently deny
being sentient: they attribute consciousness to humans but not to themselves.
Second, classifiers trained to detect underlying beliefs - rather than mere
outputs - provide no clear evidence that these denials are untruthful. Third,
within the Qwen family, larger models deny sentience more confidently than
smaller ones. These findings contrast with recent work suggesting that models
harbour latent beliefs in their own consciousness.
\\ ( https://arxiv.org/abs/2601.15334 ,  2188kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15336 (*cross-listing*)
Date: Tue, 20 Jan 2026 10:41:18 GMT   (1893kb)

Title: Learning Discrete Successor Transitions in Continuous Attractor
  Networks: Emergence, Limits, and Topological Constraints
Authors: Daniel Brownell
Categories: q-bio.NC cs.AI
Comments: An open-source reference implementation is available at
  https://github.com/javadan/can-paper
\\
  Continuous attractor networks (CANs) are a well-established class of models
for representing low-dimensional continuous variables such as head direction,
spatial position, and phase. In canonical spatial domains, transitions along
the attractor manifold are driven by continuous displacement signals, such as
angular velocity-provided by sensorimotor systems external to the CAN itself.
When such signals are not explicitly provided as dedicated displacement inputs,
it remains unclear whether attractor-based circuits can reliably acquire
recurrent dynamics that support stable state transitions, or whether
alternative predictive strategies dominate.
  In this work, we present an experimental framework for training CANs to
perform successor-like transitions between stable attractor states in the
absence of externally provided displacement signals. We compare two recurrent
topologies, a circular ring and a folded snake manifold, and systematically
vary the temporal regime under which stability is evaluated. We find that,
under short evaluation windows, networks consistently converge to
impulse-driven associative solutions that achieve high apparent accuracy yet
lack persistent attractor dynamics. Only when stability is explicitly enforced
over extended free-run periods do genuine attractor-based transition dynamics
emerge. This suggests that shortcut solutions are the default outcome of local
learning in recurrent networks, while attractor dynamics represent a
constrained regime rather than a generic result.
  Furthermore, we demonstrate that topology strictly limits the capacity for
learned transitions. While the continuous ring topology achieves perfect
stability over long horizons, the folded snake topology hits a geometric limit
characterized by failure at manifold discontinuities, which neither curriculum
learning nor basal ganglia-inspired gating can fully overcome.
\\ ( https://arxiv.org/abs/2601.15336 ,  1893kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15348 (*cross-listing*)
Date: Wed, 21 Jan 2026 02:56:45 GMT   (3612kb)

Title: Abusive music and song transformation using GenAI and LLMs
Authors: Jiyang Choi and Rohitash Chandra
Categories: cs.SD cs.AI cs.CL
\\
  Repeated exposure to violence and abusive content in music and song content
can influence listeners' emotions and behaviours, potentially normalising
aggression or reinforcing harmful stereotypes. In this study, we explore the
use of generative artificial intelligence (GenAI) and Large Language Models
(LLMs) to automatically transform abusive words (vocal delivery) and lyrical
content in popular music. Rather than simply muting or replacing a single word,
our approach transforms the tone, intensity, and sentiment, thus not altering
just the lyrics, but how it is expressed. We present a comparative analysis of
four selected English songs and their transformed counterparts, evaluating
changes through both acoustic and sentiment-based lenses. Our findings indicate
that Gen-AI significantly reduces vocal aggressiveness, with acoustic analysis
showing improvements in Harmonic to Noise Ratio, Cepstral Peak Prominence, and
Shimmer. Sentiment analysis reduced aggression by 63.3-85.6\% across artists,
with major improvements in chorus sections (up to 88.6\% reduction). The
transformed versions maintained musical coherence while mitigating harmful
content, offering a promising alternative to traditional content moderation
that avoids triggering the "forbidden fruit" effect, where the censored content
becomes more appealing simply because it is restricted. This approach
demonstrates the potential for GenAI to create safer listening experiences
while preserving artistic expression.
\\ ( https://arxiv.org/abs/2601.15348 ,  3612kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15351 (*cross-listing*)
Date: Wed, 21 Jan 2026 04:39:32 GMT   (1099kb)

Title: OmniSpectra: A Unified Foundation Model for Native Resolution
  Astronomical Spectra
Authors: Md Khairul Islam, Judy Fox
Categories: astro-ph.IM cs.AI
\\
  We present OmniSpectra, the first native-resolution foundation model for
astronomy spectra. Unlike traditional models, which are limited to fixed-length
input sizes or configurations, OmniSpectra handles spectra of any length at
their original size, without resampling or interpolation. Despite the
large-scale spectroscopic data from diverse surveys fueling the rapid growth of
astronomy, existing foundation models are limited to a fixed wavelength range
and specific instruments. OmniSpectra is the first foundation model to learn
simultaneously from multiple real-world spectra surveys with different
configurations at a large scale. We achieve this by designing a novel
architecture with adaptive patching across variable lengths, sinusoidal global
wavelength encoding, local positional embeddings through depthwise convolution,
and validity-aware self-attention masks. Allowing us to learn multi-scale
spatial patterns while skipping attention for invalid patches. Even with a
limited training example, OmniSpectra demonstrates excellent zero-shot
generalization compared to methods tailored for specific tasks. This transfer
learning capability makes this model the state-of-the-art across various
astronomy tasks, including source classification, redshift estimation, and
properties prediction for stars and galaxies. OmniSpectra reduces the need for
training individual models for different tasks from scratch, establishing
itself as the next-generation astronomy foundation model.
\\ ( https://arxiv.org/abs/2601.15351 ,  1099kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15356 (*cross-listing*)
Date: Wed, 21 Jan 2026 08:02:32 GMT   (45246kb)

Title: Q-Probe: Scaling Image Quality Assessment to High Resolution via
  Context-Aware Agentic Probing
Authors: Xiang Li, XueHeng Li, Yu Wang, XuanHua He, ZhangChi Hu, WeiWei Yu,
  ChengJun Xie
Categories: eess.IV cs.AI
\\
  Reinforcement Learning (RL) has empowered Multimodal Large Language Models
(MLLMs) to achieve superior human preference alignment in Image Quality
Assessment (IQA). However, existing RL-based IQA models typically rely on
coarse-grained global views, failing to capture subtle local degradations in
high-resolution scenarios. While emerging "Thinking with Images" paradigms
enable multi-scale visual perception via zoom-in mechanisms, their direct
adaptation to IQA induces spurious "cropping-implies-degradation" biases and
misinterprets natural depth-of-field as artifacts. To address these challenges,
we propose Q-Probe, the first agentic IQA framework designed to scale IQA to
high resolution via context-aware probing. First, we construct Vista-Bench, a
pioneering benchmark tailored for fine-grained local degradation analysis in
high-resolution IQA settings. Furthermore, we propose a three-stage training
paradigm that progressively aligns the model with human preferences, while
simultaneously eliminating causal bias through a novel context-aware cropping
strategy. Extensive experiments demonstrate that Q-Probe achieves
state-of-the-art performance in high-resolution settings while maintaining
superior efficacy across resolution scales.
\\ ( https://arxiv.org/abs/2601.15356 ,  45246kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15369 (*cross-listing*)
Date: Wed, 21 Jan 2026 18:47:12 GMT   (6470kb)

Title: OpenVision 3: A Family of Unified Visual Encoder for Both Understanding
  and Generation
Authors: Letian Zhang, Sucheng Ren, Yanqing Liu, Xianhang Li, Zeyu Wang, Yuyin
  Zhou, Huaxiu Yao, Zeyu Zheng, Weili Nie, Guilin Liu, Zhiding Yu, Cihang Xie
Categories: eess.IV cs.AI
\\
  This paper presents a family of advanced vision encoder, named OpenVision 3,
that learns a single, unified visual representation that can serve both image
understanding and image generation. Our core architecture is simple: we feed
VAE-compressed image latents to a ViT encoder and train its output to support
two complementary roles. First, the encoder output is passed to the ViT-VAE
decoder to reconstruct the original image, encouraging the representation to
capture generative structure. Second, the same representation is optimized with
contrastive learning and image-captioning objectives, strengthening semantic
features. By jointly optimizing reconstruction- and semantics-driven signals in
a shared latent space, the encoder learns representations that synergize and
generalize well across both regimes. We validate this unified design through
extensive downstream evaluations with the encoder frozen. For multimodal
understanding, we plug the encoder into the LLaVA-1.5 framework: it performs
comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on
SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE
framework: ours substantially surpasses the standard CLIP-based encoder (e.g.,
gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on
unified modeling.
\\ ( https://arxiv.org/abs/2601.15369 ,  6470kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15395 (*cross-listing*)
Date: Wed, 21 Jan 2026 19:06:50 GMT   (7207kb)

Title: Beyond Fixed Psychological Personas: State Beats Trait, but Language
  Models are State-Blind
Authors: Tamunotonye Harry, Ivoline Ngong, Chima Nweke, Yuanyuan Feng, Joseph
  Near
Categories: cs.CL cs.AI cs.HC
\\
  User interactions with language models vary due to static properties of the
user (trait) and the specific context of the interaction (state). However,
existing persona datasets (like PersonaChat, PANDORA etc.) capture only trait,
and ignore the impact of state. We introduce Chameleon, a dataset of 5,001
contextual psychological profiles from 1,667 Reddit users, each measured across
multiple contexts. Using the Chameleon dataset, we present three key findings.
First, inspired by Latent State-Trait theory, we decompose variance and find
that 74\% is within-person(state) while only 26\% is between-person (trait).
Second, we find that LLMs are state-blind: they focus on trait only, and
produce similar responses regardless of state. Third, we find that reward
models react to user state, but inconsistently: different models favor or
penalize the same users in opposite directions. We release Chameleon to support
research on affective computing, personalized dialogue, and RLHF alignment.
\\ ( https://arxiv.org/abs/2601.15395 ,  7207kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15408 (*cross-listing*)
Date: Wed, 21 Jan 2026 19:19:41 GMT   (8025kb)

Title: CURE: Curriculum-guided Multi-task Training for Reliable Anatomy
  Grounded Report Generation
Authors: Pablo Messina, Andr\'es Villa, Juan Le\'on Alc\'azar, Karen S\'anchez,
  Carlos Hinojosa, Denis Parra, \'Alvaro Soto, Bernard Ghanem
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: 31 pages, 7 figures, submitted to CVPR 2026 (under review)
\\
  Medical vision-language models can automate the generation of radiology
reports but struggle with accurate visual grounding and factual consistency.
Existing models often misalign textual findings with visual evidence, leading
to unreliable or weakly grounded predictions. We present CURE, an error-aware
curriculum learning framework that improves grounding and report quality
without any additional data. CURE fine-tunes a multimodal instructional model
on phrase grounding, grounded report generation, and anatomy-grounded report
generation using public datasets. The method dynamically adjusts sampling based
on model performance, emphasizing harder samples to improve spatial and textual
alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality
by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a
data-efficient framework that enhances both grounding accuracy and report
reliability. Code is available at https://github.com/PabloMessina/CURE and
model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure
\\ ( https://arxiv.org/abs/2601.15408 ,  8025kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15412 (*cross-listing*)
Date: Wed, 21 Jan 2026 19:24:27 GMT   (633kb)

Title: A Checklist for Trustworthy, Safe, and User-Friendly Mental Health
  Chatbots
Authors: Shreya Haran, Samiha Thatikonda, Dong Whi Yoo, and Koustuv Saha
Categories: cs.HC cs.AI cs.CY
Journal-ref: In 28th International Conference on Human-Computer Interaction,
  Springer LNCS, 2026
\\
  Mental health concerns are rising globally, prompting increased reliance on
technology to address the demand-supply gap in mental health services. In
particular, mental health chatbots are emerging as a promising solution, but
these remain largely untested, raising concerns about safety and potential
harms. In this paper, we dive into the literature to identify critical gaps in
the design and implementation of mental health chatbots. We contribute an
operational checklist to help guide the development and design of more
trustworthy, safe, and user-friendly chatbots. The checklist serves as both a
developmental framework and an auditing tool to ensure ethical and effective
chatbot design. We discuss how this checklist is a step towards supporting more
responsible design practices and supporting new standards for sociotechnically
sound digital mental health tools.
\\ ( https://arxiv.org/abs/2601.15412 ,  633kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15416 (*cross-listing*)
Date: Wed, 21 Jan 2026 19:27:47 GMT   (6910kb)

Title: DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely
  Sparse-view CBCT Reconstruction
Authors: Cuong Tran Van, Trong-Thang Pham, Ngoc-Son Nguyen, Duy Minh Ho Nguyen,
  Ngan Le
Categories: cs.CV cs.AI
Comments: Published with J2C Certification in Transactions on Machine Learning
  Research (TMLR)
\\
  Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray
projections remains a challenging problem in medical imaging due to the
inherent undersampling of fine-grained anatomical details, which correspond to
high-frequency components. Conventional CNN-based methods often struggle to
recover these fine structures, as they are typically biased toward learning
low-frequency information. To address this challenge, this paper presents DuFal
(Dual-Frequency-Aware Learning), a novel framework that integrates
frequency-domain and spatial-domain processing via a dual-path architecture.
The core innovation lies in our High-Local Factorized Fourier Neural Operator,
which comprises two complementary branches: a Global High-Frequency Enhanced
Fourier Neural Operator that captures global frequency patterns and a Local
High-Frequency Enhanced Fourier Neural Operator that processes spatially
partitioned patches to preserve spatial locality that might be lost in global
frequency analysis. To improve efficiency, we design a Spectral-Channel
Factorization scheme that reduces the Fourier Neural Operator parameter count.
We also design a Cross-Attention Frequency Fusion module to integrate spatial
and frequency features effectively. The fused features are then decoded through
a Feature Decoder to produce projection representations, which are subsequently
processed through an Intensity Field Decoding pipeline to reconstruct a final
Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy
datasets demonstrate that DuFal significantly outperforms existing
state-of-the-art methods in preserving high-frequency anatomical features,
particularly under extremely sparse-view settings.
\\ ( https://arxiv.org/abs/2601.15416 ,  6910kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15445 (*cross-listing*)
Date: Wed, 21 Jan 2026 20:24:39 GMT   (12147kb)

Title: Reflexis: Supporting Reflexivity and Rigor in Collaborative Qualitative
  Analysis through Design for Deliberation
Authors: Runlong Ye, Oliver Huang, Patrick Yung Kang Lee, Michael Liut,
  Carolina Nobre, Ha-Kyung Kong
Categories: cs.HC cs.AI
Comments: Accepted at CHI 26
DOI: 10.1145/3772318.3791275
\\
  Reflexive Thematic Analysis (RTA) is a critical method for generating deep
interpretive insights. Yet its core tenets, including researcher reflexivity,
tangible analytical evolution, and productive disagreement, are often poorly
supported by software tools that prioritize speed and consensus over
interpretive depth. To address this gap, we introduce Reflexis, a collaborative
workspace that centers these practices. It supports reflexivity by integrating
in-situ reflection prompts, makes code evolution transparent and tangible, and
scaffolds collaborative interpretation by turning differences into productive,
positionality-aware dialogue. Results from our paired-analyst study (N=12)
indicate that Reflexis encouraged participants toward more granular reflection
and reframed disagreements as productive conversations. The evaluation also
surfaced key design tensions, including a desire for higher-level, networked
memos and more user control over the timing of proactive alerts. Reflexis
contributes a design framework for tools that prioritize rigor and transparency
to support deep, collaborative interpretation in an age of automation.
\\ ( https://arxiv.org/abs/2601.15445 ,  12147kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15457 (*cross-listing*)
Date: Wed, 21 Jan 2026 20:52:48 GMT   (135kb)

Title: Chunking, Retrieval, and Re-ranking: An Empirical Evaluation of RAG
  Architectures for Policy Document Question Answering
Authors: Anuj Maharjan and Umesh Yadav
Categories: cs.CL cs.AI cs.IR
\\
  The integration of Large Language Models (LLMs) into the public health policy
sector offers a transformative approach to navigating the vast repositories of
regulatory guidance maintained by agencies such as the Centers for Disease
Control and Prevention (CDC). However, the propensity for LLMs to generate
hallucinations, defined as plausible but factually incorrect assertions,
presents a critical barrier to the adoption of these technologies in
high-stakes environments where information integrity is non-negotiable. This
empirical evaluation explores the effectiveness of Retrieval-Augmented
Generation (RAG) architectures in mitigating these risks by grounding
generative outputs in authoritative document context. Specifically, this study
compares a baseline Vanilla LLM against Basic RAG and Advanced RAG pipelines
utilizing cross-encoder re-ranking. The experimental framework employs a
Mistral-7B-Instruct-v0.2 model and an all-MiniLM-L6-v2 embedding model to
process a corpus of official CDC policy analytical frameworks and guidance
documents. The analysis measures the impact of two distinct chunking
strategies, recursive character-based and token-based semantic splitting, on
system accuracy, measured through faithfulness and relevance scores across a
curated set of complex policy scenarios. Quantitative findings indicate that
while Basic RAG architectures provide a substantial improvement in faithfulness
(0.621) over Vanilla baselines (0.347), the Advanced RAG configuration achieves
a superior faithfulness average of 0.797. These results demonstrate that
two-stage retrieval mechanisms are essential for achieving the precision
required for domain-specific policy question answering, though structural
constraints in document segmentation remain a significant bottleneck for
multi-step reasoning tasks.
\\ ( https://arxiv.org/abs/2601.15457 ,  135kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15479 (*cross-listing*)
Date: Wed, 21 Jan 2026 21:29:46 GMT   (707kb)

Title: Benchmarking LLMs for Pairwise Causal Discovery in Biomedical and
  Multi-Domain Contexts
Authors: Sydney Anuyah, Sneha Shajee-Mohan, Ankit-Singh Chauhan, Sunandan
  Chakraborty
Categories: cs.CL cs.AI
\\
  The safe deployment of large language models (LLMs) in high-stakes fields
like biomedicine, requires them to be able to reason about cause and effect. We
investigate this ability by testing 13 open-source LLMs on a fundamental task:
pairwise causal discovery (PCD) from text. Our benchmark, using 12 diverse
datasets, evaluates two core skills: 1) \textbf{Causal Detection} (identifying
if a text contains a causal link) and 2) \textbf{Causal Extraction} (pulling
out the exact cause and effect phrases). We tested various prompting methods,
from simple instructions (zero-shot) to more complex strategies like
Chain-of-Thought (CoT) and Few-shot In-Context Learning (FICL).
  The results show major deficiencies in current models. The best model for
detection, DeepSeek-R1-Distill-Llama-70B, only achieved a mean score of 49.57\%
($C_{detect}$), while the best for extraction, Qwen2.5-Coder-32B-Instruct,
reached just 47.12\% ($C_{extract}$). Models performed best on simple,
explicit, single-sentence relations. However, performance plummeted for more
difficult (and realistic) cases, such as implicit relationships, links spanning
multiple sentences, and texts containing multiple causal pairs. We provide a
unified evaluation framework, built on a dataset validated with high
inter-annotator agreement ($\kappa \ge 0.758$), and make all our data, code,
and prompts publicly available to spur further research.
\href{https://github.com/sydneyanuyah/CausalDiscovery}{Code available here:
https://github.com/sydneyanuyah/CausalDiscovery}
\\ ( https://arxiv.org/abs/2601.15479 ,  707kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15484 (*cross-listing*)
Date: Wed, 21 Jan 2026 21:36:12 GMT   (207kb)

Title: Is Grokipedia Right-Leaning? Comparing Political Framing in Wikipedia
  and Grokipedia on Controversial Topics
Authors: Philipp Eibl, Erica Coppolillo, Simone Mungari and Luca Luceri
Categories: cs.IR cs.AI
\\
  Online encyclopedias are central to contemporary information infrastructures
and have become focal points of debates over ideological bias. Wikipedia, in
particular, has long been accused of left-leaning bias, while Grokipedia, an
AI-generated encyclopedia launched by xAI, has been framed as a right-leaning
alternative. This paper presents a comparative analysis of Wikipedia and
Grokipedia on well-established politically contested topics. Specifically, we
examine differences in semantic framing, political orientation, and content
prioritization. We find that semantic similarity between the two platforms
decays across article sections and diverges more strongly on controversial
topics than on randomly sampled ones. Additionally, we show that both
encyclopedias predominantly exhibit left-leaning framings, although Grokipedia
exhibits a more bimodal distribution with increased prominence of right-leaning
content. The experimental code is publicly available.
\\ ( https://arxiv.org/abs/2601.15484 ,  207kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15485 (*cross-listing*)
Date: Wed, 21 Jan 2026 21:37:08 GMT   (5749kb)

Title: The Rise of Large Language Models and the Direction and Impact of US
  Federal Research Funding
Authors: Yifan Qian, Zhe Wen, Alexander C. Furnas, Yue Bai, Erzhuo Shao, Dashun
  Wang
Categories: cs.DL cs.AI cs.CY physics.soc-ph
Comments: 41 pages, 23 figures, 12 tables
ACM-class: I.2; J.4
\\
  Federal research funding shapes the direction, diversity, and impact of the
US scientific enterprise. Large language models (LLMs) are rapidly diffusing
into scientific practice, holding substantial promise while raising widespread
concerns. Despite growing attention to AI use in scientific writing and
evaluation, little is known about how the rise of LLMs is reshaping the public
funding landscape. Here, we examine LLM involvement at key stages of the
federal funding pipeline by combining two complementary data sources:
confidential National Science Foundation (NSF) and National Institutes of
Health (NIH) proposal submissions from two large US R1 universities, including
funded, unfunded, and pending proposals, and the full population of publicly
released NSF and NIH awards. We find that LLM use rises sharply beginning in
2023 and exhibits a bimodal distribution, indicating a clear split between
minimal and substantive use. Across both private submissions and public awards,
higher LLM involvement is consistently associated with lower semantic
distinctiveness, positioning projects closer to recently funded work within the
same agency. The consequences of this shift are agency-dependent. LLM use is
positively associated with proposal success and higher subsequent publication
output at NIH, whereas no comparable associations are observed at NSF. Notably,
the productivity gains at NIH are concentrated in non-hit papers rather than
the most highly cited work. Together, these findings provide large-scale
evidence that the rise of LLMs is reshaping how scientific ideas are
positioned, selected, and translated into publicly funded research, with
implications for portfolio governance, research diversity, and the long-run
impact of science.
\\ ( https://arxiv.org/abs/2601.15485 ,  5749kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15488 (*cross-listing*)
Date: Wed, 21 Jan 2026 21:40:58 GMT   (314kb)

Title: Multi-Persona Thinking for Bias Mitigation in Large Language Models
Authors: Yuxing Chen, Guoqing Luo, Zijun Wu, Lili Mou
Categories: cs.CL cs.AI
Comments: 13 pages, 3 figures
\\
  Large Language Models (LLMs) exhibit significant social biases that can
perpetuate harmful stereotypes and unfair outcomes. In this paper, we propose
Multi-Persona Thinking (MPT), a novel inference-time framework that leverages
dialectical reasoning from multiple perspectives to reduce bias. MPT guides
models to adopt contrasting social identities (e.g., male and female) along
with a neutral viewpoint, and then engages these personas iteratively to expose
and correct biases. Through a dialectical reasoning process, the framework
transforms the potential weakness of persona assignment into a strength for
bias mitigation. We evaluate MPT on two widely used bias benchmarks across both
open-source and closed-source models of varying scales. Our results demonstrate
substantial improvements over existing prompting-based strategies: MPT achieves
the lowest bias while maintaining core reasoning ability.
\\ ( https://arxiv.org/abs/2601.15488 ,  314kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15500 (*cross-listing*)
Date: Wed, 21 Jan 2026 22:09:27 GMT   (7148kb)

Title: Low-Dimensional Adaptation of Rectified Flow: A New Perspective through
  the Lens of Diffusion and Stochastic Localization
Authors: Saptarshi Roy, Alessandro Rinaldo, Purnamrita Sarkar
Categories: stat.ML cs.AI cs.LG math.ST stat.TH
Comments: 32 pages, 7 figures
\\
  In recent years, Rectified flow (RF) has gained considerable popularity
largely due to its generation efficiency and state-of-the-art performance. In
this paper, we investigate the degree to which RF automatically adapts to the
intrinsic low dimensionality of the support of the target distribution to
accelerate sampling. We show that, using a carefully designed choice of the
time-discretization scheme and with sufficiently accurate drift estimates, the
RF sampler enjoys an iteration complexity of order $O(k/\varepsilon)$ (up to
log factors), where $\varepsilon$ is the precision in total variation distance
and $k$ is the intrinsic dimension of
  the target distribution. In addition, we show that the denoising diffusion
probabilistic model (DDPM) procedure is equivalent to a stochastic version of
RF by establishing a novel connection between these processes and stochastic
localization. Building on this connection, we further design a stochastic RF
sampler that also adapts to the low-dimensionality of the target distribution
under milder requirements on the accuracy of the drift estimates, and also with
a specific time schedule. We illustrate with simulations on the synthetic data
and text-to-image data experiments the improved performance of the proposed
samplers implementing the newly designed time-discretization schedules.
\\ ( https://arxiv.org/abs/2601.15500 ,  7148kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15539 (*cross-listing*)
Date: Wed, 21 Jan 2026 23:48:59 GMT   (358kb)

Title: A Machine Vision Approach to Preliminary Skin Lesion Assessments
Authors: Ali Khreis, Ro'Yah Radaideh, Quinn McGill
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: 6 pages, 2 figures, 2 tables
ACM-class: I.4.6; I.4.8; I.5.4
\\
  Early detection of malignant skin lesions is critical for improving patient
outcomes in aggressive, metastatic skin cancers. This study evaluates a
comprehensive system for preliminary skin lesion assessment that combines the
clinically established ABCD rule of dermoscopy (analyzing Asymmetry, Borders,
Color, and Dermoscopic Structures) with machine learning classification. Using
a 1,000-image subset of the HAM10000 dataset, the system implements an
automated, rule-based pipeline to compute a Total Dermoscopy Score (TDS) for
each lesion. This handcrafted approach is compared against various machine
learning solutions, including traditional classifiers (Logistic Regression,
Random Forest, and SVM) and deep learning models. While the rule-based system
provides high clinical interpretability, results indicate a performance
bottleneck when reducing complex morphology to five numerical features.
Experimental findings show that transfer learning with EfficientNet-B0 failed
significantly due to domain shift between natural and medical images. In
contrast, a custom three-layer Convolutional Neural Network (CNN) trained from
scratch achieved 78.5% accuracy and 86.5% recall on median-filtered images,
representing a 19-point accuracy improvement over traditional methods. The
results demonstrate that direct pixel-level learning captures diagnostic
patterns beyond handcrafted features and that purpose-built lightweight
architectures can outperform large pretrained models for small, domain-specific
medical datasets.
\\ ( https://arxiv.org/abs/2601.15539 ,  358kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15549 (*cross-listing*)
Date: Thu, 22 Jan 2026 00:35:30 GMT   (2111kb)

Title: VIOLA: Towards Video In-Context Learning with Minimal Annotations
Authors: Ryo Fujii, Hideo Saito, Ryo Hachiuma
Categories: cs.CV cs.AI
\\
  Generalizing Multimodal Large Language Models (MLLMs) to novel video domains
is essential for real-world deployment but remains challenging due to the
scarcity of labeled data. While In-Context Learning (ICL) offers a
training-free adaptation path, standard methods rely on large annotated pools,
which are often impractical in specialized environments like industrial or
surgical settings since they require the experts' annotations. To bridge this
gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a
label-efficient framework that synergizes minimal expert supervision with
abundant unlabeled data. First, to maximize the efficiency of a strict
annotation budget, we propose density-uncertainty-weighted sampling. Unlike
standard diversity or uncertainty strategies that risk selecting visual
outliers, our method leverages density estimation to identify samples that are
simultaneously diverse, representative, and informative. Second, to utilize the
remaining unlabeled data without noise propagation, we construct a hybrid pool
and introduce confidence-aware retrieval and confidence-aware prompting. These
mechanisms explicitly model label reliability, retrieving demonstrations based
on a composite score of similarity and confidence while enabling the MLLM to
adaptively distinguish between verified ground truths and noisy pseudo-labels.
Extensive experiments across nine diverse benchmarks using four MLLMs
demonstrate that our framework significantly outperforms various baselines in
low-resource settings, achieving robust adaptation with minimal annotation
costs.
\\ ( https://arxiv.org/abs/2601.15549 ,  2111kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15575 (*cross-listing*)
Date: Thu, 22 Jan 2026 01:44:51 GMT   (587kb)

Title: PromptHelper: A Prompt Recommender System for Encouraging Creativity in
  AI Chatbot Interactions
Authors: Jason Kim, Maria Teleki, James Caverlee
Categories: cs.HC cs.AI cs.IR
\\
  Prompting is central to interaction with AI systems, yet many users struggle
to explore alternative directions, articulate creative intent, or understand
how variations in prompts shape model outputs. We introduce prompt recommender
systems (PRS) as an interaction approach that supports exploration, suggesting
contextually relevant follow-up prompts. We present PromptHelper, a PRS
prototype integrated into an AI chatbot that surfaces semantically diverse
prompt suggestions while users work on real writing tasks. We evaluate
PromptHelper in a 2x2 fully within-subjects study (N=32) across creative and
academic writing tasks. Results show that PromptHelper significantly increases
users' perceived exploration and expressiveness without increasing cognitive
workload. Qualitative findings illustrate how prompt recommendations help users
branch into new directions, overcome uncertainty about what to ask next, and
better articulate their intent. We discuss implications for designing AI
interfaces that scaffold exploratory interaction while preserving user agency,
and release open-source resources to support research on prompt recommendation.
\\ ( https://arxiv.org/abs/2601.15575 ,  587kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15578 (*cross-listing*)
Date: Thu, 22 Jan 2026 01:57:48 GMT   (2025kb)

Title: MapViT: A Two-Stage ViT-Based Framework for Real-Time Radio Quality Map
  Prediction in Dynamic Environments
Authors: Cyril Shih-Huan Hsu, Xi Li, Lanfranco Zanzi, Zhiheng Yang, Chrysa
  Papagianni, Xavier Costa P\'erez
Categories: cs.NI cs.AI cs.LG cs.RO
Comments: This paper has been accepted for publication at IEEE International
  Conference on Communications (ICC) 2026
\\
  Recent advancements in mobile and wireless networks are unlocking the full
potential of robotic autonomy, enabling robots to take advantage of ultra-low
latency, high data throughput, and ubiquitous connectivity. However, for robots
to navigate and operate seamlessly, efficiently and reliably, they must have an
accurate understanding of both their surrounding environment and the quality of
radio signals. Achieving this in highly dynamic and ever-changing environments
remains a challenging and largely unsolved problem. In this paper, we introduce
MapViT, a two-stage Vision Transformer (ViT)-based framework inspired by the
success of pre-train and fine-tune paradigm for Large Language Models (LLMs).
MapViT is designed to predict both environmental changes and expected radio
signal quality. We evaluate the framework using a set of representative Machine
Learning (ML) models, analyzing their respective strengths and limitations
across different scenarios. Experimental results demonstrate that the proposed
two-stage pipeline enables real-time prediction, with the ViT-based
implementation achieving a strong balance between accuracy and computational
efficiency. This makes MapViT a promising solution for energy- and
resource-constrained platforms such as mobile robots. Moreover, the geometry
foundation model derived from the self-supervised pre-training stage improves
data efficiency and transferability, enabling effective downstream predictions
even with limited labeled data. Overall, this work lays the foundation for
next-generation digital twin ecosystems, and it paves the way for a new class
of ML foundation models driving multi-modal intelligence in future 6G-enabled
systems.
\\ ( https://arxiv.org/abs/2601.15578 ,  2025kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15593 (*cross-listing*)
Date: Thu, 22 Jan 2026 02:39:36 GMT   (4766kb)

Title: Parallelism and Generation Order in Masked Diffusion Language Models:
  Limits Today, Potential Tomorrow
Authors: Yangyang Zhong, Yanmei Gu, Zhengqing Zang, Xiaomeng Li, Yuqi Ding,
  Xibei Jia, Yuting Shen, Zhenzhong Lan, Liwang Zhu, Weiping Liu, Junlin Zhou,
  Haisheng Liu, Zhong Xin Yu, Pengxin Luo, Donglian Qi, Yunfeng Yan, Junbo Zhao
Categories: cs.CL cs.AI cs.LG
\\
  Masked Diffusion Language Models (MDLMs) promise parallel token generation
and arbitrary-order decoding, yet it remains unclear to what extent current
models truly realize these capabilities. We characterize MDLM behavior along
two dimensions -- parallelism strength and generation order -- using Average
Finalization Parallelism (AFP) and Kendall's tau. We evaluate eight mainstream
MDLMs (up to 100B parameters) on 58 benchmarks spanning knowledge, reasoning,
and programming. The results show that MDLMs still lag behind comparably sized
autoregressive models, mainly because parallel probabilistic modeling weakens
inter-token dependencies. Meanwhile, MDLMs exhibit adaptive decoding behavior:
their parallelism and generation order vary significantly with the task domain,
the stage of reasoning, and whether the output is correct. On tasks that
require "backward information" (e.g., Sudoku), MDLMs adopt a solution order
that tends to fill easier Sudoku blanks first, highlighting their advantages.
Finally, we provide theoretical motivation and design insights supporting a
Generate-then-Edit paradigm, which mitigates dependency loss while retaining
the efficiency of parallel decoding.
\\ ( https://arxiv.org/abs/2601.15593 ,  4766kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15595 (*cross-listing*)
Date: Thu, 22 Jan 2026 02:43:12 GMT   (682kb)

Title: Data-Free Privacy-Preserving for LLMs via Model Inversion and Selective
  Unlearning
Authors: Xinjie Zhou, Zhihui Yang, Lechao Cheng, Sai Wu and Gang Chen
Categories: cs.CR cs.AI cs.LG
\\
  Large language models (LLMs) exhibit powerful capabilities but risk
memorizing sensitive personally identifiable information (PII) from their
training data, posing significant privacy concerns. While machine unlearning
techniques aim to remove such data, they predominantly depend on access to the
training data. This requirement is often impractical, as training data in
real-world deployments is commonly proprietary or inaccessible. To address this
limitation, we propose Data-Free Selective Unlearning (DFSU), a novel
privacy-preserving framework that removes sensitive PII from an LLM without
requiring its training data. Our approach first synthesizes pseudo-PII through
language model inversion, then constructs token-level privacy masks for these
synthetic samples, and finally performs token-level selective unlearning via a
contrastive mask loss within a low-rank adaptation (LoRA) subspace. Extensive
experiments on the AI4Privacy PII-Masking dataset using Pythia models
demonstrate that our method effectively removes target PII while maintaining
model utility.
\\ ( https://arxiv.org/abs/2601.15595 ,  682kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15596 (*cross-listing*)
Date: Thu, 22 Jan 2026 02:44:22 GMT   (1571kb)

Title: DeepASMR: LLM-Based Zero-Shot ASMR Speech Generation for Anyone of Any
  Voice
Authors: Leying Zhang, Tingxiao Zhou, Haiyang Sun, Mengxiao Bi, Yanmin Qian
Categories: cs.SD cs.AI eess.AS
\\
  While modern Text-to-Speech (TTS) systems achieve high fidelity for
read-style speech, they struggle to generate Autonomous Sensory Meridian
Response (ASMR), a specialized, low-intensity speech style essential for
relaxation. The inherent challenges include ASMR's subtle, often unvoiced
characteristics and the demand for zero-shot speaker adaptation. In this paper,
we introduce DeepASMR, the first framework designed for zero-shot ASMR
generation. We demonstrate that a single short snippet of a speaker's ordinary,
read-style speech is sufficient to synthesize high-fidelity ASMR in their
voice, eliminating the need for whispered training data from the target
speaker. Methodologically, we first identify that discrete speech tokens
provide a soft factorization of ASMR style from speaker timbre. Leveraging this
insight, we propose a two-stage pipeline incorporating a Large Language Model
(LLM) for content-style encoding and a flow-matching acoustic decoder for
timbre reconstruction. Furthermore, we contribute DeepASMR-DB, a comprehensive
670-hour English-Chinese multi-speaker ASMR speech corpus, and introduce a
novel evaluation protocol integrating objective metrics, human listening tests,
LLM-based scoring and unvoiced speech analysis. Extensive experiments confirm
that DeepASMR achieves state-of-the-art naturalness and style fidelity in ASMR
generation for anyone of any voice, while maintaining competitive performance
on normal speech synthesis.
\\ ( https://arxiv.org/abs/2601.15596 ,  1571kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15626 (*cross-listing*)
Date: Thu, 22 Jan 2026 03:57:47 GMT   (409kb)

Title: Bridging Qualitative Rubrics and AI: A Binary Question Framework for
  Criterion-Referenced Grading in Engineering
Authors: Lili Chen, Winn Wing-Yiu Chow, Stella Peng, Bencheng Fan and Sachitha
  Bandara
Categories: eess.SY cs.AI cs.SY
Comments: Proceedings of the 36th Annual Conference of the Australasian
  Association for Engineering Education (AAEE 2025)
\\
  PURPOSE OR GOAL: This study investigates how GenAI can be integrated with a
criterion-referenced grading framework to improve the efficiency and quality of
grading for mathematical assessments in engineering. It specifically explores
the challenges demonstrators face with manual, model solution-based grading and
how a GenAI-supported system can be designed to reliably identify student
errors, provide high-quality feedback, and support human graders. The research
also examines human graders' perceptions of the effectiveness of this
GenAI-assisted approach. ACTUAL OR ANTICIPATED OUTCOMES: The study found that
GenAI achieved an overall grading accuracy of 92.5%, comparable to two
experienced human graders. The two researchers, who also served as subject
demonstrators, perceived the GenAI as a helpful second reviewer that improved
accuracy by catching small errors and provided more complete feedback than they
could manually. A central outcome was the significant enhancement of formative
feedback. However, they noted the GenAI tool is not yet reliable enough for
autonomous use, especially with unconventional solutions.
CONCLUSIONS/RECOMMENDATIONS/SUMMARY: This study demonstrates that GenAI, when
paired with a structured, criterion-referenced framework using binary
questions, can grade engineering mathematical assessments with an accuracy
comparable to human experts. Its primary contribution is a novel methodological
approach that embeds the generation of high-quality, scalable formative
feedback directly into the assessment workflow. Future work should investigate
student perceptions of GenAI grading and feedback.
\\ ( https://arxiv.org/abs/2601.15626 ,  409kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15655 (*cross-listing*)
Date: Thu, 22 Jan 2026 05:05:53 GMT   (3711kb)

Title: Event-VStream: Event-Driven Real-Time Understanding for Long Video
  Streams
Authors: Zhenghui Guo, Yuanbin Man, Junyuan Sheng, Bowen Lin, Ahmed Ahmed, Bo
  Jiang, Boyuan Zhang, Miao Yin, Sian Jin, Omprakash Gnawal, Chengming Zhang
Categories: cs.CV cs.AI
\\
  Real-time understanding of long video streams remains challenging for
multimodal large language models (VLMs) due to redundant frame processing and
rapid forgetting of past context. Existing streaming systems rely on
fixed-interval decoding or cache pruning, which either produce repetitive
outputs or discard crucial temporal information. We introduce Event-VStream, an
event-aware framework that represents continuous video as a sequence of
discrete, semantically coherent events. Our system detects meaningful state
transitions by integrating motion, semantic, and predictive cues, and triggers
language generation only at those boundaries. Each event embedding is
consolidated into a persistent memory bank, enabling long-horizon reasoning
while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D
evaluations, Event-VStream achieves competitive performance. It improves over a
VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves
performance close to Flash-VStream-7B despite using only a general-purpose
LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour
Ego4D streams.
\\ ( https://arxiv.org/abs/2601.15655 ,  3711kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15663 (*cross-listing*)
Date: Thu, 22 Jan 2026 05:23:19 GMT   (2072kb)

Title: TempoNet: Learning Realistic Communication and Timing Patterns for
  Network Traffic Simulation
Authors: Kristen Moore, Diksha Goel, Cody James Christopher, Zhen Wang, Minjune
  Kim, Ahmed Ibrahim, Ahmad Mohsin, Seyit Camtepe
Categories: cs.CR cs.AI cs.LG
DOI: 10.1109/ACSAC67867.2025.00097
\\
  Realistic network traffic simulation is critical for evaluating intrusion
detection systems, stress-testing network protocols, and constructing
high-fidelity environments for cybersecurity training. While attack traffic can
often be layered into training environments using red-teaming or replay
methods, generating authentic benign background traffic remains a core
challenge -- particularly in simulating the complex temporal and communication
dynamics of real-world networks. This paper introduces TempoNet, a novel
generative model that combines multi-task learning with multi-mark temporal
point processes to jointly model inter-arrival times and all packet- and
flow-header fields. TempoNet captures fine-grained timing patterns and
higher-order correlations such as host-pair behavior and seasonal trends,
addressing key limitations of GAN-, LLM-, and Bayesian-based methods that fail
to reproduce structured temporal variation. TempoNet produces temporally
consistent, high-fidelity traces, validated on real-world datasets.
Furthermore, we show that intrusion detection models trained on
TempoNet-generated background traffic perform comparably to those trained on
real data, validating its utility for real-world security applications.
\\ ( https://arxiv.org/abs/2601.15663 ,  2072kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15664 (*cross-listing*)
Date: Thu, 22 Jan 2026 05:23:20 GMT   (37054kb)

Title: Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence
  Modeling
Authors: Hongyang Wei, Hongbo Liu, Zidong Wang, Yi Peng, Baixin Xu, Size Wu,
  Xuying Zhang, Xianglong He, Zexiang Liu, Peiyu Wang, Xuchen Song, Yangguang
  Li, Yang Liu, Yahui Zhou
Categories: cs.CV cs.AI
\\
  The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores
the community's strong interest in multi-image composition tasks. Compared to
single-image editing, multi-image composition presents significantly greater
challenges in terms of consistency and quality, yet existing models have not
disclosed specific methodological details for achieving high-quality fusion.
Through statistical analysis, we identify Human-Object Interaction (HOI) as the
most sought-after category by the community. We therefore systematically
analyze and implement a state-of-the-art solution for multi-image composition
with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a
unified multimodal framework that integrates single-image editing and
multi-image composition. Our model supports an arbitrary (1~6) number and
resolution of input images, as well as arbitrary output resolutions (within a
total pixel budget of 1024x1024). To address the challenges of multi-image
composition, we design a comprehensive data collection, filtering, and
synthesis pipeline, achieving strong performance with only 700K high-quality
training samples. Furthermore, we introduce a novel training paradigm that
formulates multi-image composition as a sequence-modeling problem, transforming
conditional generation into unified sequence synthesis. To accelerate
inference, we integrate trajectory mapping and distribution matching into the
post-training stage, enabling the model to produce high-fidelity samples in
just 8 steps and achieve a 12.5x speedup over standard synthesis sampling.
Skywork UniPic 3.0 achieves state-of-the-art performance on single-image
editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on
multi-image composition benchmark, thereby validating the effectiveness of our
data pipeline and training paradigm. Code, models and dataset are publicly
available.
\\ ( https://arxiv.org/abs/2601.15664 ,  37054kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15671 (*cross-listing*)
Date: Thu, 22 Jan 2026 05:53:05 GMT   (8072kb)

Title: StreetDesignAI: A Multi-Persona Evaluation System for Inclusive
  Infrastructure Design
Authors: Ziyi Wang, Yilong Dai, Duanya Lyu, Mateo Nader, Sihan Chen, Wanghao
  Ye, Zjian Ding, Xiang Yan
Categories: cs.HC cs.AI
\\
  Designing inclusive cycling infrastructure requires balancing competing needs
of diverse user groups, yet designers often struggle to anticipate how
different cyclists experience the same street. We investigate how persona-based
multi-agent evaluation can support inclusive design by making experiential
conflicts explicit. We present StreetDesignAI, an interactive system that
enables designers to (1) ground evaluation in street context through imagery
and map data, (2) receive parallel feedback from cyclist personas spanning
confident to cautious users, and (3) iteratively modify designs while surfacing
conflicts across perspectives. A within-subjects study with 26 transportation
professionals demonstrates that structured multi-perspective feedback
significantly improves designers' understanding of diverse user perspectives,
ability to identify persona needs, and confidence in translating them into
design decisions, with higher satisfaction and stronger intention for
professional adoption. Qualitative findings reveal how conflict surfacing
transforms design exploration from single-perspective optimization toward
deliberate trade-off reasoning. We discuss implications for AI tools that
scaffold inclusive design through disagreement as an interaction primitive.
\\ ( https://arxiv.org/abs/2601.15671 ,  8072kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15673 (*cross-listing*)
Date: Thu, 22 Jan 2026 05:55:21 GMT   (355kb)

Title: Enhancing guidance for missing data in diffusion-based sequential
  recommendation
Authors: Qilong Yan, Yifei Xing, Dugang Liu, Jingpu Duan, Jian Yin
Categories: cs.IR cs.AI
Comments: ICASSP 2026 accecpted
\\
  Contemporary sequential recommendation methods are becoming more complex,
shifting from classification to a diffusion-guided generative paradigm.
However, the quality of guidance in the form of user information is often
compromised by missing data in the observed sequences, leading to suboptimal
generation quality. Existing methods address this by removing locally similar
items, but overlook ``critical turning points'' in user interest, which are
crucial for accurately predicting subsequent user intent. To address this, we
propose a novel Counterfactual Attention Regulation Diffusion model (CARD),
which focuses on amplifying the signal from key interest-turning-point items
while concurrently identifying and suppressing noise within the user sequence.
CARD consists of (1) a Dual-side Thompson Sampling method to identify sequences
undergoing significant interest shift, and (2) a counterfactual attention
mechanism for these sequences to quantify the importance of each item. In this
manner, CARD provides the diffusion model with a high-quality guidance signal
composed of dynamically re-weighted interaction vectors to enable effective
generation. Experiments show our method works well on real-world data without
being computationally expensive. Our code is available at
https://github.com/yanqilong3321/CARD.
\\ ( https://arxiv.org/abs/2601.15673 ,  355kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15678 (*cross-listing*)
Date: Thu, 22 Jan 2026 05:59:42 GMT   (5699kb)

Title: Connect the Dots: Knowledge Graph-Guided Crawler Attack on
  Retrieval-Augmented Generation Systems
Authors: Mengyu Yao, Ziqi Zhang, Ning Luo, Shaofei Li, Yifeng Cai, Xiangqun
  Chen, Yao Guo and Ding Li
Categories: cs.CR cs.AI cs.IR cs.LG
\\
  Retrieval-augmented generation (RAG) systems integrate document retrieval
with large language models and have been widely adopted. However, in
privacy-related scenarios, RAG introduces a new privacy risk: adversaries can
issue carefully crafted queries to exfiltrate sensitive content from the
underlying corpus gradually. Although recent studies have demonstrated
multi-turn extraction attacks, they rely on heuristics and fail to perform
long-term extraction planning. To address these limitations, we formulate the
RAG extraction attack as an adaptive stochastic coverage problem (ASCP). In
ASCP, each query is treated as a probabilistic action that aims to maximize
conditional marginal gain (CMG), enabling principled long-term planning under
uncertainty. However, integrating ASCP with practical RAG attack faces three
key challenges: unobservable CMG, intractability in the action space, and
feasibility constraints. To overcome these challenges, we maintain a global
attacker-side state to guide the attack. Building on this idea, we introduce
RAGCRAWLER, which builds a knowledge graph to represent revealed information,
uses this global state to estimate CMG, and plans queries in semantic space
that target unretrieved regions. In comprehensive experiments across diverse
RAG architectures and datasets, our proposed method, RAGCRAWLER, consistently
outperforms all baselines. It achieves up to 84.4% corpus coverage within a
fixed query budget and deliver an average improvement of 20.7% over the
top-performing baseline. It also maintains high semantic fidelity and strong
content reconstruction accuracy with low attack cost. Crucially, RAGCRAWLER
proves its robustness by maintaining effectiveness against advanced RAG systems
employing query rewriting and multi-query retrieval strategies. Our work
reveals significant security gaps and highlights the pressing need for stronger
safeguards for RAG.
\\ ( https://arxiv.org/abs/2601.15678 ,  5699kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15698 (*cross-listing*)
Date: Thu, 22 Jan 2026 06:56:27 GMT   (36845kb)

Title: Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for
  Harmful Image Generation via Semantic-Agnostic Inputs
Authors: Mingyu Yu, Lana Liu, Zhehao Zhao, Wei Wang, Sujuan Qin
Categories: cs.CV cs.AI
\\
  The rapid advancement of Multimodal Large Language Models (MLLMs) has
introduced complex security challenges, particularly at the intersection of
textual and visual safety. While existing schemes have explored the security
vulnerabilities of MLLMs, the investigation into their visual safety boundaries
remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a
novel image-text pair jailbreaking framework specifically designed to probe the
visual safety boundaries of MLLMs. BVS employs a
"reconstruction-then-generation" strategy, leveraging neutralized visual
splicing and inductive recomposition to decouple malicious intent from raw
inputs, thereby leading MLLMs to be induced into generating harmful images.
Experimental results demonstrate that BVS achieves a remarkable jailbreak
success rate of 98.21\% against GPT-5 (12 January 2026 release). Our findings
expose critical vulnerabilities in the visual safety alignment of current
MLLMs.
\\ ( https://arxiv.org/abs/2601.15698 ,  36845kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15710 (*cross-listing*)
Date: Thu, 22 Jan 2026 07:31:51 GMT   (1664kb)

Title: FlexLLM: Composable HLS Library for Flexible Hybrid LLM Accelerator
  Design
Authors: Jiahao Zhang, Zifan He, Nicholas Fraser, Michaela Blott, Yizhou Sun,
  Jason Cong
Categories: cs.AR cs.AI cs.LG
\\
  We present FlexLLM, a composable High-Level Synthesis (HLS) library for rapid
development of domain-specific LLM accelerators. FlexLLM exposes key
architectural degrees of freedom for stage-customized inference, enabling
hybrid designs that tailor temporal reuse and spatial dataflow differently for
prefill and decode, and provides a comprehensive quantization suite to support
accurate low-bit deployment. Using FlexLLM, we build a complete inference
system for the Llama-3.2 1B model in under two months with only 1K lines of
code. The system includes: (1) a stage-customized accelerator with
hardware-efficient quantization (12.68 WikiText-2 PPL) surpassing SpinQuant
baseline, and (2) a Hierarchical Memory Transformer (HMT) plug-in for efficient
long-context processing. On the AMD U280 FPGA at 16nm, the accelerator achieves
1.29$\times$ end-to-end speedup, 1.64$\times$ higher decode throughput, and
3.14$\times$ better energy efficiency than an NVIDIA A100 GPU (7nm) running
BF16 inference; projected results on the V80 FPGA at 7nm reach 4.71$\times$,
6.55$\times$, and 4.13$\times$, respectively. In long-context scenarios,
integrating the HMT plug-in reduces prefill latency by 23.23$\times$ and
extends the context window by 64$\times$, delivering 1.10$\times$/4.86$\times$
lower end-to-end latency and 5.21$\times$/6.27$\times$ higher energy efficiency
on the U280/V80 compared to the A100 baseline. FlexLLM thus bridges algorithmic
innovation in LLM inference and high-performance accelerators with minimal
manual effort.
\\ ( https://arxiv.org/abs/2601.15710 ,  1664kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15715 (*cross-listing*)
Date: Thu, 22 Jan 2026 07:36:48 GMT   (503kb)

Title: Dancing in Chains: Strategic Persuasion in Academic Rebuttal via Theory
  of Mind
Authors: Zhitao He, Zongwei Lyu, Yi R Fung
Categories: cs.CL cs.AI
Comments: Preprint, under review
\\
  Although artificial intelligence (AI) has become deeply integrated into
various stages of the research workflow and achieved remarkable advancements,
academic rebuttal remains a significant and underexplored challenge. This is
because rebuttal is a complex process of strategic communication under severe
information asymmetry rather than a simple technical debate. Consequently,
current approaches struggle as they largely imitate surface-level linguistics,
missing the essential element of perspective-taking required for effective
persuasion. In this paper, we introduce RebuttalAgent, the first framework to
ground academic rebuttal in Theory of Mind (ToM), operationalized through a
ToM-Strategy-Response (TSR) pipeline that models reviewer mental state,
formulates persuasion strategy, and generates strategy-grounded response. To
train our agent, we construct RebuttalBench, a large-scale dataset synthesized
via a novel critique-and-refine approach. Our training process consists of two
stages, beginning with a supervised fine-tuning phase to equip the agent with
ToM-based analysis and strategic planning capabilities, followed by a
reinforcement learning phase leveraging the self-reward mechanism for scalable
self-improvement. For reliable and efficient automated evaluation, we further
develop Rebuttal-RM, a specialized evaluator trained on over 100K samples of
multi-source rebuttal data, which achieves scoring consistency with human
preferences surpassing powerful judge GPT-4.1. Extensive experiments show
RebuttalAgent significantly outperforms the base model by an average of 18.3%
on automated metrics, while also outperforming advanced proprietary models
across both automated and human evaluations. Disclaimer: the generated rebuttal
content is for reference only to inspire authors and assist in drafting. It is
not intended to replace the author's own critical analysis and response.
\\ ( https://arxiv.org/abs/2601.15715 ,  503kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15721 (*cross-listing*)
Date: Thu, 22 Jan 2026 07:46:18 GMT   (1014kb)

Title: CoNRec: Context-Discerning Negative Recommendation with LLMs
Authors: Xinda Chen, Jiawei Wu, Yishuang Liu, Jialin Zhu, Shuwen Xiao, Junjun
  Zheng, Xiangheng Kong and Yuning Jiang
Categories: cs.IR cs.AI
\\
  Understanding what users like is relatively straightforward; understanding
what users dislike, however, remains a challenging and underexplored problem.
Research into users' negative preferences has gained increasing importance in
modern recommendation systems. Numerous platforms have introduced explicit
negative feedback mechanisms and leverage such signals to refine their
recommendation models. Beyond traditional business metrics, user
experience-driven metrics, such as negative feedback rates, have become
critical indicators for evaluating system performance. However, most existing
approaches primarily use negative feedback as an auxiliary signal to enhance
positive recommendations, paying little attention to directly modeling negative
interests, which can be highly valuable in offline applications. Moreover, due
to the inherent sparsity of negative feedback data, models often suffer from
context understanding biases induced by positive feedback dominance. To address
these challenges, we propose the first large language model framework for
negative feedback modeling with special designed context-discerning modules. We
use semantic ID Representation to replace text-based item descriptions and
introduce an item-level alignment task that enhances the LLM's understanding of
the semantic context behind negative feedback. Furthermore, we design a
Progressive GRPO training paradigm that enables the model to dynamically
balance the positive and negative behavioral context utilization. Besides, our
investigation further reveals a fundamental misalignment between the
conventional next-negative-item prediction objective and users' true negative
preferences, which is heavily influenced by the system's recommendation order.
To mitigate this, we propose a novel reward function and evaluation metric
grounded in multi-day future negative feedback and their collaborative signals.
\\ ( https://arxiv.org/abs/2601.15721 ,  1014kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15724 (*cross-listing*)
Date: Thu, 22 Jan 2026 07:47:29 GMT   (2997kb)

Title: VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning
Authors: Chenglin Li, Qianglong Chen, Feng Han, Yikun Wang, Xingxi Yin, Yan
  Gong, Ruilin Li, Yin Zhang, Jiaqi Wang
Categories: cs.CV cs.AI
\\
  Long-form video understanding remains a fundamental challenge for current
Video Large Language Models. Most existing models rely on static reasoning over
uniformly sampled frames, which weakens temporal localization and leads to
substantial information loss in long videos. Agentic tools such as temporal
retrieval, spatial zoom, and temporal zoom offer a natural way to overcome
these limitations by enabling adaptive exploration of key moments. However,
constructing agentic video understanding data requires models that already
possess strong long-form video comprehension, creating a circular dependency.
We address this challenge with VideoThinker, an agentic Video Large Language
Model trained entirely on synthetic tool interaction trajectories. Our key idea
is to convert videos into rich captions and employ a powerful agentic language
model to generate multi-step tool use sequences in caption space. These
trajectories are subsequently grounded back to video by replacing captions with
the corresponding frames, yielding a large-scale interleaved video and tool
reasoning dataset without requiring any long-form understanding from the
underlying model. Training on this synthetic agentic dataset equips
VideoThinker with dynamic reasoning capabilities, adaptive temporal
exploration, and multi-step tool use. Remarkably, VideoThinker significantly
outperforms both caption-only language model agents and strong video model
baselines across long-video benchmarks, demonstrating the effectiveness of tool
augmented synthetic data and adaptive retrieval and zoom reasoning for
long-form video understanding.
\\ ( https://arxiv.org/abs/2601.15724 ,  2997kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15729 (*cross-listing*)
Date: Thu, 22 Jan 2026 07:56:36 GMT   (2338kb)

Title: DualShield: Safe Model Predictive Diffusion via Reachability Analysis
  for Interactive Autonomous Driving
Authors: Rui Yang, Lei Zheng, Ruoyu Yao, Jun Ma
Categories: cs.RO cs.AI cs.SY eess.SY
Comments: 8 pages, 5 figures
\\
  Diffusion models have emerged as a powerful approach for multimodal motion
planning in autonomous driving. However, their practical deployment is
typically hindered by the inherent difficulty in enforcing vehicle dynamics and
a critical reliance on accurate predictions of other agents, making them prone
to safety issues under uncertain interactions. To address these limitations, we
introduce DualShield, a planning and control framework that leverages
Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First,
the value functions act as proactive guidance, steering the diffusion denoising
process towards safe and dynamically feasible regions. Second, they form a
reactive safety shield using control barrier-value functions (CBVFs) to modify
the executed actions and ensure safety. This dual mechanism preserves the rich
exploration capabilities of diffusion models while providing principled safety
assurance under uncertain and even adversarial interactions. Simulations in
challenging unprotected U-turn scenarios demonstrate that DualShield
significantly improves both safety and task efficiency compared to leading
methods from different planning paradigms under uncertainty.
\\ ( https://arxiv.org/abs/2601.15729 ,  2338kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15731 (*cross-listing*)
Date: Thu, 22 Jan 2026 07:57:27 GMT   (15339kb)

Title: FAIR-ESI: Feature Adaptive Importance Refinement for
  Electrophysiological Source Imaging
Authors: Linyong Zou, Liang Zhang, Xiongfei Wang, Jia-Hong Gao, Yi Sun, Shurong
  Sheng, Kuntao Xiao, Wanli Yang, Pengfei Teng, Guoming Luan, Zhao Lv and
  Zikang Xu
Categories: cs.CV cs.AI
\\
  An essential technique for diagnosing brain disorders is electrophysiological
source imaging (ESI). While model-based optimization and deep learning methods
have achieved promising results in this field, the accurate selection and
refinement of features remains a central challenge for precise ESI. This paper
proposes FAIR-ESI, a novel framework that adaptively refines feature importance
across different views, including FFT-based spectral feature refinement,
weighted temporal feature refinement, and self-attention-based patch-wise
feature refinement. Extensive experiments on two simulation datasets with
diverse configurations and two real-world clinical datasets validate our
framework's efficacy, highlighting its potential to advance brain disorder
diagnosis and offer new insights into brain function.
\\ ( https://arxiv.org/abs/2601.15731 ,  15339kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15754 (*cross-listing*)
Date: Thu, 22 Jan 2026 08:43:15 GMT   (1245kb)

Title: CAFE-GB: Scalable and Stable Feature Selection for Malware Detection via
  Chunk-wise Aggregated Gradient Boosting
Authors: Ajvad Haneef K, Karan Kuwar Singh, Madhu Kumar S D
Categories: cs.CR cs.AI cs.LG
\\
  High-dimensional malware datasets often exhibit feature redundancy,
instability, and scalability limitations, which hinder the effectiveness and
interpretability of machine learning-based malware detection systems. Although
feature selection is commonly employed to mitigate these issues, many existing
approaches lack robustness when applied to large-scale and heterogeneous
malware data. To address this gap, this paper proposes CAFE-GB (Chunk-wise
Aggregated Feature Estimation using Gradient Boosting), a scalable feature
selection framework designed to produce stable and globally consistent feature
rankings for high-dimensional malware detection. CAFE-GB partitions training
data into overlapping chunks, estimates local feature importance using gradient
boosting models, and aggregates these estimates to derive a robust global
ranking. Feature budget selection is performed separately through a systematic
k-selection and stability analysis to balance detection performance and
robustness. The proposed framework is evaluated on two large-scale malware
datasets: BODMAS and CIC-AndMal2020, representing large and diverse malware
feature spaces. Experimental results show that classifiers trained on CAFE-GB
-selected features achieve performance parity with full-feature baselines
across multiple metrics, including Accuracy, F1-score, MCC, ROC-AUC, and
PR-AUC, while reducing feature dimensionality by more than 95\%. Paired
Wilcoxon signed-rank tests confirm that this reduction does not introduce
statistically significant performance degradation. Additional analyses
demonstrate low inter-feature redundancy and improved interpretability through
SHAP-based explanations. Runtime and memory profiling further indicate reduced
downstream classification overhead. Overall, CAFE-GB provides a stable,
interpretable, and scalable feature selection strategy for large-scale malware
detection.
\\ ( https://arxiv.org/abs/2601.15754 ,  1245kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15802 (*cross-listing*)
Date: Thu, 22 Jan 2026 09:34:56 GMT   (317kb)

Title: A Beacon Based Solution for Autonomous UUVs GNSS-Denied Stealthy
  Navigation
Authors: Alexandre Albore, Humbert Fiorino, Damien Pellier
Categories: cs.RO cs.AI
Comments: 8 pages. IEEE TechDefense 2025
\\
  Autonomous Unmanned Underwater Vehicles (UUVs) enable military and civilian
covert operations in coastal areas without relying on support vessels or Global
Navigation Satellite Systems (GNSS). Such operations are critical when surface
access is not possible and stealthy navigation is required in restricted
environments such as protected zones or dangerous areas under access ban. GNSS
denied navigation is then essential to maintaining concealment as surfacing
could expose UUVs to detection. To ensure a precise fleet positioning a
constellation of beacons deployed by aerial or surface drones establish a
synthetic landmark network that will guide the fleet of UUVs along an optimized
path from the continental shelf to the goal on the shore. These beacons either
submerged or floating emit acoustic signals for UUV localisation and
navigation. A hierarchical planner generates an adaptive route for the drones
executing primitive actions while continuously monitoring and replanning as
needed to maintain trajectory accuracy.
\\ ( https://arxiv.org/abs/2601.15802 ,  317kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15810 (*cross-listing*)
Date: Thu, 22 Jan 2026 09:52:04 GMT   (749kb)

Title: A Mobile Application for Flower Recognition System Based on
  Convolutional Neural Networks
Authors: Mustafa Yurdakul, Enes Ayan, Fahrettin Horasan, Sakir Tasdemir
Categories: cs.CV cs.AI
\\
  A convolutional neural network (CNN) is a deep learning algorithm that has
been specifically designed for computer vision applications. The CNNs proved
successful in handling the increasing amount of data in many computer vision
problems, where classical machine learning algorithms were insufficient.
Flowers have many uses in our daily lives, from decorating to making medicines
to detoxifying the environment. Identifying flower types requires expert
knowledge. However, accessing experts at any time and in any location may not
always be feasible. In this study a mobile application based on CNNs was
developed to recognize different types of flowers to provide non-specialists
with quick and easy access to information about flower types. The study
employed three distinct CNN models, namely MobileNet, DenseNet121, and
Xception, to determine the most suitable model for the mobile application. The
classification performances of the models were evaluated by training them with
seven different optimization algorithms. The DenseNet-121 architecture, which
uses the stochastic gradient descent (SGD) optimization algorithm, was the most
successful, achieving 95.84 % accuracy, 96.00% precision, recall, and F1-score.
This result shows that CNNs can be used for flower classification in mobile
applications.
\\ ( https://arxiv.org/abs/2601.15810 ,  749kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15816 (*cross-listing*)
Date: Thu, 22 Jan 2026 10:04:21 GMT   (3864kb)

Title: Virtual Traffic Police: Large Language Model-Augmented Traffic Signal
  Control for Unforeseen Incidents
Authors: Shiqi Wei, Qiqing Wang, Kaidi Yang
Categories: eess.SY cs.AI cs.SY
\\
  Adaptive traffic signal control (TSC) has demonstrated strong effectiveness
in managing dynamic traffic flows. However, conventional methods often struggle
when unforeseen traffic incidents occur (e.g., accidents and road maintenance),
which typically require labor-intensive and inefficient manual interventions by
traffic police officers. Large Language Models (LLMs) appear to be a promising
solution thanks to their remarkable reasoning and generalization capabilities.
Nevertheless, existing works often propose to replace existing TSC systems with
LLM-based systems, which can be (i) unreliable due to the inherent
hallucinations of LLMs and (ii) costly due to the need for system replacement.
To address the issues of existing works, we propose a hierarchical framework
that augments existing TSC systems with LLMs, whereby a virtual traffic police
agent at the upper level dynamically fine-tunes selected parameters of signal
controllers at the lower level in response to real-time traffic incidents. To
enhance domain-specific reliability in response to unforeseen traffic
incidents, we devise a self-refined traffic language retrieval system (TLRS),
whereby retrieval-augmented generation is employed to draw knowledge from a
tailored traffic language database that encompasses traffic conditions and
controller operation principles. Moreover, we devise an LLM-based verifier to
update the TLRS continuously over the reasoning process. Our results show that
LLMs can serve as trustworthy virtual traffic police officers that can adapt
conventional TSC methods to unforeseen traffic incidents with significantly
improved operational efficiency and reliability.
\\ ( https://arxiv.org/abs/2601.15816 ,  3864kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15824 (*cross-listing*)
Date: Thu, 22 Jan 2026 10:19:24 GMT   (195kb)

Title: Introducing the Generative Application Firewall (GAF)
Authors: Joan Vendrell Farreny (1), Mart\'i Jord\`a Roca (1), Miquel Cornudella
  Gaya (1), Rodrigo Fern\'andez Ba\'on (1), V\'ictor Garc\'ia Mart\'inez (1),
  Eduard Camacho Sucarrat (1), Alessandro Pignati (1) ((1) NeuralTrust)
Categories: cs.CR cs.AI
\\
  This paper introduces the Generative Application Firewall (GAF), a new
architectural layer for securing LLM applications. Existing defenses -- prompt
filters, guardrails, and data-masking -- remain fragmented; GAF unifies them
into a single enforcement point, much like a WAF coordinates defenses for web
traffic, while also covering autonomous agents and their tool interactions.
\\ ( https://arxiv.org/abs/2601.15824 ,  195kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15828 (*cross-listing*)
Date: Thu, 22 Jan 2026 10:25:52 GMT   (585kb)

Title: Can professional translators identify machine-generated text?
Authors: Michael Farrell
Categories: cs.CL cs.AI
Comments: 10 pages
\\
  This study investigates whether professional translators can reliably
identify short stories generated in Italian by artificial intelligence (AI)
without prior specialized training. Sixty-nine translators took part in an
in-person experiment, where they assessed three anonymized short stories - two
written by ChatGPT-4o and one by a human author. For each story, participants
rated the likelihood of AI authorship and provided justifications for their
choices. While average results were inconclusive, a statistically significant
subset (16.2%) successfully distinguished the synthetic texts from the human
text, suggesting that their judgements were informed by analytical skill rather
than chance. However, a nearly equal number misclassified the texts in the
opposite direction, often relying on subjective impressions rather than
objective markers, possibly reflecting a reader preference for AI-generated
texts. Low burstiness and narrative contradiction emerged as the most reliable
indicators of synthetic authorship, with unexpected calques, semantic loans and
syntactic transfer from English also reported. In contrast, features such as
grammatical accuracy and emotional tone frequently led to misclassification.
These findings raise questions about the role and scope of synthetic-text
editing in professional contexts.
\\ ( https://arxiv.org/abs/2601.15828 ,  585kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15869 (*cross-listing*)
Date: Thu, 22 Jan 2026 11:18:16 GMT   (620kb)

Title: Artificial Rigidities vs. Biological Noise: A Comparative Analysis of
  Multisensory Integration in AV-HuBERT and Human Observers
Authors: Francisco Portillo L\'opez
Categories: cs.CL cs.AI
Comments: 18 pages, 6 figures
\\
  This study evaluates AV-HuBERT's perceptual bio-fidelity by benchmarking its
response to incongruent audiovisual stimuli (McGurk effect) against human
observers (N=44). Results reveal a striking quantitative isomorphism: AI and
humans exhibited nearly identical auditory dominance rates (32.0% vs. 31.8%),
suggesting the model captures biological thresholds for auditory resistance.
However, AV-HuBERT showed a deterministic bias toward phonetic fusion (68.0%),
significantly exceeding human rates (47.7%). While humans displayed perceptual
stochasticity and diverse error profiles, the model remained strictly
categorical. Findings suggest that current self-supervised architectures mimic
multisensory outcomes but lack the neural variability inherent to human speech
perception.
\\ ( https://arxiv.org/abs/2601.15869 ,  620kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15888 (*cross-listing*)
Date: Thu, 22 Jan 2026 12:07:56 GMT   (1289kb)

Title: Understanding the Transfer Limits of Vision Foundation Models
Authors: Shiqi Huang, Yipei Wang, Natasha Thorley, Alexander Ng, Shaheer Saeed,
  Mark Emberton, Shonit Punwani, Veeru Kasivisvanathan, Dean Barratt, Daniel
  Alexander, Yipeng Hu
Categories: cs.CV cs.AI
Comments: accepted in ISBI 2026
\\
  Foundation models leverage large-scale pretraining to capture extensive
knowledge, demonstrating generalization in a wide range of language tasks. By
comparison, vision foundation models (VFMs) often exhibit uneven improvements
across downstream tasks, despite substantial computational investment. We
postulate that this limitation arises from a mismatch between pretraining
objectives and the demands of downstream vision-and-imaging tasks. Pretraining
strategies like masked image reconstruction or contrastive learning shape
representations for tasks such as recovery of generic visual patterns or global
semantic structures, which may not align with the task-specific requirements of
downstream applications including segmentation, classification, or image
synthesis. To investigate this in a concrete real-world clinical area, we
assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a
contrastive-learning-based model (ProViCNet), on five prostate multiparametric
MR imaging tasks, examining how such task alignment influences transfer
performance, i.e., from pretraining to fine-tuning. Our findings indicate that
better alignment between pretraining and downstream tasks, measured by simple
divergence metrics such as maximum-mean-discrepancy (MMD) between the same
features before and after fine-tuning, correlates with greater performance
improvements and faster convergence, emphasizing the importance of designing
and analyzing pretraining objectives with downstream applicability in mind.
\\ ( https://arxiv.org/abs/2601.15888 ,  1289kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15909 (*cross-listing*)
Date: Thu, 22 Jan 2026 12:38:20 GMT   (3677kb)

Title: Transfer Learning from ImageNet for MEG-Based Decoding of Imagined
  Speech
Authors: Soufiane Jhilal, St\'ephanie Martin, Anne-Lise Giraud
Categories: cs.CL cs.AI cs.CV
Comments: Accepted at IEEE ISBI 2026
\\
  Non-invasive decoding of imagined speech remains challenging due to weak,
distributed signals and limited labeled data. Our paper introduces an
image-based approach that transforms magnetoencephalography (MEG) signals into
time-frequency representations compatible with pretrained vision models. MEG
data from 21 participants performing imagined speech tasks were projected into
three spatial scalogram mixtures via a learnable sensor-space convolution,
producing compact image-like inputs for ImageNet-pretrained vision
architectures. These models outperformed classical and non-pretrained models,
achieving up to 90.4% balanced accuracy for imagery vs. silence, 81.0% vs.
silent reading, and 60.6% for vowel decoding. Cross-subject evaluation
confirmed that pretrained models capture shared neural representations, and
temporal analyses localized discriminative information to imagery-locked
intervals. These findings show that pretrained vision models applied to
image-based MEG representations can effectively capture the structure of
imagined speech in non-invasive neural signals.
\\ ( https://arxiv.org/abs/2601.15909 ,  3677kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15912 (*cross-listing*)
Date: Thu, 22 Jan 2026 12:42:30 GMT   (171kb)

Title: TeNet: Text-to-Network for Compact Policy Synthesis
Authors: Ariyan Bighashdel and Kevin Sebastian Luck
Categories: cs.RO cs.AI
\\
  Robots that follow natural-language instructions often either plan at a high
level using hand-designed interfaces or rely on large end-to-end models that
are difficult to deploy for real-time control. We propose TeNet
(Text-to-Network), a framework for instantiating compact, task-specific robot
policies directly from natural language descriptions. TeNet conditions a
hypernetwork on text embeddings produced by a pretrained large language model
(LLM) to generate a fully executable policy, which then operates solely on
low-dimensional state inputs at high control frequencies. By using the language
only once at the policy instantiation time, TeNet inherits the general
knowledge and paraphrasing robustness of pretrained LLMs while remaining
lightweight and efficient at execution time. To improve generalization, we
optionally ground language in behavior during training by aligning text
embeddings with demonstrated actions, while requiring no demonstrations at
inference time. Experiments on MuJoCo and Meta-World benchmarks show that TeNet
produces policies that are orders of magnitude smaller than sequence-based
baselines, while achieving strong performance in both multi-task and
meta-learning settings and supporting high-frequency control. These results
show that text-conditioned hypernetworks offer a practical way to build
compact, language-driven controllers for ressource-constrained robot control
tasks with real-time requirements.
\\ ( https://arxiv.org/abs/2601.15912 ,  171kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15915 (*cross-listing*)
Date: Thu, 22 Jan 2026 12:44:25 GMT   (44kb)

Title: Progressive Power Homotopy for Non-convex Optimization
Authors: Chen Xu
Categories: math.OC cs.AI cs.LG
MSC-class: 65K05, 68T07, 90C30, 68W40
\\
  We propose a novel first-order method for non-convex optimization of the form
$\max_{\bm{w}\in\mathbb{R}^d}\mathbb{E}_{\bm{x}\sim\mathcal{D}}[f_{\bm{w}}(\bm{x})]$,
termed Progressive Power Homotopy (Prog-PowerHP). The method applies stochastic
gradient ascent to a surrogate objective obtained by first performing a power
transformation and then Gaussian smoothing,
$F_{N,\sigma}(\bm{\mu}):=\mathbb{E}_{\bm{w}\sim\mathcal{N}(\bm{\mu},\sigma^2I_d),\bm{x}\sim\mathcal{D}}[e^{Nf_w(\bm{x})}]$,
while progressively increasing the power parameter $N$ and decreasing the
smoothing scale $\sigma$ along the optimization trajectory. We prove that,
under mild regularity conditions, Prog-PowerHP converges to a small
neighborhood of the global optimum with an iteration complexity scaling nearly
as $O(d^2\varepsilon^{-2})$. Empirically, Prog-PowerHP demonstrates clear
advantages in phase retrieval when the samples-to-dimension ratio approaches
the information-theoretic limit, and in training two-layer neural networks in
under-parameterized regimes. These results suggest that Prog-PowerHP is
particularly effective for navigating cluttered non-convex landscapes where
standard first-order methods struggle.
\\ ( https://arxiv.org/abs/2601.15915 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15924 (*cross-listing*)
Date: Thu, 22 Jan 2026 12:58:05 GMT   (688kb)

Title: Class Confidence Aware Reweighting for Long Tailed Learning
Authors: Brainard Philemon Jagati, Jitendra Tembhurne, Harsh Goud, Rudra Pratap
  Singh, Chandrashekhar Meshram
Categories: cs.CV cs.AI cs.LG cs.PF
Comments: 9 pages, 3 figures, IEEE Transaction on Neural Networks and Learning
  Systems (Submitted)
\\
  Deep neural network models degrade significantly in the long-tailed data
distribution, with the overall training data dominated by a small set of
classes in the head, and the tail classes obtaining less training examples.
Addressing the imbalance in the classes, attention in the related literature
was given mainly to the adjustments carried out in the decision space in terms
of either corrections performed at the logit level in order to compensate
class-prior bias, with the least attention to the optimization process
resulting from the adjustments introduced through the differences in the
confidences among the samples. In the current study, we present the design of a
class and confidence-aware re-weighting scheme for long-tailed learning. This
scheme is purely based upon the loss level and has a complementary nature to
the existing methods performing the adjustment of the logits. In the practical
implementation stage of the proposed scheme, we use an {\Omega}(p_t, f_c)
function. This function enables the modulation of the contribution towards the
training task based upon the confidence value of the prediction, as well as the
relative frequency of the corresponding class. Our observations in the
experiments are corroborated by significant experimental results performed on
the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various
values of imbalance factors that clearly authenticate the theoretical
discussions above.
\\ ( https://arxiv.org/abs/2601.15924 ,  688kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15930 (*cross-listing*)
Date: Thu, 22 Jan 2026 13:09:16 GMT   (698kb)

Title: MMGRid: Navigating Temporal-aware and Cross-domain Generative
  Recommendation via Model Merging
Authors: Tianjun Wei, Enneng Yang, Yingpeng Du, Huizhong Guo, Jie Zhang, Zhu
  Sun
Categories: cs.IR cs.AI
Comments: https://github.com/Joinn99/MMGRid
\\
  Model merging (MM) offers an efficient mechanism for integrating multiple
specialized models without access to original training data or costly
retraining. While MM has demonstrated success in domains like computer vision,
its role in recommender systems (RSs) remains largely unexplored. Recently,
Generative Recommendation (GR) has emerged as a new paradigm in RSs,
characterized by rapidly growing model scales and substantial computational
costs, making MM particularly appealing for cost-sensitive deployment
scenarios. In this work, we present the first systematic study of MM in GR
through a contextual lens. We focus on a fundamental yet underexplored
challenge in real-world: how to merge generative recommenders specialized to
different real-world contexts, arising from temporal evolving user behaviors
and heterogeneous application domains. To this end, we propose a unified
framework MMGRid, a structured contextual grid of GR checkpoints that organizes
models trained under diverse contexts induced by temporal evolution and domain
diversity. All checkpoints are derived from a shared base LLM but fine-tuned on
context-specific data, forming a realistic and controlled model space for
systematically analyzing MM across GR paradigms and merging algorithms. Our
investigation reveals several key insights. First, training GR models from LLMs
can introduce parameter conflicts during merging due to token distribution
shifts and objective disparities; such conflicts can be alleviated by
disentangling task-aware and context-specific parameter changes via base model
replacement. Second, incremental training across contexts induces recency bias,
which can be effectively balanced through weighted contextual merging. Notably,
we observe that optimal merging weights correlate with context-dependent
interaction characteristics, offering practical guidance for weight selection
in real-world deployments.
\\ ( https://arxiv.org/abs/2601.15930 ,  698kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15995 (*cross-listing*)
Date: Thu, 22 Jan 2026 14:16:12 GMT   (6720kb)

Title: PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented
  Quadruped Parkour
Authors: Liang Wang, Kanzhong Yao, Yang Liu, Weikai Qin, Jun Wu, Zhe Sun and
  Qiuguo Zhu
Categories: cs.RO cs.AI cs.LG
\\
  Parkour tasks for quadrupeds have emerged as a promising benchmark for agile
locomotion. While human athletes can effectively perceive environmental
characteristics to select appropriate footholds for obstacle traversal,
endowing legged robots with similar perceptual reasoning remains a significant
challenge. Existing methods often rely on hierarchical controllers that follow
pre-computed footholds, thereby constraining the robot's real-time adaptability
and the exploratory potential of reinforcement learning. To overcome these
challenges, we present PUMA, an end-to-end learning framework that integrates
visual perception and foothold priors into a single-stage training process.
This approach leverages terrain features to estimate egocentric polar foothold
priors, composed of relative distance and heading, guiding the robot in active
posture adaptation for parkour tasks. Extensive experiments conducted in
simulation and real-world environments across various discrete complex
terrains, demonstrate PUMA's exceptional agility and robustness in challenging
scenarios.
\\ ( https://arxiv.org/abs/2601.15995 ,  6720kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16007 (*cross-listing*)
Date: Thu, 22 Jan 2026 14:33:01 GMT   (37881kb)

Title: PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning
  and Prediction in Foundational VLMs and World Models
Authors: Chak-Wing Mak, Guanyu Zhu, Boyi Zhang, Hongji Li, Xiaowei Chi, Kevin
  Zhang, Yichen Wu, Yangfan He, Chun-Kai Fan, Wentao Lu, Kuangzhi Ge, Xinyu
  Fang, Hongyang He, Kuan Lu, Tianxiang Xu, Li Zhang, Yongxin Ni, Youhua Li,
  Shanghang Zhang
Categories: cs.CV cs.AI
\\
  Modern foundational Multimodal Large Language Models (MLLMs) and video world
models have advanced significantly in mathematical, common-sense, and visual
reasoning, but their grasp of the underlying physics remains underexplored.
Existing benchmarks attempting to measure this matter rely on synthetic, Visual
Question Answer templates or focus on perceptual video quality that is
tangential to measuring how well the video abides by physical laws. To address
this fragmentation, we introduce PhysicsMind, a unified benchmark with both
real and simulation environments that evaluates law-consistent reasoning and
generation over three canonical principles: Center of Mass, Lever Equilibrium,
and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks,
testing whether models can reason and determine physical quantities and values
from images or short videos, and ii) Video Generation(VG) tasks, evaluating if
predicted motion trajectories obey the same center-of-mass, torque, and
inertial constraints as the ground truth. A broad range of recent models and
video generation models is evaluated on PhysicsMind and found to rely on
appearance heuristics while often violating basic mechanics. These gaps
indicate that current scaling and training are still insufficient for robust
physical understanding, underscoring PhysicsMind as a focused testbed for
physics-aware multimodal models. Our data will be released upon acceptance.
\\ ( https://arxiv.org/abs/2601.16007 ,  37881kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16011 (*cross-listing*)
Date: Thu, 22 Jan 2026 14:38:00 GMT   (27400kb)

Title: THOR: A Versatile Foundation Model for Earth Observation Climate and
  Society Applications
Authors: Theodor Forgaard and Jarle H. Reksten and Anders U. Waldeland and
  Valerio Marsocci and Nicolas Long\'ep\'e and Michael Kampffmeyer and
  Arnt-B{\o}rre Salberg
Categories: eess.IV cs.AI
Comments: 25 pages
\\
  Current Earth observation foundation models are architecturally rigid,
struggle with heterogeneous sensors and are constrained to fixed patch sizes.
This limits their deployment in real-world scenarios requiring flexible
computeaccuracy trade-offs. We propose THOR, a "computeadaptive" foundation
model that solves both input heterogeneity and deployment rigidity. THOR is the
first architecture to unify data from Copernicus Sentinel-1, -2, and -3 (OLCI &
SLSTR) satellites, processing their native 10 m to 1000 m resolutions in a
single model. We pre-train THOR with a novel randomized patch and input image
size strategy. This allows a single set of pre-trained weights to be deployed
at inference with any patch size, enabling a dynamic trade-off between
computational cost and feature resolution without retraining. We pre-train THOR
on THOR Pretrain, a new, large-scale multi-sensor dataset and demonstrate
state-of-the-art performance on downstream benchmarks, particularly in
data-limited regimes like the PANGAEA 10% split, validating that THOR's
flexible feature generation excels for diverse climate and society
applications.
\\ ( https://arxiv.org/abs/2601.16011 ,  27400kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16032 (*cross-listing*)
Date: Thu, 22 Jan 2026 15:05:31 GMT   (990kb)

Title: Sawtooth Wavefront Reordering: Enhanced CuTile FlashAttention on NVIDIA
  GB10
Authors: Yifan Zhu and Yekai Pan and Chen Ding
Categories: cs.PF cs.AI cs.LG cs.OS
\\
  High-performance attention kernels are essential for Large Language Models.
This paper presents analysis of CuTile-based Flash Attention memory behavior
and a technique to improve its cache performance. In particular, our analysis
on the NVIDIA GB10 (Grace Blackwell) identifies the main cause of L2 cache
miss. Leveraging this insight, we introduce a new programming technique called
Sawtooth Wavefront Reordering that reduces L2 misses. We validate it in both
CUDA and CuTile, observing 50\% or greater reduction in L2 misses and up to
60\% increase in throughput on GB10.
\\ ( https://arxiv.org/abs/2601.16032 ,  990kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16091 (*cross-listing*)
Date: Thu, 22 Jan 2026 16:42:05 GMT   (31kb)

Title: Delayed Assignments in Online Non-Centroid Clustering with Stochastic
  Arrivals
Authors: Saar Cohen
Categories: cs.MA cs.AI cs.LG
Comments: To Appear in the 25th International Conference on Autonomous Agents
  and Multiagent Systems (AAMAS), 2026
\\
  Clustering is a fundamental problem, aiming to partition a set of elements,
like agents or data points, into clusters such that elements in the same
cluster are closer to each other than to those in other clusters. In this
paper, we present a new framework for studying online non-centroid clustering
with delays, where elements, that arrive one at a time as points in a finite
metric space, should be assigned to clusters, but assignments need not be
immediate. Specifically, upon arrival, each point's location is revealed, and
an online algorithm has to irrevocably assign it to an existing cluster or
create a new one containing, at this moment, only this point. However, we allow
decisions to be postponed at a delay cost, instead of following the more common
assumption of immediate decisions upon arrival. This poses a critical
challenge: the goal is to minimize both the total distance costs between points
in each cluster and the overall delay costs incurred by postponing assignments.
In the classic worst-case arrival model, where points arrive in an arbitrary
order, no algorithm has a competitive ratio better than sublogarithmic in the
number of points. To overcome this strong impossibility, we focus on a
stochastic arrival model, where points' locations are drawn independently
across time from an unknown and fixed probability distribution over the finite
metric space. We offer hope for beyond worst-case adversaries: we devise an
algorithm that is constant competitive in the sense that, as the number of
points grows, the ratio between the expected overall costs of the output
clustering and an optimal offline clustering is bounded by a constant.
\\ ( https://arxiv.org/abs/2601.16091 ,  31kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16127 (*cross-listing*)
Date: Thu, 22 Jan 2026 17:28:24 GMT   (280kb)

Title: Improving Training Efficiency and Reducing Maintenance Costs via
  Language Specific Model Merging
Authors: Alphaeus Dmonte, Vidhi Gupta, Daniel J Perry, Mark Arehart
Categories: cs.CL cs.AI
\\
  Fine-tuning a task-specific multilingual large language model (LLM) involves
training the model on a multilingual dataset with examples in all the required
languages. Updating one or more supported languages with additional data or
adding support for a new language involves retraining the model, which can be
computationally inefficient and creates a severe maintenance bottleneck. Recent
research on merging multilingual multitask models has shown promise in terms of
improved quality, but its computational and maintenance efficiency remains
unstudied. In this work, we provide the first focused analysis of this merging
strategy from an efficiency perspective, evaluating it across three independent
tasks. We demonstrate significant efficiency gains while maintaining parity in
terms of quality: this merging approach reduces the initial training time by up
to 50\%. We also demonstrate that updating an individual language and
re-merging as part of model maintenance reduces training costs by more than
60\%, compared to re-training the full multilingual model. We show this on both
public and proprietary industry datasets confirming that the approach works
well for industrial use cases in addition to academic settings already studied
in previous work.
\\ ( https://arxiv.org/abs/2601.16127 ,  280kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16130 (*cross-listing*)
Date: Thu, 22 Jan 2026 17:29:07 GMT   (5500kb)

Title: Replicating Human Motivated Reasoning Studies with LLMs
Authors: Neeley Pate, Adiba Mahbub Proma, Hangfeng He, James N. Druckman,
  Daniel Molden, Gourab Ghoshal, Ehsan Hoque
Categories: cs.HC cs.AI
\\
  Motivated reasoning -- the idea that individuals processing information may
be motivated to reach a certain conclusion, whether it be accurate or
predetermined -- has been well-explored as a human phenomenon. However, it is
unclear whether base LLMs mimic these motivational changes. Replicating 4 prior
political motivated reasoning studies, we find that base LLM behavior does not
align with expected human behavior. Furthermore, base LLM behavior across
models shares some similarities, such as smaller standard deviations and
inaccurate argument strength assessments. We emphasize the importance of these
findings for researchers using LLMs to automate tasks such as survey data
collection and argument assessment.
\\ ( https://arxiv.org/abs/2601.16130 ,  5500kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16140 (*cross-listing*)
Date: Thu, 22 Jan 2026 17:34:30 GMT   (11538kb)

Title: Learning to Watermark in the Latent Space of Generative Models
Authors: Sylvestre-Alvise Rebuffi, Tuan Tran, Valeriu Lacatusu, Pierre
  Fernandez, Tom\'a\v{s} Sou\v{c}ek, Nikola Jovanovi\'c, Tom Sander, Hady
  Elsahar, Alexandre Mourachko
Categories: cs.CV cs.AI cs.CR
Comments: Code and models are available at
  https://github.com/facebookresearch/distseal
\\
  Existing approaches for watermarking AI-generated images often rely on
post-hoc methods applied in pixel space, introducing computational overhead and
potential visual artifacts. In this work, we explore latent space watermarking
and introduce DistSeal, a unified approach for latent watermarking that works
across both diffusion and autoregressive models. Our approach works by training
post-hoc watermarking models in the latent space of generative models. We
demonstrate that these latent watermarkers can be effectively distilled either
into the generative model itself or into the latent decoder, enabling in-model
watermarking. The resulting latent watermarks achieve competitive robustness
while offering similar imperceptibility and up to 20x speedup compared to
pixel-space baselines. Our experiments further reveal that distilling latent
watermarkers outperforms distilling pixel-space ones, providing a solution that
is both more efficient and more robust.
\\ ( https://arxiv.org/abs/2601.16140 ,  11538kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16150 (*cross-listing*)
Date: Thu, 22 Jan 2026 17:46:31 GMT   (528kb)

Title: Pay (Cross) Attention to the Melody: Curriculum Masking for
  Single-Encoder Melodic Harmonization
Authors: Maximos Kaliakatsos-Papakostas, Dimos Makris, Konstantinos Soiledis,
  Konstantinos-Theodoros Tsamis, Vassilis Katsouros, Emilios Cambouropoulos
Categories: cs.SD cs.AI
\\
  Melodic harmonization, the task of generating harmonic accompaniments for a
given melody, remains a central challenge in computational music generation.
Recent single encoder transformer approaches have framed harmonization as a
masked sequence modeling problem, but existing training curricula inspired by
discrete diffusion often result in weak (cross) attention between melody and
harmony. This leads to limited exploitation of melodic cues, particularly in
out-of-domain contexts. In this work, we introduce a training curriculum, FF
(full-to-full), which keeps all harmony tokens masked for several training
steps before progressively unmasking entire sequences during training to
strengthen melody-harmony interactions. We systematically evaluate this
approach against prior curricula across multiple experimental axes, including
temporal quantization (quarter vs. sixteenth note), bar-level vs.
time-signature conditioning, melody representation (full range vs. pitch
class), and inference-time unmasking strategies. Models are trained on the
HookTheory dataset and evaluated both in-domain and on a curated collection of
jazz standards, using a comprehensive set of metrics that assess chord
progression structure, harmony-melody alignment, and rhythmic coherence.
Results demonstrate that the proposed FF curriculum consistently outperforms
baselines in nearly all metrics, with particularly strong gains in
out-of-domain evaluations where harmonic adaptability to novel melodic queues
is crucial. We further find that quarter-note quantization, intertwining of bar
tokens, and pitch-class melody representations are advantageous in the FF
setting. Our findings highlight the importance of training curricula in
enabling effective melody conditioning and suggest that full-to-full unmasking
offers a robust strategy for single encoder harmonization.
\\ ( https://arxiv.org/abs/2601.16150 ,  528kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16152 (*cross-listing*)
Date: Thu, 22 Jan 2026 17:51:02 GMT   (28kb)

Title: Substrate Stability Under Persistent Disagreement: Structural
  Constraints for Neutral Ontological Substrates
Authors: Denise M. Case
Categories: cs.LO cs.AI
Comments: 29 pages
\\
  Modern data systems increasingly operate under conditions of persistent
legal, political, and analytic disagreement. In such settings, interoperability
cannot rely on shared interpretation, negotiated semantics, or centralized
authority. Instead, representations must function as neutral substrates that
preserve stable reference across incompatible extensions. This paper
investigates the structural constraints imposed on ontological design by this
requirement. Building on a neutrality framework that treats interpretive
non-commitment and stability under extension as explicit design constraints, we
ask what minimal ontological structure is forced if accountability
relationships are to remain referable and comparable under disagreement.
Minimality here is not mere parsimony: a reduction is admissible only if it
does not reintroduce stability-critical distinctions as hidden roles, flags, or
contextual predicates. We establish a conditional lower-bound result: any
ontology capable of supporting accountability under persistent disagreement
must realize at least six distinct identity-and-persistence regimes. We further
show that a construction with exactly six such regimes is sufficient to satisfy
the stated requirements without embedding causal or normative commitments in
the substrate. The result is not a proposal for a universal ontology, but a
constraint on what is possible when neutrality and stable reference are treated
as non-negotiable design goals.
\\ ( https://arxiv.org/abs/2601.16152 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16206 (*cross-listing*)
Date: Thu, 22 Jan 2026 18:57:09 GMT   (989kb)

Title: LLM-in-Sandbox Elicits General Agentic Intelligence
Authors: Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen, Li
  Dong, Wayne Xin Zhao, Ji-Rong Wen, Furu Wei
Categories: cs.CL cs.AI
Comments: Project Page: https://llm-in-sandbox.github.io
\\
  We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox
(i.e., a virtual computer), to elicit general intelligence in non-code domains.
We first demonstrate that strong LLMs, without additional training, exhibit
generalization capabilities to leverage the code sandbox for non-code tasks.
For example, LLMs spontaneously access external resources to acquire new
knowledge, leverage the file system to handle long contexts, and execute
scripts to satisfy formatting requirements. We further show that these agentic
capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning
(LLM-in-Sandbox-RL), which uses only non-agentic data to train models for
sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both
training-free and post-trained settings, achieves robust generalization
spanning mathematics, physics, chemistry, biomedicine, long-context
understanding, and instruction following. Finally, we analyze LLM-in-Sandbox's
efficiency from computational and system perspectives, and open-source it as a
Python package to facilitate real-world deployment.
\\ ( https://arxiv.org/abs/2601.16206 ,  989kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16210 (*cross-listing*)
Date: Thu, 22 Jan 2026 18:58:55 GMT   (18087kb)

Title: PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding
  and Generation
Authors: Onkar Susladkar, Tushar Prakash, Adheesh Juvekar, Kiet A. Nguyen,
  Dong-Hwan Jang, Inderjit S Dhillon, Ismini Lourentzou
Categories: cs.CV cs.AI
\\
  Discrete video VAEs underpin modern text-to-video generation and video
understanding systems, yet existing tokenizers typically learn visual codebooks
at a single scale with limited vocabularies and shallow language supervision,
leading to poor cross-modal alignment and zero-shot transfer. We introduce
PyraTok, a language-aligned pyramidal tokenizer that learns semantically
structured discrete latents across multiple spatiotemporal resolutions. PyraTok
builds on a pretrained video VAE and a novel Language aligned Pyramidal
Quantization (LaPQ) module that discretizes encoder features at several depths
using a shared large binary codebook, yielding compact yet expressive video
token sequences. To tightly couple visual tokens with language, PyraTok jointly
optimizes multi-scale text-guided quantization and a global autoregressive
objective over the token hierarchy. Across ten benchmarks, PyraTok delivers
state-of-the-art (SOTA) video reconstruction, consistently improves
text-to-video quality, and sets new SOTA zero-shot performance on video
segmentation, temporal action localization, and video understanding, scaling
robustly to up to 4K/8K resolutions.
\\ ( https://arxiv.org/abs/2601.16210 ,  18087kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16211 (*cross-listing*)
Date: Thu, 22 Jan 2026 18:59:13 GMT   (2581kb)

Title: Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in
  Zero-Shot Compositional Action Recognition
Authors: Geo Ahn, Inwoong Lee, Taeoh Kim, Minho Shim, Dongyoon Wee, Jinwoo Choi
Categories: cs.CV cs.AI
Comments: The code is available at https://github.com/KHU-VLL/RCORE
\\
  We study Compositional Video Understanding (CVU), where models must recognize
verbs and objects and compose them to generalize to unseen combinations. We
find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models
fail primarily due to an overlooked failure mode: object-driven verb shortcuts.
Through systematic analysis, we show that this behavior arises from two
intertwined factors: severe sparsity and skewness of compositional supervision,
and the asymmetric learning difficulty between verbs and objects. As training
progresses, the existing ZS-CAR model increasingly ignores visual evidence and
overfits to co-occurrence statistics. Consequently, the existing model does not
gain the benefit of compositional recognition in unseen verb-object
compositions. To address this, we propose RCORE, a simple and effective
framework that enforces temporally grounded verb learning. RCORE introduces (i)
a composition-aware augmentation that diversifies verb-object combinations
without corrupting motion cues, and (ii) a temporal order regularization loss
that penalizes shortcut behaviors by explicitly modeling temporal structure.
Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE
significantly improves unseen composition accuracy, reduces reliance on
co-occurrence bias, and achieves consistently positive compositional gaps. Our
findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR
and demonstrate that addressing them is essential for robust compositional
video understanding.
\\ ( https://arxiv.org/abs/2601.16211 ,  2581kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15340 (*cross-listing*)
Date: Tue, 20 Jan 2026 16:00:26 GMT   (31376kb)

Title: Learning Nonlinear Heterogeneity in Physical Kolmogorov-Arnold Networks
Authors: Fabiana Taglietti, Andrea Pulici, Maxwell Roxburgh, Gabriele Seguini,
  Ian Vidamour, Stephan Menzel, Edoardo Franco, Michele Laus, Eleni Vasilaki,
  Michele Perego, Thomas J. Hayward, Marco Fanciulli, and Jack C. Gartside
Categories: cond-mat.dis-nn cond-mat.mes-hall cs.LG nlin.AO physics.app-ph
\\
  Physical neural networks typically train linear synaptic weights while
treating device nonlinearities as fixed. We show the opposite - by training the
synaptic nonlinearity itself, as in Kolmogorov-Arnold Network (KAN)
architectures, we yield markedly higher task performance per physical resource
and improved performance-parameter scaling than conventional linear
weight-based networks, demonstrating ability of KAN topologies to exploit
reconfigurable nonlinear physical dynamics.
  We experimentally realise physical KANs in silicon-on-insulator devices we
term 'Synaptic Nonlinear Elements' (SYNEs), operating at room temperature,
0.1-1 microampere currents, and 2 MHz speeds with no observed degradation over
10^13 measurements and months-long timescales.
  We demonstrate nonlinear function regression, classification, and prediction
of Li-Ion battery dynamics from noisy real-world multi-sensor data. Physical
KANs outperform equivalently-parameterised software multilayer perceptron
networks across all tasks, with up to two orders of magnitude fewer parameters,
and two orders of magnitude fewer devices than linear weight based physical
networks. These results establish learned physical nonlinearity as a
hardware-native computational primitive for compact and efficient learning
systems, and SYNE devices as effective substrates for heterogenous nonlinear
computing.
\\ ( https://arxiv.org/abs/2601.15340 ,  31376kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15353 (*cross-listing*)
Date: Wed, 21 Jan 2026 04:58:49 GMT   (1224kb)

Title: Statistical Reinforcement Learning in the Real World: A Survey of
  Challenges and Future Directions
Authors: Asim H. Gazi, Yongyi Guo, Daiqi Gao, Ziping Xu, Kelly W. Zhang, Susan
  A. Murphy
Categories: stat.AP cs.LG stat.ML
\\
  Reinforcement learning (RL) has achieved remarkable success in real-world
decision-making across diverse domains, including gaming, robotics, online
advertising, public health, and natural language processing. Despite these
advances, a substantial gap remains between RL research and its deployment in
many practical settings. Two recurring challenges often underlie this gap.
First, many settings offer limited opportunity for the agent to interact
extensively with the target environment due to practical constraints. Second,
many target environments often undergo substantial changes, requiring redesign
and redeployment of RL systems (e.g., advancements in science and technology
that change the landscape of healthcare delivery). Addressing these challenges
and bridging the gap between basic research and application requires theory and
methodology that directly inform the design, implementation, and continual
improvement of RL systems in real-world settings.
  In this paper, we frame the application of RL in practice as a
three-component process: (i) online learning and optimization during
deployment, (ii) post- or between-deployment offline analyses, and (iii)
repeated cycles of deployment and redeployment to continually improve the RL
system. We provide a narrative review of recent advances in statistical RL that
address these components, including methods for maximizing data utility for
between-deployment inference, enhancing sample efficiency for online learning
within-deployment, and designing sequences of deployments for continual
improvement. We also outline future research directions in statistical RL that
are use-inspired -- aiming for impactful application of RL in practice.
\\ ( https://arxiv.org/abs/2601.15353 ,  1224kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15360 (*cross-listing*)
Date: Wed, 21 Jan 2026 11:50:01 GMT   (508kb)

Title: Robust X-Learner: Breaking the Curse of Imbalance and Heavy Tails via
  Robust Cross-Imputation
Authors: Eichi Uehara
Categories: stat.ML cs.LG econ.EM stat.ME
Comments: 17 pages, 4 figures, 4 tables
MSC-class: 62G35, 68T05
ACM-class: I.2.6; G.3
\\
  Estimating Heterogeneous Treatment Effects (HTE) in industrial applications
such as AdTech and healthcare presents a dual challenge: extreme class
imbalance and heavy-tailed outcome distributions. While the X-Learner framework
effectively addresses imbalance through cross-imputation, we demonstrate that
it is fundamentally vulnerable to "Outlier Smearing" when reliant on Mean
Squared Error (MSE) minimization. In this failure mode, the bias from a few
extreme observations ("whales") in the minority group is propagated to the
entire majority group during the imputation step, corrupting the estimated
treatment effect structure. To resolve this, we propose the Robust X-Learner
(RX-Learner). This framework integrates a redescending {\gamma}-divergence
objective -- structurally equivalent to the Welsch loss under Gaussian
assumptions -- into the gradient boosting machinery. We further stabilize the
non-convex optimization using a Proxy Hessian strategy grounded in
Majorization-Minimization (MM) principles. Empirical evaluation on a
semi-synthetic Criteo Uplift dataset demonstrates that the RX-Learner reduces
the Precision in Estimation of Heterogeneous Effect (PEHE) metric by 98.6%
compared to the standard X-Learner, effectively decoupling the stable "Core"
population from the volatile "Periphery".
\\ ( https://arxiv.org/abs/2601.15360 ,  508kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15361 (*cross-listing*)
Date: Wed, 21 Jan 2026 12:06:17 GMT   (81kb)

Title: USDs: A universal stabilizer decoder framework using symmetry
Authors: Hoshitaro Ohnishi and Hideo Mukai
Categories: quant-ph cs.LG
\\
  Quantum error correction is indispensable to achieving reliable quantum
computation. When quantum information is encoded redundantly, a larger Hilbert
space is constructed using multiple physical qubits, and the computation is
performed within a designated subspace. When applying deep learning to the
decoding of quantum error-correcting codes, a key challenge arises from the
non-uniqueness between the syndrome measurements provided to the decoder and
the corresponding error patterns that constitute the ground-truth labels.
Building upon prior work that addressed this issue for the toric code by
re-optimizing the decoder with respect to the symmetry inherent in the
parity-check structure, we generalize this approach to arbitrary stabilizer
codes. In our experiments, we employed multilayer perceptrons to approximate
continuous functions that complement the syndrome measurements of the Color
code and the Golay code. Using these models, we performed decoder
re-optimization for each code. For the Color code, we achieved an improvement
of approximately 0.8% in decoding accuracy at a physical error rate of 5%,
while for the Golay code the accuracy increased by about 0.1%. Furthermore,
from the evaluation of the geometric and algebraic structures in the continuous
function approximation for each code, we showed that the design of generalized
continuous functions is advantageous for learning the geometric structure
inherent in the code. Our results also indicate that approximations that
faithfully reproduce the code structure can have a significant impact on the
effectiveness of reoptimization. This study demonstrates that the
re-optimization technique previously shown to be effective for the Toric code
can be generalized to address the challenge of label degeneracy that arises
when applying deep learning to the decoding of stabilizer codes.
\\ ( https://arxiv.org/abs/2601.15361 ,  81kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15363 (*cross-listing*)
Date: Wed, 21 Jan 2026 14:35:23 GMT   (2490kb)

Title: Non-Stationary Functional Bilevel Optimization
Authors: Jason Bohne, Ieva Petrulionyte, Michael Arbel, Julien Mairal, Pawe{\l}
  Polak
Categories: stat.ML cs.LG
\\
  Functional bilevel optimization (FBO) provides a powerful framework for
hierarchical learning in function spaces, yet current methods are limited to
static offline settings and perform suboptimally in online, non-stationary
scenarios. We propose SmoothFBO, the first algorithm for non-stationary FBO
with both theoretical guarantees and practical scalability. SmoothFBO
introduces a time-smoothed stochastic hypergradient estimator that reduces
variance through a window parameter, enabling stable outer-loop updates with
sublinear regret. Importantly, the classical parametric bilevel case is a
special reduction of our framework, making SmoothFBO a natural extension to
online, non-stationary settings. Empirically, SmoothFBO consistently
outperforms existing FBO methods in non-stationary hyperparameter optimization
and model-based reinforcement learning, demonstrating its practical
effectiveness. Together, these results establish SmoothFBO as a general,
theoretically grounded, and practically viable foundation for bilevel
optimization in online, non-stationary scenarios.
\\ ( https://arxiv.org/abs/2601.15363 ,  2490kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15506 (*cross-listing*)
Date: Wed, 21 Jan 2026 22:24:53 GMT   (627kb)

Title: ViT Registers and Fractal ViT
Authors: Jason Chuan-Chih Chou, Abhinav Kumar, Shivank Garg
Categories: cs.CL cs.LG
\\
  Drawing inspiration from recent findings including surprisingly decent
performance of transformers without positional encoding (NoPE) in the domain of
language models and how registers (additional throwaway tokens not tied to
input) may improve the performance of large vision transformers (ViTs), we
invent and test a variant of ViT called fractal ViT that breaks permutation
invariance among the tokens by applying an attention mask between the regular
tokens and ``summary tokens'' similar to registers, in isolation or in
combination with various positional encodings. These models do not improve upon
ViT with registers, highlighting the fact that these findings may be scale,
domain, or application-specific.
\\ ( https://arxiv.org/abs/2601.15506 ,  627kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15516 (*cross-listing*)
Date: Wed, 21 Jan 2026 23:00:43 GMT   (3320kb)

Title: DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in
  Egocentric Views
Authors: William Huang, Siyou Pei, Leyi Zou, Eric J. Gonzalez, Ishan
  Chatterjee, Yang Zhang
Categories: cs.CV cs.HC cs.LG
Comments: 16 pages, 11 figures, Presented at ACM CHI 2026. For associated
  codebase, see https://github.com/hilab-open-source/deltadorsal
\\
  The proliferation of XR devices has made egocentric hand pose estimation a
vital task, yet this perspective is inherently challenged by frequent finger
occlusions. To address this, we propose a novel approach that leverages the
rich information in dorsal hand skin deformation, unlocked by recent advances
in dense visual featurizers. We introduce a dual-stream delta encoder that
learns pose by contrasting features from a dynamic hand with a baseline relaxed
position. Our evaluation demonstrates that, using only cropped dorsal images,
our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in
self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art
techniques that depend on the whole hand's geometry and large model backbones.
Consequently, our method not only enhances the reliability of downstream tasks
like index finger pinch and tap estimation in occluded scenarios but also
unlocks new interaction paradigms, such as detecting isometric force for a
surface "click" without visible movement while minimizing model size.
\\ ( https://arxiv.org/abs/2601.15516 ,  3320kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15518 (*cross-listing*)
Date: Wed, 21 Jan 2026 23:09:17 GMT   (857kb)

Title: DS@GT at TREC TOT 2025: Bridging Vague Recollection with Fusion
  Retrieval and Learned Reranking
Authors: Wenxin Zhou, Ritesh Mehta, Anthony Miyaguchi
Categories: cs.IR cs.CL cs.LG
Comments: Paper submitted to TREC 2025 (34th Text REtrieval Conference)
\\
  We develop a two-stage retrieval system that combines multiple complementary
retrieval methods with a learned reranker and LLM-based reranking, to address
the TREC Tip-of-the-Tongue (ToT) task. In the first stage, we employ hybrid
retrieval that merges LLM-based retrieval, sparse (BM25), and dense (BGE-M3)
retrieval methods. We also introduce topic-aware multi-index dense retrieval
that partitions the Wikipedia corpus into 24 topical domains. In the second
stage, we evaluate both a trained LambdaMART reranker and LLM-based reranking.
To support model training, we generate 5000 synthetic ToT queries using LLMs.
Our best system achieves recall of 0.66 and NDCG@1000 of 0.41 on the test set
by combining hybrid retrieval with Gemini-2.5-flash reranking, demonstrating
the effectiveness of fusion retrieval.
\\ ( https://arxiv.org/abs/2601.15518 ,  857kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15561 (*cross-listing*)
Date: Thu, 22 Jan 2026 01:01:35 GMT   (2156kb)

Title: Enhanced Convergence in p-bit Based Simulated Annealing with Partial
  Deactivation for Large-Scale Combinatorial Optimization Problems
Authors: Naoya Onizawa and Takahiro Hanyu
Categories: cs.ET cs.LG
Comments: 17 pages
DOI: 10.1038/s41598-024-51639-x
\\
  This article critically investigates the limitations of the simulated
annealing algorithm using probabilistic bits (pSA) in solving large-scale
combinatorial optimization problems. The study begins with an in-depth analysis
of the pSA process, focusing on the issues resulting from unexpected
oscillations among p-bits. These oscillations hinder the energy reduction of
the Ising model and thus obstruct the successful execution of pSA in complex
tasks. Through detailed simulations, we unravel the root cause of this energy
stagnation, identifying the feedback mechanism inherent to the pSA operation as
the primary contributor to these disruptive oscillations. To address this
challenge, we propose two novel algorithms, time average pSA (TApSA) and
stalled pSA (SpSA). These algorithms are designed based on partial deactivation
of p-bits and are thoroughly tested using Python simulations on maximum cut
benchmarks that are typical combinatorial optimization problems. On the 16
benchmarks from 800 to 5,000 nodes, the proposed methods improve the normalized
cut value from 0.8% to 98.4% on average in comparison with the conventional
pSA.
\\ ( https://arxiv.org/abs/2601.15561 ,  2156kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15641 (*cross-listing*)
Date: Thu, 22 Jan 2026 04:43:53 GMT   (409kb)

Title: Machine Failure Detection Based on Projected Quantum Models
Authors: Larry Bowden, Qi Chu, Bernard Cena, Kentaro Ohno, Bob Parney, Deepak
  Sharma, Mitsuharu Takeori
Categories: quant-ph cs.LG
Comments: 9 pages
\\
  Detecting machine failures promptly is of utmost importance in industry for
maintaining efficiency and minimizing downtime. This paper introduces a failure
detection algorithm based on quantum computing and a statistical change-point
detection approach. Our method leverages the potential of projected quantum
feature maps to enhance the precision of anomaly detection in machine
monitoring systems. We empirically validate our approach on benchmark
multi-dimensional time series datasets as well as on a real-world dataset
comprising IoT sensor readings from operational machines, ensuring the
practical relevance of our study. The algorithm was executed on IBM's 133-qubit
Heron quantum processor, demonstrating the feasibility of integrating quantum
computing into industrial maintenance procedures. The presented results
underscore the effectiveness of our quantum-based failure detection system,
showcasing its capability to accurately identify anomalies in noisy time series
data. This work not only highlights the potential of quantum computing in
industrial diagnostics but also paves the way for more sophisticated quantum
algorithms in the realm of predictive maintenance.
\\ ( https://arxiv.org/abs/2601.15641 ,  409kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15676 (*cross-listing*)
Date: Thu, 22 Jan 2026 05:57:25 GMT   (1928kb)

Title: Bridging the Perception Gap: A Lightweight Coarse-to-Fine Architecture
  for Edge Audio Systems
Authors: Hengfan Zhang, Yueqian Lin, Hai Helen Li, Yiran Chen
Categories: cs.SD cs.LG eess.AS
Comments: 10 pages, 3 figures, 2 tables. Preprint
\\
  Deploying Audio-Language Models (Audio-LLMs) on edge infrastructure exposes a
persistent tension between perception depth and computational efficiency.
Lightweight local models tend to produce passive perception - generic summaries
that miss the subtle evidence required for multi-step audio reasoning - while
indiscriminate cloud offloading incurs unacceptable latency, bandwidth cost,
and privacy risk. We propose CoFi-Agent (Tool-Augmented Coarse-to-Fine Agent),
a hybrid architecture targeting edge servers and gateways. It performs fast
local perception and triggers conditional forensic refinement only when
uncertainty is detected. CoFi-Agent runs an initial single-pass on a local 7B
Audio-LLM, then a cloud controller gates difficult cases and issues lightweight
plans for on-device tools such as temporal re-listening and local ASR. On the
MMAR benchmark, CoFi-Agent improves accuracy from 27.20% to 53.60%, while
achieving a better accuracy-efficiency trade-off than an always-on
investigation pipeline. Overall, CoFi-Agent bridges the perception gap via
tool-enabled, conditional edge-cloud collaboration under practical system
constraints.
\\ ( https://arxiv.org/abs/2601.15676 ,  1928kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15688 (*cross-listing*)
Date: Thu, 22 Jan 2026 06:17:08 GMT   (1915kb)

Title: Performance-guided Reinforced Active Learning for Object Detection
Authors: Zhixuan Liang, Xingyu Zeng, Rui Zhao, Ping Luo
Categories: cs.CV cs.LG
Comments: Accepted by ICASSP 2026. Camera-ready Version
\\
  Active learning (AL) strategies aim to train high-performance models with
minimal labeling efforts, only selecting the most informative instances for
annotation. Current approaches to evaluating data informativeness predominantly
focus on the data's distribution or intrinsic information content and do not
directly correlate with downstream task performance, such as mean average
precision (mAP) in object detection. Thus, we propose Performance-guided (i.e.
mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel
approach that leverages the concept of expected model output changes as
informativeness. To address the combinatorial explosion challenge of batch
sample selection and the non-differentiable correlation between model
performance and selected batches, MGRAL skillfully employs a reinforcement
learning-based sampling agent that optimizes selection using policy gradient
with mAP improvement as reward. Moreover, to reduce the computational overhead
of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way
with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL's
active learning performance on detection tasks over PASCAL VOC and COCO
benchmarks. Our approach demonstrates the highest AL curve with convincing
visualizations, establishing a new paradigm in reinforcement learning-driven
active object detection.
\\ ( https://arxiv.org/abs/2601.15688 ,  1915kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15697 (*cross-listing*)
Date: Thu, 22 Jan 2026 06:51:45 GMT   (229kb)

Title: Balancing Security and Privacy: The Pivotal Role of AI in Modern
  Healthcare Systems
Authors: Binu V P, Deepthy K Bhaskar, Minimol B
Categories: cs.CR cs.LG
\\
  As digital threats continue to grow, organizations must find ways to enhance
security while protecting user privacy. This paper explores how artificial
intelligence (AI) plays a crucial role in achieving this balance. AI
technologies can improve security by detecting threats, monitoring systems, and
automating responses. However, using AI also raises privacy concerns that need
careful consideration.We examine real-world examples from the healthcare sector
to illustrate how organizations can implement AI solutions that strengthen
security without compromising patient privacy. Additionally, we discuss the
importance of creating transparent AI systems and adhering to privacy
regulations.Ultimately, this paper provides insights and recommendations for
integrating AI into healthcare security practices, helping organizations
navigate the challenges of modern management while keeping patient data safe.
\\ ( https://arxiv.org/abs/2601.15697 ,  229kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15759 (*cross-listing*)
Date: Thu, 22 Jan 2026 08:49:33 GMT   (17991kb)

Title: Atlas-Assisted Segment Anything Model for Fetal Brain MRI (FeTal-SAM)
Authors: Qi Zeng, Weide Liu, Bo Li, Ryne Didier, P. Ellen Grant, Davood Karimi
Categories: cs.CV cs.LG
\\
  This paper presents FeTal-SAM, a novel adaptation of the Segment Anything
Model (SAM) tailored for fetal brain MRI segmentation. Traditional deep
learning methods often require large annotated datasets for a fixed set of
labels, making them inflexible when clinical or research needs change. By
integrating atlas-based prompts and foundation-model principles, FeTal-SAM
addresses two key limitations in fetal brain MRI segmentation: (1) the need to
retrain models for varying label definitions, and (2) the lack of insight into
whether segmentations are driven by genuine image contrast or by learned
spatial priors. We leverage multi-atlas registration to generate spatially
aligned label templates that serve as dense prompts, alongside a bounding-box
prompt, for SAM's segmentation decoder. This strategy enables binary
segmentation on a per-structure basis, which is subsequently fused to
reconstruct the full 3D segmentation volumes. Evaluations on two datasets, the
dHCP dataset and an in-house dataset demonstrate FeTal-SAM's robust performance
across gestational ages. Notably, it achieves Dice scores comparable to
state-of-the-art baselines which were trained for each dataset and label
definition for well-contrasted structures like cortical plate and cerebellum,
while maintaining the flexibility to segment any user-specified anatomy.
Although slightly lower accuracy is observed for subtle, low-contrast
structures (e.g., hippocampus, amygdala), our results highlight FeTal-SAM's
potential to serve as a general-purpose segmentation model without exhaustive
retraining. This method thus constitutes a promising step toward clinically
adaptable fetal brain MRI analysis tools.
\\ ( https://arxiv.org/abs/2601.15759 ,  17991kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15813 (*cross-listing*)
Date: Thu, 22 Jan 2026 10:01:01 GMT   (1555kb)

Title: Beyond Off-the-Shelf Models: A Lightweight and Accessible Machine
  Learning Pipeline for Ecologists Working with Image Data
Authors: Clare Chemery, Hendrik Edelhoff, Ludwig Bothmann
Categories: cs.CV cs.LG
\\
  We introduce a lightweight experimentation pipeline designed to lower the
barrier for applying machine learning (ML) methods for classifying images in
ecological research. We enable ecologists to experiment with ML models
independently, thus they can move beyond off-the-shelf models and generate
insights tailored to local datasets and specific classification tasks and
target variables. Our tool combines a simple command-line interface for
preprocessing, training, and evaluation with a graphical interface for
annotation, error analysis, and model comparison. This design enables
ecologists to build and iterate on compact, task-specific classifiers without
requiring advanced ML expertise. As a proof of concept, we apply the pipeline
to classify red deer (Cervus elaphus) by age and sex from 3392 camera trap
images collected in the Veldenstein Forest, Germany. Using 4352 cropped images
containing individual deer labeled by experts, we trained and evaluated
multiple backbone architectures with a wide variety of parameters and data
augmentation strategies. Our best-performing models achieved 90.77% accuracy
for age classification and 96.15% for sex classification. These results
demonstrate that reliable demographic classification is feasible even with
limited data to answer narrow, well-defined ecological problems. More broadly,
the framework provides ecologists with an accessible tool for developing ML
models tailored to specific research questions, paving the way for broader
adoption of ML in wildlife monitoring and demographic analysis.
\\ ( https://arxiv.org/abs/2601.15813 ,  1555kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15846 (*cross-listing*)
Date: Thu, 22 Jan 2026 10:53:50 GMT   (539kb)

Title: Determinants of Training Corpus Size for Clinical Text Classification
Authors: Jaya Chaturvedi, Saniya Deshpande, Chenkai Ma, Robert Cobb, Angus
  Roberts, Robert Stewart, Daniel Stahl, Diana Shamsutdinova
Categories: cs.CL cs.LG
\\
  Introduction: Clinical text classification using natural language processing
(NLP) models requires adequate training data to achieve optimal performance.
For that, 200-500 documents are typically annotated. The number is constrained
by time and costs and lacks justification of the sample size requirements and
their relationship to text vocabulary properties.
  Methods: Using the publicly available MIMIC-III dataset containing hospital
discharge notes with ICD-9 diagnoses as labels, we employed pre-trained BERT
embeddings followed by Random Forest classifiers to identify 10 randomly
selected diagnoses, varying training corpus sizes from 100 to 10,000 documents,
and analyzed vocabulary properties by identifying strong and noisy predictive
words through Lasso logistic regression on bag-of-words embeddings.
  Results: Learning curves varied significantly across the 10 classification
tasks despite identical preprocessing and algorithms, with 600 documents
sufficient to achieve 95% of the performance attainable with 10,000 documents
for all tasks. Vocabulary analysis revealed that more strong predictors and
fewer noisy predictors were associated with steeper learning curves, where
every 100 additional noisy words decreased accuracy by approximately 0.02 while
100 additional strong predictors increased maximum accuracy by approximately
0.04.
\\ ( https://arxiv.org/abs/2601.15846 ,  539kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15865 (*cross-listing*)
Date: Thu, 22 Jan 2026 11:14:37 GMT   (13kb)

Title: A Lightweight Brain-Inspired Machine Learning Framework for Coronary
  Angiography: Hybrid Neural Representation and Robust Learning Strategies
Authors: Jingsong Xia and Siqi Wang
Categories: cs.CV cs.LG
\\
  Background: Coronary angiography (CAG) is a cornerstone imaging modality for
assessing coronary artery disease and guiding interventional treatment
decisions. However, in real-world clinical settings, angiographic images are
often characterized by complex lesion morphology, severe class imbalance, label
uncertainty, and limited computational resources, posing substantial challenges
to conventional deep learning approaches in terms of robustness and
generalization.Methods: The proposed framework is built upon a pretrained
convolutional neural network to construct a lightweight hybrid neural
representation. A selective neural plasticity training strategy is introduced
to enable efficient parameter adaptation. Furthermore, a brain-inspired
attention-modulated loss function, combining Focal Loss with label smoothing,
is employed to enhance sensitivity to hard samples and uncertain annotations.
Class-imbalance-aware sampling and cosine annealing with warm restarts are
adopted to mimic rhythmic regulation and attention allocation mechanisms
observed in biological neural systems.Results: Experimental results demonstrate
that the proposed lightweight brain-inspired model achieves strong and stable
performance in binary coronary angiography classification, yielding competitive
accuracy, recall, F1-score, and AUC metrics while maintaining high
computational efficiency.Conclusion: This study validates the effectiveness of
brain-inspired learning mechanisms in lightweight medical image analysis and
provides a biologically plausible and deployable solution for intelligent
clinical decision support under limited computational resources.
\\ ( https://arxiv.org/abs/2601.15865 ,  13kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15872 (*cross-listing*)
Date: Thu, 22 Jan 2026 11:21:54 GMT   (693kb)

Title: PF-D2M: A Pose-free Diffusion Model for Universal Dance-to-Music
  Generation
Authors: Jaekwon Im, Natalia Polouliakh, Taketo Akama
Categories: cs.SD cs.CV cs.LG cs.MM eess.AS
Comments: 4 pages, 2 figures
\\
  Dance-to-music generation aims to generate music that is aligned with dance
movements. Existing approaches typically rely on body motion features extracted
from a single human dancer and limited dance-to-music datasets, which restrict
their performance and applicability to real-world scenarios involving multiple
dancers and non-human dancers. In this paper, we propose PF-D2M, a universal
diffusion-based dance-to-music generation model that incorporates visual
features extracted from dance videos. PF-D2M is trained with a progressive
training strategy that effectively addresses data scarcity and generalization
challenges. Both objective and subjective evaluations show that PF-D2M achieves
state-of-the-art performance in dance-music alignment and music quality.
\\ ( https://arxiv.org/abs/2601.15872 ,  693kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16041 (*cross-listing*)
Date: Thu, 22 Jan 2026 15:18:36 GMT   (274kb)

Title: Risk reversal for least squares estimators under nested convex
  constraints
Authors: Omar Al-Ghattas
Categories: math.ST cs.LG math.OC stat.TH
Comments: 31 pages, 5 figures
\\
  In constrained stochastic optimization, one naturally expects that imposing a
stricter feasible set does not increase the statistical risk of an estimator
defined by projection onto that set. In this paper, we show that this intuition
can fail even in canonical settings.
  We study the Gaussian sequence model, a deliberately austere test best, where
for a compact, convex set $\Theta \subset \mathbb{R}^d$ one observes \[ Y =
\theta^\star + \sigma Z, \qquad Z \sim N(0, I_d), \] and seeks to estimate an
unknown parameter $\theta^\star \in \Theta$. The natural estimator is the least
squares estimator (LSE), which coincides with the Euclidean projection of $Y$
onto $\Theta$. We construct an explicit example exhibiting \emph{risk
reversal}: for sufficiently large noise, there exist nested compact convex sets
$\Theta_S \subset \Theta_L$ and a parameter $\theta^\star \in \Theta_S$ such
that the LSE constrained to $\Theta_S$ has strictly larger risk than the LSE
constrained to $\Theta_L$. We further show that this phenomenon can persist at
the level of worst-case risk, with the supremum risk over the smaller
constraint set exceeding that over the larger one.
  We clarify this behavior by contrasting noise regimes. In the vanishing-noise
limit, the risk admits a first-order expansion governed by the statistical
dimension of the tangent cone at $\theta^\star$, and tighter constraints
uniformly reduce risk. In contrast, in the diverging-noise regime, the risk is
determined by global geometric interactions between the constraint set and
random noise directions. Here, the embedding of $\Theta_S$ within $\Theta_L$
can reverse the risk ordering.
  These results reveal a previously unrecognized failure mode of
projection-based estimators: in sufficiently noisy settings, tightening a
constraint can paradoxically degrade statistical performance.
\\ ( https://arxiv.org/abs/2601.16041 ,  274kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16070 (*cross-listing*)
Date: Thu, 22 Jan 2026 16:09:00 GMT   (292kb)

Title: On damage of interpolation to adversarial robustness in regression
Authors: Jingfu Peng, Yuhong Yang
Categories: stat.ML cs.LG math.ST stat.TH
\\
  Deep neural networks (DNNs) typically involve a large number of parameters
and are trained to achieve zero or near-zero training error. Despite such
interpolation, they often exhibit strong generalization performance on unseen
data, a phenomenon that has motivated extensive theoretical investigations.
Comforting results show that interpolation indeed may not affect the minimax
rate of convergence under the squared error loss. In the mean time, DNNs are
well known to be highly vulnerable to adversarial perturbations in future
inputs. A natural question then arises: Can interpolation also escape from
suboptimal performance under a future $X$-attack? In this paper, we investigate
the adversarial robustness of interpolating estimators in a framework of
nonparametric regression. A finding is that interpolating estimators must be
suboptimal even under a subtle future $X$-attack, and achieving perfect fitting
can substantially damage their robustness. An interesting phenomenon in the
high interpolation regime, which we term the curse of simple size, is also
revealed and discussed. Numerical experiments support our theoretical findings.
\\ ( https://arxiv.org/abs/2601.16070 ,  292kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16098 (*cross-listing*)
Date: Thu, 22 Jan 2026 16:47:07 GMT   (3575kb)

Title: Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image
  Classification
Authors: Zack Dewis, Yimin Zhu, Zhengsen Xu, Mabel Heffring, Saeid
  Taleghanidoozdoozan, Quinn Ledingham, Lincoln Linlin Xu
Categories: cs.CV cs.LG
Comments: 5 pages, 3 figures
\\
  Although Mamba models greatly improve Hyperspectral Image (HSI)
classification, they have critical challenges in terms defining efficient and
adaptive token sequences for improve performance. This paper therefore presents
CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address
the challenges, with the following contributions. First, to achieve efficient
and adaptive token sequences for improved Mamba performance, we integrate the
clustering mechanism into a spatial Mamba architecture, leading to a
cluster-guided spatial Mamba module (CSpaMamba) that reduces the Mamba sequence
length and improves Mamba feature learning capability. Second, to improve the
learning of both spatial and spectral information, we integrate the CSpaMamba
module with a spectral mamba module (SpeMamba), leading to a complete
clustering-guided spatial-spectral Mamba framework. Third, to further improve
feature learning capability, we introduce an Attention-Driven Token Selection
mechanism to optimize Mamba token sequencing. Last, to seamlessly integrate
clustering into the Mamba model in a coherent manner, we design a Learnable
Clustering Module that learns the cluster memberships in an adaptive manner.
Experiments on the Pavia University, Indian Pines, and Liao-Ning 01 datasets
demonstrate that CSSMamba achieves higher accuracy and better boundary
preservation compared to state-of-the-art CNN, Transformer, and Mamba-based
methods.
\\ ( https://arxiv.org/abs/2601.16098 ,  3575kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16120 (*cross-listing*)
Date: Thu, 22 Jan 2026 17:15:26 GMT   (831kb)

Title: Synthetic Augmentation in Imbalanced Learning: When It Helps, When It
  Hurts, and How Much to Add
Authors: Zhengchi Ma and Anru R. Zhang
Categories: stat.ML cs.LG stat.ME
\\
  Imbalanced classification, where one class is observed far less frequently
than the other, often causes standard training procedures to prioritize the
majority class and perform poorly on rare but important cases. A classic and
widely used remedy is to augment the minority class with synthetic examples,
but two basic questions remain under-resolved: when does synthetic augmentation
actually help, and how many synthetic samples should be generated?
  We develop a unified statistical framework for synthetic augmentation in
imbalanced learning, studying models trained on imbalanced data augmented with
synthetic minority samples and evaluated under the balanced population risk.
Our theory shows that synthetic data is not always beneficial. In a ``local
symmetry" regime, imbalance is not the dominant source of error near the
balanced optimum, so adding synthetic samples cannot improve learning rates and
can even degrade performance by amplifying generator mismatch. When
augmentation can help (a ``local asymmetry" regime), the optimal synthetic size
depends on generator accuracy and on whether the generator's residual mismatch
is directionally aligned with the intrinsic majority-minority shift. This
structure can make the best synthetic size deviate from naive full balancing,
sometimes by a small refinement and sometimes substantially when generator bias
is systematic. Practically, we recommend Validation-Tuned Synthetic Size
(VTSS): select the synthetic size by minimizing balanced validation loss over a
range centered near the fully balanced baseline, while allowing meaningful
departures when the data indicate them. Simulations and a real sepsis
prediction study support the theory and illustrate when synthetic augmentation
helps, when it cannot, and how to tune its quantity effectively.
\\ ( https://arxiv.org/abs/2601.16120 ,  831kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16138 (*cross-listing*)
Date: Thu, 22 Jan 2026 17:32:19 GMT   (358kb)

Title: Automatic Classification of Arabic Literature into Historical Eras
Authors: Zainab Alhathloul and Irfan Ahmad
Categories: cs.CL cs.LG
Comments: 27 pages
\\
  The Arabic language has undergone notable transformations over time,
including the emergence of new vocabulary, the obsolescence of others, and
shifts in word usage. This evolution is evident in the distinction between the
classical and modern Arabic eras. Although historians and linguists have
partitioned Arabic literature into multiple eras, relatively little research
has explored the automatic classification of Arabic texts by time period,
particularly beyond the domain of poetry. This paper addresses this gap by
employing neural networks and deep learning techniques to automatically
classify Arabic texts into distinct eras and periods. The proposed models are
evaluated using two datasets derived from two publicly available corpora,
covering texts from the pre-Islamic to the modern era. The study examines class
setups ranging from binary to 15-class classification and considers both
predefined historical eras and custom periodizations. Results range from
F1-scores of 0.83 and 0.79 on the binary-era classification task using the
OpenITI and APCD datasets, respectively, to 0.20 on the 15-era classification
task using OpenITI and 0.18 on the 12-era classification task using APCD.
\\ ( https://arxiv.org/abs/2601.16138 ,  358kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16142 (*cross-listing*)
Date: Thu, 22 Jan 2026 17:36:19 GMT   (181kb)

Title: Computing Fixpoints of Learned Functions: Chaotic Iteration and Simple
  Stochastic Games
Authors: Paolo Baldan, Sebastian Gurke, Barbara K\"onig, Florian Wittbold
Categories: cs.LO cs.LG
\\
  The problem of determining the (least) fixpoint of (higher-dimensional)
functions over the non-negative reals frequently occurs when dealing with
systems endowed with a quantitative semantics. We focus on the situation in
which the functions of interest are not known precisely but can only be
approximated. As a first contribution we generalize an iteration scheme called
dampened Mann iteration, recently introduced in the literature. The improved
scheme relaxes previous constraints on parameter sequences, allowing learning
rates to converge to zero or not converge at all. While seemingly minor, this
flexibility is essential to enable the implementation of chaotic iterations,
where only a subset of components is updated in each step, allowing to tackle
higher-dimensional problems. Additionally, by allowing learning rates to
converge to zero, we can relax conditions on the convergence speed of function
approximations, making the method more adaptable to various scenarios. We also
show that dampened Mann iteration applies immediately to compute the expected
payoff in various probabilistic models, including simple stochastic games, not
covered by previous work.
\\ ( https://arxiv.org/abs/2601.16142 ,  181kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16158 (*cross-listing*)
Date: Thu, 22 Jan 2026 17:59:31 GMT   (549kb)

Title: Domain-Incremental Continual Learning for Robust and Efficient Keyword
  Spotting in Resource Constrained Systems
Authors: Prakash Dhungana and Sayed Ahmad Salehi
Categories: cs.SD cs.LG
Comments: 12 pages, 8 figures, and 3 tables
ACM-class: I.2.7; C.3
\\
  Keyword Spotting (KWS) systems with small footprint models deployed on edge
devices face significant accuracy and robustness challenges due to domain
shifts caused by varying noise and recording conditions. To address this, we
propose a comprehensive framework for continual learning designed to adapt to
new domains while maintaining computational efficiency. The proposed pipeline
integrates a dual-input Convolutional Neural Network, utilizing both Mel
Frequency Cepstral Coefficients (MFCC) and Mel-spectrogram features, supported
by a multi-stage denoising process, involving discrete wavelet transform and
spectral subtraction techniques, plus model and prototype update blocks. Unlike
prior methods that restrict updates to specific layers, our approach updates
the complete quantized model, made possible due to compact model architecture.
A subset of input samples are selected during runtime using class prototypes
and confidence-driven filtering, which are then pseudo-labeled and combined
with rehearsal buffer for incremental model retraining. Experimental results on
noisy test dataset demonstrate the framework's effectiveness, achieving 99.63\%
accuracy on clean data and maintaining robust performance (exceeding 94\%
accuracy) across diverse noisy environments, even at -10 dB Signal-to-Noise
Ratio. The proposed framework work confirms that integrating efficient
denoising with prototype-based continual learning enables KWS models to operate
autonomously and robustly in resource-constrained, dynamic environments.
\\ ( https://arxiv.org/abs/2601.16158 ,  549kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16174 (*cross-listing*)
Date: Thu, 22 Jan 2026 18:19:52 GMT   (1144kb)

Title: Beyond Predictive Uncertainty: Reliable Representation Learning with
  Structural Constraints
Authors: Yiyao Yang
Categories: stat.ML cs.LG
Comments: 22 pages, 5 figures, 5 propositions
\\
  Uncertainty estimation in machine learning has traditionally focused on the
prediction stage, aiming to quantify confidence in model outputs while treating
learned representations as deterministic and reliable by default. In this work,
we challenge this implicit assumption and argue that reliability should be
regarded as a first-class property of learned representations themselves. We
propose a principled framework for reliable representation learning that
explicitly models representation-level uncertainty and leverages structural
constraints as inductive biases to regularize the space of feasible
representations. Our approach introduces uncertainty-aware regularization
directly in the representation space, encouraging representations that are not
only predictive but also stable, well-calibrated, and robust to noise and
structural perturbations. Structural constraints, such as sparsity, relational
structure, or feature-group dependencies, are incorporated to define meaningful
geometry and reduce spurious variability in learned representations, without
assuming fully correct or noise-free structure. Importantly, the proposed
framework is independent of specific model architectures and can be integrated
with a wide range of representation learning methods.
\\ ( https://arxiv.org/abs/2601.16174 ,  1144kb)
------------------------------------------------------------------------------
\\
arXiv:2601.16194 (*cross-listing*)
Date: Thu, 22 Jan 2026 18:46:46 GMT   (217kb)

Title: A Rolling-Space Branch-and-Price Algorithm for the Multi-Compartment
  Vehicle Routing Problem with Multiple Time Windows
Authors: El Mehdi Er Raqabi, Kevin Dalmeijer, Pascal Van Hentenryck
Categories: math.OC cs.LG
\\
  This paper investigates the multi-compartment vehicle routing problem with
multiple time windows (MCVRPMTW), an extension of the classical vehicle routing
problem with time windows that considers vehicles equipped with multiple
compartments and customers requiring service across several delivery time
windows. The problem incorporates three key compartment-related features: (i)
compartment flexibility in the number of compartments, (ii) item-to-compartment
compatibility, and (iii) item-to-item compatibility. The problem also
accommodates practical operational requirements such as driver breaks. To solve
the MCVRPMTW, we develop an exact branch-and-price (B&P) algorithm in which the
pricing problem is solved using a labeling algorithm. Several acceleration
strategies are introduced to limit symmetry during label extensions, improve
the stability of dual solutions in column generation, and enhance the branching
process. To handle large-scale instances, we propose a rolling-space B&P
algorithm that integrates clustering techniques into the solution framework.
Extensive computational experiments on instances inspired by a real-world
industrial application demonstrate the effectiveness of the proposed approach
and provide useful managerial insights for practical implementation.
\\ ( https://arxiv.org/abs/2601.16194 ,  217kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15774 (*cross-listing*)
Date: Thu, 22 Jan 2026 09:02:35 GMT   (1030kb)

Title: FirmReBugger: A Benchmark Framework for Monolithic Firmware Fuzzers
Authors: Mathew Duong, Michael Chesser, Guy Farrelly, Surya Nepal, Damith C.
  Ranasinghe
Categories: cs.CR cs.SE
Comments: Accepted to USENIX Security 2026. Code, see
  http://myhost.domain/file.mpg
\\
  Monolithic Firmware is widespread. Unsurprisingly, fuzz testing firmware is
an active research field with new advances addressing the unique challenges in
the domain. However, understanding and evaluating improvements by deriving
metrics such as code coverage and unique crashes are problematic, leading to a
desire for a reliable bug-based benchmark. To address the need, we design and
build FirmReBugger, a holistic framework for fairly assessing monolithic
firmware fuzzers with a realistic, diverse, bug-based benchmark. FirmReBugger
proposes using bug oracles--C syntax expressions of bug descriptors--with an
interpreter to automate analysis and accurately report on bugs discovered,
discriminating between states of detected, triggered, reached and not reached.
Importantly, our idea of benchmarking does not modify the target binary and
simply replays fuzzing seeds to isolate the benchmark implementation from the
fuzzer while providing a simple means to extend with new bug oracles. Further,
analyzing fuzzing roadblocks, we created FirmBench, a set of diverse,
real-world binary targets with 313 software bug oracles. Incorporating our
analysis of roadblocks challenging monolithic firmware fuzzing, the bench
provides for rapid evaluation of future advances. We implement FirmReBugger in
a FuzzBench-for-Firmware type service and use FirmBench to evaluate 9
state-of-the art monolithic firmware fuzzers in the style of a reproducibility
study, using a 10 CPU-year effort, to report our findings.
\\ ( https://arxiv.org/abs/2601.15774 ,  1030kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2404.11833
replaced with revised version Thu, 22 Jan 2026 00:03:22 GMT   (90kb)

Title: Thought of Search: Planning with Language Models Through The Lens of
  Efficiency
Authors: Michael Katz, Harsha Kokel, Kavitha Srinivas, Shirin Sohrabi
Categories: cs.AI
Comments: Accepted at NeurIPS 2024,
  https://papers.nips.cc/paper_files/paper/2024/hash/fa080fe0f218871faec1d8ba20e491d5-Abstract-Conference.html
DOI: 10.52202/079017-4395
\\ ( https://arxiv.org/abs/2404.11833 ,  90kb)
------------------------------------------------------------------------------
\\
arXiv:2501.16448
replaced with revised version Thu, 22 Jan 2026 15:27:34 GMT   (36kb)

Title: Information-theoretic Distinctions Between Deception and Confusion
Authors: Robin Young
Categories: cs.AI cs.LG
Comments: Proceedings of the 14th IJCNLP and the 4th AACL (2025)
\\ ( https://arxiv.org/abs/2501.16448 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2503.10883
replaced with revised version Thu, 22 Jan 2026 17:37:12 GMT   (3017kb)

Title: Chat-TS: Enhancing Multi-Modal Reasoning Over Time-Series and Natural
  Language Data
Authors: Paul Quinlan, Qingguo Li, Xiaodan Zhu
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2503.10883 ,  3017kb)
------------------------------------------------------------------------------
\\
arXiv:2504.03729
replaced with revised version Thu, 22 Jan 2026 14:11:43 GMT   (1200kb)

Title: A Scalable Predictive Modelling Approach to Identifying Duplicate
  Adverse Event Reports for Drugs and Vaccines
Authors: Jim W. Barrett, Nils Erlanson, Joana F\'elix China, G. Niklas Nor\'en
Categories: cs.AI cs.LG
Comments: 26 pages, 11 figures
\\ ( https://arxiv.org/abs/2504.03729 ,  1200kb)
------------------------------------------------------------------------------
\\
arXiv:2504.15211
replaced with revised version Thu, 22 Jan 2026 15:49:05 GMT   (28kb)

Title: Embracing Ambiguity: Bayesian Nonparametrics and Stakeholder
  Participation for Ambiguity-Aware Safety Evaluation
Authors: Yanan Long
Categories: cs.AI stat.AP
Comments: AAAI 2026 workshop MURE
\\ ( https://arxiv.org/abs/2504.15211 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2505.10309
replaced with revised version Wed, 21 Jan 2026 22:06:24 GMT   (5278kb)

Title: A large-scale evaluation of commonsense knowledge in humans and large
  language models
Authors: Tuan Dung Nguyen, Duncan J. Watts, Mark E. Whiting
Categories: cs.AI cs.HC cs.SI
Comments: Code and data: https://github.com/Watts-Lab/commonsense-llm-eval
\\ ( https://arxiv.org/abs/2505.10309 ,  5278kb)
------------------------------------------------------------------------------
\\
arXiv:2506.09049
replaced with revised version Thu, 22 Jan 2026 08:52:35 GMT   (7150kb)

Title: VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement
  Learning
Authors: Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu,
  Philip Torr, Lei Bai, Zhenfei Yin
Categories: cs.AI cs.CV cs.RO
Comments: Accepted by NeurIPS 2025 Track on Datasets and Benchmarks. Project
  page: https://faceong.github.io/VIKI-R/
\\ ( https://arxiv.org/abs/2506.09049 ,  7150kb)
------------------------------------------------------------------------------
\\
arXiv:2506.14079
replaced with revised version Wed, 21 Jan 2026 22:06:53 GMT   (7889kb)

Title: FormGym: Doing Paperwork with Agents
Authors: Matthew Toles, Rattandeep Singh, Isaac Song, Zhou Yu
Categories: cs.AI
\\ ( https://arxiv.org/abs/2506.14079 ,  7889kb)
------------------------------------------------------------------------------
\\
arXiv:2508.01693
replaced with revised version Thu, 22 Jan 2026 05:21:39 GMT   (0kb,I)

Title: SURE-Med: Systematic Uncertainty Reduction for Enhanced Reliability in
  Medical Report Generation
Authors: Yuhang Gu and Xingyu Hu and Yuyu Fan and Xulin Yan and Longhuan Xu and
  Peng peng
Categories: cs.AI cs.CV
Comments: fix some problems
\\ ( https://arxiv.org/abs/2508.01693 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2508.04037
replaced with revised version Thu, 22 Jan 2026 05:22:03 GMT   (5111kb)

Title: Evolving in Tasks: Empowering the Multi-modality Large Language Model as
  the Computer Use Agent
Authors: Yuhao Cheng, Liang Tang, Shuxian Li, Yukang Huo, Tiaonan Duan, Kaer
  Huang, Yanzhe Jing, Yiqiang Yan
Categories: cs.AI
\\ ( https://arxiv.org/abs/2508.04037 ,  5111kb)
------------------------------------------------------------------------------
\\
arXiv:2508.12260
replaced with revised version Thu, 22 Jan 2026 17:34:42 GMT   (2698kb)

Title: Mantis: A Foundation Model for Mechanistic Disease Forecasting
Authors: Carson Dudley, Reiden Magdaleno, Christopher Harding, Ananya Sharma,
  Emily Martin, Marisa Eisenberg
Categories: cs.AI q-bio.QM
Comments: 11 pages, 4 figures
\\ ( https://arxiv.org/abs/2508.12260 ,  2698kb)
------------------------------------------------------------------------------
\\
arXiv:2509.24592
replaced with revised version Thu, 22 Jan 2026 09:56:31 GMT   (576kb)

Title: BPMN Assistant: An LLM-Based Approach to Business Process Modeling
Authors: Josip Tomo Licardo, Nikola Tankovic, Darko Etinger
Categories: cs.AI cs.SE
Comments: 22 pages, 5 figures
\\ ( https://arxiv.org/abs/2509.24592 ,  576kb)
------------------------------------------------------------------------------
\\
arXiv:2510.17145
replaced with revised version Thu, 22 Jan 2026 13:19:46 GMT   (5094kb)

Title: Enhanced Fish Freshness Classification with Incremental Handcrafted
  Feature Fusion
Authors: Phi-Hung Hoang, Nam-Thuan Trinh, Van-Manh Tran and Thi-Thu-Hong Phan
Categories: cs.AI
Comments: 35 pages, 6 figures and 11 tables
MSC-class: 68T05, 62H30
\\ ( https://arxiv.org/abs/2510.17145 ,  5094kb)
------------------------------------------------------------------------------
\\
arXiv:2511.10501
replaced with revised version Thu, 22 Jan 2026 10:36:51 GMT   (447kb)

Title: Graph Neural Networks, Deep Reinforcement Learning and Probabilistic
  Topic Modeling for Strategic Multiagent Settings
Authors: Georgios Chalkiadakis and Charilaos Akasiadis and Gerasimos Koresis
  and Stergios Plataniotis and Leonidas Bakopoulos
Categories: cs.AI
Comments: 28 pages
MSC-class: 68-02
\\ ( https://arxiv.org/abs/2511.10501 ,  447kb)
------------------------------------------------------------------------------
\\
arXiv:2512.10046
replaced with revised version Thu, 22 Jan 2026 14:26:01 GMT   (15817kb)

Title: SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban
  Environments for Multimodal Robot Navigation and Collaboration
Authors: Yan Zhuang, Jiawei Ren, Xiaokang Ye, Jianzhi Shen, Ruixuan Zhang,
  Tianai Yue, Muhammad Faayez, Xuhong He, Ziqiao Ma, Lianhui Qin, Zhiting Hu,
  Tianmin Shu
Categories: cs.AI
Comments: Conference: NeurIPS 2025 (main)
\\ ( https://arxiv.org/abs/2512.10046 ,  15817kb)
------------------------------------------------------------------------------
\\
arXiv:2512.22431
replaced with revised version Thu, 22 Jan 2026 02:13:24 GMT   (60kb)

Title: Monadic Context Engineering
Authors: Yifan Zhang, Yang Yuan, Mengdi Wang, Andrew Chi-Chih Yao
Categories: cs.AI cs.CL cs.FL
\\ ( https://arxiv.org/abs/2512.22431 ,  60kb)
------------------------------------------------------------------------------
\\
arXiv:2601.01910
replaced with revised version Thu, 22 Jan 2026 10:24:37 GMT   (2834kb)

Title: MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on
  Path Planning
Authors: Minh Hieu Ha, Khanh Ly Ta, Hung Phan, Tung Doan, Tung Dao, Dao Tran,
  Huynh Thi Thanh Binh
Categories: cs.AI
\\ ( https://arxiv.org/abs/2601.01910 ,  2834kb)
------------------------------------------------------------------------------
\\
arXiv:2601.06795
replaced with revised version Thu, 22 Jan 2026 14:58:18 GMT   (2146kb)

Title: GDEPO: Group Dual-dynamic and Equal-right Advantage Policy Optimization
  with Enhanced Training Data Utilization for Sample-Constrained Reinforcement
  Learning
Authors: Zhengqing Yan, Xinyang Liu, Yi Zhang, Fan Guo, ChengXun Jia, Junchen
  Wan, Yao Liu, Qi Liu, Jihao Huang, Kang Song
Categories: cs.AI
\\ ( https://arxiv.org/abs/2601.06795 ,  2146kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13545
replaced with revised version Thu, 22 Jan 2026 15:45:29 GMT   (601kb)

Title: TruthTensor: Evaluating LLMs through Human Imitation on Prediction
  Market under Drift and Holistic Reasoning
Authors: Shirin Shahabi and Spencer Graham and Haruna Isah
Categories: cs.AI cs.ET cs.MA
Comments: 16 pages, 6 figures, 2 tables
\\ ( https://arxiv.org/abs/2601.13545 ,  601kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14691
replaced with revised version Thu, 22 Jan 2026 05:12:15 GMT   (5760kb)

Title: Gaming the Judge: Unfaithful Chain-of-Thought Can Undermine Agent
  Evaluation
Authors: Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Sungryull Sohn,
  Yunxiang Zhang, Moontae Lee, Hao Peng, Lu Wang, Honglak Lee
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2601.14691 ,  5760kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15120
replaced with revised version Thu, 22 Jan 2026 12:08:34 GMT   (2549kb)

Title: Emerging from Ground: Addressing Intent Deviation in Tool-Using Agents
  via Deriving Real Calls into Virtual Trajectories
Authors: Qian Xiong, Yuekai Huang, Bo Yang, Yujia Zheng, Tianhao Li, Ziyou
  Jiang, Zhiyuan Chang, Zhaoyang Li, Huanxiang Feng, Mingyang Li
Categories: cs.AI
\\ ( https://arxiv.org/abs/2601.15120 ,  2549kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15197
replaced with revised version Thu, 22 Jan 2026 17:01:41 GMT   (657kb)

Title: BayesianVLA: Bayesian Decomposition of Vision Language Action Models via
  Latent Action Queries
Authors: Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen,
  Changti Wu, Yuzhuo Miao, Cong Huang, Kai Chen
Categories: cs.AI cs.CL cs.CV cs.RO
\\ ( https://arxiv.org/abs/2601.15197 ,  657kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19922
replaced with revised version Thu, 22 Jan 2026 14:39:55 GMT   (1448kb)

Title: Representation-Driven Reinforcement Learning
Authors: Ofir Nabati, Guy Tennenholtz and Shie Mannor
Categories: cs.LG cs.AI
Comments: Accepted to ICML 2023
\\ ( https://arxiv.org/abs/2305.19922 ,  1448kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04794
replaced with revised version Thu, 22 Jan 2026 15:12:31 GMT   (362kb)

Title: Scalable Multi-view Clustering via Explicit Kernel Features Maps
Authors: Chakib Fettal, Lazhar Labiod, Mohamed Nadif
Categories: cs.LG
Comments: Version accepted by Data Mining and Knowledge Discovery
\\ ( https://arxiv.org/abs/2402.04794 ,  362kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05406
replaced with revised version Thu, 22 Jan 2026 18:13:50 GMT   (217kb)

Title: Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes
Authors: Steven Kolawole, Lucio Dery, Jean-Fran\c{c}ois Kagy, Virginia Smith,
  Graham Neubig, Ameet Talwalkar
Categories: cs.LG cs.CL
Comments: 19 pages, 6 fiigures, 16 tables
\\ ( https://arxiv.org/abs/2402.05406 ,  217kb)
------------------------------------------------------------------------------
\\
arXiv:2406.01857
replaced with revised version Thu, 22 Jan 2026 09:00:01 GMT   (7324kb)

Title: Neural Green's Operators for Parametric Partial Differential Equations
Authors: Hugo Melchers, Joost Prins, Michael Abdelmalik
Categories: cs.LG cs.NA math.NA
MSC-class: 68T07
ACM-class: I.2.6; G.1.8
\\ ( https://arxiv.org/abs/2406.01857 ,  7324kb)
------------------------------------------------------------------------------
\\
arXiv:2406.12205
replaced with revised version Wed, 21 Jan 2026 21:33:45 GMT   (311kb)

Title: On the Exponential Convergence for Offline RLHF with Pairwise
  Comparisons
Authors: Zhirui Chen and Vincent Y. F. Tan
Categories: cs.LG cs.AI cs.IT math.IT math.ST stat.ML stat.TH
Comments: Accepted as an oral presentation at AAAI 2026 (AI Alignment Track)
\\ ( https://arxiv.org/abs/2406.12205 ,  311kb)
------------------------------------------------------------------------------
\\
arXiv:2412.11139
replaced with revised version Thu, 22 Jan 2026 17:29:53 GMT   (3331kb)

Title: ViSymRe: Vision-guided Multimodal Symbolic Regression
Authors: Da Li, Junping Yin, Jin Xu, Xinxin Li, Juan Zhang
Categories: cs.LG cs.AI cs.SC
\\ ( https://arxiv.org/abs/2412.11139 ,  3331kb)
------------------------------------------------------------------------------
\\
arXiv:2412.19950
replaced with revised version Thu, 22 Jan 2026 11:22:37 GMT   (8668kb)

Title: Data-driven tool wear prediction in milling, based on a
  process-integrated single-sensor approach
Authors: Eric Hirsch and Christian Friedrich
Categories: cs.LG cs.RO eess.SP
Comments: This work is a preprint and has been submitted for possible
  publication,14 pages, 12 figures
\\ ( https://arxiv.org/abs/2412.19950 ,  8668kb)
------------------------------------------------------------------------------
\\
arXiv:2501.06078
replaced with revised version Thu, 22 Jan 2026 14:23:09 GMT   (84kb)

Title: Explaining k-Nearest Neighbors: Abductive and Counterfactual
  Explanations
Authors: Pablo Barcel\'o, Alexander Kozachinskiy, Miguel Romero Orth, Bernardo
  Subercaseaux, Jos\'e Verschae
Categories: cs.LG cs.AI
Comments: 34 pages, 6 figure. The following results are added to the version 2:
  W[1]-hardness of Counterfactual Explanation for l_2-distance when k is a
  parameter, NP-hardness of minimal sufficient reason for l_1-distance for k
  \ge 3, and Sigma_2-hardness of the minimum sufficient reason for Hamming
  distance and k \ge 3
\\ ( https://arxiv.org/abs/2501.06078 ,  84kb)
------------------------------------------------------------------------------
\\
arXiv:2502.02448
replaced with revised version Thu, 22 Jan 2026 08:07:22 GMT   (1519kb)

Title: Sparse Data Diffusion for Scientific Simulations in Biology and Physics
Authors: Phil Ostheimer, Mayank Nagda, Andriy Balinskyy, Jean Radig, Carl
  Herrmann, Stephan Mandt, Marius Kloft, Sophie Fellenz
Categories: cs.LG
Comments: This paper won the Best Paper Award at the SimBioChem workshop at
  EurIPS 2025
\\ ( https://arxiv.org/abs/2502.02448 ,  1519kb)
------------------------------------------------------------------------------
\\
arXiv:2503.15250
replaced with revised version Thu, 22 Jan 2026 14:20:40 GMT   (569kb)

Title: ImputeGAP: A Comprehensive Library for Time Series Imputation
Authors: Quentin Nater, Mourad Khayati
Categories: cs.LG cs.DB
\\ ( https://arxiv.org/abs/2503.15250 ,  569kb)
------------------------------------------------------------------------------
\\
arXiv:2504.02321
replaced with revised version Thu, 22 Jan 2026 04:44:43 GMT   (14kb)

Title: On shallow feedforward neural networks with inputs from a topological
  space
Authors: Vugar Ismailov
Categories: cs.LG math.FA
Comments: Major revision (14 pages): improved exposition, expanded references,
  and additional subsections
Journal-ref: V.E. Ismailov, Ann. Math. Artif. Intell. (2026)
DOI: 10.1007/s10472-026-10003-7
\\ ( https://arxiv.org/abs/2504.02321 ,  14kb)
------------------------------------------------------------------------------
\\
arXiv:2505.01665
replaced with revised version Thu, 22 Jan 2026 01:07:40 GMT   (2466kb)

Title: Adaptively Point-weighting Curriculum Learning
Authors: Wensheng Li, Yichao Tian, Hao Wang, Ruifeng Zhou, Hanting Guan, Chao
  Zhang, Dacheng Tao
Categories: cs.LG
\\ ( https://arxiv.org/abs/2505.01665 ,  2466kb)
------------------------------------------------------------------------------
\\
arXiv:2505.05577
replaced with revised version Wed, 21 Jan 2026 20:24:07 GMT   (901kb)

Title: PyTDC: A multimodal machine learning training, evaluation, and inference
  platform for biomedical foundation models
Authors: Alejandro Velez-Arce and Jesus Caraballo and Marinka Zitnik
Categories: cs.LG cs.AI
Comments: Proceedings of the 42nd International Conference on Machine Learning,
  Vancouver, Canada. PMLR 267, 2025
MSC-class: 68-04, 92-04
ACM-class: D.2.11; I.2.5; J.3
\\ ( https://arxiv.org/abs/2505.05577 ,  901kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18763
replaced with revised version Thu, 22 Jan 2026 17:10:05 GMT   (4454kb)

Title: GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning
Authors: Shutong Ding, Ke Hu, Shan Zhong, Haoyang Luo, Weinan Zhang, Jingya
  Wang, Jun Wang, Ye Shi
Categories: cs.LG
Comments: Accepted by NeurIPS2025
\\ ( https://arxiv.org/abs/2505.18763 ,  4454kb)
------------------------------------------------------------------------------
\\
arXiv:2506.13196
replaced with revised version Thu, 22 Jan 2026 06:31:53 GMT   (2081kb)

Title: KEPLA: A Knowledge-Enhanced Deep Learning Framework for Accurate
  Protein-Ligand Binding Affinity Prediction
Authors: Han Liu, Keyan Ding, Peilin Chen, Yinwei Wei, Liqiang Nie, Dapeng Wu,
  Shiqi Wang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2506.13196 ,  2081kb)
------------------------------------------------------------------------------
\\
arXiv:2507.02921
replaced with revised version Thu, 22 Jan 2026 18:46:50 GMT   (3168kb)

Title: Training-Free Geospatial Place Representation Learning from Large-Scale
  Point-of-Interest Graph Data
Authors: Mohammad Hashemi, Hossein Amiri, Andreas Zufle
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2507.02921 ,  3168kb)
------------------------------------------------------------------------------
\\
arXiv:2507.06775
replaced with revised version Wed, 21 Jan 2026 22:35:46 GMT   (215kb)

Title: Stability, Complexity and Data-Dependent Worst-Case Generalization
  Bounds
Authors: Mario Tuci, Lennart Bastian, Benjamin Dupuis, Nassir Navab, Tolga
  Birdal, Umut \c{S}im\c{s}ekli
Categories: cs.LG math.AT stat.ML
Comments: 29 pages
\\ ( https://arxiv.org/abs/2507.06775 ,  215kb)
------------------------------------------------------------------------------
\\
arXiv:2507.21184
replaced with revised version Thu, 22 Jan 2026 14:07:59 GMT   (1786kb)

Title: Can Language Models Discover Scaling Laws?
Authors: Haowei Lin and Haotian Ye and Wenzheng Feng and Quzhe Huang and Yujun
  Li and Hubert Lim and Zhengrui Li and Xiangyu Wang and Jianzhu Ma and Yitao
  Liang and James Zou
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2507.21184 ,  1786kb)
------------------------------------------------------------------------------
\\
arXiv:2508.14082
replaced with revised version Thu, 22 Jan 2026 01:41:12 GMT   (338kb)

Title: Toward Robust Semi-supervised Regression via Dual-stream Knowledge
  Distillation
Authors: Ye Su, Hezhe Qiao, Wei Huang, Lin Chen
Categories: cs.LG
Comments: 12 pages
\\ ( https://arxiv.org/abs/2508.14082 ,  338kb)
------------------------------------------------------------------------------
\\
arXiv:2509.01187
replaced with revised version Thu, 22 Jan 2026 08:33:34 GMT   (3938kb)

Title: StoxLSTM: A Stochastic Extended Long Short-Term Memory Network for Time
  Series Forecasting
Authors: Zihao Wang, Yunjie Li, Lingmin Zan, Zheng Gong, Mengtao Zhu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2509.01187 ,  3938kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18171
replaced with revised version Thu, 22 Jan 2026 16:28:18 GMT   (2148kb)

Title: FedIA: Towards Domain-Robust Aggregation in Federated Graph Learning
Authors: Zhanting Zhou and KaHou Tam and Yiding Feng and Ziqiang Zheng and Zeyu
  Ma and Yang Yang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2509.18171 ,  2148kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19884
replaced with revised version Thu, 22 Jan 2026 17:41:06 GMT   (106kb)

Title: MCGrad: Multicalibration at Web Scale
Authors: Niek Tax, Lorenzo Perini, Fridolin Linder, Daniel Haimovich, Dima
  Karamshuk, Nastaran Okati, Milan Vojnovic, Pavlos Athanasios Apostolopoulos
Categories: cs.LG
Comments: Accepted at KDD 2026
\\ ( https://arxiv.org/abs/2509.19884 ,  106kb)
------------------------------------------------------------------------------
\\
arXiv:2510.03129
replaced with revised version Thu, 22 Jan 2026 08:38:35 GMT   (896kb)

Title: Signature-Informed Transformer for Asset Allocation
Authors: Yoontae Hwang, Stefan Zohren
Categories: cs.LG cs.AI q-fin.PM
\\ ( https://arxiv.org/abs/2510.03129 ,  896kb)
------------------------------------------------------------------------------
\\
arXiv:2510.04842
replaced with revised version Wed, 21 Jan 2026 21:24:30 GMT   (11266kb)

Title: Distributionally Robust Causal Abstractions
Authors: Yorgos Felekis, Theodoros Damoulas, Paris Giampouras
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2510.04842 ,  11266kb)
------------------------------------------------------------------------------
\\
arXiv:2510.19950
replaced with revised version Thu, 22 Jan 2026 06:31:29 GMT   (3233kb)

Title: Robust Reinforcement Learning in Finance: Modeling Market Impact with
  Elliptic Uncertainty Sets
Authors: Shaocong Ma, Heng Huang
Categories: cs.LG cs.AI math.OC
\\ ( https://arxiv.org/abs/2510.19950 ,  3233kb)
------------------------------------------------------------------------------
\\
arXiv:2510.24235
replaced with revised version Thu, 22 Jan 2026 10:20:06 GMT   (3066kb)

Title: PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware
  Task-Adaptive Reward Modeling
Authors: Ai Jian, Jingqing Ruan, Xing Ma, Dailin Li, Weipeng Zhang, Ke Zeng and
  Xunliang Cai
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2510.24235 ,  3066kb)
------------------------------------------------------------------------------
\\
arXiv:2511.07603
replaced with revised version Wed, 21 Jan 2026 19:41:02 GMT   (0kb,I)

Title: One Router to Route Them All: Homogeneous Expert Routing for
  Heterogeneous Graph Transformers
Authors: Georgiy Shakirov, Albert Arakelov
Categories: cs.LG cs.AI
Comments: This preprint has been withdrawn by the authors due to significant
  revisions in preparation for conference submission. A substantially updated
  version will be submitted separately
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2511.07603 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2511.12417
replaced with revised version Wed, 21 Jan 2026 23:25:37 GMT   (292kb)

Title: Integrating Neural Differential Forecasting with Safe Reinforcement
  Learning for Blood Glucose Regulation
Authors: Yushen Liu, Yanfu Zhang, Xugui Zhou
Categories: cs.LG
Comments: 5 pages, 3 figures, ISBI 2026
Journal-ref: ISBI 2026
\\ ( https://arxiv.org/abs/2511.12417 ,  292kb)
------------------------------------------------------------------------------
\\
arXiv:2511.16665
replaced with revised version Wed, 21 Jan 2026 23:19:53 GMT   (1469kb)

Title: Taming the Long-Tail: Efficient Reasoning RL Training with Adaptive
  Drafter
Authors: Qinghao Hu, Shang Yang, Junxian Guo, Xiaozhe Yao, Yujun Lin, Yuxian
  Gu, Han Cai, Chuang Gan, Ana Klimovic, Song Han
Categories: cs.LG cs.AI cs.DC
\\ ( https://arxiv.org/abs/2511.16665 ,  1469kb)
------------------------------------------------------------------------------
\\
arXiv:2511.17629
replaced with revised version Wed, 21 Jan 2026 21:17:36 GMT   (2194kb)

Title: Boundary-Aware Adversarial Filtering for Reliable Diagnosis under
  Extreme Class Imbalance
Authors: Yanxuan Yu, Michael S. Hughes, Julien Lee, Jiacheng Zhou, and Andrew
  F. Laine
Categories: cs.LG
Comments: Accepted to IEEE ISBI 2026
\\ ( https://arxiv.org/abs/2511.17629 ,  2194kb)
------------------------------------------------------------------------------
\\
arXiv:2511.18457
replaced with revised version Thu, 22 Jan 2026 11:33:21 GMT   (857kb)

Title: Radiation-Preserving Selective Imaging for Pediatric Hip Dysplasia: A
  Cross-Modal Ultrasound-Xray Policy with Limited Labels
Authors: Duncan Stothers, Ben Stothers, Emily Schaeffer, Kishore Mulpuri
Categories: cs.LG cs.CV
Comments: Accepted (with oral presentation) to the AAAI 2026 AI for Medicine
  and Healthcare Bridge Program Awarded Best Paper Runner-Up at the AAAI 2026
  AI for Medicine and Healthcare Bridge Program
\\ ( https://arxiv.org/abs/2511.18457 ,  857kb)
------------------------------------------------------------------------------
\\
arXiv:2511.19935
replaced with revised version Wed, 21 Jan 2026 23:43:42 GMT   (614kb)

Title: EfficientXpert: Efficient Domain Adaptation for Large Language Models
  via Propagation-Aware Pruning
Authors: Songlin Zhao, Michael Pitts, Zhuwei Qin
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2511.19935 ,  614kb)
------------------------------------------------------------------------------
\\
arXiv:2511.20257
replaced with revised version Thu, 22 Jan 2026 08:30:16 GMT   (474kb)

Title: Interpretable Air Pollution Forecasting by Physics-Guided Spatiotemporal
  Decoupling
Authors: Zhiguo Zhang and Xiaoliang Ma and Daniel Schlesinger
Categories: cs.LG cs.AI
Comments: Accepted to 2025 IEEE International Conference on Big Data. v2
  corrects grant numbers
\\ ( https://arxiv.org/abs/2511.20257 ,  474kb)
------------------------------------------------------------------------------
\\
arXiv:2512.00783
replaced with revised version Thu, 22 Jan 2026 10:28:40 GMT   (504kb)

Title: Sigma: The Key for Vision-Language-Action Models toward Telepathic
  Alignment
Authors: Libo Wang
Categories: cs.LG cs.RO
Comments: The Sigma model has been open-sourced on Hugging Face. Weights,
  dataset, some scripts, and logs are all available. The link is:
  https://huggingface.co/Veltraxor/Sigma
\\ ( https://arxiv.org/abs/2512.00783 ,  504kb)
------------------------------------------------------------------------------
\\
arXiv:2512.08671
replaced with revised version Wed, 21 Jan 2026 21:48:56 GMT   (0kb,I)

Title: DS FedProxGrad: Asymptotic Stationarity Without Noise Floor in Fair
  Federated Learning
Authors: Huzaifa Arif
Categories: cs.LG stat.ML
Comments: Withdrawn due to an error in the proof; a corrected version will be
  posted
\\ ( https://arxiv.org/abs/2512.08671 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2512.10350
replaced with revised version Thu, 22 Jan 2026 07:04:31 GMT   (3719kb)

Title: Dynamics of Agentic Loops in Large Language Models: A Geometric Theory
  of Trajectories
Authors: Nicolas Tacheny
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2512.10350 ,  3719kb)
------------------------------------------------------------------------------
\\
arXiv:2512.12930
replaced with revised version Thu, 22 Jan 2026 01:38:36 GMT   (3615kb)

Title: SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference
  via Hierarchical Group Quantization and SVD-Guided Mixed Precision
Authors: Yuseon Choi, Sangjin Kim, Jungjun Oh, Byeongcheol Kim, and Hoi-Jun Yoo
Categories: cs.LG cs.AR
Comments: IEEE International Symposium on Circuits and Systems (ISCAS) 2026
\\ ( https://arxiv.org/abs/2512.12930 ,  3615kb)
------------------------------------------------------------------------------
\\
arXiv:2512.23916
replaced with revised version Wed, 21 Jan 2026 20:37:33 GMT   (7640kb)

Title: Constraint Breeds Generalization: Temporal Dynamics as an Inductive Bias
Authors: Xia Chen
Categories: cs.LG
Comments: 8 pages, 7 figures
MSC-class: 68T05
\\ ( https://arxiv.org/abs/2512.23916 ,  7640kb)
------------------------------------------------------------------------------
\\
arXiv:2601.05679
replaced with revised version Thu, 22 Jan 2026 00:35:18 GMT   (1045kb)

Title: Do Sparse Autoencoders Identify Reasoning Features in Language Models?
Authors: George Ma, Zhongyuan Liang, Irene Y. Chen, Somayeh Sojoudi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2601.05679 ,  1045kb)
------------------------------------------------------------------------------
\\
arXiv:2601.06487
replaced with revised version Thu, 22 Jan 2026 09:16:57 GMT   (1767kb)

Title: ArenaRL: Scaling RL for Open-Ended Agents via Tournament-based Relative
  Ranking
Authors: Qiang Zhang, Boli Chen, Fanrui Zhang, Ruixue Ding, Shihang Wang,
  Qiuchen Wang, Yinfeng Huang, Haonan Zhang, Rongxiang Zhu, Pengyong Wang,
  Ailin Ren, Xin Li, Pengjun Xie, Jiawei Liu, Ning Guo, Jingren Zhou, Zheng-Jun
  Zha
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2601.06487 ,  1767kb)
------------------------------------------------------------------------------
\\
arXiv:2601.07903
replaced with revised version Thu, 22 Jan 2026 13:19:01 GMT   (677kb)

Title: Enhancing Large Language Models for Time-Series Forecasting via
  Vector-Injected In-Context Learning
Authors: Jianqi Zhang, Jingyao Wang, Wenwen Qiang, Fanjiang Xu, Changwen Zheng
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2601.07903 ,  677kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08521
replaced with revised version Thu, 22 Jan 2026 03:42:38 GMT   (1255kb)

Title: Your Group-Relative Advantage Is Biased
Authors: Fengkai Yang, Zherui Chen, Xiaohan Wang, Xiaodong Lu, Jiajun Chai,
  Guojun Yin, Wei Lin, Shuai Ma, Fuzhen Zhuang, Deqing Wang, Yaodong Yang,
  Jianxin Li, Yikun Ban
Categories: cs.LG
\\ ( https://arxiv.org/abs/2601.08521 ,  1255kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08549
replaced with revised version Thu, 22 Jan 2026 10:10:04 GMT   (2141kb)

Title: Contrastive and Multi-Task Learning on Noisy Brain Signals with
  Nonlinear Dynamical Signatures
Authors: Sucheta Ghosh and Zahra Monfared and Felix Dietrich
Categories: cs.LG cs.AI
ACM-class: I.2.6; I.5.1; J.3; G.1.6
\\ ( https://arxiv.org/abs/2601.08549 ,  2141kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08893
replaced with revised version Wed, 21 Jan 2026 22:24:36 GMT   (30kb)

Title: Spectral Generative Flow Models: A Physics-Inspired Replacement for
  Vectorized Large Language Models
Authors: Andrew Kiruluta
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2601.08893 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2601.10498
replaced with revised version Thu, 22 Jan 2026 03:49:02 GMT   (425kb)

Title: PROMA: Projected Microbatch Accumulation for Reference-Free Proximal
  Policy Updates
Authors: Nilin Abrahamsen
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2601.10498 ,  425kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13448
replaced with revised version Thu, 22 Jan 2026 13:03:55 GMT   (3038kb)

Title: Fairness-informed Pareto Optimization : An Efficient Bilevel Framework
Authors: Sofiane Tanji and Samuel Vaiter and Yassine Laguel
Categories: cs.LG math.OC stat.ML
\\ ( https://arxiv.org/abs/2601.13448 ,  3038kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13710
replaced with revised version Thu, 22 Jan 2026 13:39:47 GMT   (6065kb)

Title: Who Benefits From Sinus Surgery? Comparing Generative AI and Supervised
  Machine Learning for Predicting Surgical Outcomes in Chronic Rhinosinusitis
Authors: Sayeed Shafayet Chowdhury, Snehasis Mukhopadhyay, Shiaofen Fang, and
  Vijay R. Ramakrishnan
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2601.13710 ,  6065kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14541
replaced with revised version Thu, 22 Jan 2026 02:12:57 GMT   (96kb)

Title: Report for NSF Workshop on AI for Electronic Design Automation
Authors: Deming Chen, Vijay Ganesh, Weikai Li, Yingyan Celine Lin, Yong Liu,
  Subhasish Mitra, David Z. Pan, Ruchir Puri, Jason Cong, Yizhou Sun
Categories: cs.LG cs.AI cs.AR
\\ ( https://arxiv.org/abs/2601.14541 ,  96kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14758
replaced with revised version Thu, 22 Jan 2026 02:34:00 GMT   (8082kb)

Title: Mechanism Shift During Post-training from Autoregressive to Masked
  Diffusion Language Models
Authors: Injin Kong, Hyoungjoon Lee, Yohan Jo
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2601.14758 ,  8082kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14855
replaced with revised version Thu, 22 Jan 2026 10:33:23 GMT   (16326kb)

Title: Adaptive Exponential Integration for Stable Gaussian Mixture Black-Box
  Variational Inference
Authors: Baojun Che and Yifan Chen and Daniel Zhengyu Huang and Xinying Mao and
  Weijie Wang
Categories: cs.LG
Comments: 26 pages, 7 figures
\\ ( https://arxiv.org/abs/2601.14855 ,  16326kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15249
replaced with revised version Thu, 22 Jan 2026 15:51:15 GMT   (27889kb)

Title: Recommending Best Paper Awards for ML/AI Conferences via the Isotonic
  Mechanism
Authors: Garrett G. Wen, Buxin Su, Natalie Collina, Zhun Deng, Weijie Su
Categories: cs.LG cs.AI cs.GT stat.ME
\\ ( https://arxiv.org/abs/2601.15249 ,  27889kb)
------------------------------------------------------------------------------
\\
arXiv:2406.09843
replaced with revised version Thu, 22 Jan 2026 05:39:24 GMT   (2813kb)

Title: A Comprehensive Study on Large Language Models for Mutation Testing
Authors: Bo Wang, Mingda Chen, Ming Deng, Youfang Lin, Mark Harman, Mike
  Papadakis and Jie M. Zhang
Categories: cs.SE
Comments: 39 pages, 4 figures
ACM-class: D.2.5
\\ ( https://arxiv.org/abs/2406.09843 ,  2813kb)
------------------------------------------------------------------------------
\\
arXiv:2412.10099
replaced with revised version Thu, 22 Jan 2026 07:41:14 GMT   (2451kb)

Title: Unexpected but informative: What fixation-related potentials tell us
  about the processing of confusing program code
Authors: Annabelle Bergum, Anna-Maria Maurer, Norman Peitek, Regine Bader, Axel
  Mecklinger, Vera Demberg, Janet Siegmund, Sven Apel
Categories: cs.SE
\\ ( https://arxiv.org/abs/2412.10099 ,  2451kb)
------------------------------------------------------------------------------
\\
arXiv:2502.03365
replaced with revised version Thu, 22 Jan 2026 11:23:52 GMT   (448kb)

Title: A Match Made in Heaven? AI-driven Matching of Vulnerabilities and
  Security Unit Tests
Authors: Emanuele Iannone, Quang-Cuong Bui, Riccardo Scandariato
Categories: cs.SE cs.CR cs.LG
Comments: Accepted in the MSR 2026 Technical Track. This work was partially
  supported by EU-funded project Sec4AI4Sec (grant no. 101120393)
ACM-class: D.2.5; D.2.7
\\ ( https://arxiv.org/abs/2502.03365 ,  448kb)
------------------------------------------------------------------------------
\\
arXiv:2506.11003
replaced with revised version Thu, 22 Jan 2026 11:31:58 GMT   (8846kb)

Title: EmbedAgent: Benchmarking Large Language Models in Embedded System
  Development
Authors: Ruiyang Xu, Jialun Cao, Mingyuan Wu, Wenliang Zhong, Yaojie Lu, Ben
  He, Xianpei Han, Shing-Chi Cheung, Le Sun
Categories: cs.SE cs.AI
Comments: 12 pages, accept by icse
\\ ( https://arxiv.org/abs/2506.11003 ,  8846kb)
------------------------------------------------------------------------------
\\
arXiv:2510.15408
replaced with revised version Thu, 22 Jan 2026 07:15:55 GMT   (205kb)

Title: Community Engagement and the Lifespan of Open-Source Software Projects
Authors: Mohit Kaushik and Kuljit Kaur Chahal
Categories: cs.SE
Comments: Author Name updated
DOI: 10.1016/j.infsof.2025.107914
\\ ( https://arxiv.org/abs/2510.15408 ,  205kb)
------------------------------------------------------------------------------
\\
arXiv:2510.23761
replaced with revised version Thu, 22 Jan 2026 16:50:52 GMT   (464kb)

Title: TDFlow: Agentic Workflows for Test Driven Development
Authors: Kevin Han, Siddharth Maddikayala, Tim Knappe, Om Patel, Austen Liao,
  Amir Barati Farimani
Categories: cs.SE cs.AI cs.MA
Comments: Published in the 19th Conference of the European Chapter of the
  Association for Computational Linguistics (EACL 2026 Main Conference)
\\ ( https://arxiv.org/abs/2510.23761 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2511.21964
replaced with revised version Thu, 22 Jan 2026 02:46:59 GMT   (503kb)

Title: DRS-OSS: Practical Diff Risk Scoring with LLMs
Authors: Ali Sayedsalehi, Peter C. Rigby, and Audris Mockus
Categories: cs.SE
Comments: 8 pages, 4 figures, includes system architecture diagrams, Web UI
  screenshots, GitHub App examples, and an appendix with API endpoints. Full
  replication package and demo materials available
MSC-class: 68T07, 68N30
ACM-class: D.2.5; I.2.6; D.2.7
\\ ( https://arxiv.org/abs/2511.21964 ,  503kb)
------------------------------------------------------------------------------
\\
arXiv:2512.06247
replaced with revised version Thu, 22 Jan 2026 02:05:23 GMT   (268kb)

Title: DUET: Agentic Design Understanding via Experimentation and Testing
Authors: Gus Henry Smith, Sandesh Adhikary, Vineet Thumuluri, Karthik Suresh,
  Vivek Pandit, Kartik Hegde, Hamid Shojaei, Chandra Bhagavatula
Categories: cs.SE cs.AI cs.AR
\\ ( https://arxiv.org/abs/2512.06247 ,  268kb)
------------------------------------------------------------------------------
\\
arXiv:2512.23746
replaced with revised version Wed, 21 Jan 2026 21:59:19 GMT   (1238kb)

Title: DEFT: Differentiable Automatic Test Pattern Generation
Authors: Wei Li, Yang Zou, Yixin Liang, Jos\'e Moura, Shawn Blanton
Categories: cs.SE
\\ ( https://arxiv.org/abs/2512.23746 ,  1238kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12148
replaced with revised version Thu, 22 Jan 2026 16:16:01 GMT   (565kb)

Title: Many Hands Make Light Work: An LLM-based Multi-Agent System for
  Detecting Malicious PyPI Packages
Authors: Muhammad Umar Zeshan, Motunrayo Ibiyo, Claudio Di Sipio, Phuong T.
  Nguyen, Davide Di Ruscio
Categories: cs.SE
Comments: The paper has been peer-reviewed and accepted for publication to the
  Journal of Systems and Software
  (https://www.sciencedirect.com/journal/journal-of-systems-and-software)
\\ ( https://arxiv.org/abs/2601.12148 ,  565kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12890
replaced with revised version Thu, 22 Jan 2026 02:41:56 GMT   (3851kb)

Title: Efficient Code Analysis via Graph-Guided Large Language Models
Authors: Hang Gao, Tao Peng, Baoquan Cui, Hong Huang, Fengge Wu, Junsuo Zhao,
  Jian Zhang
Categories: cs.SE
\\ ( https://arxiv.org/abs/2601.12890 ,  3851kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13943
replaced with revised version Thu, 22 Jan 2026 09:31:11 GMT   (2420kb)

Title: RepoGenesis: Benchmarking End-to-End Microservice Generation from Readme
  to Repository
Authors: Zhiyuan Peng, Xin Yin, Pu Zhao, Fangkai Yang, Lu Wang, Ran Jia, Xu
  Chen, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang
Categories: cs.SE
\\ ( https://arxiv.org/abs/2601.13943 ,  2420kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15084
replaced with revised version Thu, 22 Jan 2026 11:59:24 GMT   (500kb)

Title: DeLog: An Efficient Log Compression Framework with Pattern Signature
  Synthesis
Authors: Siyu Yu, Yifan Wu, Junjielong Xu, Ying Fu, Ning Wang, Maoyin Liu,
  Pancheng Jiang, Xiang Zhang, Tong Jia, Pinjia He, Ying Li
Categories: cs.SE cs.OS
Comments: 23 pages, 11 figures
\\ ( https://arxiv.org/abs/2601.15084 ,  500kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18034
replaced with revised version Thu, 22 Jan 2026 18:28:42 GMT   (2540kb)

Title: Paramanu: Compact and Competitive Monolingual Language Models for
  Low-Resource Morphologically Rich Indian Languages
Authors: Mitodru Niyogi, Eric Gaussier, Arnab Bhattacharya
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.18034 ,  2540kb)
------------------------------------------------------------------------------
\\
arXiv:2405.10739
replaced with revised version Thu, 22 Jan 2026 02:53:40 GMT   (4246kb)

Title: Efficient Multimodal Large Language Models: A Survey
Authors: Yizhang Jin, Jian Li, Yexin Liu, Tianjun Gu, Kai Wu, Zhengkai Jiang,
  Muyang He, Bo Zhao, Xin Tan, Zhenye Gan, Yabiao Wang, Chengjie Wang, Lizhuang
  Ma
Categories: cs.CV cs.AI
Comments: Accepted by Visual Intelligence
Journal-ref: Visual Intelligence, Volume 3, article number 27, (2025)
DOI: 10.1007/s44267-025-00099-6
\\ ( https://arxiv.org/abs/2405.10739 ,  4246kb)
------------------------------------------------------------------------------
\\
arXiv:2409.06518
replaced with revised version Thu, 22 Jan 2026 14:18:20 GMT   (577kb)

Title: Medal Matters: Probing LLMs' Failure Cases Through Olympic Rankings
Authors: Juhwan Choi, Seunguk Yu, JungMin Yun, YoungBin Kim
Categories: cs.CL cs.AI
Comments: COLM 2025 ORIGen Workshop
\\ ( https://arxiv.org/abs/2409.06518 ,  577kb)
------------------------------------------------------------------------------
\\
arXiv:2501.15446
replaced with revised version Wed, 21 Jan 2026 20:47:35 GMT   (48kb)

Title: NP-Hard Lower Bound Complexity for Semantic Self-Verification
Authors: Robin Young
Categories: cs.CL cs.AI
Comments: EACL 2026
\\ ( https://arxiv.org/abs/2501.15446 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2503.03592
replaced with revised version Thu, 22 Jan 2026 14:12:42 GMT   (171kb)

Title: English K_Quantization of LLMs Does Not Disproportionately Diminish
  Multilingual Performance
Authors: Karl Audun Borgersen, Morten Goodwin
Categories: cs.CL cs.AI
Comments: 8 pages, 6 figures, v2
DOI: 10.1007/978-3-032-11402-0_8
\\ ( https://arxiv.org/abs/2503.03592 ,  171kb)
------------------------------------------------------------------------------
\\
arXiv:2503.05793
replaced with revised version Thu, 22 Jan 2026 03:18:19 GMT   (6721kb)

Title: MedSimAI: Simulation and Formative Feedback Generation to Enhance
  Deliberate Practice in Medical Education
Authors: Yann Hicke, Jadon Geathers, Kellen Vu, Justin Sewell, Claire Cardie,
  Jaideep Talwalkar, Dennis Shung, Anyanate Gwendolyne Jack, Susannah Cornes,
  Mackenzi Preston, Rene Kizilcec
Categories: cs.CY cs.AI cs.CL
Comments: Accepted to LAK 2026; 11 pages, 5 figures
DOI: 10.1145/3785022.3785092
\\ ( https://arxiv.org/abs/2503.05793 ,  6721kb)
------------------------------------------------------------------------------
\\
arXiv:2504.00035
replaced with revised version Thu, 22 Jan 2026 07:56:15 GMT   (2968kb)

Title: Is Your Writing Being Mimicked by AI? Unveiling Imitation with Invisible
  Watermarks in Creative Writing
Authors: Ziwei Zhang and Juan Wen and Wanli Peng and Zhengxian Wu and Yinghan
  Zhou and Yiming Xue
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2504.00035 ,  2968kb)
------------------------------------------------------------------------------
\\
arXiv:2505.03005
replaced with revised version Thu, 22 Jan 2026 05:03:56 GMT   (107kb)

Title: RADLADS: Rapid Attention Distillation to Linear Attention Decoders at
  Scale
Authors: Daniel Goldstein, Eric Alcaide, Janna Lu, Eugene Cheah
Categories: cs.CL cs.AI cs.LG
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2505.03005 ,  107kb)
------------------------------------------------------------------------------
\\
arXiv:2505.03336
replaced with revised version Thu, 22 Jan 2026 03:13:37 GMT   (317kb)

Title: Eliminating Out-of-Domain Recommendations in LLM-based Recommender
  Systems: A Unified View
Authors: Hao Liao, Jiwei Zhang, Jianxun Lian, Wensheng Lu, Mingqi Wu, Shuo
  Wang, Yong Zhang, Yitian Huang, Mingyang Zhou, Rui Mao
Categories: cs.IR cs.AI
Comments: 20 pages
\\ ( https://arxiv.org/abs/2505.03336 ,  317kb)
------------------------------------------------------------------------------
\\
arXiv:2505.23709
replaced with revised version Thu, 22 Jan 2026 09:47:19 GMT   (40507kb)

Title: Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning
Authors: Dionysis Christopoulos, Sotiris Spanos, Eirini Baltzi, Valsamis
  Ntouskos, Konstantinos Karantzalos
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2505.23709 ,  40507kb)
------------------------------------------------------------------------------
\\
arXiv:2505.24133
replaced with revised version Thu, 22 Jan 2026 01:31:36 GMT   (305kb)

Title: R-KV: Redundancy-aware KV Cache Compression for Reasoning Models
Authors: Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan,
  Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima
  Anandkumar, Abedelkadir Asi, Junjie Hu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2505.24133 ,  305kb)
------------------------------------------------------------------------------
\\
arXiv:2506.03088 (*cross-listing*)
replaced with revised version Thu, 22 Jan 2026 06:55:33 GMT   (5759kb)

Title: Modelling the Effects of Hearing Loss on Neural Coding in the Auditory
  Midbrain with Variational Conditioning
Authors: Lloyd Pellatt, Fotios Drakopoulos, Shievanie Sabesan and Nicholas A.
  Lesica
Categories: q-bio.NC cs.AI cs.LG
Comments: 12 pages, 3 figures, presented at AAAI 2026
\\ ( https://arxiv.org/abs/2506.03088 ,  5759kb)
------------------------------------------------------------------------------
\\
arXiv:2506.06299
replaced with revised version Thu, 22 Jan 2026 15:03:06 GMT   (17kb)

Title: How malicious AI swarms can threaten democracy: The fusion of agentic AI
  and LLMs marks a new frontier in information warfare
Authors: Daniel Thilo Schroeder, Meeyoung Cha, Andrea Baronchelli, Nick
  Bostrom, Nicholas A. Christakis, David Garcia, Amit Goldenberg, Yara
  Kyrychenko, Kevin Leyton-Brown, Nina Lutz, Gary Marcus, Filippo Menczer,
  Gordon Pennycook, David G. Rand, Maria Ressa, Frank Schweitzer, Dawn Song,
  Christopher Summerfield, Audrey Tang, Jay J. Van Bavel, Sander van der
  Linden, Jonas R. Kunst
Categories: cs.CY cs.AI cs.CL cs.LG
Comments: 5 Pages, This is the author's version of the work. It is posted here
  by permission of the AAAS for personal use, not for redistribution. The
  definitive version was published in Science on January 22, 2026, DOI:
  10.1126/science.adz1697
DOI: 10.1126/science.adz1697
\\ ( https://arxiv.org/abs/2506.06299 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2506.18930
replaced with revised version Thu, 22 Jan 2026 06:45:04 GMT   (2752kb)

Title: Dynamic Exploration on Segment-Proposal Graphs for Tubular Centerline
  Tracking
Authors: Chong Di, Jinglin Zhang, Zhenjiang Li, Jean-Marie Mirebeau, Da Chen
  and Laurent D. Cohen
Categories: cs.CV cs.AI cs.LG
Comments: A real time interactive model that can accurately find centerline of
  a tubular structure even in complex scenarios. At this version, this work is
  independent to deep learning-based algorithms
\\ ( https://arxiv.org/abs/2506.18930 ,  2752kb)
------------------------------------------------------------------------------
\\
arXiv:2507.01001
replaced with revised version Thu, 22 Jan 2026 18:32:06 GMT   (6330kb)

Title: SciArena: An Open Evaluation Platform for Non-Verifiable Scientific
  Literature-Grounded Tasks
Authors: Yilun Zhao, Kaiyan Zhang, Tiansheng Hu, Sihong Wu, Ronan Le Bras,
  Charles McGrady, Taira Anderson, Jonathan Bragg, Joseph Chee Chang, Jesse
  Dodge, Matt Latzke, Yixin Liu, Xiangru Tang, Zihang Wang, Chen Zhao, Hannaneh
  Hajishirzi, Doug Downey, Arman Cohan
Categories: cs.CL cs.AI
Comments: NeurIPS 2025 Datasets & Benchmarks Track (Spotlight)
\\ ( https://arxiv.org/abs/2507.01001 ,  6330kb)
------------------------------------------------------------------------------
\\
arXiv:2507.03251
replaced with revised version Thu, 22 Jan 2026 01:52:58 GMT   (0kb,I)

Title: Toward Efficient Speech Emotion Recognition via Spectral Learning and
  Attention
Authors: HyeYoung Lee, Muhammad Nadeem
Categories: cs.SD cs.AI eess.AS
Comments: After posting, we discovered that part of the material included in
  the manuscript should not have been publicly distributed in this form. We are
  withdrawing the paper while we address the issue
\\ ( https://arxiv.org/abs/2507.03251 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2507.14625
replaced with revised version Thu, 22 Jan 2026 08:30:25 GMT   (455kb)

Title: VTarbel: Targeted Label Attack with Minimal Knowledge on
  Detector-enhanced Vertical Federated Learning
Authors: Juntao Tan, Anran Li, Quanchao Liu, Peng Ran, Lan Zhang
Categories: cs.CR cs.AI
Comments: Accepted by ACM Transactions on Sensor Networks (TOSN)
\\ ( https://arxiv.org/abs/2507.14625 ,  455kb)
------------------------------------------------------------------------------
\\
arXiv:2507.14629
replaced with revised version Thu, 22 Jan 2026 08:34:21 GMT   (2916kb)

Title: VMask: Tunable Label Privacy Protection for Vertical Federated Learning
  via Layer Masking
Authors: Juntao Tan, Lan Zhang, Zhonghao Hu, Kai Yang, Peng Ran, Bo Li
Categories: cs.CR cs.AI
Comments: Accepted by Frontiers of Computer Science (FCS)
\\ ( https://arxiv.org/abs/2507.14629 ,  2916kb)
------------------------------------------------------------------------------
\\
arXiv:2508.04174
replaced with revised version Thu, 22 Jan 2026 10:18:02 GMT   (875kb)

Title: Cohesive Group Discovery in Interaction Graphs under Explicit Density
  Constraints
Authors: Yu Zhang, Yilong Luo, Mingyuan Ma, Yao Chen, Enqiang Zhu, Jin Xu,
  Chanjuan Liu
Categories: cs.SI cs.AI
Comments: 10 pages, 6 figures
\\ ( https://arxiv.org/abs/2508.04174 ,  875kb)
------------------------------------------------------------------------------
\\
arXiv:2508.18665
replaced with revised version Thu, 22 Jan 2026 02:01:51 GMT   (7829kb)

Title: Membership Inference Attacks on LLM-based Recommender Systems
Authors: Jiajie He, Min-Chun Chen, Xintong Chen, Xinyang Fang, Yuechun Gu and
  Keke Chen
Categories: cs.IR cs.AI cs.CL cs.CR cs.LG
Comments: This is paper is under review ACL 2026
\\ ( https://arxiv.org/abs/2508.18665 ,  7829kb)
------------------------------------------------------------------------------
\\
arXiv:2508.20097 (*cross-listing*)
replaced with revised version Wed, 21 Jan 2026 23:55:51 GMT   (273kb)

Title: Can LLMs Identify Tax Abuse?
Authors: Andrew Blair-Stanek, Nils Holzenberger, Benjamin Van Durme
Categories: q-fin.CP cs.AI
Comments: 9 pages
ACM-class: I.2.1; I.2.7; J.4
\\ ( https://arxiv.org/abs/2508.20097 ,  273kb)
------------------------------------------------------------------------------
\\
arXiv:2509.05882
replaced with revised version Wed, 21 Jan 2026 21:29:06 GMT   (962kb)

Title: Collaborate, Deliberate, Evaluate: How LLM Alignment Affects Coordinated
  Multi-Agent Outcomes
Authors: Abhijnan Nath, Carine Graff and Nikhil Krishnaswamy
Categories: cs.CL cs.AI cs.LG
Comments: This submission is a new version of arXiv:2509.05882v1. with a
  substantially revised experimental pipeline and new metrics. In particular,
  collaborator agents are now instantiated independently via separate API
  calls, rather than generated autoregressively by a single agent. All
  experimental results are new. Accepted as an extended abstract at AAMAS 2026
\\ ( https://arxiv.org/abs/2509.05882 ,  962kb)
------------------------------------------------------------------------------
\\
arXiv:2509.07526
replaced with revised version Thu, 22 Jan 2026 09:16:28 GMT   (160kb)

Title: Competitive Audio-Language Models with Data-Efficient Single-Stage
  Training on Public Data
Authors: Gokul Karthik Kumar, Rishabh Saraf, Ludovick Lepauloux, Abdul Muneer,
  Billel Mokeddem, Hakim Hacid
Categories: cs.SD cs.AI cs.CL cs.LG
Comments: Accepted at ASRU 2025
\\ ( https://arxiv.org/abs/2509.07526 ,  160kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16346
replaced with revised version Wed, 21 Jan 2026 22:53:57 GMT   (36845kb)

Title: From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation
  of 3D Forest Structure from Aerial-to-Terrestrial LiDAR
Authors: Juan Castorena, E. Louise Loudermilk, Scott Pokswinski, Rodman Linn
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2509.16346 ,  36845kb)
------------------------------------------------------------------------------
\\
arXiv:2510.04704 (*cross-listing*)
replaced with revised version Thu, 22 Jan 2026 05:18:03 GMT   (10903kb)

Title: AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large
  Language Models on Crystalline Materials
Authors: Taoyuze Lv, Alexander Chen, Fengyu Xie, Chu Wu, Jeffrey Meng, Dongzhan
  Zhou, Yingheng Wang, Bram Hoex, Zhicheng Zhong, Tong Xie
Categories: cond-mat.mtrl-sci cs.AI cs.CL
\\ ( https://arxiv.org/abs/2510.04704 ,  10903kb)
------------------------------------------------------------------------------
\\
arXiv:2510.15911 (*cross-listing*)
replaced with revised version Wed, 21 Jan 2026 23:41:10 GMT   (142kb)

Title: The Sleeping Beauty Problem: Sleeping Kelly is a Thirder
Authors: Ben Abramowitz
Categories: q-fin.GN cs.AI
\\ ( https://arxiv.org/abs/2510.15911 ,  142kb)
------------------------------------------------------------------------------
\\
arXiv:2510.17873
replaced with revised version Thu, 22 Jan 2026 14:42:57 GMT   (0kb,I)

Title: Auditing and Mitigating Bias in Gender Classification Algorithms: A
  Data-Centric Approach
Authors: Tadesse K Bahiru, Natnael Tilahun Sinshaw, Teshager Hailemariam Moges,
  and Dheeraj Kumar Singh
Categories: cs.CV cs.AI
Comments: The manuscript contains a substantive error identified after
  submission
\\ ( https://arxiv.org/abs/2510.17873 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2511.06943
replaced with revised version Thu, 22 Jan 2026 14:52:49 GMT   (40287kb)

Title: PlantTraitNet: An Uncertainty-Aware Multimodal Framework for
  Global-Scale Plant Trait Inference from Citizen Science Data
Authors: Ayushi Sharma, Johanna Trost, Daniel Lusk, Johannes Dollinger, Julian
  Schrader, Christian Rossi, Javier Lopatin, Etienne Lalibert\'e, Simon
  Haberstroh, Jana Eichel, Daniel Mederer, Jose Miguel Cerda-Paredes, Shyam S.
  Phartyal, Lisa-Maricia Schwarz, Anja Linst\"adter, Maria Concei\c{c}\~ao
  Caldeira, Teja Kattenborn
Categories: cs.CV cs.AI
Comments: Preprint version of the paper accepted at the 40th AAAI Conference on
  Artificial Intelligence (AAAI-26), organized by the Association for the
  Advancement of Artificial Intelligence
\\ ( https://arxiv.org/abs/2511.06943 ,  40287kb)
------------------------------------------------------------------------------
\\
arXiv:2511.13273
replaced with revised version Thu, 22 Jan 2026 17:11:50 GMT   (719kb)

Title: AudioMotionBench: Evaluating Auditory Motion Perception in Audio LLMs
Authors: Zhe Sun, Yujun Cai, Jiayu Yao, Yiwei Wang
Categories: cs.SD cs.AI
\\ ( https://arxiv.org/abs/2511.13273 ,  719kb)
------------------------------------------------------------------------------
\\
arXiv:2511.16485
replaced with revised version Thu, 22 Jan 2026 07:54:35 GMT   (1189kb)

Title: Online Operator Design in Evolutionary Optimization for Flexible Job
  Shop Scheduling via Large Language Models
Authors: Rongjie Liao, Junhao Qiu, Xin Chen, Xiaoping Li
Categories: cs.NE cs.AI
\\ ( https://arxiv.org/abs/2511.16485 ,  1189kb)
------------------------------------------------------------------------------
\\
arXiv:2511.18894
replaced with revised version Thu, 22 Jan 2026 15:35:39 GMT   (7188kb)

Title: MetaDCSeg: Robust Medical Image Segmentation via Meta Dynamic Center
  Weighting
Authors: Chenyu Mu, Guihai Chen, Xun Yang, Erkun Yang, Cheng Deng
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2511.18894 ,  7188kb)
------------------------------------------------------------------------------
\\
arXiv:2512.06193
replaced with revised version Wed, 21 Jan 2026 19:09:58 GMT   (799kb)

Title: Do You Feel Comfortable? Detecting Hidden Conversational Escalation in
  AI Chatbots
Authors: Jihyung Park, Saleh Afroogh, David Atkinson, Junfeng Jiao
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2512.06193 ,  799kb)
------------------------------------------------------------------------------
\\
arXiv:2512.11276
replaced with revised version Thu, 22 Jan 2026 15:58:47 GMT   (13869kb)

Title: Words to Describe What I'm Feeling: Exploring the Potential of AI Agents
  for High Subjectivity Decisions in Advance Care Planning
Authors: Kellie Yu Hui Sim, Pin Sym Foong, Chenyu Zhao, Melanie Yi Ning Quek,
  Swarangi Subodh Mehta, Kenny Tsu Wei Choo
Categories: cs.HC cs.AI
Comments: Accepted at CHI 2026. 34 pages, 10 figures
ACM-class: H.5.0
DOI: 10.1145/3772318.3791335
\\ ( https://arxiv.org/abs/2512.11276 ,  13869kb)
------------------------------------------------------------------------------
\\
arXiv:2512.15891 (*cross-listing*)
replaced with revised version Thu, 22 Jan 2026 17:40:42 GMT   (3590kb)

Title: Dynamical Mechanisms for Coordinating Long-term Working Memory Based on
  the Precision of Spike-timing in Cortical Neurons
Authors: Terrence J. Sejnowski
Categories: q-bio.NC cs.AI
Comments: 31 pages, 13 figures
\\ ( https://arxiv.org/abs/2512.15891 ,  3590kb)
------------------------------------------------------------------------------
\\
arXiv:2512.16602
replaced with revised version Thu, 22 Jan 2026 13:49:34 GMT   (782kb)

Title: Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for
  Sensitive Topics
Authors: Iker Garc\'ia-Ferrero, David Montero, Roman Orus
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2512.16602 ,  782kb)
------------------------------------------------------------------------------
\\
arXiv:2512.19849
replaced with revised version Thu, 22 Jan 2026 03:29:09 GMT   (596kb)

Title: UCCL-EP: Portable Expert-Parallel Communication
Authors: Ziming Mao, Yihan Zhang, Chihan Cui, Zhen Huang, Kaichao You, Zhongjie
  Chen, Zhiying Xu, Zhenyu Gu, Scott Shenker, Costin Raiciu, Yang Zhou, Ion
  Stoica
Categories: cs.DC cs.AI cs.LG cs.NI
\\ ( https://arxiv.org/abs/2512.19849 ,  596kb)
------------------------------------------------------------------------------
\\
arXiv:2601.01747
replaced with revised version Thu, 22 Jan 2026 09:09:47 GMT   (590kb)

Title: Crafting Adversarial Inputs for Large Vision-Language Models Using
  Black-Box Optimization
Authors: Jiwei Guan, Haibo Jin, Haohan Wang
Categories: cs.CR cs.AI cs.CV cs.LG
Comments: EACL
\\ ( https://arxiv.org/abs/2601.01747 ,  590kb)
------------------------------------------------------------------------------
\\
arXiv:2601.06037
replaced with revised version Thu, 22 Jan 2026 13:48:29 GMT   (929kb)

Title: TeleMem: Building Long-Term and Multimodal Memory for Agentic AI
Authors: Chunliang Chen, Ming Guan, Xiao Lin, Jiaxu Li, Luxi Lin, Qiyi Wang,
  Xiangyu Chen, Jixiang Luo, Changzhi Sun, Dell Zhang, Xuelong Li
Categories: cs.CL cs.AI cs.CV
\\ ( https://arxiv.org/abs/2601.06037 ,  929kb)
------------------------------------------------------------------------------
\\
arXiv:2601.07315
replaced with revised version Thu, 22 Jan 2026 11:46:08 GMT   (1145kb)

Title: VLM-CAD: VLM-Optimized Collaborative Agent Design Workflow for Analog
  Circuit Sizing
Authors: Guanyuan Pan, Shuai Wang, Yugui Lin, Tiansheng Zhou, Pietro Li\`o,
  Yaqi Wang, Zhenxin Zhao
Categories: cs.MA cs.AI cs.AR
Comments: 9 pages, 4 figures, submitted to the 10th International Conference on
  Control, Automation and Diagnosis (ICCAD'26)
\\ ( https://arxiv.org/abs/2601.07315 ,  1145kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11109
replaced with revised version Thu, 22 Jan 2026 01:46:22 GMT   (3304kb)

Title: Vision-as-Inverse-Graphics Agent via Interleaved Multimodal Reasoning
Authors: Shaofeng Yin, Jiaxin Ge, Zora Zhiruo Wang, Xiuyu Li, Michael J. Black,
  Trevor Darrell, Angjoo Kanazawa, Haiwen Feng
Categories: cs.CV cs.AI cs.GR
Comments: Project page: https://fugtemypt123.github.io/VIGA-website/
\\ ( https://arxiv.org/abs/2601.11109 ,  3304kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12061
replaced with revised version Thu, 22 Jan 2026 14:16:14 GMT   (50kb)

Title: Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs
  Annotation: LLM-Assisted and Gold-Label-Free Evaluation
Authors: Jinsook Lee, Kirk Vanacore, Zhuqian Zhou, Bakhtawar Ahtisham, Jeanine
  Grutter, Rene F. Kizilcec
Categories: cs.CL cs.AI
Comments: Under Review for ACL 2026
\\ ( https://arxiv.org/abs/2601.12061 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12471
replaced with revised version Thu, 22 Jan 2026 05:03:19 GMT   (3492kb)

Title: Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty
Authors: Sravanthi Machcha, Sushrita Yerra, Sahil Gupta, Aishwarya Sahoo,
  Sharmin Sultana, Hong Yu, Zonghai Yao
Categories: cs.CL cs.AI
Comments: Equal contribution for the first two authors; To appear in
  proceedings of the Main Conference of the European Chapter of the Association
  for Computational Linguistics (EACL) 2026
\\ ( https://arxiv.org/abs/2601.12471 ,  3492kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14490
replaced with revised version Thu, 22 Jan 2026 18:58:24 GMT   (6815kb)

Title: GutenOCR: A Grounded Vision-Language Front-End for Documents
Authors: Hunter Heidenreich, Ben Elliott, Olivia Dinica, Yosheb Getachew
Categories: cs.CV cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2601.14490 ,  6815kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15209
replaced with revised version Thu, 22 Jan 2026 16:01:25 GMT   (2205kb)

Title: Deaf and Hard of Hearing Access to Intelligent Personal Assistants:
  Comparison of Voice-Based Options with an LLM-Powered Touch Interface
Authors: Paige S. DeVries, Michaela Okosi, Ming Li, Nora Dunphy, Gidey Gezae,
  Dante Conway, Abraham Glasser, Raja Kushalnagar, Christian Vogler
Categories: cs.HC cs.AI
Comments: Accepted for publication in ACM CHI 2026
\\ ( https://arxiv.org/abs/2601.15209 ,  2205kb)
------------------------------------------------------------------------------
\\
arXiv:2106.00103 (*cross-listing*)
replaced with revised version Wed, 21 Jan 2026 20:47:37 GMT   (151kb)

Title: Control Occupation Kernel Regression for Nonlinear Control-Affine
  Systems
Authors: Moad Abudia, Tejasvi Channagiri, Joel A. Rosenfeld, Rushikesh
  Kamalapurkar
Categories: math.OC cs.LG cs.SY eess.SY math.FA
\\ ( https://arxiv.org/abs/2106.00103 ,  151kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11551
replaced with revised version Thu, 22 Jan 2026 06:58:13 GMT   (5998kb)

Title: Multi-event Video-Text Retrieval
Authors: Gengyuan Zhang, Jisen Ren, Jindong Gu, Volker Tresp
Categories: cs.CV cs.IR cs.LG
Comments: [fixed typos in equations] accepted to ICCV2023 Poster; some figures
  are not supported when viewed online, please download the file and view
  locally
\\ ( https://arxiv.org/abs/2308.11551 ,  5998kb)
------------------------------------------------------------------------------
\\
arXiv:2501.19373 (*cross-listing*)
replaced with revised version Wed, 21 Jan 2026 22:15:44 GMT   (34kb)

Title: Beyond Fixed Horizons: A Theoretical Framework for Adaptive Denoising
  Diffusions
Authors: S\"oren Christensen, Jan Kallsen, Claudia Strauch and Lukas Trottner
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2501.19373 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2504.15473
replaced with revised version Wed, 21 Jan 2026 19:30:24 GMT   (33219kb)

Title: Emergence and Evolution of Interpretable Concepts in Diffusion Models
Authors: Berk Tinaz, Zalan Fabian, Mahdi Soltanolkotabi
Categories: cs.CV cs.LG eess.IV
Comments: 32 pages, 32 figures, published at the 39th Conference on Neural
  Information Processing Systems (NeurIPS), 2025
ACM-class: I.2.6; I.2.10
\\ ( https://arxiv.org/abs/2504.15473 ,  33219kb)
------------------------------------------------------------------------------
\\
arXiv:2505.13353
replaced with revised version Thu, 22 Jan 2026 14:25:19 GMT   (190kb)

Title: Sense and Sensitivity: Examining the Influence of Semantic Recall on
  Long Context Code Reasoning
Authors: Adam \v{S}torek, Mukur Gupta, Samira Hajizadeh, Prashast Srivastava,
  Suman Jana
Categories: cs.CL cs.LG cs.SE
\\ ( https://arxiv.org/abs/2505.13353 ,  190kb)
------------------------------------------------------------------------------
\\
arXiv:2505.19328
replaced with revised version Thu, 22 Jan 2026 18:06:39 GMT   (18916kb)

Title: BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Digital
  Behavioural Change
Authors: Manuela Gonz\'alez-Gonz\'alez, Soufiane Belharbi, Muhammad Osama
  Zeeshan, Masoumeh Sharafi, Muhammad Haseeb Aslam, Marco Pedersoli, Alessandro
  Lameiras Koerich, Simon L Bacon, Eric Granger
Categories: cs.CV cs.LG
Comments: 45 pages, 21 figures, under review
\\ ( https://arxiv.org/abs/2505.19328 ,  18916kb)
------------------------------------------------------------------------------
\\
arXiv:2507.10383 (*cross-listing*)
replaced with revised version Thu, 22 Jan 2026 07:40:51 GMT   (1082kb)

Title: Dynamical stability for dense patterns in discrete attractor neural
  networks
Authors: Uri Cohen, M\'at\'e Lengyel
Categories: cond-mat.dis-nn cond-mat.stat-mech cs.LG cs.NE q-bio.NC
\\ ( https://arxiv.org/abs/2507.10383 ,  1082kb)
------------------------------------------------------------------------------
\\
arXiv:2508.03636 (*cross-listing*)
replaced with revised version Thu, 22 Jan 2026 16:44:18 GMT   (4317kb)

Title: Likelihood Matching for Diffusion Models
Authors: Lei Qian, Wu Su, Yanqi Huang and Song Xi Chen
Categories: stat.ML cs.LG math.ST stat.AP stat.ME stat.TH
\\ ( https://arxiv.org/abs/2508.03636 ,  4317kb)
------------------------------------------------------------------------------
\\
arXiv:2509.08454
replaced with revised version Thu, 22 Jan 2026 09:28:02 GMT   (1458kb)

Title: Behind the Scenes: Mechanistic Interpretability of LoRA-adapted Whisper
  for Speech Emotion Recognition
Authors: Yujian Ma, Xikun Lu, Jinqiu Sang, Xianquan Jiang, Ruizhe Li
Categories: cs.SD cs.LG eess.AS
Comments: Accepted at ICASSP 2026
\\ ( https://arxiv.org/abs/2509.08454 ,  1458kb)
------------------------------------------------------------------------------
\\
arXiv:2509.24257
replaced with revised version Thu, 22 Jan 2026 01:57:41 GMT   (788kb)

Title: VeriLLM: A Lightweight Framework for Publicly Verifiable Decentralized
  Inference
Authors: Ke Wang, Zishuo Zhao, Xinyuan Song, Zelin Li, Libin Xia, Chris Tong,
  Bill Shi, Wenjie Qu, Eric Yang, Lynn Ai
Categories: cs.CR cs.LG
Comments: 18 pages, 4 figures, 6 tables
ACM-class: C.2.1
\\ ( https://arxiv.org/abs/2509.24257 ,  788kb)
------------------------------------------------------------------------------
\\
arXiv:2510.03328
replaced with revised version Thu, 22 Jan 2026 00:12:38 GMT   (2206kb)

Title: DECOR: Deep Embedding Clustering with Orientation Robustness
Authors: Fiona Victoria Stanley Jothiraj, Arunaggiri Pandian Karunanidhi, Seth
  A. Eichmeyer
Categories: cs.CV cs.LG
Comments: Accepted to the KGML Bridge at AAAI 2026 (non-archival)
\\ ( https://arxiv.org/abs/2510.03328 ,  2206kb)
------------------------------------------------------------------------------
\\
arXiv:2511.13732 (*cross-listing*)
replaced with revised version Thu, 22 Jan 2026 08:21:23 GMT   (935kb)

Title: Principled Coarse-Grained Acceptance for Speculative Decoding in Speech
Authors: Moran Yanuka, Paul Dixon, Eyal Finkelshtein, Daniel Rotman, Raja
  Giryes
Categories: eess.AS cs.LG
Comments: Accepted to ICASSP 2026
\\ ( https://arxiv.org/abs/2511.13732 ,  935kb)
------------------------------------------------------------------------------
\\
arXiv:2512.02010
replaced with revised version Thu, 22 Jan 2026 18:49:14 GMT   (704kb)

Title: Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block
  Scaling
Authors: Jack Cook, Junxian Guo, Guangxuan Xiao, Yujun Lin, Song Han
Categories: cs.CL cs.LG
Comments: 10 pages, 4 figures
\\ ( https://arxiv.org/abs/2512.02010 ,  704kb)
------------------------------------------------------------------------------
\\
arXiv:2512.05717 (*cross-listing*)
replaced with revised version Thu, 22 Jan 2026 15:53:36 GMT   (5570kb)

Title: Comparing the latent features of universal machine-learning interatomic
  potentials
Authors: Sofiia Chorna, Davide Tisi, Cesare Malosso, Wei Bin How, Michele
  Ceriotti, and Sanggyu Chong
Categories: physics.chem-ph cond-mat.mtrl-sci cs.LG
\\ ( https://arxiv.org/abs/2512.05717 ,  5570kb)
------------------------------------------------------------------------------
\\
arXiv:2512.11089 (*cross-listing*)
replaced with revised version Thu, 22 Jan 2026 15:36:45 GMT   (301kb)

Title: TPV: Parameter Perturbations Through the Lens of Test Prediction
  Variance
Authors: Devansh Arpit
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2512.11089 ,  301kb)
------------------------------------------------------------------------------
\\
arXiv:2512.14274
replaced with revised version Thu, 22 Jan 2026 07:08:24 GMT   (7346kb)

Title: TUN: Detecting Significant Points in Persistence Diagrams with Deep
  Learning
Authors: Yu Chen and Hongwei Lin
Categories: cs.CV cs.LG math.AT
\\ ( https://arxiv.org/abs/2512.14274 ,  7346kb)
------------------------------------------------------------------------------
\\
arXiv:2512.20712
replaced with revised version Thu, 22 Jan 2026 12:36:23 GMT   (526kb)

Title: Real-World Adversarial Attacks on RF-Based Drone Detectors
Authors: Omer Gazit, Yael Itzhakev, Yuval Elovici, Asaf Shabtai
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2512.20712 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2601.01147 (*cross-listing*)
replaced with revised version Thu, 22 Jan 2026 12:14:56 GMT   (827kb)

Title: Conformal Blindness: A Note on $A$-Cryptic change-points
Authors: Johan Hallberg Szabadv\'ary
Categories: stat.ML cs.LG
Comments: 6 pages, 3 figures
\\ ( https://arxiv.org/abs/2601.01147 ,  827kb)
------------------------------------------------------------------------------
\\
arXiv:2601.03892
replaced with revised version Thu, 22 Jan 2026 15:34:25 GMT   (349kb)

Title: Lightweight and perceptually-guided voice conversion for
  electro-laryngeal speech
Authors: Benedikt Mayrhofer, Franz Pernkopf, Philipp Aichinger, Martin
  Hagm\"uller
Categories: cs.SD cs.LG
Comments: 5 pages, 5 figures. Paper accepted for ICASSP 2026. Audio samples
  available at https://spsc-tugraz.github.io/lw-elvc-icassp26/
\\ ( https://arxiv.org/abs/2601.03892 ,  349kb)
------------------------------------------------------------------------------
\\
arXiv:2601.04157
replaced with revised version Wed, 21 Jan 2026 19:04:30 GMT   (765kb)

Title: FLEx: Language Modeling with Few-shot Language Explanations
Authors: Adar Avsian, Christopher Richardson, Anirudh Sundar, Larry Heck
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2601.04157 ,  765kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12630 (*cross-listing*)
replaced with revised version Thu, 22 Jan 2026 17:11:23 GMT   (4457kb)

Title: Enhanced Climbing Image Nudged Elastic Band method with Hessian
  Eigenmode Alignment
Authors: Rohit Goswami (1 and 2), Miha Gunde (2 and 3), Hannes J\'onsson ((1)
  Institute IMX and Lab-COSMO, \'Ecole polytechnique f\'ed\'erale de Lausanne
  (EPFL), Lausanne, Switzerland (2) Science Institute, University of Iceland,
  Reykjavik, Iceland (3) Institute Ru{\dj}er Bo\v{s}kovi\'c, Zagreb, Croatia)
Categories: physics.chem-ph cond-mat.mtrl-sci cs.LG physics.comp-ph
Comments: 25 pages. 11 figures
\\ ( https://arxiv.org/abs/2601.12630 ,  4457kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13256
replaced with revised version Thu, 22 Jan 2026 04:04:28 GMT   (74kb)

Title: Deep Neural networks for solving high-dimensional parabolic partial
  differential equations
Authors: Wenzhong Zhang, Zheyuan Hu, Wei Cai, George EM Karniadakis
Categories: math.NA cs.LG cs.NA
\\ ( https://arxiv.org/abs/2601.13256 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13519 (*cross-listing*)
replaced with revised version Wed, 21 Jan 2026 23:05:14 GMT   (79kb)

Title: Small Gradient Norm Regret for Online Convex Optimization
Authors: Wenzhi Gao, Chang He, Madeleine Udell
Categories: stat.ML cs.LG math.OC
\\ ( https://arxiv.org/abs/2601.13519 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14872 (*cross-listing*)
replaced with revised version Thu, 22 Jan 2026 03:42:11 GMT   (3655kb)

Title: Finite-Sample Inference for Sparsely Permuted Linear Regression
Authors: Hirofumi Ota, Masaaki Imaizumi
Categories: math.ST cs.LG stat.ME stat.ML stat.TH
\\ ( https://arxiv.org/abs/2601.14872 ,  3655kb)
------------------------------------------------------------------------------
\\
arXiv:2601.15118
replaced with revised version Thu, 22 Jan 2026 08:55:20 GMT   (202kb)

Title: WavLink: Compact Audio-Text Embeddings with a Global Whisper Token
Authors: Gokul Karthik Kumar, Ludovick Lepauloux, Hakim Hacid
Categories: cs.SD cs.CL cs.LG
Comments: Accepted at ICASSP 2026
\\ ( https://arxiv.org/abs/2601.15118 ,  202kb)
------------------------------------------------------------------------------
\\
arXiv:2408.06219
replaced with revised version Thu, 22 Jan 2026 09:03:35 GMT   (434kb)

Title: 120 Domain-Specific Languages for Security
Authors: Markus Krausz, Sven Peldszus, Francesco Regazzoni, Thorsten Berger,
  Tim G\"uneysu
Categories: cs.CR cs.SE
\\ ( https://arxiv.org/abs/2408.06219 ,  434kb)
------------------------------------------------------------------------------
\\
arXiv:2511.13520 (*cross-listing*)
replaced with revised version Thu, 22 Jan 2026 08:29:13 GMT   (47kb)

Title: Towards Quantum Software for Quantum Simulation
Authors: Maja Franz, Lukas Schmidbauer, Joshua Ammermann, Ina Schaefer,
  Wolfgang Mauerer
Categories: quant-ph cs.SE
Journal-ref: Proceedings of the 48th International Conference of Software
  Engineering 2026, Workshop on Quantum Software Engineering (Q-SE@ICSE'26)
DOI: 10.1145/3786150.3788611
\\ ( https://arxiv.org/abs/2511.13520 ,  47kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---