------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Machine Learning
Software Engineering
 received from  Fri 16 Jan 26 19:00:00 GMT  to  Tue 20 Jan 26 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2601.11559
Date: Thu, 18 Dec 2025 05:03:49 GMT   (704kb)

Title: MIMIC-RD: Can LLMs differentially diagnose rare diseases in real-world
  clinical settings?
Authors: Zilal Eiz AlDin, John Wu, Jeffrey Paul Fung, Jennifer King, Mya Watts,
  Lauren ONeill, Adam Richard Cross and Jimeng Sun
Categories: cs.AI cs.CL cs.LG
Comments: 5 pages
\\
  Despite rare diseases affecting 1 in 10 Americans, their differential
diagnosis remains challenging. Due to their impressive recall abilities, large
language models (LLMs) have been recently explored for differential diagnosis.
Existing approaches to evaluating LLM-based rare disease diagnosis suffer from
two critical limitations: they rely on idealized clinical case studies that
fail to capture real-world clinical complexity, or they use ICD codes as
disease labels, which significantly undercounts rare diseases since many lack
direct mappings to comprehensive rare disease databases like Orphanet. To
address these limitations, we explore MIMIC-RD, a rare disease differential
diagnosis benchmark constructed by directly mapping clinical text entities to
Orphanet. Our methodology involved an initial LLM-based mining process followed
by validation from four medical annotators to confirm identified entities were
genuine rare diseases. We evaluated various models on our dataset of 145
patients and found that current state-of-the-art LLMs perform poorly on rare
disease differential diagnosis, highlighting the substantial gap between
existing capabilities and clinical needs. From our findings, we outline several
future steps towards improving differential diagnosis of rare diseases.
\\ ( https://arxiv.org/abs/2601.11559 ,  704kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11620
Date: Sun, 11 Jan 2026 01:08:33 GMT   (97kb)

Title: A Mind Cannot Be Smeared Across Time
Authors: Michael Timothy Bennett
Categories: cs.AI
\\
  Whether machines can be conscious depends not only on what they compute, but
\emph{when} they compute it. Most deployed artificial systems realise their
functions via sequential or time-multiplexed updates. Conscious experience
appears unified and simultaneous. I show that this difference matters formally.
I augment Stack Theory with algebraic laws relating within time-window
constraint satisfaction to conjunction. I introduce a precise temporal
semantics over windowed trajectories $\tau^{\Delta,s}$ and prove that
existential temporal realisation $\Diamond_{\Delta}$ does not preserve
conjunction. A system can realise all the ingredients of experience across time
without ever instantiating the experienced conjunction itself. I then
distinguish two postulates. StrongSync requires objective co-instantiation of
the grounded conjunction within the window, while WeakSync permits temporal
``smearing''. I formalise concurrency-capacity to measure what is needed to
satisfy StrongSync. Finally, I review neurophysiological evidence suggesting
that consciousness depends on phase synchrony and effective connectivity, and
that loss of consciousness is often associated with its breakdown. This
evidence makes WeakSync less plausible. Under StrongSync, software
consciousness on strictly sequential substrates is impossible for contents
whose grounding requires two or more simultaneous contributors. The more parts
from which simultaneous contribution required, the more concurrency capacity is
required. The hardware matters. Consciousness attribution therefore requires
architectural inspection, not just functional performance.
\\ ( https://arxiv.org/abs/2601.11620 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11622
Date: Sun, 11 Jan 2026 21:57:52 GMT   (163kb)

Title: Dynamical Systems Analysis Reveals Functional Regimes in Large Language
  Models
Authors: Hassan Ugail and Newton Howard
Categories: cs.AI cs.LG
\\
  Large language models perform text generation through high-dimensional
internal dynamics, yet the temporal organisation of these dynamics remains
poorly understood. Most interpretability approaches emphasise static
representations or causal interventions, leaving temporal structure largely
unexplored. Drawing on neuroscience, where temporal integration and
metastability are core markers of neural organisation, we adapt these concepts
to transformer models and discuss a composite dynamical metric, computed from
activation time-series during autoregressive generation. We evaluate this
metric in GPT-2-medium across five conditions: structured reasoning, forced
repetition, high-temperature noisy sampling, attention-head pruning, and
weight-noise injection. Structured reasoning consistently exhibits elevated
metric relative to repetitive, noisy, and perturbed regimes, with statistically
significant differences confirmed by one-way ANOVA and large effect sizes in
key comparisons. These results are robust to layer selection, channel
subsampling, and random seeds. Our findings demonstrate that
neuroscience-inspired dynamical metrics can reliably characterise differences
in computational organisation across functional regimes in large language
models. We stress that the proposed metric captures formal dynamical properties
and does not imply subjective experience.
\\ ( https://arxiv.org/abs/2601.11622 ,  163kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11625
Date: Mon, 12 Jan 2026 17:48:05 GMT   (1188kb)

Title: Reasoning Stabilization Point: A Training-Time Signal for Stable
  Evidence and Shortcut Reliance
Authors: Sahil Rajesh Dhayalkar
Categories: cs.AI cs.LG
Comments: 8 pages, Submitted to ACL Rolling Review and is under review
\\
  Fine-tuning pretrained language models can improve task performance while
subtly altering the evidence a model relies on. We propose a training-time
interpretability view that tracks token-level attributions across finetuning
epochs. We define explanation driftas the epoch-to-epoch change in normalized
token attributions on a fixed probe set, and introduce the Reasoning
Stabilization Point(RSP), the earliest epoch after which drift remains
consistently low. RSP is computed from within-run drift dynamics and requires
no tuning on out-of-distribution data. Across multiple lightweight transformer
classifiers and benchmark classification tasks, drift typically collapses into
a low, stable regime early in training, while validation accuracy continues to
change only marginally. In a controlled shortcut setting with label-correlated
trigger tokens, attribution dynamics expose increasing reliance on the shortcut
even when validation accuracy remains competitive. Overall, explanation drift
provides a simple, low-cost diagnostic for monitoring how decision evidence
evolves during fine-tuning and for selecting checkpoints in a stable-evidence
regime.
\\ ( https://arxiv.org/abs/2601.11625 ,  1188kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11747
Date: Fri, 16 Jan 2026 19:56:13 GMT   (14183kb)

Title: PRISM: Learning Design Knowledge from Data for Stylistic Design
  Improvement
Authors: Huaxiaoyue Wang, Sunav Choudhary, Franck Dernoncourt, Yu Shen, Stefano
  Petrangeli
Categories: cs.AI
\\
  Graphic design often involves exploring different stylistic directions, which
can be time-consuming for non-experts. We address this problem of stylistically
improving designs based on natural language instructions. While VLMs have shown
initial success in graphic design, their pretrained knowledge on styles is
often too general and misaligned with specific domain data. For example, VLMs
may associate minimalism with abstract designs, whereas designers emphasize
shape and color choices. Our key insight is to leverage design data -- a
collection of real-world designs that implicitly capture designer's principles
-- to learn design knowledge and guide stylistic improvement. We propose PRISM
(PRior-Informed Stylistic Modification) that constructs and applies a design
knowledge base through three stages: (1) clustering high-variance designs to
capture diversity within a style, (2) summarizing each cluster into actionable
design knowledge, and (3) retrieving relevant knowledge during inference to
enable style-aware improvement. Experiments on the Crello dataset show that
PRISM achieves the highest average rank of 1.49 (closer to 1 is better) over
baselines in style alignment. User studies further validate these results,
showing that PRISM is consistently preferred by designers.
\\ ( https://arxiv.org/abs/2601.11747 ,  14183kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11781
Date: Fri, 16 Jan 2026 21:08:01 GMT   (6791kb)

Title: Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response
  for Autonomous Vehicles
Authors: Dawood Wasif, Terrence J. Moore, Seunghyun Yoon, Hyuk Lim, Dan
  Dongseong Kim, Frederica F. Nelson, and Jin-Hee Cho
Categories: cs.AI cs.CV
Comments: Submitted to ICRA 2026 (under review)
\\
  Autonomous vehicles must remain safe and effective when encountering rare
long-tailed scenarios or cyber-physical intrusions during driving. We present
RAIL, a risk-aware human-in-the-loop framework that turns heterogeneous runtime
signals into calibrated control adaptations and focused learning. RAIL fuses
three cues (curvature actuation integrity, time-to-collision proximity, and
observation-shift consistency) into an Intrusion Risk Score (IRS) via a
weighted Noisy-OR. When IRS exceeds a threshold, actions are blended with a
cue-specific shield using a learned authority, while human override remains
available; when risk is low, the nominal policy executes. A contextual bandit
arbitrates among shields based on the cue vector, improving mitigation choices
online. RAIL couples Soft Actor-Critic (SAC) with risk-prioritized replay and
dual rewards so that takeovers and near misses steer learning while nominal
behavior remains covered. On MetaDrive, RAIL achieves a Test Return (TR) of
360.65, a Test Success Rate (TSR) of 0.85, a Test Safety Violation (TSV) of
0.75, and a Disturbance Rate (DR) of 0.0027, while logging only 29.07 training
safety violations, outperforming RL, safe RL, offline/imitation learning, and
prior HITL baselines. Under Controller Area Network (CAN) injection and LiDAR
spoofing attacks, it improves Success Rate (SR) to 0.68 and 0.80, lowers the
Disengagement Rate under Attack (DRA) to 0.37 and 0.03, and reduces the Attack
Success Rate (ASR) to 0.34 and 0.11. In CARLA, RAIL attains a TR of 1609.70 and
TSR of 0.41 with only 8000 steps.
\\ ( https://arxiv.org/abs/2601.11781 ,  6791kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11792
Date: Fri, 16 Jan 2026 21:36:04 GMT   (3320kb)

Title: A self-evolving multi-role collaborative framework with fine-grained
  difficulty guidance for innovative mathematical problem generation
Authors: Yifei Sun, Yongan Li, A.K. Qin, Sicheng Hou, Tamas Pflanzner
Categories: cs.AI cs.CL
\\
  Mathematical problem generation (MPG) is a significant research direction in
the field of intelligent education. In recent years, the rapid development of
large language models (LLMs) has enabled new technological approaches to
problem-generation tasks. Although existing LLMs can achieve high correctness
rates, they generally lack innovation and exhibit poor discrimination. In this
paper, we propose the task of innovative math problem generation (IMPG). To
solve the IMPG task, this paper proposes a self-evolving, multi-role
collaborative framework with fine-grained difficulty guidance. First, a
multi-role collaborative mechanism comprising a sampler, generator, evaluator,
state machine, and memory is constructed, ensuring the correctness of generated
problems through iterative optimization informed by self-assessment and
external feedback. Second, we introduce an improved difficulty model to
quantify difficulty and provide fine-grained guidance. We adopt the data-driven
association-guided path sampling (DAPS) algorithm to enhance the semantic
rationality of sampled encodings. Third, we construct the HSM3K-CN dataset,
which comprises high-quality high school math problems. A multi-stage training
pipeline is adopted, incorporating continual pre-training (CPT), supervised
fine-tuning (SFT), and group relative policy optimization (GRPO), to enhance
the generation and evaluation capabilities of the base model. Finally, system
self-evolution is achieved by transferring evaluation capabilities from the
expert model to the apprentice model via distillation. Experiments show that,
compared to baseline models, our proposed method significantly improves the
innovation of the generated problems while maintaining a high correctness rate.
\\ ( https://arxiv.org/abs/2601.11792 ,  3320kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11809
Date: Fri, 16 Jan 2026 22:22:05 GMT   (2179kb)

Title: Multi-agent DRL-based Lane Change Decision Model for Cooperative
  Planning in Mixed Traffic
Authors: Zeyu Mu, Shangtong Zhang, and B. Brian Park
Categories: cs.AI
Comments: Under review at IEEE Transactions on Intelligent Transportation
  Systems
\\
  Connected automated vehicles (CAVs) possess the ability to communicate and
coordinate with one another, enabling cooperative platooning that enhances both
energy efficiency and traffic flow. However, during the initial stage of CAV
deployment, the sparse distribution of CAVs among human-driven vehicles reduces
the likelihood of forming effective cooperative platoons. To address this
challenge, this study proposes a hybrid multi-agent lane change decision model
aimed at increasing CAV participation in cooperative platooning and maximizing
its associated benefits. The proposed model employs the QMIX framework,
integrating traffic data processed through a convolutional neural network
(CNN-QMIX). This architecture addresses a critical issue in dynamic traffic
scenarios by enabling CAVs to make optimal decisions irrespective of the
varying number of CAVs present in mixed traffic. Additionally, a trajectory
planner and a model predictive controller are designed to ensure smooth and
safe lane-change execution. The proposed model is trained and evaluated within
a microsimulation environment under varying CAV market penetration rates. The
results demonstrate that the proposed model efficiently manages fluctuating
traffic agent numbers, significantly outperforming the baseline rule-based
models. Notably, it enhances cooperative platooning rates up to 26.2\%,
showcasing its potential to optimize CAV cooperation and traffic dynamics
during the early stage of deployment.
\\ ( https://arxiv.org/abs/2601.11809 ,  2179kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11816
Date: Fri, 16 Jan 2026 22:38:21 GMT   (5098kb)

Title: POLARIS: Typed Planning and Governed Execution for Agentic AI in
  Back-Office Automation
Authors: Zahra Moslemi, Keerthi Koneru, Yen-Ting Lee, Sheethal Kumar, Ramesh
  Radhakrishnan
Categories: cs.AI
Comments: Workshop on Agentic AI Benchmarks and Applications for Enterprise
  Tasks: AAAI 2026
\\
  Enterprise back office workflows require agentic systems that are auditable,
policy-aligned, and operationally predictable, capabilities that generic
multi-agent setups often fail to deliver. We present POLARIS (Policy-Aware LLM
Agentic Reasoning for Integrated Systems), a governed orchestration framework
that treats automation as typed plan synthesis and validated execution over LLM
agents. A planner proposes structurally diverse, type checked directed acyclic
graphs (DAGs), a rubric guided reasoning module selects a single compliant
plan, and execution is guarded by validator gated checks, a bounded repair
loop, and compiled policy guardrails that block or route side effects before
they occur. Applied to document centric finance tasks, POLARIS produces
decision grade artifacts and full execution traces while reducing human
intervention. Empirically, POLARIS achieves a micro F1 of 0.81 on the SROIE
dataset and, on a controlled synthetic suite, achieves 0.95 to 1.00 precision
for anomaly routing with preserved audit trails. These evaluations constitute
an initial benchmark for governed Agentic AI. POLARIS provides a methodological
and benchmark reference for policy-aligned Agentic AI. Keywords Agentic AI,
Enterprise Automation, Back-Office Tasks, Benchmarks, Governance, Typed
Planning, Evaluation
\\ ( https://arxiv.org/abs/2601.11816 ,  5098kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11825
Date: Fri, 16 Jan 2026 23:07:58 GMT   (1751kb)

Title: AI Co-Scientist for Knowledge Synthesis in Medical Contexts: A Proof of
  Concept
Authors: Arya Rahgozar, Pouria Mortezaagha
Categories: cs.AI cs.IR
\\
  Research waste in biomedical science is driven by redundant studies,
incomplete reporting, and the limited scalability of traditional evidence
synthesis workflows. We present an AI co-scientist for scalable and transparent
knowledge synthesis based on explicit formalization of Population,
Intervention, Comparator, Outcome, and Study design (PICOS). The platform
integrates relational storage, vector-based semantic retrieval, and a Neo4j
knowledge graph. Evaluation was conducted on dementia-sport and
non-communicable disease corpora. Automated PICOS compliance and study design
classification from titles and abstracts were performed using a Bidirectional
Long Short-Term Memory baseline and a transformer-based multi-task classifier
fine-tuned from PubMedBERT. Full-text synthesis employed retrieval-augmented
generation with hybrid vector and graph retrieval, while BERTopic was used to
identify thematic structure, redundancy, and evidence gaps. The transformer
model achieved 95.7% accuracy for study design classification with strong
agreement against expert annotations, while the Bi-LSTM achieved 87% accuracy
for PICOS compliance detection. Retrieval-augmented generation outperformed
non-retrieval generation for queries requiring structured constraints,
cross-study integration, and graph-based reasoning, whereas non-retrieval
approaches remained competitive for high-level summaries. Topic modeling
revealed substantial thematic redundancy and identified underexplored research
areas. These results demonstrate that PICOS-aware and explainable natural
language processing can improve the scalability, transparency, and efficiency
of evidence synthesis. The proposed architecture is domain-agnostic and offers
a practical framework for reducing research waste across biomedical
disciplines.
\\ ( https://arxiv.org/abs/2601.11825 ,  1751kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11840
Date: Sat, 17 Jan 2026 00:16:41 GMT   (8854kb)

Title: Imandra CodeLogician: Neuro-Symbolic Reasoning for Precise Analysis of
  Software Logic
Authors: Hongyu Lin, Samer Abdallah, Makar Valentinov, Paul Brennan, Elijah
  Kagan, Christoph M. Wintersteiger, Denis Ignatovich, Grant Passmore
Categories: cs.AI cs.LO cs.SE
Comments: 52 pages, 23 figures. Includes a new benchmark dataset
  (code-logic-bench) and evaluation of neurosymbolic reasoning for software
  analysis
MSC-class: 68N30
ACM-class: F.3.1; D.2.4; I.2.3; I.2.4
\\
  Large Language Models (LLMs) have shown strong performance on code
understanding tasks, yet they fundamentally lack the ability to perform
precise, exhaustive mathematical reasoning about program behavior. Existing
benchmarks either focus on mathematical proof automation, largely disconnected
from real-world software, or on engineering tasks that do not require semantic
rigor.
  We present CodeLogician, a neurosymbolic agent for precise analysis of
software logic, integrated with ImandraX, an industrial automated reasoning
engine deployed in financial markets and safety-critical systems. Unlike prior
approaches that use formal methods primarily to validate LLM outputs,
CodeLogician uses LLMs to construct explicit formal models of software systems,
enabling automated reasoning to answer rich semantic questions beyond binary
verification outcomes.
  To rigorously evaluate mathematical reasoning about software logic, we
introduce code-logic-bench, a benchmark targeting the middle ground between
theorem proving and software engineering benchmarks. It measures reasoning
correctness about program state spaces, control flow, coverage constraints, and
edge cases, with ground truth defined via formal modeling and region
decomposition.
  Comparing LLM-only reasoning against LLMs augmented with CodeLogician, formal
augmentation yields substantial improvements, closing a 41-47 percentage point
gap in reasoning accuracy. These results demonstrate that neurosymbolic
integration is essential for scaling program analysis toward rigorous,
autonomous software understanding.
\\ ( https://arxiv.org/abs/2601.11840 ,  8854kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11850
Date: Sat, 17 Jan 2026 00:38:36 GMT   (1130kb)

Title: Human-AI Collaborative Inductive Thematic Analysis: AI Guided Analysis
  and Human Interpretive Authority
Authors: Matthew Nyaaba, Min SungEun, Mary Abiswin Apam, Kwame Owoahene
  Acheampong, Emmanuel Dwamena, Xiaoming Zhai
Categories: cs.AI cs.CY
\\
  The increasing use of generative artificial intelligence (GenAI) in
qualitative research raises important questions about analytic practice and
interpretive authority. This study examines how researchers interact with an
Inductive Thematic Analysis GPT (ITA-GPT), a purpose-built AI tool designed to
support inductive thematic analysis through structured, semi-automated prompts
aligned with reflexive thematic analysis and verbatim coding principles. Guided
by a Human-Artificial Intelligence Collaborative Inductive Thematic Analysis
(HACITA) framework, the study focuses on analytic process rather than
substantive findings. Three experienced qualitative researchers conducted
ITA-GPT assisted analyses of interview transcripts from education research in
the Ghanaian teacher education context. The tool supported familiarization,
verbatim in vivo coding, gerund-based descriptive coding, and theme
development, while enforcing trace to text integrity, coverage checks, and
auditability. Data sources included interaction logs, AI-generated tables,
researcher revisions, deletions, insertions, comments, and reflexive memos.
Findings show that ITA-GPT functioned as a procedural scaffold that structured
analytic workflow and enhanced transparency. However, interpretive authority
remained with human researchers, who exercised judgment through recurrent
analytic actions including modification, deletion, rejection, insertion, and
commenting. The study demonstrates how inductive thematic analysis is enacted
through responsible human AI collaboration.
\\ ( https://arxiv.org/abs/2601.11850 ,  1130kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11885
Date: Sat, 17 Jan 2026 02:51:42 GMT   (850kb)

Title: MyGram: Modality-aware Graph Transformer with Global Distribution for
  Multi-modal Entity Alignment
Authors: Zhifei Li, Ziyue Qin, Xiangyu Luo, Xiaoju Hou, Yue Zhao, Miao Zhang,
  Zhifang Huang, Kui Xiao, Bing Yang
Categories: cs.AI
Comments: Accepted by AAAI 2026
\\
  Multi-modal entity alignment aims to identify equivalent entities between two
multi-modal Knowledge graphs by integrating multi-modal data, such as images
and text, to enrich the semantic representations of entities. However, existing
methods may overlook the structural contextual information within each
modality, making them vulnerable to interference from shallow features. To
address these challenges, we propose MyGram, a modality-aware graph transformer
with global distribution for multi-modal entity alignment. Specifically, we
develop a modality diffusion learning module to capture deep structural
contextual information within modalities and enable fine-grained multi-modal
fusion. In addition, we introduce a Gram Loss that acts as a regularization
constraint by minimizing the volume of a 4-dimensional parallelotope formed by
multi-modal features, thereby achieving global distribution consistency across
modalities. We conduct experiments on five public datasets. Results show that
MyGram outperforms baseline models, achieving a maximum improvement of 4.8% in
Hits@1 on FBDB15K, 9.9% on FBYG15K, and 4.3% on DBP15K.
\\ ( https://arxiv.org/abs/2601.11885 ,  850kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11903
Date: Sat, 17 Jan 2026 04:09:02 GMT   (662kb)

Title: AEMA: Verifiable Evaluation Framework for Trustworthy and Controlled
  Agentic LLM Systems
Authors: YenTing Lee, Keerthi Koneru, Zahra Moslemi, Sheethal Kumar, Ramesh
  Radhakrishnan
Categories: cs.AI
Comments: Workshop on W51: How Can We Trust and Control Agentic AI? Toward
  Alignment, Robustness, and Verifiability in Autonomous LLM Agents at AAAI
  2026
\\
  Evaluating large language model (LLM)-based multi-agent systems remains a
critical challenge, as these systems must exhibit reliable coordination,
transparent decision-making, and verifiable performance across evolving tasks.
Existing evaluation approaches often limit themselves to single-response
scoring or narrow benchmarks, which lack stability, extensibility, and
automation when deployed in enterprise settings at multi-agent scale. We
present AEMA (Adaptive Evaluation Multi-Agent), a process-aware and auditable
framework that plans, executes, and aggregates multi-step evaluations across
heterogeneous agentic workflows under human oversight. Compared to a single
LLM-as-a-Judge, AEMA achieves greater stability, human alignment, and traceable
records that support accountable automation. Our results on enterprise-style
agent workflows simulated using realistic business scenarios demonstrate that
AEMA provides a transparent and reproducible pathway toward responsible
evaluation of LLM-based multi-agent systems.
  Keywords Agentic AI, Multi-Agent Systems, Trustworthy AI, Verifiable
Evaluation, Human Oversight
\\ ( https://arxiv.org/abs/2601.11903 ,  662kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11905
Date: Sat, 17 Jan 2026 04:37:20 GMT   (2627kb)

Title: LIBRA: Language Model Informed Bandit Recourse Algorithm for
  Personalized Treatment Planning
Authors: Junyu Cao, Ruijiang Gao, Esmaeil Keyvanshokooh, Jianhao Ma
Categories: cs.AI cs.LG math.ST stat.TH
Comments: 50 pages. Previous version with human-AI collaboration:
  arXiv:2410.14640
\\
  We introduce a unified framework that seamlessly integrates algorithmic
recourse, contextual bandits, and large language models (LLMs) to support
sequential decision-making in high-stakes settings such as personalized
medicine. We first introduce the recourse bandit problem, where a
decision-maker must select both a treatment action and a feasible, minimal
modification to mutable patient features. To address this problem, we develop
the Generalized Linear Recourse Bandit (GLRB) algorithm. Building on this
foundation, we propose LIBRA, a Language Model-Informed Bandit Recourse
Algorithm that strategically combines domain knowledge from LLMs with the
statistical rigor of bandit learning. LIBRA offers three key guarantees: (i) a
warm-start guarantee, showing that LIBRA significantly reduces initial regret
when LLM recommendations are near-optimal; (ii) an LLM-effort guarantee,
proving that the algorithm consults the LLM only $O(\log^2 T)$ times, where $T$
is the time horizon, ensuring long-term autonomy; and (iii) a robustness
guarantee, showing that LIBRA never performs worse than a pure bandit algorithm
even when the LLM is unreliable. We further establish matching lower bounds
that characterize the fundamental difficulty of the recourse bandit problem and
demonstrate the near-optimality of our algorithms. Experiments on synthetic
environments and a real hypertension-management case study confirm that GLRB
and LIBRA improve regret, treatment quality, and sample efficiency compared
with standard contextual bandits and LLM-only benchmarks. Our results highlight
the promise of recourse-aware, LLM-assisted bandit algorithms for trustworthy
LLM-bandits collaboration in personalized high-stakes decision-making.
\\ ( https://arxiv.org/abs/2601.11905 ,  2627kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11940
Date: Sat, 17 Jan 2026 07:26:02 GMT   (1616kb)

Title: Thinking Traps in Long Chain-of-Thought: A Measurable Study and
  Trap-Aware Adaptive Restart
Authors: Kang Chen, Fan Yu, Junjie Nian, Shihan Zhao, Zhuoka Feng, Zijun Yao,
  Heng Wang, Minshen Yu, Yixin Cao
Categories: cs.AI cs.CL
\\
  Scaling test-time compute via Long Chain-of-Thought (Long-CoT) significantly
enhances reasoning capabilities, yet extended generation does not guarantee
correctness: after an early wrong commitment, models may keep elaborating a
self-consistent but incorrect prefix. Through fine-grained trajectory analysis,
we identify Thinking Traps, prefix-dominant deadlocks where later reflection,
alternative attempts, or verification fails to revise the root error. On a
curated subset of DAPO-MATH, 89\% of failures exhibit such traps. To solve this
problem, we introduce TAAR (Trap-Aware Adaptive Restart), a test-time control
framework that trains a diagnostic policy to predict two signals from partial
trajectories: a trap index for where to truncate and an escape probability for
whether and how strongly to intervene. At inference time, TAAR truncates the
trajectory before the predicted trap segment and adaptively restarts decoding;
for severely trapped cases, it applies stronger perturbations, including
higher-temperature resampling and an optional structured reboot suffix.
Experiments on challenging mathematical and scientific reasoning benchmarks
(AIME24, AIME25, GPQA-Diamond, HMMT25, BRUMO25) show that TAAR improves
reasoning performance without fine-tuning base model parameters.
\\ ( https://arxiv.org/abs/2601.11940 ,  1616kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11974
Date: Sat, 17 Jan 2026 09:12:26 GMT   (3620kb)

Title: Learn Like Humans: Use Meta-cognitive Reflection for Efficient
  Self-Improvement
Authors: Xinmeng Hou, Peiliang Gong, Bohao Qu, Wuqi Wang, Qing Guo, Yang Liu
Categories: cs.AI
\\
  While Large Language Models (LLMs) enable complex autonomous behavior,
current agents remain constrained by static, human-designed prompts that limit
adaptability. Existing self-improving frameworks attempt to bridge this gap but
typically rely on inefficient, multi-turn recursive loops that incur high
computational costs. To address this, we propose Metacognitive Agent Reflective
Self-improvement (MARS), a framework that achieves efficient self-evolution
within a single recurrence cycle. Inspired by educational psychology, MARS
mimics human learning by integrating principle-based reflection (abstracting
normative rules to avoid errors) and procedural reflection (deriving
step-by-step strategies for success). By synthesizing these insights into
optimized instructions, MARS allows agents to systematically refine their
reasoning logic without continuous online feedback. Extensive experiments on
six benchmarks demonstrate that MARS outperforms state-of-the-art self-evolving
systems while significantly reducing computational overhead.
\\ ( https://arxiv.org/abs/2601.11974 ,  3620kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11979
Date: Sat, 17 Jan 2026 09:20:06 GMT   (1562kb)

Title: Process In-Context Learning: Enhancing Mathematical Reasoning via
  Dynamic Demonstration Insertion
Authors: Ang Gao, Changshuo Zhang, Xiao Zhang, Deyang Li, Minjun Zhao, Fangchao
  Liu, Xinyu Zhang
Categories: cs.AI cs.LG
\\
  In-context learning (ICL) has proven highly effective across diverse large
language model (LLM) tasks. However, its potential for enhancing tasks that
demand step-by-step logical deduction, such as mathematical reasoning, remains
underexplored. A core limitation of existing ICL approaches is their static use
of demonstrations: examples are pre-selected before inference and remain fixed,
failing to adapt to the dynamic confusion points that often arise during
multi-step reasoning such as ambiguous calculations or logical gaps. These
unresolved confusion points can lead to cascading errors that degrade final
accuracy. To tackle this issue, we propose Process In-Context Learning (PICL),
a dynamic demonstration integration framework designed to boost mathematical
reasoning by responding to real-time inference needs. PICL operates in two
stages: 1)~it identifies potential confusion points by analyzing semantics and
entropy in the reasoning process and summarizes their core characteristics;
2)~upon encountering these points, it retrieves relevant demonstrations from
the demonstration pool that match the confusion context and inserts them
directly into the ongoing reasoning process to guide subsequent steps.
Experiments show that PICL outperforms baseline methods by mitigating
mid-inference confusion, highlighting the value of adaptive demonstration
insertion in complex mathematical reasoning.
\\ ( https://arxiv.org/abs/2601.11979 ,  1562kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12002
Date: Sat, 17 Jan 2026 10:42:35 GMT   (3407kb)

Title: Kernel-Based Learning of Safety Barriers
Authors: Oliver Sch\"on, Zhengang Zhong, Sadegh Soudjani
Categories: cs.AI cs.LG cs.SY eess.SY
Comments: 44 pages, 9 figures
\\
  The rapid integration of AI algorithms in safety-critical applications such
as autonomous driving and healthcare is raising significant concerns about the
ability to meet stringent safety standards. Traditional tools for formal safety
verification struggle with the black-box nature of AI-driven systems and lack
the flexibility needed to scale to the complexity of real-world applications.
In this paper, we present a data-driven approach for safety verification and
synthesis of black-box systems with discrete-time stochastic dynamics. We
employ the concept of control barrier certificates, which can guarantee safety
of the system, and learn the certificate directly from a set of system
trajectories. We use conditional mean embeddings to embed data from the system
into a reproducing kernel Hilbert space (RKHS) and construct an RKHS ambiguity
set that can be inflated to robustify the result to out-of-distribution
behavior. We provide the theoretical results on how to apply the approach to
general classes of temporal logic specifications beyond safety. For the
data-driven computation of safety barriers, we leverage a finite Fourier
expansion to cast a typically intractable semi-infinite optimization problem as
a linear program. The resulting spectral barrier allows us to leverage the fast
Fourier transform to generate the relaxed problem efficiently, offering a
scalable yet distributionally robust framework for verifying safety. Our work
moves beyond restrictive assumptions on system dynamics and uncertainty, as
demonstrated on two case studies including a black-box system with a neural
network controller.
\\ ( https://arxiv.org/abs/2601.12002 ,  3407kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12014
Date: Sat, 17 Jan 2026 11:42:02 GMT   (288kb)

Title: Are LLMs Ready for TOON? Benchmarking Structural
  Correctness-Sustainability Trade-offs in Novel Structured Output Formats
Authors: Elio Masciari, Vincenzo Moscato, Enea Vincenzo Napolitano, Gian Marco
  Orlando, Marco Perillo, Diego Russo
Categories: cs.AI cs.SE
\\
  Large Language Models (LLMs) are increasingly required to generate
structured, machine-readable outputs for downstream systems. While recent
benchmarks have focused on evaluating the structural correctness of such
outputs, the environmental impact of inference for different output formats has
largely been overlooked. In this paper, we argue that structured output formats
should be assessed not only in terms of correctness, but also with respect to
their environmental efficiency. To this end, we introduce a
sustainability-aware evaluation framework for structured generation that
measures token usage, generation time, and estimated carbon emissions. Within
this framework, we propose the Environment-Aware Generation Correctness Score
(GCS_env), a unified metric that integrates structural correctness with
carbon-aware efficiency. Using this framework, we systematically benchmark the
novel TOON format against established representations (JSON, XML, YAML) across
multiple LLMs spanning different architectures and parameter scales.
  Our results reveal a consistent trade-off: TOON yields markedly more compact
outputs and lower emissions, but lower structural correctness when models lack
native support. We show that increased model capacity reduces this gap and that
environment-aware scoring can shift format rankings depending on deployment
priorities. highlighting the need for sustainability-inclusive benchmarking and
provides empirical evidence that compact representations such as TOON can offer
practical advantages in large-scale, carbon-conscious LLM deployments.
\\ ( https://arxiv.org/abs/2601.12014 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12024
Date: Sat, 17 Jan 2026 12:07:55 GMT   (1469kb)

Title: A Multi-Agent System for Generating Actionable Business Advice
Authors: Kartikey Singh Bhandari, Tanish Jain, Archit Agrawal, Dhruv Kumar,
  Praveen Kumar, Pratik Narang
Categories: cs.AI cs.CL
\\
  Customer reviews contain rich signals about product weaknesses and unmet user
needs, yet existing analytic methods rarely move beyond descriptive tasks such
as sentiment analysis or aspect extraction. While large language models (LLMs)
can generate free-form suggestions, their outputs often lack accuracy and depth
of reasoning. In this paper, we present a multi-agent, LLM-based framework for
prescriptive decision support, which transforms large scale review corpora into
actionable business advice. The framework integrates four components:
clustering to select representative reviews, generation of advices, iterative
evaluation, and feasibility based ranking. This design couples corpus
distillation with feedback driven advice refinement to produce outputs that are
specific, actionable, and practical. Experiments across three service domains
and multiple model families show that our framework consistently outperform
single model baselines on actionability, specificity, and non-redundancy, with
medium sized models approaching the performance of large model frameworks.
\\ ( https://arxiv.org/abs/2601.12024 ,  1469kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12030
Date: Sat, 17 Jan 2026 12:17:50 GMT   (209kb)

Title: ARC: Active and Reflection-driven Context Management for Long-Horizon
  Information Seeking Agents
Authors: Yilun Yao, Shan Huang, Elsie Dai, Zhewen Tan, Zhenyu Duan, Shousheng
  Jia, Yanbing Jiang, Tong Yang
Categories: cs.AI
Comments: 15 pages, 5 figures
\\
  Large language models are increasingly deployed as research agents for deep
search and long-horizon information seeking, yet their performance often
degrades as interaction histories grow. This degradation, known as context rot,
reflects a failure to maintain coherent and task-relevant internal states over
extended reasoning horizons. Existing approaches primarily manage context
through raw accumulation or passive summarization, treating it as a static
artifact and allowing early errors or misplaced emphasis to persist. Motivated
by this perspective, we propose ARC, which is the first framework to
systematically formulate context management as an active, reflection-driven
process that treats context as a dynamic internal reasoning state during
execution. ARC operationalizes this view through reflection-driven monitoring
and revision, allowing agents to actively reorganize their working context when
misalignment or degradation is detected. Experiments on challenging
long-horizon information-seeking benchmarks show that ARC consistently
outperforms passive context compression methods, achieving up to an 11%
absolute improvement in accuracy on BrowseComp-ZH with Qwen2.5-32B-Instruct.
\\ ( https://arxiv.org/abs/2601.12030 ,  209kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12038
Date: Sat, 17 Jan 2026 12:54:10 GMT   (38kb)

Title: Abstract Argumentation with Subargument Relations
Authors: Beishui Liao
Categories: cs.AI
Comments: 11 pages
\\
  Dung's abstract argumentation framework characterises argument acceptability
solely via an attack relation, deliberately abstracting from the internal
structure of arguments. While this level of abstraction has enabled a rich body
of results, it limits the ability to represent structural dependencies that are
central in many structured argumentation formalisms, in particular subargument
relations. Existing extensions, including bipolar argumentation frameworks,
introduce support relations, but these do not capture the asymmetric and
constitutive nature of subarguments or their interaction with attacks. In this
paper, we study abstract argumentation frameworks enriched with an explicit
subargument relation, treated alongside attack as a basic relation. We analyse
how subargument relations interact with attacks and examine their impact on
fundamental semantic properties. This framework provides a principled
abstraction of structural information and clarifies the role of subarguments in
abstract acceptability reasoning.
\\ ( https://arxiv.org/abs/2601.12038 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12040
Date: Sat, 17 Jan 2026 13:00:17 GMT   (275kb)

Title: Partial Reasoning in Language Models: Search and Refinement Guided by
  Uncertainty
Authors: Murilo da Luz, Bruno Brand\~ao, Luana Martins, Gustavo Oliveira, Bryan
  de Oliveira, Luckeciano Melo and Telma Soares
Categories: cs.AI
\\
  The use of Large Language Models (LLMs) for reasoning and planning tasks has
drawn increasing attention in Artificial Intelligence research. Despite their
remarkable progress, these models still exhibit limitations in multi-step
inference scenarios, particularly in mathematical and logical reasoning. We
introduce PREGU (Partial Reasoning Guided by Uncertainty). PREGU monitors the
entropy of the output distribution during autoregressive generation and halts
the process whenever entropy exceeds a defined threshold, signaling
uncertainty. From that point, a localized search is performed in the latent
space to refine the partial reasoning and select the most coherent answer,
using the Soft Reasoning method. Experiments conducted with LLaMA-3-8B,
Mistral-7B, and Qwen2-7B across four reasoning benchmarks (GSM8K, GSM-Hard,
SVAMP, and StrategyQA) showed performance greater than or similar to Soft
Reasoning, indicating that entropy can serve as an effective signal to trigger
selective refinement during reasoning.
\\ ( https://arxiv.org/abs/2601.12040 ,  275kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12126
Date: Sat, 17 Jan 2026 17:56:49 GMT   (27458kb)

Title: UniMo: Unified Motion Generation and Understanding with Chain of Thought
Authors: Guocun Wang, Kenkun Liu, Jing Lin, Guorui Song, Jian Li, Xiaoguang Han
Categories: cs.AI
\\
  Existing 3D human motion generation and understanding methods often exhibit
limited interpretability, restricting effective mutual enhancement between
these inherently related tasks. While current unified frameworks based on large
language models (LLMs) leverage linguistic priors, they frequently encounter
challenges in semantic alignment and task coherence. Moreover, the next-token
prediction paradigm in LLMs is ill-suited for motion sequences, causing
cumulative prediction errors. To address these limitations, we propose UniMo, a
novel framework that integrates motion-language information and interpretable
chain of thought (CoT) reasoning into the LLM via supervised fine-tuning (SFT).
We further introduce reinforcement learning with Group Relative Policy
Optimization (GRPO) as a post-training strategy that optimizes over groups of
tokens to enforce structural correctness and semantic alignment, mitigating
cumulative errors in motion token prediction. Extensive experiments demonstrate
that UniMo significantly outperforms existing unified and task-specific models,
achieving state-of-the-art performance in both motion generation and
understanding.
\\ ( https://arxiv.org/abs/2601.12126 ,  27458kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12138
Date: Sat, 17 Jan 2026 18:50:47 GMT   (6925kb)

Title: DriveSafe: A Hierarchical Risk Taxonomy for Safety-Critical LLM-Based
  Driving Assistants
Authors: Abhishek Kumar, Riya Tapwal, Carsten Maple
Categories: cs.AI
\\
  Large Language Models (LLMs) are increasingly integrated into vehicle-based
digital assistants, where unsafe, ambiguous, or legally incorrect responses can
lead to serious safety, ethical, and regulatory consequences. Despite growing
interest in LLM safety, existing taxonomies and evaluation frameworks remain
largely general-purpose and fail to capture the domain-specific risks inherent
to real-world driving scenarios. In this paper, we introduce DriveSafe, a
hierarchical, four-level risk taxonomy designed to systematically characterize
safety-critical failure modes of LLM-based driving assistants. The taxonomy
comprises 129 fine-grained atomic risk categories spanning technical, legal,
societal, and ethical dimensions, grounded in real-world driving regulations
and safety principles and reviewed by domain experts. To validate the safety
relevance and realism of the constructed prompts, we evaluate their refusal
behavior across six widely deployed LLMs. Our analysis shows that the evaluated
models often fail to appropriately refuse unsafe or non-compliant
driving-related queries, underscoring the limitations of general-purpose safety
alignment in driving contexts.
\\ ( https://arxiv.org/abs/2601.12138 ,  6925kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12141
Date: Sat, 17 Jan 2026 19:07:03 GMT   (1006kb)

Title: TIDE: A Trace-Informed Depth-First Exploration for Planning with
  Temporally Extended Goals
Authors: Yuliia Suprun, Khen Elimelech, Lydia E. Kavraki, and Moshe Y. Vardi
Categories: cs.AI
\\
  Task planning with temporally extended goals (TEGs) is a critical challenge
in AI and robotics, enabling agents to achieve complex sequences of objectives
over time rather than addressing isolated, immediate tasks. Linear Temporal
Logic on finite traces (LTLf ) provides a robust formalism for encoding these
temporal goals. Traditional LTLf task planning approaches often transform the
temporal planning problem into a classical planning problem with reachability
goals, which are then solved using off-the-shelf planners. However, these
methods often lack informed heuristics to provide a guided search for temporal
goals. We introduce TIDE (Trace-Informed Depth-first Exploration), a novel
approach that addresses this limitation by decomposing a temporal problem into
a sequence of smaller, manageable reach-avoid sub-problems, each solvable using
an off-the-shelf planner. TIDE identifies and prioritizes promising automaton
traces within the domain graph, using cost-driven heuristics to guide
exploration. Its adaptive backtracking mechanism systematically recovers from
failed plans by recalculating costs and penalizing infeasible transitions,
ensuring completeness and efficiency. Experimental results demonstrate that
TIDE achieves promising performance and is a valuable addition to the portfolio
of planning methods for temporally extended goals.
\\ ( https://arxiv.org/abs/2601.12141 ,  1006kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12242
Date: Sun, 18 Jan 2026 03:37:40 GMT   (939kb)

Title: Optimal Power Allocation and Sub-Optimal Channel Assignment for Downlink
  NOMA Systems Using Deep Reinforcement Learning
Authors: WooSeok Kim, Jeonghoon Lee, Sangho Kim, Taesun An, WonMin Lee, Dowon
  Kim, Kyungseop Shin
Categories: cs.AI cs.LG cs.NI
Journal-ref: J. Korean Inst. Commun. Inf. Sci. (J-KICS), vol. 50, no. 3, pp.
  406-419, 2025
DOI: 10.7840/kics.2025.50.3.406
\\
  In recent years, Non-Orthogonal Multiple Access (NOMA) system has emerged as
a promising candidate for multiple access frameworks due to the evolution of
deep machine learning, trying to incorporate deep machine learning into the
NOMA system. The main motivation for such active studies is the growing need to
optimize the utilization of network resources as the expansion of the internet
of things (IoT) caused a scarcity of network resources. The NOMA addresses this
need by power multiplexing, allowing multiple users to access the network
simultaneously. Nevertheless, the NOMA system has few limitations. Several
works have proposed to mitigate this, including the optimization of power
allocation known as joint resource allocation(JRA) method, and integration of
the JRA method and deep reinforcement learning (JRA-DRL). Despite this, the
channel assignment problem remains unclear and requires further investigation.
In this paper, we propose a deep reinforcement learning framework incorporating
replay memory with an on-policy algorithm, allocating network resources in a
NOMA system to generalize the learning. Also, we provide extensive simulations
to evaluate the effects of varying the learning rate, batch size, type of
model, and the number of features in the state.
\\ ( https://arxiv.org/abs/2601.12242 ,  939kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12256
Date: Sun, 18 Jan 2026 04:38:19 GMT   (457kb)

Title: Improving Large Molecular Language Model via Relation-aware Multimodal
  Collaboration
Authors: Jinyoung Park, Minseong Bae, Jeehye Na, Hyunwoo J. Kim
Categories: cs.AI
\\
  Large language models (LLMs) have demonstrated their instruction-following
capabilities and achieved powerful performance on various tasks. Inspired by
their success, recent works in the molecular domain have led to the development
of large molecular language models (LMLMs) that integrate 1D molecular strings
or 2D molecular graphs into the language models. However, existing LMLMs often
suffer from hallucination and limited robustness, largely due to inadequate
integration of diverse molecular modalities such as 1D sequences, 2D molecular
graphs, and 3D conformations. To address these limitations, we propose CoLLaMo,
a large language model-based molecular assistant equipped with a multi-level
molecular modality-collaborative projector. The relation-aware
modality-collaborative attention mechanism in the projector facilitates
fine-grained and relation-guided information exchange between atoms by
incorporating 2D structural and 3D spatial relations. Furthermore, we present a
molecule-centric new automatic measurement, including a hallucination
assessment metric and GPT-based caption quality evaluation to address the
limitations of token-based generic evaluation metrics (i.e., BLEU) widely used
in assessing molecular comprehension of LMLMs. Our extensive experiments
demonstrate that our CoLLaMo enhances the molecular modality generalization
capabilities of LMLMs, achieving the best performance on multiple tasks,
including molecule captioning, computed property QA, descriptive property QA,
motif counting, and IUPAC name prediction.
\\ ( https://arxiv.org/abs/2601.12256 ,  457kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12259
Date: Sun, 18 Jan 2026 04:44:49 GMT   (894kb)

Title: FutureX-Pro: Extending Future Prediction to High-Value Vertical Domains
Authors: Jiashuo Liu, Siyuan Chen, Zaiyuan Wang, Zhiyuan Zeng, Jiacheng Guo,
  Liang Hu, Lingyue Yin, Suozhi Huang, Wenxin Hao, Yang Yang, Zerui Cheng,
  Zixin Yao, Lingyue Yin, Haoxin Liu, Jiayi Cheng, Yuzhen Li, Zezhong Ma,
  Bingjie Wang, Bingsen Qiu, Xiao Liu, Zeyang Zhang, Zijian Liu, Jinpeng Wang,
  Mingren Yin, Tianci He, Yali Liao, Yixiao Tian, Zhenwei Zhu, Anqi Dai, Ge
  Zhang, Jingkai Liu, Kaiyuan Zhang, Wenlong Wu, Xiang Gao, Xinjie Chen, Zhixin
  Yao, Zhoufutu Wen, B. Aditya Prakash, Jose Blanchet, Mengdi Wang, Nian Si,
  Wenhao Huang
Categories: cs.AI cs.CE cs.LG
Comments: 21 pages
\\
  Building upon FutureX, which established a live benchmark for general-purpose
future prediction, this report introduces FutureX-Pro, including
FutureX-Finance, FutureX-Retail, FutureX-PublicHealth, FutureX-NaturalDisaster,
and FutureX-Search. These together form a specialized framework extending
agentic future prediction to high-value vertical domains. While generalist
agents demonstrate proficiency in open-domain search, their reliability in
capital-intensive and safety-critical sectors remains under-explored.
FutureX-Pro targets four economically and socially pivotal verticals: Finance,
Retail, Public Health, and Natural Disaster. We benchmark agentic Large
Language Models (LLMs) on entry-level yet foundational prediction tasks --
ranging from forecasting market indicators and supply chain demands to tracking
epidemic trends and natural disasters. By adapting the contamination-free,
live-evaluation pipeline of FutureX, we assess whether current State-of-the-Art
(SOTA) agentic LLMs possess the domain grounding necessary for industrial
deployment. Our findings reveal the performance gap between generalist
reasoning and the precision required for high-value vertical applications.
\\ ( https://arxiv.org/abs/2601.12259 ,  894kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12260
Date: Sun, 18 Jan 2026 04:45:09 GMT   (2044kb)

Title: Docs2Synth: A Synthetic Data Trained Retriever Framework for Scanned
  Visually Rich Documents Understanding
Authors: Yihao Ding, Qiang Sun, Puzhen Wu, Sirui Li, Siwen Luo, Wei Liu
Categories: cs.AI
Comments: Accepted at WWW 2026 Demo Track
\\
  Document understanding (VRDU) in regulated domains is particularly
challenging, since scanned documents often contain sensitive, evolving, and
domain specific knowledge. This leads to two major challenges: the lack of
manual annotations for model adaptation and the difficulty for pretrained
models to stay up-to-date with domain-specific facts. While Multimodal Large
Language Models (MLLMs) show strong zero-shot abilities, they still suffer from
hallucination and limited domain grounding. In contrast, discriminative
Vision-Language Pre-trained Models (VLPMs) provide reliable grounding but
require costly annotations to cover new domains. We introduce Docs2Synth, a
synthetic-supervision framework that enables retrieval-guided inference for
private and low-resource domains. Docs2Synth automatically processes raw
document collections, generates and verifies diverse QA pairs via an
agent-based system, and trains a lightweight visual retriever to extract
domain-relevant evidence. During inference, the retriever collaborates with an
MLLM through an iterative retrieval--generation loop, reducing hallucination
and improving response consistency. We further deliver Docs2Synth as an
easy-to-use Python package, enabling plug-and-play deployment across diverse
real-world scenarios. Experiments on multiple VRDU benchmarks show that
Docs2Synth substantially enhances grounding and domain generalization without
requiring human annotations.
\\ ( https://arxiv.org/abs/2601.12260 ,  2044kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12294
Date: Sun, 18 Jan 2026 07:48:36 GMT   (814kb)

Title: ToolPRMBench: Evaluating and Advancing Process Reward Models for
  Tool-using Agents
Authors: Dawei Li, Yuguang Yao, Zhen Tan, Huan Liu, Ruocheng Guo
Categories: cs.AI cs.SE
Comments: under review
\\
  Reward-guided search methods have demonstrated strong potential in enhancing
tool-using agents by effectively guiding sampling and exploration over complex
action spaces. As a core design, those search methods utilize process reward
models (PRMs) to provide step-level rewards, enabling more fine-grained
monitoring. However, there is a lack of systematic and reliable evaluation
benchmarks for PRMs in tool-using settings. In this paper, we introduce
ToolPRMBench, a large-scale benchmark specifically designed to evaluate PRMs
for tool-using agents. ToolPRMBench is built on top of several representative
tool-using benchmarks and converts agent trajectories into step-level test
cases. Each case contains the interaction history, a correct action, a
plausible but incorrect alternative, and relevant tool metadata. We
respectively utilize offline sampling to isolate local single-step errors and
online sampling to capture realistic multi-step failures from full agent
rollouts. A multi-LLM verification pipeline is proposed to reduce label noise
and ensure data quality. We conduct extensive experiments across large language
models, general PRMs, and tool-specialized PRMs on ToolPRMBench. The results
reveal clear differences in PRM effectiveness and highlight the potential of
specialized PRMs for tool-using. Code and data will be released at
https://github.com/David-Li0406/ToolPRMBench.
\\ ( https://arxiv.org/abs/2601.12294 ,  814kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12310
Date: Sun, 18 Jan 2026 08:35:56 GMT   (5197kb)

Title: Survival is the Only Reward: Sustainable Self-Training Through
  Environment-Mediated Selection
Authors: Jennifer Dodgson, Alfath Daryl Alhajir, Michael Joedhitya, Akira
  Rafhael Janson Pattirane, Surender Suresh Kumar, Joseph Lim, C.H. Peh, Adith
  Ramdas and Steven Zhang Zhexu
Categories: cs.AI
\\
  Self-training systems often degenerate due to the lack of an external
criterion for judging data quality, leading to reward hacking and semantic
drift. This paper provides a proof-of-concept system architecture for stable
self-training under sparse external feedback and bounded memory, and
empirically characterises its learning dynamics and failure modes.
  We introduce a self-training architecture in which learning is mediated
exclusively by environmental viability, rather than by reward, objective
functions, or externally defined fitness criteria. Candidate behaviours are
executed under real resource constraints, and only those whose environmental
effects both persist and preserve the possibility of future interaction are
propagated. The environment does not provide semantic feedback, dense rewards,
or task-specific supervision; selection operates solely through differential
survival of behaviours as world-altering events, making proxy optimisation
impossible and rendering reward-hacking evolutionarily unstable.
  Analysis of semantic dynamics shows that improvement arises primarily through
the persistence of effective and repeatable strategies under a regime of
consolidation and pruning, a paradigm we refer to as negative-space learning
(NSL), and that models develop meta-learning strategies (such as deliberate
experimental failure in order to elicit informative error messages) without
explicit instruction. This work establishes that environment-grounded selection
enables sustainable open-ended self-improvement, offering a viable path toward
more robust and generalisable autonomous systems without reliance on
human-curated data or complex reward shaping.
\\ ( https://arxiv.org/abs/2601.12310 ,  5197kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12318
Date: Sun, 18 Jan 2026 09:01:18 GMT   (2093kb)

Title: Beyond Human Annotation: Recent Advances in Data Generation Methods for
  Document Intelligence
Authors: Dehao Ying, Fengchang Yu, Haihua Chen, Changjiang Jiang, Yurong Li,
  and Wei Lu
Categories: cs.AI
\\
  The advancement of Document Intelligence (DI) demands large-scale,
high-quality training data, yet manual annotation remains a critical
bottleneck. While data generation methods are evolving rapidly, existing
surveys are constrained by fragmented focuses on single modalities or specific
tasks, lacking a unified perspective aligned with real-world workflows. To fill
this gap, this survey establishes the first comprehensive technical map for
data generation in DI. Data generation is redefined as supervisory signal
production, and a novel taxonomy is introduced based on the "availability of
data and labels." This framework organizes methodologies into four
resource-centric paradigms: Data Augmentation, Data Generation from Scratch,
Automated Data Annotation, and Self-Supervised Signal Construction.
Furthermore, a multi-level evaluation framework is established to integrate
intrinsic quality and extrinsic utility, compiling performance gains across
diverse DI benchmarks. Guided by this unified structure, the methodological
landscape is dissected to reveal critical challenges such as fidelity gaps and
frontiers including co-evolutionary ecosystems. Ultimately, by systematizing
this fragmented field, data generation is positioned as the central engine for
next-generation DI.
\\ ( https://arxiv.org/abs/2601.12318 ,  2093kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12323
Date: Sun, 18 Jan 2026 09:10:08 GMT   (1921kb)

Title: MARO: Learning Stronger Reasoning from Social Interaction
Authors: Yin Cai, Zhouhong Gu, Juntao Zhang, Ping Chen
Categories: cs.AI
\\
  Humans face countless scenarios that require reasoning and judgment in daily
life. However, existing large language model training methods primarily allow
models to learn from existing textual content or solve predetermined problems,
lacking experience in real scenarios involving interaction, negotiation, and
competition with others. To address this, this paper proposes Multi-Agent
Reward Optimization (MARO), a method that enables large language models (LLMs)
to acquire stronger reasoning abilities by learning and practicing in
multi-agent social environments. Specifically, MARO first addresses the sparse
learning signal problem by decomposing final success or failure outcomes into
each specific behavior during the interaction process; second, it handles the
uneven role distribution problem by balancing the training sample weights of
different roles; finally, it addresses environmental instability issues by
directly evaluating the utility of each behavior. Experimental results
demonstrate that MARO not only achieves significant improvements in social
reasoning capabilities, but also that the abilities acquired through social
simulation learning can effectively transfer to other tasks such as
mathematical reasoning and instruction following. This reveals the tremendous
potential of multi-agent social learning in enhancing the general reasoning
capabilities of LLMs.
\\ ( https://arxiv.org/abs/2601.12323 ,  1921kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12338
Date: Sun, 18 Jan 2026 10:11:29 GMT   (336kb)

Title: Actionable Advice from Reviews via Mixture of LoRA Experts: A Two-LLM
  Pipeline for Issue Extraction and Business Recommendations
Authors: Kartikey Singh Bhandari, Manav Ganesh, Yashwant Viswanathan, Archit
  Agrawal, Dhruv Kumar, Pratik Narang
Categories: cs.AI
\\
  Customer reviews contain detailed, domain specific signals about service
failures and user expectations, but converting this unstructured feedback into
actionable business decisions remains difficult. We study review-to-action
generation: producing concrete, implementable recommendations grounded in
review text. We propose a modular two-LLM framework in which an Issue model
extracts salient issues and assigns coarse themes, and an Advice model
generates targeted operational fixes conditioned on the extracted issue
representation. To enable specialization without expensive full fine-tuning, we
adapt the Advice model using a mixture of LoRA experts strategy: multiple
low-rank adapters are trained and a lightweight gating mechanism performs
token-level expert mixing at inference, combining complementary expertise
across issue types. We construct synthetic review-issue-advice triples from
Yelp reviews (airlines and restaurants) to supervise training, and evaluate
recommendations using an eight dimension operational rubric spanning
actionability, specificity, feasibility, expected impact, novelty,
non-redundancy, bias, and clarity. Across both domains, our approach
consistently outperforms prompting-only and single-adapter baselines, yielding
higher actionability and specificity while retaining favorable
efficiency-quality trade-offs.
\\ ( https://arxiv.org/abs/2601.12338 ,  336kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12392
Date: Sun, 18 Jan 2026 13:06:13 GMT   (21817kb)

Title: Psych\=eChat: An Empathic Framework Focused on Emotion Shift Tracking
  and Safety Risk Analysis in Psychological Counseling
Authors: Zhentao Xia, Yongqi Fan, Yuxiang Chu, Yichao Yin, Liangliang Chen,
  Tong Ruan, Weiyan Zhang
Categories: cs.AI
\\
  Large language models (LLMs) have demonstrated notable advancements in
psychological counseling. However, existing models generally do not explicitly
model seekers' emotion shifts across counseling sessions, a core focus in
classical psychological schools. Moreover, how to align counselor models'
responses with these emotion shifts while proactively mitigating safety risks
remains underexplored. To bridge these gaps, we propose Psych\=eChat, which
explicitly integrates emotion shift tracking and safety risk analysis for
psychological counseling. Specifically, we employ interactive role-playing to
synthesize counselor--seeker dialogues, incorporating two modules: Emotion
Management Module, to capture seekers' current emotions and emotion shifts; and
Risk Control Module, to anticipate seekers' subsequent reactions and identify
potential risks. Furthermore, we introduce two modeling paradigms. The Agent
Mode structures emotion management, risk control, and counselor responses into
a collaborative multi-agent pipeline. The LLM Mode integrates these stages into
a unified chain-of-thought for end-to-end inference, balancing efficiency and
performance. Extensive experiments, including interactive scoring,
dialogue-level evaluation, and human assessment, demonstrate that Psych\=eChat
outperforms existing methods for emotional insight and safety control.
\\ ( https://arxiv.org/abs/2601.12392 ,  21817kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12410
Date: Sun, 18 Jan 2026 13:53:24 GMT   (1819kb)

Title: Are LLMs Smarter Than Chimpanzees? An Evaluation on Perspective Taking
  and Knowledge State Estimation
Authors: Dingyi Yang, Junqi Zhao, Xue Li, Ce Li, Boyang Li
Categories: cs.AI
Comments: 23 pages, 11 figures
\\
  Cognitive anthropology suggests that the distinction of human intelligence
lies in the ability to infer other individuals' knowledge states and understand
their intentions. In comparison, our closest animal relative, chimpanzees, lack
the capacity to do so. With this paper, we aim to evaluate LLM performance in
the area of knowledge state tracking and estimation. We design two tasks to
test (1) if LLMs can detect when story characters, through their actions,
demonstrate knowledge they should not possess, and (2) if LLMs can predict
story characters' next actions based on their own knowledge vs. objective
truths they do not know. Results reveal that most current state-of-the-art LLMs
achieve near-random performance on both tasks, and are substantially inferior
to humans. We argue future LLM research should place more weight on the
abilities of knowledge estimation and intention understanding.
\\ ( https://arxiv.org/abs/2601.12410 ,  1819kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12444
Date: Sun, 18 Jan 2026 14:57:57 GMT   (2018kb)

Title: Large Language Model for OWL Proofs
Authors: Hui Yang, Jiaoyan Chen, Uli Sattler
Categories: cs.AI cs.LO
\\
  The ability of Large Language Models (LLMs) to perform reasoning tasks such
as deduction has been widely investigated in recent years. Yet, their capacity
to generate proofs-faithful, human-readable explanations of why conclusions
follow-remains largely under explored. In this work, we study proof generation
in the context of OWL ontologies, which are widely adopted for representing and
reasoning over complex knowledge, by developing an automated dataset
construction and evaluation framework. Our evaluation encompassing three
sequential tasks for complete proving: Extraction, Simplification, and
Explanation, as well as an additional task of assessing Logic Completeness of
the premise. Through extensive experiments on widely used reasoning LLMs, we
achieve important findings including: (1) Some models achieve overall strong
results but remain limited on complex cases; (2) Logical complexity, rather
than representation format (formal logic language versus natural language), is
the dominant factor shaping LLM performance; and (3) Noise and incompleteness
in input data substantially diminish LLMs' performance. Together, these results
underscore both the promise of LLMs for explanation with rigorous logics and
the gap of supporting resilient reasoning under complex or imperfect
conditions. Code and data are available at
https://github.com/HuiYang1997/LLMOwlR.
\\ ( https://arxiv.org/abs/2601.12444 ,  2018kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12499
Date: Sun, 18 Jan 2026 17:16:04 GMT   (5751kb)

Title: Failure Modes in Multi-Hop QA: The Weakest Link Law and the Recognition
  Bottleneck
Authors: Meiru Zhang, Zaiqiao Meng, Nigel Collier
Categories: cs.AI cs.LG
Comments: preprint
\\
  Despite scaling to massive context windows, Large Language Models (LLMs)
struggle with multi-hop reasoning due to inherent position bias, which causes
them to overlook information at certain positions. Whether these failures stem
from an inability to locate evidence (recognition failure) or integrate it
(synthesis failure) is unclear. We introduce Multi-Focus Attention Instruction
(MFAI), a semantic probe to disentangle these mechanisms by explicitly steering
attention towards selected positions. Across 5 LLMs on two multi-hop QA tasks
(MuSiQue and NeoQA), we establish the "Weakest Link Law": multi-hop reasoning
performance collapses to the performance level of the least visible evidence.
Crucially, this failure is governed by absolute position rather than the linear
distance between facts (performance variance $<3%$). We further identify a
duality in attention steering: while matched MFAI resolves recognition
bottlenecks, improving accuracy by up to 11.5% in low-visibility positions,
misleading MFAI triggers confusion in real-world tasks but is successfully
filtered in synthetic tasks. Finally, we demonstrate that "thinking" models
that utilize System-2 reasoning, effectively locate and integrate the required
information, matching gold-only baselines even in noisy, long-context settings.
\\ ( https://arxiv.org/abs/2601.12499 ,  5751kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12538
Date: Sun, 18 Jan 2026 18:58:23 GMT   (8703kb)

Title: Agentic Reasoning for Large Language Models
Authors: Tianxin Wei, Ting-Wei Li, Zhining Liu, Xuying Ning, Ze Yang, Jiaru
  Zou, Zhichen Zeng, Ruizhong Qiu, Xiao Lin, Dongqi Fu, Zihao Li, Mengting Ai,
  Duo Zhou, Wenxuan Bao, Yunzhe Li, Gaotang Li, Cheng Qian, Yu Wang, Xiangru
  Tang, Yin Xiao, Liri Fang, Hui Liu, Xianfeng Tang, Yuji Zhang, Chi Wang,
  Jiaxuan You, Heng Ji, Hanghang Tong, Jingrui He
Categories: cs.AI cs.CL
Comments: Project: https://github.com/weitianxin/Awesome-Agentic-Reasoning
\\
  Reasoning is a fundamental cognitive process underlying inference,
problem-solving, and decision-making. While large language models (LLMs)
demonstrate strong reasoning capabilities in closed-world settings, they
struggle in open-ended and dynamic environments. Agentic reasoning marks a
paradigm shift by reframing LLMs as autonomous agents that plan, act, and learn
through continual interaction. In this survey, we organize agentic reasoning
along three complementary dimensions. First, we characterize environmental
dynamics through three layers: foundational agentic reasoning, which
establishes core single-agent capabilities including planning, tool use, and
search in stable environments; self-evolving agentic reasoning, which studies
how agents refine these capabilities through feedback, memory, and adaptation;
and collective multi-agent reasoning, which extends intelligence to
collaborative settings involving coordination, knowledge sharing, and shared
goals. Across these layers, we distinguish in-context reasoning, which scales
test-time interaction through structured orchestration, from post-training
reasoning, which optimizes behaviors via reinforcement learning and supervised
fine-tuning. We further review representative agentic reasoning frameworks
across real-world applications and benchmarks, including science, robotics,
healthcare, autonomous research, and mathematics. This survey synthesizes
agentic reasoning methods into a unified roadmap bridging thought and action,
and outlines open challenges and future directions, including personalization,
long-horizon interaction, world modeling, scalable multi-agent training, and
governance for real-world deployment.
\\ ( https://arxiv.org/abs/2601.12538 ,  8703kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12539
Date: Sun, 18 Jan 2026 19:01:03 GMT   (1476kb)

Title: MemeLens: Multilingual Multitask VLMs for Memes
Authors: Ali Ezzat Shahroor, Mohamed Bayan Kmainasi, Abul Hasnat, Dimitar
  Dimitrov, Giovanni Da San Martino, Preslav Nakov, Firoj Alam
Categories: cs.AI cs.CL
Comments: disinformation, misinformation, factuality, harmfulness, fake news,
  propaganda, hateful meme, multimodality, text, images
MSC-class: 68T50
ACM-class: I.2.7
\\
  Memes are a dominant medium for online communication and manipulation because
meaning emerges from interactions between embedded text, imagery, and cultural
context. Existing meme research is distributed across tasks (hate, misogyny,
propaganda, sentiment, humour) and languages, which limits cross-domain
generalization. To address this gap we propose MemeLens, a unified multilingual
and multitask explanation-enhanced Vision Language Model (VLM) for meme
understanding. We consolidate 38 public meme datasets, filter and map
dataset-specific labels into a shared taxonomy of $20$ tasks spanning harm,
targets, figurative/pragmatic intent, and affect. We present a comprehensive
empirical analysis across modeling paradigms, task categories, and datasets.
Our findings suggest that robust meme understanding requires multimodal
training, exhibits substantial variation across semantic categories, and
remains sensitive to over-specialization when models are fine-tuned on
individual datasets rather than trained in a unified setting. We will make the
experimental resources and datasets publicly available for the community.
\\ ( https://arxiv.org/abs/2601.12539 ,  1476kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12542
Date: Sun, 18 Jan 2026 19:12:41 GMT   (10256kb)

Title: Rethinking the AI Scientist: Interactive Multi-Agent Workflows for
  Scientific Discovery
Authors: Lukas Weidener, Marko Brki\'c, Mihailo Jovanovi\'c, Ritvik Singh,
  Chiara Baccin, Emre Ulgac, Alex Dobrin, Aakaash Meduri
Categories: cs.AI
\\
  Artificial intelligence systems for scientific discovery have demonstrated
remarkable potential, yet existing approaches remain largely proprietary and
operate in batch-processing modes requiring hours per research cycle,
precluding real-time researcher guidance. This paper introduces Deep Research,
a multi-agent system enabling interactive scientific investigation with
turnaround times measured in minutes. The architecture comprises specialized
agents for planning, data analysis, literature search, and novelty detection,
unified through a persistent world state that maintains context across
iterative research cycles. Two operational modes support different workflows:
semi-autonomous mode with selective human checkpoints, and fully autonomous
mode for extended investigations. Evaluation on the BixBench computational
biology benchmark demonstrated state-of-the-art performance, achieving 48.8%
accuracy on open response and 64.5% on multiple-choice evaluation, exceeding
existing baselines by 14 to 26 percentage points. Analysis of architectural
constraints, including open access literature limitations and challenges
inherent to automated novelty assessment, informs practical deployment
considerations for AI-assisted scientific workflows.
\\ ( https://arxiv.org/abs/2601.12542 ,  10256kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12547
Date: Sun, 18 Jan 2026 19:19:41 GMT   (42kb)

Title: How Clinicians Think and What AI Can Learn From It
Authors: Dipayan Sengupta, Saumya Panda
Categories: cs.AI
Comments: 34 pages
\\
  Most clinical AI systems operate as prediction engines -- producing labels or
risk scores -- yet real clinical reasoning is a time-bounded, sequential
control problem under uncertainty. Clinicians interleave information gathering
with irreversible actions, guided by regret, constraints and patient values. We
argue that the dominant computational substrate of clinician reasoning is not
cardinal optimization but ordinal, non-compensatory decision-making: Clinicians
frequently rely on fast-and-frugal, lexicographic heuristics (e.g.,
fast-and-frugal trees) that stop early after checking a small, fixed sequence
of cues. We provide a normative rationale for why such algorithms are not
merely bounded rationality shortcuts, but can be epistemically preferred in
medicine. First, many clinical trade-offs are constructed through human
judgment and are only weakly measurable on absolute scales; without strong
measurement axioms, only orderings are invariant, motivating an
ordinal-by-default stance. Second, preference and signal elicitation are
structurally crude: The mapping from truth $\to$ perception $\to$ inference
$\to$ recorded variables introduces layered noise, leaving a persistent
uncertainty floor. When this 'crudeness' overwhelms the decision margin,
plug-in expected-utility optimization becomes brittle (high flip probability
under small perturbations), whereas robust dominance/filtering rules
($\epsilon$-dominance, maximin) stabilize decisions.Finally, we outline a
clinician-aligned AI blueprint: Use rich models for beliefs and trajectories,
but choose actions through robust ordinal rules; treat heuristics as the
low-dimensional special case; and deploy AI as 'selective complexity' --
invoked mainly for tie-breaking when decisions are fragile and information has
positive expected impact.
\\ ( https://arxiv.org/abs/2601.12547 ,  42kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12560
Date: Sun, 18 Jan 2026 19:51:16 GMT   (1304kb)

Title: Agentic Artificial Intelligence (AI): Architectures, Taxonomies, and
  Evaluation of Large Language Model Agents
Authors: Arunkumar V, Gangadharan G.R., Rajkumar Buyya
Categories: cs.AI cs.MA
Comments: 28 pages, 4 figures, 5 tables
\\
  Artificial Intelligence is moving from models that only generate text to
Agentic AI, where systems behave as autonomous entities that can perceive,
reason, plan, and act. Large Language Models (LLMs) are no longer used only as
passive knowledge engines but as cognitive controllers that combine memory,
tool use, and feedback from their environment to pursue extended goals. This
shift already supports the automation of complex workflows in software
engineering, scientific discovery, and web navigation, yet the variety of
emerging designs, from simple single loop agents to hierarchical multi agent
systems, makes the landscape hard to navigate. In this paper, we investigate
architectures and propose a unified taxonomy that breaks agents into
Perception, Brain, Planning, Action, Tool Use, and Collaboration. We use this
lens to describe the move from linear reasoning procedures to native inference
time reasoning models, and the transition from fixed API calls to open
standards like the Model Context Protocol (MCP) and Native Computer Use. We
also group the environments in which these agents operate, including digital
operating systems, embodied robotics, and other specialized domains, and we
review current evaluation practices. Finally, we highlight open challenges,
such as hallucination in action, infinite loops, and prompt injection, and
outline future research directions toward more robust and reliable autonomous
systems.
\\ ( https://arxiv.org/abs/2601.12560 ,  1304kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12641
Date: Mon, 19 Jan 2026 01:10:49 GMT   (877kb)

Title: STEP-LLM: Generating CAD STEP Models from Natural Language with Large
  Language Models
Authors: Xiangyu Shi, Junyang Ding, Xu Zhao, Sinong Zhan, Payal Mohapatra,
  Daniel Quispe, Kojo Welbeck, Jian Cao, Wei Chen, Ping Guo, Qi Zhu
Categories: cs.AI
Comments: Accepted to the Design, Automation & Test in Europe Conference (DATE)
  2026
\\
  Computer-aided design (CAD) is vital to modern manufacturing, yet model
creation remains labor-intensive and expertise-heavy. To enable non-experts to
translate intuitive design intent into manufacturable artifacts, recent large
language models-based text-to-CAD efforts focus on command sequences or
script-based formats like CadQuery. However, these formats are kernel-dependent
and lack universality for manufacturing. In contrast, the Standard for the
Exchange of Product Data (STEP, ISO 10303) file is a widely adopted, neutral
boundary representation (B-rep) format directly compatible with manufacturing,
but its graph-structured, cross-referenced nature poses unique challenges for
auto-regressive LLMs. To address this, we curate a dataset of ~40K STEP-caption
pairs and introduce novel preprocessing tailored for the graph-structured
format of STEP, including a depth-first search-based reserialization that
linearizes cross-references while preserving locality and
chain-of-thought(CoT)-style structural annotations that guide global coherence.
We integrate retrieval-augmented generation to ground predictions in relevant
examples for supervised fine-tuning, and refine generation quality through
reinforcement learning with a specific Chamfer Distance-based geometric reward.
Experiments demonstrate consistent gains of our STEP-LLM in geometric fidelity
over the Text2CAD baseline, with improvements arising from multiple stages of
our framework: the RAG module substantially enhances completeness and
renderability, the DFS-based reserialization strengthens overall accuracy, and
the RL further reduces geometric discrepancy. Both metrics and visual
comparisons confirm that STEP-LLM generates shapes with higher fidelity than
Text2CAD. These results show the feasibility of LLM-driven STEP model
generation from natural language, showing its potential to democratize CAD
design for manufacturing.
\\ ( https://arxiv.org/abs/2601.12641 ,  877kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12661
Date: Mon, 19 Jan 2026 02:18:10 GMT   (4683kb)

Title: MedConsultBench: A Full-Cycle, Fine-Grained, Process-Aware Benchmark for
  Medical Consultation Agents
Authors: Chuhan Qiao, Jianghua Huang, Daxing Zhao, Ziding Liu, Yanjun Shen,
  Bing Cheng, Wei Lin, Kai Wu
Categories: cs.AI
\\
  Current evaluations of medical consultation agents often prioritize
outcome-oriented tasks, frequently overlooking the end-to-end process integrity
and clinical safety essential for real-world practice. While recent interactive
benchmarks have introduced dynamic scenarios, they often remain fragmented and
coarse-grained, failing to capture the structured inquiry logic and diagnostic
rigor required in professional consultations. To bridge this gap, we propose
MedConsultBench, a comprehensive framework designed to evaluate the complete
online consultation cycle by covering the entire clinical workflow from history
taking and diagnosis to treatment planning and follow-up Q\&A. Our methodology
introduces Atomic Information Units (AIUs) to track clinical information
acquisition at a sub-turn level, enabling precise monitoring of how key facts
are elicited through 22 fine-grained metrics. By addressing the
underspecification and ambiguity inherent in online consultations, the
benchmark evaluates uncertainty-aware yet concise inquiry while emphasizing
medication regimen compatibility and the ability to handle realistic
post-prescription follow-up Q\&A via constraint-respecting plan revisions.
Systematic evaluation of 19 large language models reveals that high diagnostic
accuracy often masks significant deficiencies in information-gathering
efficiency and medication safety. These results underscore a critical gap
between theoretical medical knowledge and clinical practice ability,
establishing MedConsultBench as a rigorous foundation for aligning medical AI
with the nuanced requirements of real-world clinical care.
\\ ( https://arxiv.org/abs/2601.12661 ,  4683kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12667
Date: Mon, 19 Jan 2026 02:28:27 GMT   (3486kb)

Title: Empowering All-in-Loop Health Management of Spacecraft Power System in
  the Mega-Constellation Era via Human-AI Collaboration
Authors: Yi Di, Zhibin Zhao, Fujin Wang, Xue Liu, Jiafeng Tang, Jiaxin Ren, Zhi
  Zhai, Xuefeng Chen
Categories: cs.AI
\\
  It is foreseeable that the number of spacecraft will increase exponentially,
ushering in an era dominated by satellite mega-constellations (SMC). This
necessitates a focus on energy in space: spacecraft power systems (SPS),
especially their health management (HM), given their role in power supply and
high failure rates. Providing health management for dozens of SPS and for
thousands of SPS represents two fundamentally different paradigms. Therefore,
to adapt the health management in the SMC era, this work proposes a principle
of aligning underlying capabilities (AUC principle) and develops SpaceHMchat,
an open-source Human-AI collaboration (HAIC) framework for all-in-loop health
management (AIL HM). SpaceHMchat serves across the entire loop of work
condition recognition, anomaly detection, fault localization, and maintenance
decision making, achieving goals such as conversational task completion,
adaptive human-in-the-loop learning, personnel structure optimization,
knowledge sharing, efficiency enhancement, as well as transparent reasoning and
improved interpretability. Meanwhile, to validate this exploration, a
hardware-realistic fault injection experimental platform is established, and
its simulation model is built and open-sourced, both fully replicating the real
SPS. The corresponding experimental results demonstrate that SpaceHMchat
achieves excellent performance across 23 quantitative metrics, such as 100%
conclusion accuracy in logical reasoning of work condition recognition, over
99% success rate in anomaly detection tool invocation, over 90% precision in
fault localization, and knowledge base search time under 3 minutes in
maintenance decision-making. Another contribution of this work is the release
of the first-ever AIL HM dataset of SPS. This dataset contains four
sub-datasets, involving 4 types of AIL HM sub-tasks, 17 types of faults, and
over 700,000 timestamps.
\\ ( https://arxiv.org/abs/2601.12667 ,  3486kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12688
Date: Mon, 19 Jan 2026 03:20:36 GMT   (4495kb)

Title: Logic-Guided Multistage Inference for Explainable Multidefendant
  Judgment Prediction
Authors: Xu Zhang, Qinghua Wang, Mengyang Zhao, Fang Wang, Cunquan Qu
Categories: cs.AI cs.LG
\\
  Crime disrupts societal stability, making law essential for balance. In
multidefendant cases, assigning responsibility is complex and challenges
fairness, requiring precise role differentiation. However, judicial phrasing
often obscures the roles of the defendants, hindering effective AI-driven
analyses. To address this issue, we incorporate sentencing logic into a
pretrained Transformer encoder framework to enhance the intelligent assistance
in multidefendant cases while ensuring legal interpretability. Within this
framework an oriented masking mechanism clarifies roles and a comparative data
construction strategy improves the model's sensitivity to culpability
distinctions between principals and accomplices. Predicted guilt labels are
further incorporated into a regression model through broadcasting,
consolidating crime descriptions and court views. Our proposed masked
multistage inference (MMSI) framework, evaluated on the custom IMLJP dataset
for intentional injury cases, achieves significant accuracy improvements,
outperforming baselines in role-based culpability differentiation. This work
offers a robust solution for enhancing intelligent judicial systems, with
publicly code available.
\\ ( https://arxiv.org/abs/2601.12688 ,  4495kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12711
Date: Mon, 19 Jan 2026 04:24:49 GMT   (706kb)

Title: Neurosymbolic LoRA: Why and When to Tune Weights vs. Rewrite Prompts
Authors: Kevin Wang, Neel P. Bhatt, Cong Liu, Junbo Li, Runjin Chen, Yihan Xi,
  Timothy Barclay, Alvaro Velasquez, Ufuk Topcu, Zhangyang Wang
Categories: cs.AI cs.LG cs.SC
\\
  Large language models (LLMs) can be adapted either through numerical updates
that alter model parameters or symbolic manipulations that work on discrete
prompts or logical constraints. While numerical fine-tuning excels at injecting
new factual knowledge, symbolic updates offer flexible control of style and
alignment without retraining. We introduce a neurosymbolic LoRA framework that
dynamically combines these two complementary strategies. Specifically, we
present a unified monitoring signal and a reward-based classifier to decide
when to employ LoRA for deeper factual reconstruction and when to apply
TextGrad for token-level edits. Our approach remains memory-efficient by
offloading the symbolic transformations to an external LLM only when needed.
Additionally, the refined prompts produced during symbolic editing serve as
high-quality, reusable training data, an important benefit in data-scarce
domains like mathematical reasoning. Extensive experiments across multiple LLM
backbones show that neurosymbolic LoRA consistently outperforms purely
numerical or purely symbolic baselines, demonstrating superior adaptability and
improved performance. Our findings highlight the value of interleaving
numerical and symbolic updates to unlock a new level of versatility in language
model fine-tuning.
\\ ( https://arxiv.org/abs/2601.12711 ,  706kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12720
Date: Mon, 19 Jan 2026 04:51:53 GMT   (810kb)

Title: Teaching Large Reasoning Models Effective Reflection
Authors: Hanbin Wang, Jingwei Song, Jinpeng Li, Qi Zhu, Fei Mi, Ganqu Cui,
  Yasheng Wang, Lifeng Shang
Categories: cs.AI
Comments: 14 pages (including appendix), 5 figures
\\
  Large Reasoning Models (LRMs) have recently shown impressive performance on
complex reasoning tasks, often by engaging in self-reflective behaviors such as
self-critique and backtracking. However, not all reflections are
beneficial-many are superficial, offering little to no improvement over the
original answer and incurring computation overhead. In this paper, we identify
and address the problem of superficial reflection in LRMs. We first propose
Self-Critique Fine-Tuning (SCFT), a training framework that enhances the
model's reflective reasoning ability using only self-generated critiques. SCFT
prompts models to critique their own outputs, filters high-quality critiques
through rejection sampling, and fine-tunes the model using a critique-based
objective. Building on this strong foundation, we further introduce
Reinforcement Learning with Effective Reflection Rewards (RLERR). RLERR
leverages the high-quality reflections initialized by SCFT to construct reward
signals, guiding the model to internalize the self-correction process via
reinforcement learning. Experiments on two challenging benchmarks, AIME2024 and
AIME2025, show that SCFT and RLERR significantly improve both reasoning
accuracy and reflection quality, outperforming state-of-the-art baselines. All
data and codes are available at https://github.com/wanghanbinpanda/SCFT.
\\ ( https://arxiv.org/abs/2601.12720 ,  810kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12744
Date: Mon, 19 Jan 2026 05:57:58 GMT   (8360kb)

Title: Vision Language Models for Optimization-Driven Intent Processing in
  Autonomous Networks
Authors: Tasnim Ahmed, Yifan Zhu, Salimur Choudhury
Categories: cs.AI cs.NI cs.SE
Comments: Accepted for presentation at The IEEE International Conference on
  Communications (ICC) 2026
\\
  Intent-Based Networking (IBN) allows operators to specify high-level network
goals rather than low-level configurations. While recent work demonstrates that
large language models can automate configuration tasks, a distinct class of
intents requires generating optimization code to compute provably optimal
solutions for traffic engineering, routing, and resource allocation. Current
systems assume text-based intent expression, requiring operators to enumerate
topologies and parameters in prose. Network practitioners naturally reason
about structure through diagrams, yet whether Vision-Language Models (VLMs) can
process annotated network sketches into correct optimization code remains
unexplored. We present IntentOpt, a benchmark of 85 optimization problems
across 17 categories, evaluating four VLMs (GPT-5-Mini, Claude-Haiku-4.5,
Gemini-2.5-Flash, Llama-3.2-11B-Vision) under three prompting strategies on
multimodal versus text-only inputs. Our evaluation shows that visual parameter
extraction reduces execution success by 12-21 percentage points (pp), with
GPT-5-Mini dropping from 93% to 72%. Program-of-thought prompting decreases
performance by up to 13 pp, and open-source models lag behind closed-source
ones, with Llama-3.2-11B-Vision reaching 18% compared to 75% for GPT-5-Mini.
These results establish baseline capabilities and limitations of current VLMs
for optimization code generation within an IBN system. We also demonstrate
practical feasibility through a case study that deploys VLM-generated code to
network testbed infrastructure using Model Context Protocol.
\\ ( https://arxiv.org/abs/2601.12744 ,  8360kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12781
Date: Mon, 19 Jan 2026 07:21:19 GMT   (1949kb)

Title: VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification
  for Referring Expression Comprehension
Authors: Hyejin Park, Junhyuk Kwon, Suha Kwak, Jungseul Ok
Categories: cs.AI cs.CV
\\
  Referring Expression Comprehension (REC) aims to localize the image region
corresponding to a natural-language query. Recent neuro-symbolic REC approaches
leverage large language models (LLMs) and vision-language models (VLMs) to
perform compositional reasoning, decomposing queries 4 structured programs and
executing them step-by-step. While such approaches achieve interpretable
reasoning and strong zero-shot generalization, they assume that intermediate
reasoning steps are accurate. However, this assumption causes cascading errors:
false detections and invalid relations propagate through the reasoning chain,
yielding high-confidence false positives even when no target is present in the
image. To address this limitation, we introduce Verification-Integrated
Reasoning Operators (VIRO), a neuro-symbolic framework that embeds lightweight
operator-level verifiers within reasoning steps. Each operator executes and
validates its output, such as object existence or spatial relationship, thereby
allowing the system to robustly handle no-target cases when verification
conditions are not met. Our framework achieves state-of-the-art performance,
reaching 61.1% balanced accuracy across target-present and no-target settings,
and demonstrates generalization to real-world egocentric data. Furthermore,
VIRO shows superior computational efficiency in terms of throughput, high
reliability with a program failure rate of less than 0.3%, and scalability
through decoupled program generation from execution.
\\ ( https://arxiv.org/abs/2601.12781 ,  1949kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12804
Date: Mon, 19 Jan 2026 08:05:28 GMT   (8923kb)

Title: SL-CBM: Enhancing Concept Bottleneck Models with Semantic Locality for
  Better Interpretability
Authors: Hanwei Zhang, Luo Cheng, Rui Wen, Yang Zhang, Lijun Zhang, Holger
  Hermanns
Categories: cs.AI cs.LG
\\
  Explainable AI (XAI) is crucial for building transparent and trustworthy
machine learning systems, especially in high-stakes domains. Concept Bottleneck
Models (CBMs) have emerged as a promising ante-hoc approach that provides
interpretable, concept-level explanations by explicitly modeling
human-understandable concepts. However, existing CBMs often suffer from poor
locality faithfulness, failing to spatially align concepts with meaningful
image regions, which limits their interpretability and reliability. In this
work, we propose SL-CBM (CBM with Semantic Locality), a novel extension that
enforces locality faithfulness by generating spatially coherent saliency maps
at both concept and class levels. SL-CBM integrates a 1x1 convolutional layer
with a cross-attention mechanism to enhance alignment between concepts, image
regions, and final predictions. Unlike prior methods, SL-CBM produces faithful
saliency maps inherently tied to the model's internal reasoning, facilitating
more effective debugging and intervention. Extensive experiments on image
datasets demonstrate that SL-CBM substantially improves locality faithfulness,
explanation quality, and intervention efficacy while maintaining competitive
classification accuracy. Our ablation studies highlight the importance of
contrastive and entropy-based regularization for balancing accuracy, sparsity,
and faithfulness. Overall, SL-CBM bridges the gap between concept-based
reasoning and spatial explainability, setting a new standard for interpretable
and trustworthy concept-based models.
\\ ( https://arxiv.org/abs/2601.12804 ,  8923kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12822
Date: Mon, 19 Jan 2026 08:32:09 GMT   (2628kb)

Title: MirrorGuard: Toward Secure Computer-Use Agents via Simulation-to-Real
  Reasoning Correction
Authors: Wenqi Zhang, Yulin Shen, Changyue Jiang, Jiarun Dai, Geng Hong, Xudong
  Pan
Categories: cs.AI
\\
  Large foundation models are integrated into Computer Use Agents (CUAs),
enabling autonomous interaction with operating systems through graphical user
interfaces (GUIs) to perform complex tasks. This autonomy introduces serious
security risks: malicious instructions or visual prompt injections can trigger
unsafe reasoning and cause harmful system-level actions. Existing defenses,
such as detection-based blocking, prevent damage but often abort tasks
prematurely, reducing agent utility. In this paper, we present MirrorGuard, a
plug-and-play defense framework that uses simulation-based training to improve
CUA security in the real world. To reduce the cost of large-scale training in
operating systems, we propose a novel neural-symbolic simulation pipeline,
which generates realistic, high-risk GUI interaction trajectories entirely in a
text-based simulated environment, which captures unsafe reasoning patterns and
potential system hazards without executing real operations. In the simulation
environment, MirrorGuard learns to intercept and rectify insecure reasoning
chains of CUAs before they produce and execute unsafe actions. In real-world
testing, extensive evaluations across diverse benchmarks and CUA architectures
show that MirrorGuard significantly mitigates security risks. For instance, on
the ByteDance UI-TARS system, it reduces the unsafe rate from 66.5% to 13.0%
while maintaining a marginal false refusal rate (FRR). In contrast, the
state-of-the-art GuardAgent only achieves a reduction to 53.9% and suffers from
a 15.4% higher FRR. Our work proves that simulation-derived defenses can
provide robust, real-world protection while maintaining the fundamental utility
of the agent. Our code and model are publicly available at
https://bmz-q-q.github.io/MirrorGuard/.
\\ ( https://arxiv.org/abs/2601.12822 ,  2628kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12842
Date: Mon, 19 Jan 2026 08:55:46 GMT   (542kb)

Title: SCULPT: Constraint-Guided Pruned MCTS that Carves Efficient Paths for
  Mathematical Reasoning
Authors: Qitong Fang (1), Haotian Li (1), Xu Wang (1)((1) Jilin Jianzhu
  University)
Categories: cs.AI cs.LG
Comments: 11 pages, 3 figures. Equal contribution: Qitong Fang and Haotian Li.
  Corresponding authors: Qitong Fang (fangqitong@student.jlju.edu.cn), Haotian
  Li (lihaotian@student.jlju.edu.cn), Xu Wang (wangxu@jlju.edu.cn)
ACM-class: I.2.8; I.2.6; I.2.7
\\
  Automated agent workflows can enhance the problem-solving ability of large
language models (LLMs), but common search strategies rely on stochastic
exploration and often traverse implausible branches. This occurs because
current pipelines sample candidate steps from generic prompts or learned
policies with weak domain priors, yielding near-random walks over operators,
units, and formats. To promote ordered exploration, this paper introduces
SCULPT, a constraint-guided approach for Monte Carlo Tree Search (MCTS) that
integrates domain-aware scoring into selection, expansion, simulation, and
backpropagation. SCULPT scores and prunes actions using a combination of
symbolic checks (dimensional consistency, type compatibility, magnitude sanity,
depth control, and diversity) and structural pattern guidance, thereby steering
the search toward plausible reasoning paths. Under matched LLM configurations,
SCULPT yields stable improvements on multiple datasets; additional results with
GPT-5.2 assess executor transferability and performance on frontier reasoning
models. Overall, domain-aware constraints can improve accuracy while
maintaining efficiency and reasoning stability.
\\ ( https://arxiv.org/abs/2601.12842 ,  542kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12856
Date: Mon, 19 Jan 2026 09:10:50 GMT   (1434kb)

Title: Mining Citywide Dengue Spread Patterns in Singapore Through Hotspot
  Dynamics from Open Web Data
Authors: Liping Huang, Gaoxi Xiao, Stefan Ma, Hechang Chen, Shisong Tang, Flora
  Salim
Categories: cs.AI cs.LG
Comments: 9 pages, 9 figures. It's accepted by WWW 2026 Web4Good Track. To make
  accessible earlier, authors would like to put it on arxiv before the
  conference
MSC-class: 68T05
ACM-class: J.3; I.6.5; H.2.8; D.2.8
Journal-ref: WWW 2026, i.e., The Web Conference 2026
\\
  Dengue, a mosquito-borne disease, continues to pose a persistent public
health challenge in urban areas, particularly in tropical regions such as
Singapore. Effective and affordable control requires anticipating where
transmission risks are likely to emerge so that interventions can be deployed
proactively rather than reactively. This study introduces a novel framework
that uncovers and exploits latent transmission links between urban regions,
mined directly from publicly available dengue case data. Instead of treating
cases as isolated reports, we model how hotspot formation in one area is
influenced by epidemic dynamics in neighboring regions. While mosquito movement
is highly localized, long-distance transmission is often driven by human
mobility, and in our case study, the learned network aligns closely with
commuting flows, providing an interpretable explanation for citywide spread.
These hidden links are optimized through gradient descent and used not only to
forecast hotspot status but also to verify the consistency of spreading
patterns, by examining the stability of the inferred network across consecutive
weeks. Case studies on Singapore during 2013-2018 and 2020 show that four weeks
of hotspot history are sufficient to achieve an average F-score of 0.79.
Importantly, the learned transmission links align with commuting flows,
highlighting the interpretable interplay between hidden epidemic spread and
human mobility. By shifting from simply reporting dengue cases to mining and
validating hidden spreading dynamics, this work transforms open web-based case
data into a predictive and explanatory resource. The proposed framework
advances epidemic modeling while providing a scalable, low-cost tool for public
health planning, early intervention, and urban resilience.
\\ ( https://arxiv.org/abs/2601.12856 ,  1434kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12912
Date: Mon, 19 Jan 2026 10:06:21 GMT   (1109kb)

Title: Human Emotion Verification by Action Languages via Answer Set
  Programming
Authors: Andreas Br\"annstr\"om and Juan Carlos Nieves
Categories: cs.AI
Comments: Under consideration in Theory and Practice of Logic Programming
  (TPLP)
\\
  In this paper, we introduce the action language C-MT (Mind Transition
Language). It is built on top of answer set programming (ASP) and transition
systems to represent how human mental states evolve in response to sequences of
observable actions. Drawing on well-established psychological theories, such as
the Appraisal Theory of Emotion, we formalize mental states, such as emotions,
as multi-dimensional configurations. With the objective to address the need for
controlled agent behaviors and to restrict unwanted mental side-effects of
actions, we extend the language with a novel causal rule, forbids to cause,
along with expressions specialized for mental state dynamics, which enables the
modeling of principles for valid transitions between mental states. These
principles of mental change are translated into transition constraints, and
properties of invariance, which are rigorously evaluated using transition
systems in terms of so-called trajectories. This enables controlled reasoning
about the dynamic evolution of human mental states. Furthermore, the framework
supports the comparison of different dynamics of change by analyzing
trajectories that adhere to different psychological principles. We apply the
action language to design models for emotion verification. Under consideration
in Theory and Practice of Logic Programming (TPLP).
\\ ( https://arxiv.org/abs/2601.12912 ,  1109kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12913
Date: Mon, 19 Jan 2026 10:10:17 GMT   (1523kb)

Title: Actionable Interpretability Must Be Defined in Terms of Symmetries
Authors: Pietro Barbiero, Mateo Espinosa Zarlenga, Francesco Giannini, Alberto
  Termine, Filippo Bonchi, Mateja Jamnik, Giuseppe Marra
Categories: cs.AI cs.LG cs.NE
\\
  This paper argues that interpretability research in Artificial Intelligence
is fundamentally ill-posed as existing definitions of interpretability are not
*actionable*: they fail to provide formal principles from which concrete
modelling and inferential rules can be derived. We posit that for a definition
of interpretability to be actionable, it must be given in terms of
*symmetries*. We hypothesise that four symmetries suffice to (i) motivate core
interpretability properties, (ii) characterize the class of interpretable
models, and (iii) derive a unified formulation of interpretable inference
(e.g., alignment, interventions, and counterfactuals) as a form of Bayesian
inversion.
\\ ( https://arxiv.org/abs/2601.12913 ,  1523kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13060
Date: Mon, 19 Jan 2026 13:50:43 GMT   (9323kb)

Title: MagicGUI-RMS: A Multi-Agent Reward Model System for Self-Evolving GUI
  Agents via Automated Feedback Reflux
Authors: Zecheng Li, Zhihui Cao, Wenke Huang, Yudong Zhang, Keying Qi, Rui
  Wang, Zeyu Zheng, Jian Zhao, Hao Zhu, Hengxin Wu, Yuran Wang, Guitao Fan,
  Guokun Wu, Yicong Liu, Zhilin Gao, Haikun Xu, He Yang, Minqi Xiang, Xingyu
  Liu, Zuojian Wang
Categories: cs.AI
\\
  Graphical user interface (GUI) agents are rapidly progressing toward
autonomous interaction and reliable task execution across diverse applications.
However, two central challenges remain unresolved: automating the evaluation of
agent trajectories and generating high-quality training data at scale to enable
continual improvement. Existing approaches often depend on manual annotation or
static rule-based verification, which restricts scalability and limits
adaptability in dynamic environments. We present MagicGUI-RMS, a multi-agent
reward model system that delivers adaptive trajectory evaluation, corrective
feedback, and self-evolving learning capabilities. MagicGUI-RMS integrates a
Domain-Specific Reward Model (DS-RM) with a General-Purpose Reward Model
(GP-RM), enabling fine-grained action assessment and robust generalization
across heterogeneous GUI tasks. To support reward learning at scale, we design
a structured data construction pipeline that automatically produces balanced
and diverse reward datasets, effectively reducing annotation costs while
maintaining sample fidelity. During execution, the reward model system
identifies erroneous actions, proposes refined alternatives, and continuously
enhances agent behavior through an automated data-reflux mechanism. Extensive
experiments demonstrate that MagicGUI-RMS yields substantial gains in task
accuracy, behavioral robustness. These results establish MagicGUI-RMS as a
principled and effective foundation for building self-improving GUI agents
driven by reward-based adaptation.
\\ ( https://arxiv.org/abs/2601.13060 ,  9323kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13122
Date: Mon, 19 Jan 2026 15:10:59 GMT   (1178kb)

Title: Responsible AI for General-Purpose Systems: Overview, Challenges, and A
  Path Forward
Authors: Gourab K Patro, Himanshi Agrawal, Himanshu Gharat, Supriya Panigrahi,
  Nim Sherpa, Vishal Vaddina, Dagnachew Birru
Categories: cs.AI
\\
  Modern general-purpose AI systems made using large language and vision
models, are capable of performing a range of tasks like writing text articles,
generating and debugging codes, querying databases, and translating from one
language to another, which has made them quite popular across industries.
However, there are risks like hallucinations, toxicity, and stereotypes in
their output that make them untrustworthy. We review various risks and
vulnerabilities of modern general-purpose AI along eight widely accepted
responsible AI (RAI) principles (fairness, privacy, explainability, robustness,
safety, truthfulness, governance, and sustainability) and compare how they are
non-existent or less severe and easily mitigable in traditional task-specific
counterparts. We argue that this is due to the non-deterministically high
Degree of Freedom in output (DoFo) of general-purpose AI (unlike the
deterministically constant or low DoFo of traditional task-specific AI
systems), and there is a need to rethink our approach to RAI for
general-purpose AI. Following this, we derive C2V2 (Control, Consistency,
Value, Veracity) desiderata to meet the RAI requirements for future
general-purpose AI systems, and discuss how recent efforts in AI alignment,
retrieval-augmented generation, reasoning enhancements, etc. fare along one or
more of the desiderata. We believe that the goal of developing responsible
general-purpose AI can be achieved by formally modeling application- or
domain-dependent RAI requirements along C2V2 dimensions, and taking a system
design approach to suitably combine various techniques to meet the desiderata.
\\ ( https://arxiv.org/abs/2601.13122 ,  1178kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13186
Date: Mon, 19 Jan 2026 16:10:11 GMT   (4212kb)

Title: Prompt Injection Mitigation with Agentic AI, Nested Learning, and AI
  Sustainability via Semantic Caching
Authors: Diego Gosmar, Deborah A. Dahl
Categories: cs.AI cs.MA
Comments: 33 pages, 19 figures
\\
  Prompt injection remains a central obstacle to the safe deployment of large
language models, particularly in multi-agent settings where intermediate
outputs can propagate or amplify malicious instructions. Building on earlier
work that introduced a four-metric Total Injection Vulnerability Score (TIVS),
this paper extends the evaluation framework with semantic similarity-based
caching and a fifth metric (Observability Score Ratio) to yield TIVS-O,
investigating how defence effectiveness interacts with transparency in a
HOPE-inspired Nested Learning architecture. The proposed system combines an
agentic pipeline with Continuum Memory Systems that implement semantic
similarity-based caching across 301 synthetically generated injection-focused
prompts drawn from ten attack families, while a fourth agent performs
comprehensive security analysis using five key performance indicators. In
addition to traditional injection metrics, OSR quantifies the richness and
clarity of security-relevant reasoning exposed by each agent, enabling an
explicit analysis of trade-offs between strict mitigation and auditability.
Experiments show that the system achieves secure responses with zero high-risk
breaches, while semantic caching delivers substantial computational savings,
achieving a 41.6% reduction in LLM calls and corresponding decreases in
latency, energy consumption, and carbon emissions. Five TIVS-O configurations
reveal optimal trade-offs between mitigation strictness and forensic
transparency. These results indicate that observability-aware evaluation can
reveal non-monotonic effects within multi-agent pipelines and that
memory-augmented agents can jointly maximize security robustness, real-time
performance, operational cost savings, and environmental sustainability without
modifying underlying model weights, providing a production-ready pathway for
secure and green LLM deployments.
\\ ( https://arxiv.org/abs/2601.13186 ,  4212kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13206
Date: Mon, 19 Jan 2026 16:31:07 GMT   (1728kb)

Title: Real-Time Deadlines Reveal Temporal Awareness Failures in LLM Strategic
  Dialogues
Authors: Neil K. R. Sehgal, Sharath Chandra Guntuku, Lyle Ungar
Categories: cs.AI
\\
  Large Language Models (LLMs) generate text token-by-token in discrete time,
yet real-world communication, from therapy sessions to business negotiations,
critically depends on continuous time constraints. Current LLM architectures
and evaluation protocols rarely test for temporal awareness under real-time
deadlines. We use simulated negotiations between paired agents under strict
deadlines to investigate how LLMs adjust their behavior in time-sensitive
settings. In a control condition, agents know only the global time limit. In a
time-aware condition, they receive remaining-time updates at each turn. Deal
closure rates are substantially higher (32\% vs. 4\% for GPT-5.1) and offer
acceptances are sixfold higher in the time-aware condition than in the control,
suggesting LLMs struggle to internally track elapsed time. However, the same
LLMs achieve near-perfect deal closure rates ($\geq$95\%) under turn-based
limits, revealing the failure is in temporal tracking rather than strategic
reasoning. These effects replicate across negotiation scenarios and models,
illustrating a systematic lack of LLM time awareness that will constrain LLM
deployment in many time-sensitive applications.
\\ ( https://arxiv.org/abs/2601.13206 ,  1728kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13233
Date: Mon, 19 Jan 2026 17:06:12 GMT   (3204kb)

Title: RAG: A Random-Forest-Based Generative Design Framework for
  Uncertainty-Aware Design of Metamaterials with Complex Functional Response
  Requirements
Authors: Bolin Chen, Dex Doksoo Lee, Wei "Wayne'' Chen, Wei Chen
Categories: cs.AI cs.CE
\\
  Metamaterials design for advanced functionality often entails the inverse
design on nonlinear and condition-dependent responses (e.g., stress-strain
relation and dispersion relation), which are described by continuous functions.
Most existing design methods focus on vector-valued responses (e.g., Young's
modulus and bandgap width), while the inverse design of functional responses
remains challenging due to their high-dimensionality, the complexity of
accommodating design requirements in inverse-design frameworks, and
non-existence or non-uniqueness of feasible solutions. Although generative
design approaches have shown promise, they are often data-hungry, handle design
requirements heuristically, and may generate infeasible designs without
uncertainty quantification. To address these challenges, we introduce a
RAndom-forest-based Generative approach (RAG). By leveraging the small-data
compatibility of random forests, RAG enables data-efficient predictions of
high-dimensional functional responses. During the inverse design, the framework
estimates the likelihood through the ensemble which quantifies the
trustworthiness of generated designs while reflecting the relative difficulty
across different requirements. The one-to-many mapping is addressed through
single-shot design generation by sampling from the conditional likelihood. We
demonstrate RAG on: 1) acoustic metamaterials with prescribed partial
passbands/stopbands, and 2) mechanical metamaterials with targeted snap-through
responses, using 500 and 1057 samples, respectively. Its data-efficiency is
benchmarked against neural networks on a public mechanical metamaterial dataset
with nonlinear stress-strain relations. Our framework provides a lightweight,
trustworthy pathway to inverse design involving functional responses, expensive
simulations, and complex design requirements, beyond metamaterials.
\\ ( https://arxiv.org/abs/2601.13233 ,  3204kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13262
Date: Mon, 19 Jan 2026 17:51:00 GMT   (1038kb)

Title: CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual
  Medical Reasoning
Authors: Eric Onyame, Akash Ghosh, Subhadip Baidya, Sriparna Saha, Xiuying
  Chen, Chirag Agarwal
Categories: cs.AI cs.CL
\\
  While large language models (LLMs) have shown to perform well on monolingual
mathematical and commonsense reasoning, they remain unreliable for multilingual
medical reasoning applications, hindering their deployment in multilingual
healthcare settings. We address this by first introducing CUREMED-BENCH, a
high-quality multilingual medical reasoning dataset with open-ended reasoning
queries with a single verifiable answer, spanning thirteen languages, including
underrepresented languages such as Amharic, Yoruba, and Swahili. Building on
this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning
framework that integrates code-switching-aware supervised fine-tuning and Group
Relative Policy Optimization to jointly improve logical correctness and
language stability. Across thirteen languages, our approach consistently
outperforms strong baselines and scales effectively, achieving 85.21% language
consistency and 54.35% logical correctness at 7B parameters, and 94.96%
language consistency and 70.04% logical correctness at 32B parameters. These
results support reliable and equitable multilingual medical reasoning in LLMs.
The code and dataset are available at https://cure-med.github.io/
\\ ( https://arxiv.org/abs/2601.13262 ,  1038kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13268
Date: Mon, 19 Jan 2026 18:10:34 GMT   (282kb)

Title: Improving the Safety and Trustworthiness of Medical AI via Multi-Agent
  Evaluation Loops
Authors: Zainab Ghafoor, Md Shafiqul Islam, Koushik Howlader, Md Rasel
  Khondokar, Tanusree Bhattacharjee, Sayantan Chakraborty, Adrito Roy, Ushashi
  Bhattacharjee, and Tirtho Roy
Categories: cs.AI
\\
  Large Language Models (LLMs) are increasingly applied in healthcare, yet
ensuring their ethical integrity and safety compliance remains a major barrier
to clinical deployment. This work introduces a multi-agent refinement framework
designed to enhance the safety and reliability of medical LLMs through
structured, iterative alignment. Our system combines two generative models -
DeepSeek R1 and Med-PaLM - with two evaluation agents, LLaMA 3.1 and Phi-4,
which assess responses using the American Medical Association's (AMA)
Principles of Medical Ethics and a five-tier Safety Risk Assessment (SRA-5)
protocol. We evaluate performance across 900 clinically diverse queries
spanning nine ethical domains, measuring convergence efficiency, ethical
violation reduction, and domain-specific risk behavior. Results demonstrate
that DeepSeek R1 achieves faster convergence (mean 2.34 vs. 2.67 iterations),
while Med-PaLM shows superior handling of privacy-sensitive scenarios. The
iterative multi-agent loop achieved an 89% reduction in ethical violations and
a 92% risk downgrade rate, underscoring the effectiveness of our approach. This
study presents a scalable, regulator-aligned, and cost-efficient paradigm for
governing medical AI safety.
\\ ( https://arxiv.org/abs/2601.13268 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13327
Date: Mon, 19 Jan 2026 19:07:32 GMT   (2966kb)

Title: PepEDiff: Zero-Shot Peptide Binder Design via Protein Embedding
  Diffusion
Authors: Po-Yu Liang, Tobo Duran, Jun Bai
Categories: cs.AI
\\
  We present PepEDiff, a novel peptide binder generator that designs binding
sequences given a target receptor protein sequence and its pocket residues.
Peptide binder generation is critical in therapeutic and biochemical
applications, yet many existing methods rely heavily on intermediate structure
prediction, adding complexity and limiting sequence diversity. Our approach
departs from this paradigm by generating binder sequences directly in a
continuous latent space derived from a pretrained protein embedding model,
without relying on predicted structures, thereby improving structural and
sequence diversity. To encourage the model to capture binding-relevant features
rather than memorizing known sequences, we perform latent-space exploration and
diffusion-based sampling, enabling the generation of peptides beyond the
limited distribution of known binders. This zero-shot generative strategy
leverages the global protein embedding manifold as a semantic prior, allowing
the model to propose novel peptide sequences in previously unseen regions of
the protein space. We evaluate PepEDiff on TIGIT, a challenging target with a
large, flat protein-protein interaction interface that lacks a druggable
pocket. Despite its simplicity, our method outperforms state-of-the-art
approaches across benchmark tests and in the TIGIT case study, demonstrating
its potential as a general, structure-free framework for zero-shot peptide
binder design. The code for this research is available at GitHub:
https://github.com/LabJunBMI/PepEDiff-An-Peptide-binder-Embedding-Diffusion-Model
\\ ( https://arxiv.org/abs/2601.13327 ,  2966kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13358
Date: Mon, 19 Jan 2026 19:53:37 GMT   (723kb)

Title: The Geometry of Thought: How Scale Restructures Reasoning In Large
  Language Models
Authors: Samuel Cyrenius Anderson
Categories: cs.AI cs.LG
Comments: 34 pages, 10 figures
ACM-class: I.2.6; I.2.7
\\
  Scale does not uniformly improve reasoning - it restructures it. Analyzing
25,000+ chain-of-thought trajectories across four domains (Law, Science, Code,
Math) and two scales (8B, 70B parameters), we discover that neural scaling laws
trigger domain-specific phase transitions rather than uniform capability gains.
Legal reasoning undergoes Crystallization: 45% collapse in representational
dimensionality (d95: 501 -> 274), 31% increase in trajectory alignment, and 10x
manifold untangling. Scientific and mathematical reasoning remain Liquid -
geometrically invariant despite 9x parameter increase. Code reasoning forms a
discrete Lattice of strategic modes (silhouette: 0.13 -> 0.42). This geometry
predicts learnability. We introduce Neural Reasoning Operators - learned
mappings from initial to terminal hidden states. In crystalline legal
reasoning, our operator achieves 63.6% accuracy on held-out tasks via probe
decoding, predicting reasoning endpoints without traversing intermediate
states. We further identify a universal oscillatory signature (coherence ~
-0.4) invariant across domains and scales, suggesting attention and feedforward
layers drive reasoning through opposing dynamics. These findings establish that
the cost of thought is determined not by task difficulty but by manifold
geometry - offering a blueprint for inference acceleration where topology
permits.
\\ ( https://arxiv.org/abs/2601.13358 ,  723kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13383
Date: Mon, 19 Jan 2026 20:33:26 GMT   (63kb)

Title: A Lightweight Modular Framework for Constructing Autonomous Agents
  Driven by Large Language Models: Design, Implementation, and Applications in
  AgentForge
Authors: Akbar Anbar Jafari, Cagri Ozcinar, Gholamreza Anbarjafari
Categories: cs.AI
Comments: 15 pages, 3 figures
\\
  The emergence of LLMs has catalyzed a paradigm shift in autonomous agent
development, enabling systems capable of reasoning, planning, and executing
complex multi-step tasks. However, existing agent frameworks often suffer from
architectural rigidity, vendor lock-in, and prohibitive complexity that impedes
rapid prototyping and deployment. This paper presents AgentForge, a
lightweight, open-source Python framework designed to democratize the
construction of LLM-driven autonomous agents through a principled modular
architecture. AgentForge introduces three key innovations: (1) a composable
skill abstraction that enables fine-grained task decomposition with formally
defined input-output contracts, (2) a unified LLM backend interface supporting
seamless switching between cloud-based APIs and local inference engines, and
(3) a declarative YAML-based configuration system that separates agent logic
from implementation details. We formalize the skill composition mechanism as a
directed acyclic graph (DAG) and prove its expressiveness for representing
arbitrary sequential and parallel task workflows. Comprehensive experimental
evaluation across four benchmark scenarios demonstrates that AgentForge
achieves competitive task completion rates while reducing development time by
62% compared to LangChain and 78% compared to direct API integration. Latency
measurements confirm sub-100ms orchestration overhead, rendering the framework
suitable for real-time applications. The modular design facilitates extension:
we demonstrate the integration of six built-in skills and provide comprehensive
documentation for custom skill development. AgentForge addresses a critical gap
in the LLM agent ecosystem by providing researchers and practitioners with a
production-ready foundation for constructing, evaluating, and deploying
autonomous agents without sacrificing flexibility or performance.
\\ ( https://arxiv.org/abs/2601.13383 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13443
Date: Mon, 19 Jan 2026 23:00:14 GMT   (468kb)

Title: Explicit Cognitive Allocation: A Principle for Governed and Auditable
  Inference in Large Language Models
Authors: H\'ector Manuel Manzanilla-Granados, Zaira Navarrete-Cazales, Miriam
  Pescador-Rojas, Tonahtiu Ram\'irez-Romero
Categories: cs.AI
Comments: Preprint. This version corresponds to the initial public release of
  the CUA architecture and associated evaluation metrics
\\
  The rapid adoption of large language models (LLMs) has enabled new forms of
AI-assisted reasoning across scientific, technical, and organizational domains.
However, prevailing modes of LLM use remain cognitively unstructured: problem
framing, knowledge exploration, retrieval, methodological awareness, and
explanation are typically collapsed into a single generative process. This
cognitive collapse limits traceability, weakens epistemic control, and
undermines reproducibility, particularly in high-responsibility settings.
  We introduce Explicit Cognitive Allocation, a general principle for
structuring AI-assisted inference through the explicit separation and
orchestration of epistemic functions. We instantiate this principle in the
Cognitive Universal Agent (CUA), an architecture that organizes inference into
distinct stages of exploration and framing, epistemic anchoring, instrumental
and methodological mapping, and interpretive synthesis. Central to this
framework is the notion of Universal Cognitive Instruments (UCIs), which
formalize heterogeneous means, including computational, experimental,
organizational, regulatory, and educational instruments, through which abstract
inquiries become investigable.
  We evaluate the effects of explicit cognitive and instrumental allocation
through controlled comparisons between CUA-orchestrated inference and baseline
LLM inference under matched execution conditions. Across multiple prompts in
the agricultural domain, CUA inference exhibits earlier and structurally
governed epistemic convergence, higher epistemic alignment under semantic
expansion, and systematic exposure of the instrumental landscape of inquiry. In
contrast, baseline LLM inference shows greater variability in alignment and
fails to explicitly surface instrumental structure.
\\ ( https://arxiv.org/abs/2601.13443 ,  468kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13462
Date: Mon, 19 Jan 2026 23:37:10 GMT   (3641kb)

Title: SpatialBench-UC: Uncertainty-Aware Evaluation of Spatial Prompt
  Following in Text-to-Image Generation
Authors: Amine Rostane
Categories: cs.AI
Comments: 19 pages, includes figures and tables
\\
  Evaluating whether text-to-image models follow explicit spatial instructions
is difficult to automate. Object detectors may miss targets or return multiple
plausible detections, and simple geometric tests can become ambiguous in
borderline cases. Spatial evaluation is naturally a selective prediction
problem, the checker may abstain when evidence is weak and report confidence so
that results can be interpreted as a risk coverage tradeoff rather than a
single score. We introduce SpatialBench-UC, a small, reproducible benchmark for
pairwise spatial relations. The benchmark contains 200 prompts (50 object pairs
times 4 relations) grouped into 100 counterfactual pairs obtained by swapping
object roles. We release a benchmark package, versioned prompts, pinned
configs, per-sample checker outputs, and report tables, enabling reproducible
and auditable comparisons across models. We also include a lightweight human
audit used to calibrate the checker's abstention margin and confidence
threshold. We evaluate three baselines, Stable Diffusion 1.5, SD 1.5 BoxDiff,
and SD 1.4 GLIGEN. The checker reports pass rate and coverage as well as
conditional pass rates on decided samples. The results show that grounding
methods substantially improve both pass rate and coverage, while abstention
remains a dominant factor due mainly to missing detections.
\\ ( https://arxiv.org/abs/2601.13462 ,  3641kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13464
Date: Mon, 19 Jan 2026 23:40:05 GMT   (9605kb)

Title: Context and Transcripts Improve Detection of Deepfake Audios of Public
  Figures
Authors: Chongyang Gao, Marco Postiglione, Julian Baldwin, Natalia Denisenko,
  Isabel Gortner, Luke Fosdick, Chiara Pulice, Sarit Kraus, V.S. Subrahmanian
Categories: cs.AI cs.SD
\\
  Humans use context to assess the veracity of information. However, current
audio deepfake detectors only analyze the audio file without considering either
context or transcripts. We create and analyze a Journalist-provided Deepfake
Dataset (JDD) of 255 public deepfakes which were primarily contributed by over
70 journalists since early 2024. We also generate a synthetic audio dataset
(SYN) of dead public figures and propose a novel Context-based Audio Deepfake
Detector (CADD) architecture. In addition, we evaluate performance on two
large-scale datasets: ITW and P$^2$V. We show that sufficient context and/or
the transcript can significantly improve the efficacy of audio deepfake
detectors. Performance (measured via F1 score, AUC, and EER) of multiple
baseline audio deepfake detectors and traditional classifiers can be improved
by 5%-37.58% in F1-score, 3.77%-42.79% in AUC, and 6.17%-47.83% in EER. We
additionally show that CADD, via its use of context and/or transcripts, is more
robust to 5 adversarial evasion strategies, limiting performance degradation to
an average of just -0.71% across all experiments. Code, models, and datasets
are available at our project page:
https://sites.northwestern.edu/nsail/cadd-context-based-audio-deepfake-detection
(access restricted during review).
\\ ( https://arxiv.org/abs/2601.13464 ,  9605kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13465
Date: Mon, 19 Jan 2026 23:40:08 GMT   (902kb)

Title: Graph Neural Networks are Heuristics
Authors: Yimeng Min, Carla P. Gomes
Categories: cs.AI cs.LG
ACM-class: G.2.1; G.2.2; I.2.6; I.2.8
\\
  We demonstrate that a single training trajectory can transform a graph neural
network into an unsupervised heuristic for combinatorial optimization. Focusing
on the Travelling Salesman Problem, we show that encoding global structural
constraints as an inductive bias enables a non-autoregressive model to generate
solutions via direct forward passes, without search, supervision, or sequential
decision-making. At inference time, dropout and snapshot ensembling allow a
single model to act as an implicit ensemble, reducing optimality gaps through
increased solution diversity. Our results establish that graph neural networks
do not require supervised training nor explicit search to be effective.
Instead, they can internalize global combinatorial structure and function as
strong, learned heuristics. This reframes the role of learning in combinatorial
optimization: from augmenting classical algorithms to directly instantiating
new heuristics.
\\ ( https://arxiv.org/abs/2601.13465 ,  902kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13481
Date: Tue, 20 Jan 2026 00:31:19 GMT   (7376kb)

Title: Towards Efficient and Robust Linguistic Emotion Diagnosis for Mental
  Health via Multi-Agent Instruction Refinement
Authors: Jian Zhang, Zhangqi Wang, Zhiyuan Wang, Weiping Fu, Yu He, Haiping
  Zhu, Qika Lin, Jun Liu
Categories: cs.AI
\\
  Linguistic expressions of emotions such as depression, anxiety, and
trauma-related states are pervasive in clinical notes, counseling dialogues,
and online mental health communities, and accurate recognition of these
emotions is essential for clinical triage, risk assessment, and timely
intervention. Although large language models (LLMs) have demonstrated strong
generalization ability in emotion analysis tasks, their diagnostic reliability
in high-stakes, context-intensive medical settings remains highly sensitive to
prompt design. Moreover, existing methods face two key challenges: emotional
comorbidity, in which multiple intertwined emotional states complicate
prediction, and inefficient exploration of clinically relevant cues. To address
these challenges, we propose APOLO (Automated Prompt Optimization for
Linguistic Emotion Diagnosis), a framework that systematically explores a
broader and finer-grained prompt space to improve diagnostic efficiency and
robustness. APOLO formulates instruction refinement as a Partially Observable
Markov Decision Process and adopts a multi-agent collaboration mechanism
involving Planner, Teacher, Critic, Student, and Target roles. Within this
closed-loop framework, the Planner defines an optimization trajectory, while
the Teacher-Critic-Student agents iteratively refine prompts to enhance
reasoning stability and effectiveness, and the Target agent determines whether
to continue optimization based on performance evaluation. Experimental results
show that APOLO consistently improves diagnostic accuracy and robustness across
domain-specific and stratified benchmarks, demonstrating a scalable and
generalizable paradigm for trustworthy LLM applications in mental healthcare.
\\ ( https://arxiv.org/abs/2601.13481 ,  7376kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13518
Date: Tue, 20 Jan 2026 02:10:22 GMT   (2995kb)

Title: AgenticRed: Optimizing Agentic Systems for Automated Red-teaming
Authors: Jiayi Yuan, Jonathan N\"other, Natasha Jaques, Goran Radanovi\'c
Categories: cs.AI cs.NE
Comments: Website: https://yuanjiayiy.github.io/AgenticRed/
\\
  While recent automated red-teaming methods show promise for systematically
exposing model vulnerabilities, most existing approaches rely on
human-specified workflows. This dependence on manually designed workflows
suffers from human biases and makes exploring the broader design space
expensive. We introduce AgenticRed, an automated pipeline that leverages LLMs'
in-context learning to iteratively design and refine red-teaming systems
without human intervention. Rather than optimizing attacker policies within
predefined structures, AgenticRed treats red-teaming as a system design
problem. Inspired by methods like Meta Agent Search, we develop a novel
procedure for evolving agentic systems using evolutionary selection, and apply
it to the problem of automatic red-teaming. Red-teaming systems designed by
AgenticRed consistently outperform state-of-the-art approaches, achieving 96%
attack success rate (ASR) on Llama-2-7B (36% improvement) and 98% on Llama-3-8B
on HarmBench. Our approach exhibits strong transferability to proprietary
models, achieving 100% ASR on GPT-3.5-Turbo and GPT-4o-mini, and 60% on
Claude-Sonnet-3.5 (24% improvement). This work highlights automated system
design as a powerful paradigm for AI safety evaluation that can keep pace with
rapidly evolving models.
\\ ( https://arxiv.org/abs/2601.13518 ,  2995kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13533
Date: Tue, 20 Jan 2026 02:32:39 GMT   (632kb)

Title: Reasoning While Recommending: Entropy-Guided Latent Reasoning in
  Generative Re-ranking Models
Authors: Changshuo Zhang
Categories: cs.AI
\\
  Reinforcement learning plays a crucial role in generative re-ranking
scenarios due to its exploration-exploitation capabilities, but existing
generative methods mostly fail to adapt to the dynamic entropy changes in model
difficulty during list generation, making it challenging to accurately capture
complex preferences. Given that language models have achieved remarkable
breakthroughs by integrating reasoning capabilities, we draw on this approach
to introduce a latent reasoning mechanism, and experimental validation
demonstrates that this mechanism effectively reduces entropy in the model's
decision-making process. Based on these findings, we introduce the
Entropy-Guided Latent Reasoning (EGLR) recommendation model, which has three
core advantages. First, it abandons the "reason first, recommend later"
paradigm to achieve "reasoning while recommending", specifically designed for
the high-difficulty nature of list generation by enabling real-time reasoning
during generation. Second, it implements entropy-guided variable-length
reasoning using context-aware reasoning token alongside dynamic temperature
adjustment, expanding exploration breadth in reasoning and boosting
exploitation precision in recommending to achieve a more precisely adapted
exploration-exploitation trade-off. Third, the model adopts a lightweight
integration design with no complex independent modules or post-processing,
enabling easy adaptation to existing models. Experimental results on two
real-world datasets validate the model's effectiveness, and its notable
advantage lies in being compatible with existing generative re-ranking models
to enhance their performance. Further analyses also demonstrate its practical
deployment value and research potential.
\\ ( https://arxiv.org/abs/2601.13533 ,  632kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13545
Date: Tue, 20 Jan 2026 03:11:47 GMT   (600kb)

Title: TruthTensor: Evaluating LLMs Human Imitation through Prediction Market
  Drift and Holistic Reasoning
Authors: Shirin Shahabi and Spencer Graham and Haruna Isah
Categories: cs.AI cs.ET cs.MA
Comments: 16 pages, 6 figures, 2 tables
\\
  Evaluating language models and AI agents remains fundamentally challenging
because static benchmarks fail to capture real-world uncertainty, distribution
shift, and the gap between isolated task accuracy and human-aligned
decision-making under evolving conditions. This paper introduces TruthTensor, a
novel, reproducible evaluation paradigm that measures Large Language Models
(LLMs) not only as prediction engines but as human-imitation systems operating
in socially-grounded, high-entropy environments. Building on forward-looking,
contamination-free tasks, our framework anchors evaluation to live prediction
markets and combines probabilistic scoring to provide a holistic view of model
behavior. TruthTensor complements traditional correctness metrics with
drift-centric diagnostics and explicit robustness checks for reproducibility.
It specify human vs. automated evaluation roles, annotation protocols, and
statistical testing procedures to ensure interpretability and replicability of
results. In experiments across 500+ real markets (political, economic,
cultural, technological), TruthTensor demonstrates that models with similar
forecast accuracy can diverge markedly in calibration, drift, and
risk-sensitivity, underscoring the need to evaluate models along multiple axes
(accuracy, calibration, narrative stability, cost, and resource efficiency).
TruthTensor therefore operationalizes modern evaluation best practices, clear
hypothesis framing, careful metric selection, transparent compute/cost
reporting, human-in-the-loop validation, and open, versioned evaluation
contracts, to produce defensible assessments of LLMs in real-world decision
contexts. We publicly release TruthTensor at https://truthtensor.com
\\ ( https://arxiv.org/abs/2601.13545 ,  600kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13546
Date: Tue, 20 Jan 2026 03:12:37 GMT   (1870kb)

Title: ChatAD: Reasoning-Enhanced Time-Series Anomaly Detection with Multi-Turn
  Instruction Evolution
Authors: Hui Sun, Chang Xu, Haonan Xie, Hao Li, Yuhao Huang, Chuheng Zhang,
  Ming Jin, Xiaoguang Liu, Gang Wang, Jiang Bian
Categories: cs.AI
\\
  LLM-driven Anomaly Detection (AD) helps enhance the understanding and
explanatory abilities of anomalous behaviors in Time Series (TS). Existing
methods face challenges of inadequate reasoning ability, deficient multi-turn
dialogue capability, and narrow generalization. To this end, we 1) propose a
multi-agent-based TS Evolution algorithm named TSEvol. On top of it, we 2)
introduce the AD reasoning and multi-turn dialogue Dataset TSEData-20K and
contribute the Chatbot family for AD, including ChatAD-Llama3-8B, Qwen2.5-7B,
and Mistral-7B. Furthermore, 3) we propose the TS Kahneman-Tversky Optimization
(TKTO) to enhance ChatAD's cross-task generalization capability. Lastly, 4) we
propose a LLM-driven Learning-based AD Benchmark LLADBench to evaluate the
performance of ChatAD and nine baselines across seven datasets and tasks. Our
three ChatAD models achieve substantial gains, up to 34.50% in accuracy, 34.71%
in F1, and a 37.42% reduction in false positives. Besides, via KTKO, our
optimized ChatAD achieves competitive performance in reasoning and cross-task
generalization on classification, forecasting, and imputation.
\\ ( https://arxiv.org/abs/2601.13546 ,  1870kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13558
Date: Tue, 20 Jan 2026 03:28:50 GMT   (93kb)

Title: Leveraging ChatGPT and Other NLP Methods for Identifying Risk and
  Protective Behaviors in MSM: Social Media and Dating apps Text Analysis
Authors: Mehrab Beikzadeh, Chenglin Hong, Cory J Cascalheira, Callisto Boka,
  Majid Sarrafzadeh, Ian W Holloway
Categories: cs.AI cs.CL
\\
  Men who have sex with men (MSM) are at elevated risk for sexually transmitted
infections and harmful drinking compared to heterosexual men. Text data
collected from social media and dating applications may provide new
opportunities for personalized public health interventions by enabling
automatic identification of risk and protective behaviors. In this study, we
evaluated whether text from social media and dating apps can be used to predict
sexual risk behaviors, alcohol use, and pre-exposure prophylaxis (PrEP) uptake
among MSM. With participant consent, we collected textual data and trained
machine learning models using features derived from ChatGPT embeddings, BERT
embeddings, LIWC, and a dictionary-based risk term approach. The models
achieved strong performance in predicting monthly binge drinking and having
more than five sexual partners, with F1 scores of 0.78, and moderate
performance in predicting PrEP use and heavy drinking, with F1 scores of 0.64
and 0.63. These findings demonstrate that social media and dating app text data
can provide valuable insights into risk and protective behaviors and highlight
the potential of large language model-based methods to support scalable and
personalized public health interventions for MSM.
\\ ( https://arxiv.org/abs/2601.13558 ,  93kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13559
Date: Tue, 20 Jan 2026 03:29:45 GMT   (966kb)

Title: AgentGC: Evolutionary Learning-based Lossless Compression for Genomics
  Data with LLM-driven Multiple Agent
Authors: Sun Hui, Ding Yanfeng, Huidong Ma, Chang Xu, Keyan Jin, Lizheng Zu,
  Cheng Zhong, xiaoguang Liu, Gang Wang, Wentong Cai
Categories: cs.AI
\\
  Lossless compression has made significant advancements in Genomics Data (GD)
storage, sharing and management. Current learning-based methods are
non-evolvable with problems of low-level compression modeling, limited
adaptability, and user-unfriendly interface. To this end, we propose AgentGC,
the first evolutionary Agent-based GD Compressor, consisting of 3 layers with
multi-agent named Leader and Worker. Specifically, the 1) User layer provides a
user-friendly interface via Leader combined with LLM; 2) Cognitive layer,
driven by the Leader, integrates LLM to consider joint optimization of
algorithm-dataset-system, addressing the issues of low-level modeling and
limited adaptability; and 3) Compression layer, headed by Worker, performs
compression & decompression via a automated multi-knowledge learning-based
compression framework. On top of AgentGC, we design 3 modes to support diverse
scenarios: CP for compression-ratio priority, TP for throughput priority, and
BM for balanced mode. Compared with 14 baselines on 9 datasets, the average
compression ratios gains are 16.66%, 16.11%, and 16.33%, the throughput gains
are 4.73x, 9.23x, and 9.15x, respectively.
\\ ( https://arxiv.org/abs/2601.13559 ,  966kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13562
Date: Tue, 20 Jan 2026 03:37:17 GMT   (1813kb)

Title: Reasoning is a Modality
Authors: Zhiguang Liu, Yi Shang
Categories: cs.AI cs.CV cs.LG
Comments: Code access: https://github.com/lz7fd/Reasoning_is_a_Modality
ACM-class: I.2.6; I.2.10
\\
  The Abstraction and Reasoning Corpus (ARC) provides a compact laboratory for
studying abstract reasoning, an ability central to human intelligence. Modern
AI systems, including LLMs and ViTs, largely operate as sequence-of-behavior
prediction machines: they match observable behaviors by modeling token
statistics without a persistent, readable mental state. This creates a gap with
human-like behavior: humans can explain an action by decoding internal state,
while AI systems can produce fluent post-hoc rationalizations that are not
grounded in such a state. We hypothesize that reasoning is a modality:
reasoning should exist as a distinct channel separate from the low-level
workspace on which rules are applied. To test this hypothesis, on solving ARC
tasks as a visual reasoning problem, we designed a novel role-separated
transformer block that splits global controller tokens from grid workspace
tokens, enabling iterative rule execution. Trained and evaluated within the
VARC vision-centric protocol, our method achieved 62.6% accuracy on ARC-1,
surpassing average human performance (60.2%) and outperforming prior methods
significantly. Qualitatively, our models exhibit more coherent rule-application
structure than the dense ViT baseline, consistent with a shift away from
plausible probability blobs toward controller-driven reasoning.
\\ ( https://arxiv.org/abs/2601.13562 ,  1813kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13581
Date: Tue, 20 Jan 2026 04:11:00 GMT   (4829kb)

Title: SCRIPTMIND: Crime Script Inference and Cognitive Evaluation for
  LLM-based Social Engineering Scam Detection System
Authors: Heedou Kim, Changsik Kim, Sanghwa Shin, Jaewoo Kang
Categories: cs.AI
Comments: This paper has been accepted to the EACL 2026 Industry Track
\\
  Social engineering scams increasingly employ personalized, multi-turn
deception, exposing the limits of traditional detection methods. While Large
Language Models (LLMs) show promise in identifying deception, their cognitive
assistance potential remains underexplored. We propose ScriptMind, an
integrated framework for LLM-based scam detection that bridges automated
reasoning and human cognition. It comprises three components: the Crime Script
Inference Task (CSIT) for scam reasoning, the Crime Script-Aware Inference
Dataset (CSID) for fine-tuning small LLMs, and the Cognitive Simulation-based
Evaluation of Social Engineering Defense (CSED) for assessing real-time
cognitive impact. Using 571 Korean phone scam cases, we built 22,712 structured
scammer-sequence training instances. Experimental results show that the 11B
small LLM fine-tuned with ScriptMind outperformed GPT-4o by 13%, achieving
superior performance over commercial models in detection accuracy,
false-positive reduction, scammer utterance prediction, and rationale quality.
Moreover, in phone scam simulation experiments, it significantly enhanced and
sustained users' suspicion levels, improving their cognitive awareness of
scams. ScriptMind represents a step toward human-centered, cognitively adaptive
LLMs for scam defense.
\\ ( https://arxiv.org/abs/2601.13581 ,  4829kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13589
Date: Tue, 20 Jan 2026 04:42:03 GMT   (1063kb)

Title: Motion-to-Response Content Generation via Multi-Agent AI System with
  Real-Time Safety Verification
Authors: HyeYoung Lee
Categories: cs.AI cs.SD
\\
  This paper proposes a multi-agent artificial intelligence system that
generates response-oriented media content in real time based on audio-derived
emotional signals. Unlike conventional speech emotion recognition studies that
focus primarily on classification accuracy, our approach emphasizes the
transformation of inferred emotional states into safe, age-appropriate, and
controllable response content through a structured pipeline of specialized AI
agents. The proposed system comprises four cooperative agents: (1) an Emotion
Recognition Agent with CNN-based acoustic feature extraction, (2) a Response
Policy Decision Agent for mapping emotions to response modes, (3) a Content
Parameter Generation Agent for producing media control parameters, and (4) a
Safety Verification Agent enforcing age-appropriateness and stimulation
constraints. We introduce an explicit safety verification loop that filters
generated content before output, ensuring compliance with predefined rules.
Experimental results on public datasets demonstrate that the system achieves
73.2% emotion recognition accuracy, 89.4% response mode consistency, and 100%
safety compliance while maintaining sub-100ms inference latency suitable for
on-device deployment. The modular architecture enables interpretability and
extensibility, making it applicable to child-adjacent media, therapeutic
applications, and emotionally responsive smart devices.
\\ ( https://arxiv.org/abs/2601.13589 ,  1063kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13591
Date: Tue, 20 Jan 2026 04:44:36 GMT   (7363kb)

Title: DSAEval: Evaluating Data Science Agents on a Wide Range of Real-World
  Data Science Problems
Authors: Maojun Sun, Yifei Xie, Yue Wu, Ruijian Han, Binyan Jiang, Defeng Sun,
  Yancheng Yuan, Jian Huang
Categories: cs.AI cs.CL
\\
  Recent LLM-based data agents aim to automate data science tasks ranging from
data analysis to deep learning. However, the open-ended nature of real-world
data science problems, which often span multiple taxonomies and lack standard
answers, poses a significant challenge for evaluation. To address this, we
introduce DSAEval, a benchmark comprising 641 real-world data science problems
grounded in 285 diverse datasets, covering both structured and unstructured
data (e.g., vision and text). DSAEval incorporates three distinctive features:
(1) Multimodal Environment Perception, which enables agents to interpret
observations from multiple modalities including text and vision; (2)
Multi-Query Interactions, which mirror the iterative and cumulative nature of
real-world data science projects; and (3) Multi-Dimensional Evaluation, which
provides a holistic assessment across reasoning, code, and results. We
systematically evaluate 11 advanced agentic LLMs using DSAEval. Our results
show that Claude-Sonnet-4.5 achieves the strongest overall performance, GPT-5.2
is the most efficient, and MiMo-V2-Flash is the most cost-effective. We further
demonstrate that multimodal perception consistently improves performance on
vision-related tasks, with gains ranging from 2.04% to 11.30%. Overall, while
current data science agents perform well on structured data and routine data
anlysis workflows, substantial challenges remain in unstructured domains.
Finally, we offer critical insights and outline future research directions to
advance the development of data science agents.
\\ ( https://arxiv.org/abs/2601.13591 ,  7363kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13600
Date: Tue, 20 Jan 2026 05:02:35 GMT   (82kb)

Title: Foundations of Global Consistency Checking with Noisy LLM Oracles
Authors: Paul He, Elke Kirschbaum, Shiva Kasiviswanathan
Categories: cs.AI
Comments: Under Review
\\
  Ensuring that collections of natural-language facts are globally consistent
is essential for tasks such as fact-checking, summarization, and knowledge base
construction. While Large Language Models (LLMs) can assess the consistency of
small subsets of facts, their judgments are noisy, and pairwise checks are
insufficient to guarantee global coherence. We formalize this problem and show
that verifying global consistency requires exponentially many oracle queries in
the worst case. To make the task practical, we propose an adaptive
divide-and-conquer algorithm that identifies minimal inconsistent subsets
(MUSes) of facts and optionally computes minimal repairs through hitting-sets.
Our approach has low-degree polynomial query complexity. Experiments with both
synthetic and real LLM oracles show that our method efficiently detects and
localizes inconsistencies, offering a scalable framework for linguistic
consistency verification with LLM-based evaluators.
\\ ( https://arxiv.org/abs/2601.13600 ,  82kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13632
Date: Tue, 20 Jan 2026 06:06:35 GMT   (541kb)

Title: Resilient Routing: Risk-Aware Dynamic Routing in Smart Logistics via
  Spatiotemporal Graph Learning
Authors: Zhiming Xue, Sichen Zhao, Yalun Qi, Xianling Zeng, Zihan Yu
Categories: cs.AI
\\
  With the rapid development of the e-commerce industry, the logistics network
is experiencing unprecedented pressure. The traditional static routing strategy
most time cannot tolerate the traffic congestion and fluctuating retail demand.
In this paper, we propose a Risk-Aware Dynamic Routing(RADR) framework which
integrates Spatiotemporal Graph Neural Networks (ST-GNN) with combinatorial
optimization. We first construct a logistics topology graph by using the
discrete GPS data using spatial clustering methods. Subsequently, a hybrid deep
learning model combining Graph Convolutional Network (GCN) and Gated Recurrent
Unit (GRU) is adopted to extract spatial correlations and temporal dependencies
for predicting future congestion risks. These prediction results are then
integrated into a dynamic edge weight mechanism to perform path planning. We
evaluated the framework on the Smart Logistics Dataset 2024, which contains
real-world Internet of Things(IoT) sensor data. The experimental results show
that the RADR algorithm significantly enhances the resilience of the supply
chain. Particularly in the case study of high congestion scenarios, our method
reduces the potential congestion risk exposure by 19.3% while only increasing
the transportation distance by 2.1%. This empirical evidence confirms that the
proposed data-driven approach can effectively balance delivery efficiency and
operational safety.
\\ ( https://arxiv.org/abs/2601.13632 ,  541kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13687
Date: Tue, 20 Jan 2026 07:41:26 GMT   (2531kb)

Title: Understanding Mental States to Guide Social Influence in Multi-Person
  Group Dialogue
Authors: Zhichao Liang, Satoshi Nakamura
Categories: cs.AI
\\
  Existing dynamic Theory of Mind (ToM) benchmarks mostly place language models
in a passive role: the model reads a sequence of connected scenarios and
reports what people believe, feel, intend, and do as these states change. In
real social interaction, ToM is also used for action: a speaker plans what to
say in order to shift another person's mental-state trajectory toward a goal.
We introduce SocialMindChange, a benchmark that moves from tracking minds to
changing minds in social interaction. Each instance defines a social context
with 4 characters and five connected scenes. The model plays one character and
generates dialogue across the five scenes to reach the target while remaining
consistent with the evolving states of all participants. SocialMindChange also
includes selected higher-order states. Using a structured four-step framework,
we construct 1,200 social contexts, covering 6000 scenarios and over 90,000
questions, each validated for realism and quality. Evaluations on ten
state-of-the-art LLMs show that their average performance is 54.2% below human
performance. This gap suggests that current LLMs still struggle to maintain and
change mental-state representations across long, linked interactions.
\\ ( https://arxiv.org/abs/2601.13687 ,  2531kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13709
Date: Tue, 20 Jan 2026 08:07:21 GMT   (323kb)

Title: Hidden in Plain Text: Measuring LLM Deception Quality Against Human
  Baselines Using Social Deduction Games
Authors: Christopher Kao, Vanshika Vats, James Davis
Categories: cs.AI cs.CL cs.CY cs.HC cs.SI
Comments: For associated dataset, see https://github.com/cocochief4/llm-mafia.
  Published in IEEE ICA 2025, waiting for IEEEXplore proceedings
\\
  Large Language Model (LLM) agents are increasingly used in many applications,
raising concerns about their safety. While previous work has shown that LLMs
can deceive in controlled tasks, less is known about their ability to deceive
using natural language in social contexts. In this paper, we study deception in
the Social Deduction Game (SDG) Mafia, where success is dependent on deceiving
others through conversation. Unlike previous SDG studies, we use an
asynchronous multi-agent framework which better simulates realistic social
contexts. We simulate 35 Mafia games with GPT-4o LLM agents. We then create a
Mafia Detector using GPT-4-Turbo to analyze game transcripts without player
role information to predict the mafia players. We use prediction accuracy as a
surrogate marker for deception quality. We compare this prediction accuracy to
that of 28 human games and a random baseline. Results show that the Mafia
Detector's mafia prediction accuracy is lower on LLM games than on human games.
The result is consistent regardless of the game days and the number of mafias
detected. This indicates that LLMs blend in better and thus deceive more
effectively. We also release a dataset of LLM Mafia transcripts to support
future research. Our findings underscore both the sophistication and risks of
LLM deception in social contexts.
\\ ( https://arxiv.org/abs/2601.13709 ,  323kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13735
Date: Tue, 20 Jan 2026 08:46:33 GMT   (3632kb)

Title: Reasoning or Fluency? Dissecting Probabilistic Confidence in Best-of-N
  Selection
Authors: Hojin Kim and Jaehyung Kim
Categories: cs.AI
Comments: 15 pages, 4 figures
\\
  Probabilistic confidence metrics are increasingly adopted as proxies for
reasoning quality in Best-of-N selection, under the assumption that higher
confidence reflects higher reasoning fidelity. In this work, we challenge this
assumption by investigating whether these metrics truly capture inter-step
causal dependencies necessary for valid reasoning. We introduce three classes
of inter-step causality perturbations that systematically disrupt dependencies
between reasoning steps while preserving local fluency. Surprisingly, across
diverse model families and reasoning benchmarks, we find that selection
accuracy degrades only marginally under these disruptions. Even severe
interventions, such as applying hard attention masks that directly prevent the
model from attending to prior reasoning steps, do not substantially reduce
selection performance. These findings provide strong evidence that current
probabilistic metrics are largely insensitive to logical structure, and
primarily capture surface-level fluency or in-distribution priors instead.
Motivated by this gap, we propose a contrastive causality metric that
explicitly isolates inter-step causal dependencies, and demonstrate that it
yields more faithful output selection than existing probability-based
approaches.
\\ ( https://arxiv.org/abs/2601.13735 ,  3632kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13752
Date: Tue, 20 Jan 2026 09:07:01 GMT   (3457kb)

Title: Finding RELIEF: Shaping Reasoning Behavior without Reasoning Supervision
  via Belief Engineering
Authors: Chak Tou Leong, Dingwei Chen, Heming Xia, Qingyu Yin, Sunbowen Lee,
  Jian Wang, Wenjie Li
Categories: cs.AI cs.CL
Comments: Working in progress
\\
  Large reasoning models (LRMs) have achieved remarkable success in complex
problem-solving, yet they often suffer from computational redundancy or
reasoning unfaithfulness. Current methods for shaping LRM behavior typically
rely on reinforcement learning or fine-tuning with gold-standard reasoning
traces, a paradigm that is both computationally expensive and difficult to
scale. In this paper, we reveal that LRMs possess latent \textit{reasoning
beliefs} that internally track their own reasoning traits, which can be
captured through simple logit probing. Building upon this insight, we propose
Reasoning Belief Engineering (RELIEF), a simple yet effective framework that
shapes LRM behavior by aligning the model's self-concept with a target belief
blueprint. Crucially, RELIEF completely bypasses the need for reasoning-trace
supervision. It internalizes desired traits by fine-tuning on synthesized,
self-reflective question-answering pairs that affirm the target belief.
Extensive experiments on efficiency and faithfulness tasks demonstrate that
RELIEF matches or outperforms behavior-supervised and preference-based
baselines while requiring lower training costs. Further analysis validates that
shifting a model's reasoning belief effectively shapes its actual behavior.
\\ ( https://arxiv.org/abs/2601.13752 ,  3457kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13761
Date: Tue, 20 Jan 2026 09:12:27 GMT   (3961kb)

Title: DARC: Decoupled Asymmetric Reasoning Curriculum for LLM Evolution
Authors: Shengda Fan, Xuyan Ye, Yankai Lin
Categories: cs.AI cs.CL
\\
  Self-play with large language models has emerged as a promising paradigm for
achieving self-improving artificial intelligence. However, existing self-play
frameworks often suffer from optimization instability, due to (i)
non-stationary objectives induced by solver-dependent reward feedback for the
Questioner, and (ii) bootstrapping errors from self-generated pseudo-labels
used to supervise the Solver. To mitigate these challenges, we introduce DARC
(Decoupled Asymmetric Reasoning Curriculum), a two-stage framework that
stabilizes the self-evolution process. First, we train the Questioner to
synthesize difficulty-calibrated questions, conditioned on explicit difficulty
levels and external corpora. Second, we train the Solver with an asymmetric
self-distillation mechanism, where a document-augmented teacher generates
high-quality pseudo-labels to supervise the student Solver that lacks document
access. Empirical results demonstrate that DARC is model-agnostic, yielding an
average improvement of 10.9 points across nine reasoning benchmarks and three
backbone models. Moreover, DARC consistently outperforms all baselines and
approaches the performance of fully supervised models without relying on human
annotations.The code is available at https://github.com/RUCBM/DARC.
\\ ( https://arxiv.org/abs/2601.13761 ,  3961kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13770
Date: Tue, 20 Jan 2026 09:23:51 GMT   (27kb)

Title: Look-Ahead-Bench: a Standardized Benchmark of Look-ahead Bias in
  Point-in-Time LLMs for Finance
Authors: Mostapha Benhenda (LAGA)
Categories: cs.AI cs.CL cs.LG q-fin.CP q-fin.GN
\\
  We introduce Look-Ahead-Bench, a standardized benchmark measuring look-ahead
bias in Point-in-Time (PiT) Large Language Models (LLMs) within realistic and
practical financial workflows. Unlike most existing approaches that primarily
test inner lookahead knowledge via Q\\&A, our benchmark evaluates model
behavior in practical scenarios. To distinguish genuine predictive capability
from memorization-based performance, we analyze performance decay across
temporally distinct market regimes, incorporating several quantitative
baselines to establish performance thresholds. We evaluate prominent
open-source LLMs -- Llama 3.1 (8B and 70B) and DeepSeek 3.2 -- against a family
of Point-in-Time LLMs (Pitinf-Small, Pitinf-Medium, and frontier-level model
Pitinf-Large) from PiT-Inference. Results reveal significant lookahead bias in
standard LLMs, as measured with alpha decay, unlike Pitinf models, which
demonstrate improved generalization and reasoning abilities as they scale in
size. This work establishes a foundation for the standardized evaluation of
temporal bias in financial LLMs and provides a practical framework for
identifying models suitable for real-world deployment. Code is available on
GitHub: https://github.com/benstaf/lookaheadbench
\\ ( https://arxiv.org/abs/2601.13770 ,  27kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13846
Date: Tue, 20 Jan 2026 10:59:44 GMT   (28890kb)

Title: Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity.
  A Tokyo-Based Pilot Study Using Diffusion-Generated Synthetic Environments
Authors: Glinskaya Maria
Categories: cs.AI cs.CY cs.LG
\\
  This paper introduces Virtual Urbanism (VU), a multimodal AI-driven
analytical framework for quantifying urban identity through the medium of
synthetic urban replicas. The framework aims to advance computationally
tractable urban identity metrics. To demonstrate feasibility, the pilot study
Virtual Urbanism and Tokyo Microcosms is presented. A pipeline integrating
Stable Diffusion and LoRA models was used to produce synthetic replicas of nine
Tokyo areas rendered as dynamic synthetic urban sequences, excluding existing
orientation markers to elicit core identity-forming elements. Human-evaluation
experiments (I) assessed perceptual legitimacy of replicas; (II) quantified
area-level identity; (III) derived core identity-forming elements. Results
showed a mean identification accuracy of ~81%, confirming the validity of the
replicas. Urban Identity Level (UIL) metric enabled assessment of identity
levels across areas, while semantic analysis revealed culturally embedded
typologies as core identity-forming elements, positioning VU as a viable
framework for AI-augmented urban analysis, outlining a path toward automated,
multi-parameter identity metrics.
\\ ( https://arxiv.org/abs/2601.13846 ,  28890kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13880
Date: Tue, 20 Jan 2026 11:51:58 GMT   (1059kb)

Title: LifeAgentBench: A Multi-dimensional Benchmark and Agent for Personal
  Health Assistants in Digital Health
Authors: Ye Tian, Zihao Wang, Onat Gungor, Xiaoran Fan and Tajana Rosing
Categories: cs.AI
\\
  Personalized digital health support requires long-horizon, cross-dimensional
reasoning over heterogeneous lifestyle signals, and recent advances in mobile
sensing and large language models (LLMs) make such support increasingly
feasible. However, the capabilities of current LLMs in this setting remain
unclear due to the lack of systematic benchmarks. In this paper, we introduce
LifeAgentBench, a large-scale QA benchmark for long-horizon, cross-dimensional,
and multi-user lifestyle health reasoning, containing 22,573 questions spanning
from basic retrieval to complex reasoning. We release an extensible benchmark
construction pipeline and a standardized evaluation protocol to enable reliable
and scalable assessment of LLM-based health assistants. We then systematically
evaluate 11 leading LLMs on LifeAgentBench and identify key bottlenecks in
long-horizon aggregation and cross-dimensional reasoning. Motivated by these
findings, we propose LifeAgent as a strong baseline agent for health assistant
that integrates multi-step evidence retrieval with deterministic aggregation,
achieving significant improvements compared with two widely used baselines.
Case studies further demonstrate its potential in realistic daily-life
scenarios. The benchmark is publicly available at
https://anonymous.4open.science/r/LifeAgentBench-CE7B.
\\ ( https://arxiv.org/abs/2601.13880 ,  1059kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13887
Date: Tue, 20 Jan 2026 12:00:04 GMT   (34kb)

Title: Human Simulation Computation: A Human-Inspired Framework for Adaptive AI
  Systems
Authors: Hong Su
Categories: cs.AI
\\
  Large language models (LLMs) have demonstrated strong capabilities in
knowledge representation and reasoning based on textual data. However, their
reliance on language material alone limits their ability to adapt, verify
reasoning outcomes, and operate effectively in open and dynamic real-world
environments. In this paper, we propose Human Simulation Computation (HSC), a
human-inspired computational framework that models intelligence as a
continuous, closed-loop process involving thinking, action, learning,
reflection, and activity scheduling, collectively referred to as the internal
reasoning process. HSC emphasizes active participation both within the internal
reasoning process and in interactions with the environment, where actions are
used not only to achieve goals but also to automatically refine and improve
internal reasoning mechanisms without external intervention. Furthermore, HSC
incorporates commonly used human thinking strategies across all stages of the
internal reasoning process, such as main-feature-oriented reasoning, scope
expansion through action, and on-time learning driven by environmental
feedback. Through theoretical analysis, we argue that human simulation
strategies cannot be fully learned from language material alone, and that
human-like reasoning processes and action-grounded reasoning methods are
essential for robust adaptation and effective interaction with real-world
environments.
\\ ( https://arxiv.org/abs/2601.13887 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13904
Date: Tue, 20 Jan 2026 12:30:13 GMT   (3600kb)

Title: PREFAB: PREFerence-based Affective Modeling for Low-Budget
  Self-Annotation
Authors: Jaeyoung Moon, Youjin Choi, Yucheon Park, David Melhart, Georgios N.
  Yannakakis, Kyung-Joong Kim
Categories: cs.AI cs.HC
Comments: CHI '26 Accepted paper
\\
  Self-annotation is the gold standard for collecting affective state labels in
affective computing. Existing methods typically rely on full annotation,
requiring users to continuously label affective states across entire sessions.
While this process yields fine-grained data, it is time-consuming, cognitively
demanding, and prone to fatigue and errors. To address these issues, we present
PREFAB, a low-budget retrospective self-annotation method that targets
affective inflection regions rather than full annotation. Grounded in the
peak-end rule and ordinal representations of emotion, PREFAB employs a
preference-learning model to detect relative affective changes, directing
annotators to label only selected segments while interpolating the remainder of
the stimulus. We further introduce a preview mechanism that provides brief
contextual cues to assist annotation. We evaluate PREFAB through a technical
performance study and a 25-participant user study. Results show that PREFAB
outperforms baselines in modeling affective inflections while mitigating
workload (and conditionally mitigating temporal burden). Importantly PREFAB
improves annotator confidence without degrading annotation quality.
\\ ( https://arxiv.org/abs/2601.13904 ,  3600kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13969
Date: Tue, 20 Jan 2026 13:46:37 GMT   (1068kb)

Title: Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth
  Retrieval
Authors: Joaqu\'in Polonuer (1,2), Lucas Vittor (1), I\~naki Arango (1), Ayush
  Noori (1,3), David A. Clifton (3,4), Luciano Del Corro (5,6), Marinka Zitnik
  (1,7,8,9) ((1) Department of Biomedical Informatics, Harvard Medical School,
  Boston, MA, USA, (2) Departamento de Computaci\'on, FCEyN, Universidad de
  Buenos Aires, Buenos Aires, Argentina, (3) Department of Engineering Science,
  University of Oxford, Oxford, UK, (4) Oxford Suzhou Centre for Advanced
  Research, University of Oxford, Suzhou, Jiangsu, China, (5) ELIAS Lab,
  Departamento de Ingenier\'ia, Universidad de San Andr\'es, Victoria,
  Argentina, (6) Lumina Labs, Buenos Aires, Argentina, (7) Kempner Institute
  for the Study of Natural and Artificial Intelligence, Allston, MA, USA, (8)
  Broad Institute of MIT and Harvard, Cambridge, MA, USA, (9) Harvard Data
  Science Initiative, Cambridge, MA, USA)
Categories: cs.AI cs.IR cs.LG
\\
  Retrieving evidence for language model queries from knowledge graphs requires
balancing broad search across the graph with multi-hop traversal to follow
relational links. Similarity-based retrievers provide coverage but remain
shallow, whereas traversal-based methods rely on selecting seed nodes to start
exploration, which can fail when queries span multiple entities and relations.
We introduce ARK: Adaptive Retriever of Knowledge, an agentic KG retriever that
gives a language model control over this breadth-depth tradeoff using a
two-operation toolset: global lexical search over node descriptors and one-hop
neighborhood exploration that composes into multi-hop traversal. ARK alternates
between breadth-oriented discovery and depth-oriented expansion without
depending on a fragile seed selection, a pre-set hop depth, or requiring
retrieval training. ARK adapts tool use to queries, using global search for
language-heavy queries and neighborhood exploration for relation-heavy queries.
On STaRK, ARK reaches 59.1% average Hit@1 and 67.4 average MRR, improving
average Hit@1 by up to 31.4% and average MRR by up to 28.0% over
retrieval-based and agentic training-free methods. Finally, we distill ARK's
tool-use trajectories from a large teacher into an 8B model via label-free
imitation, improving Hit@1 by +7.0, +26.6, and +13.5 absolute points over the
base 8B model on AMAZON, MAG, and PRIME datasets, respectively, while retaining
up to 98.5% of the teacher's Hit@1 rate.
\\ ( https://arxiv.org/abs/2601.13969 ,  1068kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14027
Date: Tue, 20 Jan 2026 14:51:45 GMT   (217kb)

Title: Numina-Lean-Agent: An Open and General Agentic Reasoning System for
  Formal Mathematics
Authors: Junqi Liu, Zihao Zhou, Zekai Zhu, Marco Dos Santos, Weikun He, Jiawei
  Liu, Ran Wang, Yunzhou Xie, Junqiao Zhao, Qiufeng Wang, Lihong Zhi, Jia Li,
  Wenda Li
Categories: cs.AI
\\
  Agentic systems have recently become the dominant paradigm for formal theorem
proving, achieving strong performance by coordinating multiple models and
tools. However, existing approaches often rely on task-specific pipelines and
trained formal provers, limiting their flexibility and reproducibility. In this
paper, we propose the paradigm that directly uses a general coding agent as a
formal math reasoner. This paradigm is motivated by (1) A general coding agent
provides a natural interface for diverse reasoning tasks beyond proving, (2)
Performance can be improved by simply replacing the underlying base model,
without training, and (3) MCP enables flexible extension and autonomous calling
of specialized tools, avoiding complex design. Based on this paradigm, we
introduce Numina-Lean-Agent, which combines Claude Code with Numina-Lean-MCP to
enable autonomous interaction with Lean, retrieval of relevant theorems,
informal proving and auxiliary reasoning tools. Using Claude Opus 4.5 as the
base model, Numina-Lean-Agent solves all problems in Putnam 2025 (12 / 12),
matching the best closed-source system. Beyond benchmark evaluation, we further
demonstrate its generality by interacting with mathematicians to successfully
formalize the Brascamp-Lieb theorem. We release Numina-Lean-Agent and all
solutions at https://github.com/project-numina/numina-lean-agent.
\\ ( https://arxiv.org/abs/2601.14027 ,  217kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14096
Date: Tue, 20 Jan 2026 15:57:36 GMT   (10252kb)

Title: Remapping and navigation of an embedding space via error minimization: a
  fundamental organizational principle of cognition in natural and artificial
  systems
Authors: Benedikt Hartl, L\'eo Pio-Lopez, Chris Fields, Michael Levin
Categories: cs.AI
Comments: 41 pages, 5 figures
\\
  The emerging field of diverse intelligence seeks an integrated view of
problem-solving in agents of very different provenance, composition, and
substrates. From subcellular chemical networks to swarms of organisms, and
across evolved, engineered, and chimeric systems, it is hypothesized that
scale-invariant principles of decision-making can be discovered. We propose
that cognition in both natural and synthetic systems can be characterized and
understood by the interplay between two equally important invariants: (1) the
remapping of embedding spaces, and (2) the navigation within these spaces.
Biological collectives, from single cells to entire organisms (and beyond),
remap transcriptional, morphological, physiological, or 3D spaces to maintain
homeostasis and regenerate structure, while navigating these spaces through
distributed error correction. Modern Artificial Intelligence (AI) systems,
including transformers, diffusion models, and neural cellular automata enact
analogous processes by remapping data into latent embeddings and refining them
iteratively through contextualization. We argue that this dual principle -
remapping and navigation of embedding spaces via iterative error minimization -
constitutes a substrate-independent invariant of cognition. Recognizing this
shared mechanism not only illuminates deep parallels between living systems and
artificial models, but also provides a unifying framework for engineering
adaptive intelligence across scales.
\\ ( https://arxiv.org/abs/2601.14096 ,  10252kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14171
Date: Tue, 20 Jan 2026 17:23:51 GMT   (7256kb)

Title: Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response
  Assistance
Authors: Qianli Ma, Chang Guo, Zhiheng Tian, Siyu Wang, Jipeng Xiao, Yuanhao
  Yue, Zhipeng Zhang
Categories: cs.AI
\\
  Writing effective rebuttals is a high-stakes task that demands more than
linguistic fluency, as it requires precise alignment between reviewer intent
and manuscript details. Current solutions typically treat this as a
direct-to-text generation problem, suffering from hallucination, overlooked
critiques, and a lack of verifiable grounding. To address these limitations, we
introduce $\textbf{RebuttalAgent}$, the first multi-agents framework that
reframes rebuttal generation as an evidence-centric planning task. Our system
decomposes complex feedback into atomic concerns and dynamically constructs
hybrid contexts by synthesizing compressed summaries with high-fidelity text
while integrating an autonomous and on-demand external search module to resolve
concerns requiring outside literature. By generating an inspectable response
plan before drafting, $\textbf{RebuttalAgent}$ ensures that every argument is
explicitly anchored in internal or external evidence. We validate our approach
on the proposed $\textbf{RebuttalBench}$ and demonstrate that our pipeline
outperforms strong baselines in coverage, faithfulness, and strategic
coherence, offering a transparent and controllable assistant for the peer
review process. Code will be released.
\\ ( https://arxiv.org/abs/2601.14171 ,  7256kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14192
Date: Tue, 20 Jan 2026 17:51:56 GMT   (12190kb)

Title: Toward Efficient Agents: Memory, Tool learning, and Planning
Authors: Xiaofang Yang, Lijun Li, Heng Zhou, Tong Zhu, Xiaoye Qu, Yuchen Fan,
  Qianshan Wei, Rui Ye, Li Kang, Yiran Qin, Zhiqiang Kou, Daizong Liu, Qi Li,
  Ning Ding, Siheng Chen, Jing Shao
Categories: cs.AI cs.CL
Comments: 35 pages, 200 references
\\
  Recent years have witnessed increasing interest in extending large language
models into agentic systems. While the effectiveness of agents has continued to
improve, efficiency, which is crucial for real-world deployment, has often been
overlooked. This paper therefore investigates efficiency from three core
components of agents: memory, tool learning, and planning, considering costs
such as latency, tokens, steps, etc. Aimed at conducting comprehensive research
addressing the efficiency of the agentic system itself, we review a broad range
of recent approaches that differ in implementation yet frequently converge on
shared high-level principles including but not limited to bounding context via
compression and management, designing reinforcement learning rewards to
minimize tool invocation, and employing controlled search mechanisms to enhance
efficiency, which we discuss in detail. Accordingly, we characterize efficiency
in two complementary ways: comparing effectiveness under a fixed cost budget,
and comparing cost at a comparable level of effectiveness. This trade-off can
also be viewed through the Pareto frontier between effectiveness and cost. From
this perspective, we also examine efficiency oriented benchmarks by summarizing
evaluation protocols for these components and consolidating commonly reported
efficiency metrics from both benchmark and methodological studies. Moreover, we
discuss the key challenges and future directions, with the goal of providing
promising insights.
\\ ( https://arxiv.org/abs/2601.14192 ,  12190kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11556
Date: Tue, 16 Dec 2025 14:15:06 GMT   (338kb)

Title: CSyMR: Benchmarking Compositional Symbolic Muisc Reasoning With MIR Tool
  Integration
Authors: Boyang Wang, Yash Vishe, Xin Xu, Zachary Novack, Julian McAuley, Junda
  Wu
Categories: cs.LG cs.AI cs.CL cs.SD eess.AS
\\
  Large Language Models (LLMs) are leveraged in symbolic music reasoning, yet
existing benchmarks emphasize isolated knowledge or atomic analyses rather than
the integrative compositional reasoning needed to connect musical structures.
To address this, we present the Compositional Symbolic Music Reasoning
Benchmark (CSyMR-Bench), a curated multiple-choice dataset of 126 questions
derived from expert forums and professional examinations. Each item involves
combining several atomic analyses to arrive at the final answer. Furthermore,
we introduce a tool-augmented agent framework that leverages symbolic music
analysis tools from the music21 library to address the challenges posed by
CSyMR-Bench. Experiments validate that CSyMR-Bench poses a non-trivial
challenge across both community-sourced and exam-style questions, while our
tool-augmented agent consistently outperforms all baselines, achieving 5-7%
absolute accuracy gains.
\\ ( https://arxiv.org/abs/2601.11556 ,  338kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11568
Date: Sat, 27 Dec 2025 14:11:08 GMT   (54kb)

Title: AdaFRUGAL: Adaptive Memory-Efficient Training with Dynamic Control
Authors: Quang-Hung Bui and Anh Son Ta
Categories: cs.LG cs.AI cs.CL
\\
  Training Large Language Models (LLMs) is highly memory-intensive due to
optimizer state overhead. The FRUGAL framework mitigates this with gradient
splitting, but its static hyperparameters -- the subspace ratio ($\rho$) and
update frequency ($T$) -- require costly manual tuning, limiting adaptability.
We present AdaFRUGAL, which automates this process by introducing two dynamic
controls: (i) a linear decay for $\rho$ to progressively reduce memory, and
(ii) a loss-aware schedule for $T$ to lower computational overhead. Experiments
across large-scale pre-training (English C4, Vietnamese VietVault) and
fine-tuning (GLUE) demonstrate that AdaFRUGAL achieves a compelling trade-off.
It maintains competitive performance against AdamW and static FRUGAL while
significantly reducing both GPU memory and training time, offering a more
practical, autonomous solution for resource-constrained LLM training.
\\ ( https://arxiv.org/abs/2601.11568 ,  54kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11572
Date: Mon, 29 Dec 2025 15:01:43 GMT   (841kb)

Title: Discrete Semantic States and Hamiltonian Dynamics in LLM Embedding
  Spaces
Authors: Timo Aukusti Laine
Categories: cs.LG cs.AI
Comments: 23 pages, 5 figures
\\
  We investigate the structure of Large Language Model (LLM) embedding spaces
using mathematical concepts, particularly linear algebra and the Hamiltonian
formalism, drawing inspiration from analogies with quantum mechanical systems.
Motivated by the observation that LLM embeddings exhibit distinct states,
suggesting discrete semantic representations, we explore the application of
these mathematical tools to analyze semantic relationships. We demonstrate that
the L2 normalization constraint, a characteristic of many LLM architectures,
results in a structured embedding space suitable for analysis using a
Hamiltonian formalism. We derive relationships between cosine similarity and
perturbations of embedding vectors, and explore direct and indirect semantic
transitions. Furthermore, we explore a quantum-inspired perspective, deriving
an analogue of zero-point energy and discussing potential connections to
Koopman-von Neumann mechanics. While the interpretation warrants careful
consideration, our results suggest that this approach offers a promising avenue
for gaining deeper insights into LLMs and potentially informing new methods for
mitigating hallucinations.
\\ ( https://arxiv.org/abs/2601.11572 ,  841kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11574
Date: Tue, 30 Dec 2025 03:45:32 GMT   (1011kb)

Title: GRADE: Replacing Policy Gradients with Backpropagation for LLM Alignment
Authors: Lukas Abrie Nel
Categories: cs.LG cs.AI
\\
  Reinforcement learning from human feedback (RLHF) has become the dominant
paradigm for aligning large language models with human preferences. However,
policy gradient methods such as PPO suffer from high variance gradient
estimates, requiring careful hyperparameter tuning and extensive computational
resources. We introduce GRADE (Gumbel-softmax Relaxation for Alignment via
Differentiable Estimation), a method that replaces high-variance policy
gradient estimation with direct backpropagation through a differentiable
relaxation of the discrete token sampling process. Using the Gumbel-Softmax
reparameterization with straight-through estimation (GRADE-STE), we enable
end-to-end gradient flow from reward signals through generated tokens to model
parameters. On sentiment-controlled text generation using the IMDB dataset,
GRADE-STE achieves a test reward of 0.763 +- 0.344 compared to PPO's 0.510 +-
0.313 and REINFORCE's 0.617 +- 0.378, representing a 50% relative improvement
over PPO. Critically, GRADE-STE exhibits gradient variance over 14 times lower
than REINFORCE and maintains stable training dynamics throughout optimization.
Our rigorous evaluation with proper train/validation/test splits demonstrates
that these improvements generalize to held-out data, with GRADE-STE showing the
best generalization characteristics among all methods tested. GRADE offers a
simpler, more stable, and more effective alternative to reinforcement learning
for LLM alignment.
\\ ( https://arxiv.org/abs/2601.11574 ,  1011kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11604
Date: Thu, 8 Jan 2026 18:19:52 GMT   (2002kb)

Title: Hindsight Preference Replay Improves Preference-Conditioned
  Multi-Objective Reinforcement Learning
Authors: Jonaid Shianifar, Michael Schukat, Karl Mason
Categories: cs.LG cs.AI
\\
  Multi-objective reinforcement learning (MORL) enables agents to optimize
vector-valued rewards while respecting user preferences. CAPQL, a
preference-conditioned actor-critic method, achieves this by conditioning on
weight vectors w and restricts data usage to the specific preferences under
which it was collected, leaving off-policy data from other preferences unused.
We introduce Hindsight Preference Replay (HPR), a simple and general replay
augmentation strategy that retroactively relabels stored transitions with
alternative preferences. This densifies supervision across the preference
simplex without altering the CAPQL architecture or loss functions. Evaluated on
six MO-Gymnasium locomotion tasks at a fixed 300000-step budget using expected
utility (EUM), hypervolume (HV), and sparsity, HPR-CAPQL improves HV in five of
six environments and EUM in four of six. On mo-humanoid-v5, for instance, EUM
rises from $323\!\pm\!125$ to $1613\!\pm\!464$ and HV from 0.52M to 9.63M, with
strong statistical support. mo-halfcheetah-v5 remains a challenging exception
where CAPQL attains higher HV at comparable EUM. We report final summaries and
Pareto-front visualizations across all tasks.
\\ ( https://arxiv.org/abs/2601.11604 ,  2002kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11606
Date: Thu, 8 Jan 2026 20:05:05 GMT   (469kb)

Title: A Multimodal Data Processing Pipeline for MIMIC-IV Dataset
Authors: Farzana Islam Adiba, Varsha Danduri, Fahmida Liza Piya, Ali Abbasi,
  Mehak Gupta, Rahmatollah Beheshti
Categories: cs.LG
\\
  The MIMIC-IV dataset is a large, publicly available electronic health record
(EHR) resource widely used for clinical machine learning research. It comprises
multiple modalities, including structured data, clinical notes, waveforms, and
imaging data. Working with these disjointed modalities requires an extensive
manual effort to preprocess and align them for downstream analysis. While
several pipelines for MIMIC-IV data extraction are available, they target a
small subset of modalities or do not fully support arbitrary downstream
applications. In this work, we greatly expand our prior popular unimodal
pipeline and present a comprehensive and customizable multimodal pipeline that
can significantly reduce multimodal processing time and enhance the
reproducibility of MIMIC-based studies. Our pipeline systematically integrates
the listed modalities, enabling automated cohort selection, temporal alignment
across modalities, and standardized multimodal output formats suitable for
arbitrary static and time-series downstream applications. We release the code,
a simple UI, and a Python package for selective integration (with embedding) at
https://github.com/healthylaife/MIMIC-IV-Data-Pipeline.
\\ ( https://arxiv.org/abs/2601.11606 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11609
Date: Fri, 9 Jan 2026 06:23:42 GMT   (650kb)

Title: Auxiliary-predicted Compress Memory Model(ApCM Model): A Neural Memory
  Storage Model Based on Invertible Compression and Learnable Prediction
Authors: Weinuo Ou
Categories: cs.LG
Comments: 9 pages, 7 figures
\\
  Current large language models (LLMs) generally lack an effective runtime
memory mechanism,making it difficult to adapt to dynamic and personalized
interaction requirements. To address this issue, this paper proposes a novel
neural memory storage architecture--the Auxiliary Prediction Compression Memory
Model (ApCM Model).
\\ ( https://arxiv.org/abs/2601.11609 ,  650kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11611
Date: Fri, 9 Jan 2026 09:47:06 GMT   (450kb)

Title: Integrating Temporal Context into Streaming Data for Human Activity
  Recognition in Smart Home
Authors: Marina Vicini, Martin Rudorfer, Zhuangzhuang Dai, Luis J. Manso
Categories: cs.LG
Comments: Accepted to International Conference on Ubiquitous Computing and
  Ambient Intelligence (UCAmI) 2024
\\
  With the global population ageing, it is crucial to enable individuals to
live independently and safely in their homes. Using ubiquitous sensors such as
Passive InfraRed sensors (PIR) and door sensors is drawing increasing interest
for monitoring daily activities and facilitating preventative healthcare
interventions for the elderly. Human Activity Recognition (HAR) from passive
sensors mostly relies on traditional machine learning and includes data
segmentation, feature extraction, and classification. While techniques like
Sensor Weighting Mutual Information (SWMI) capture spatial context in a feature
vector, effectively leveraging temporal information remains a challenge. We
tackle this by clustering activities into morning, afternoon, and night, and
encoding them into the feature weighting method calculating distinct mutual
information matrices. We further propose to extend the feature vector by
incorporating time of day and day of week as cyclical temporal features, as
well as adding a feature to track the user's location. The experiments show
improved accuracy and F1-score over existing state-of-the-art methods in three
out of four real-world datasets, with highest gains in a low-data regime. These
results highlight the potential of our approach for developing effective smart
home solutions to support ageing in place.
\\ ( https://arxiv.org/abs/2601.11611 ,  450kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11615
Date: Fri, 9 Jan 2026 23:06:36 GMT   (1494kb)

Title: A Review on Machine Learning Approaches for the Prediction of Glucose
  Levels and Hypogylcemia
Authors: Beyza Cinar, Louisa van den Boom, and Maria Maleshkova
Categories: cs.LG
Journal-ref: Informatics in Medicine Unlocked, Volume 60, January 2026
DOI: 10.1016/j.imu.2025.101723
\\
  Type 1 Diabetes (T1D) is an autoimmune disease leading to insulin
insufficiency. Thus, patients require lifelong insulin therapy, which has a
side effect of hypoglycemia. Hypoglycemia is a critical state of decreased
blood glucose levels (BGL) below 70 mg/dL and is associated with increased risk
of mortality. Machine learning (ML) models can improve diabetes management by
predicting hypoglycemia and providing optimal prevention methods. ML models are
classified into regression and classification based, that forecast glucose
levels and identify events based on defined labels, respectively. This review
investigates state-of-the-art models trained on data of continuous glucose
monitoring (CGM) devices from patients with T1D. We compare the models'
performance across short-term (15 to 120 min) and long term (3 to more than 24
hours) prediction horizons (PHs). Particularly, we explore: 1) How much in
advance can glucose values or a hypoglycemic event be accurately predicted? 2)
Which models have the best performance? 3) Which factors impact the
performance? and 4) Does personalization increase performance? The results show
that 1) a PH of up to 1 hour provides the best results. 2) Conventional ML
methods yield the best results for classification and DL for regression. A
single model cannot adequately classify across multiple PHs. 3) The model
performance is influenced by multivariate datasets and the input sequence
length (ISL). 4) Personal data enhances performance but due to limited data
quality population-based models are preferred.
\\ ( https://arxiv.org/abs/2601.11615 ,  1494kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11616
Date: Fri, 9 Jan 2026 23:07:14 GMT   (857kb)

Title: Mixture-of-Experts as Soft Clustering: A Dual Jacobian-PCA Spectral
  Geometry Perspective
Authors: Feilong Liu
Categories: cs.LG cs.AI cs.CL
\\
  Mixture-of-Experts (MoE) architectures are commonly motivated by efficiency
and conditional computation, but their effect on the geometry of learned
functions and representations remains poorly characterized. In this work, we
study MoEs through a geometric lens, interpreting routing as a form of soft
partitioning of the representation space into overlapping local charts. We
introduce a Dual Jacobian-PCA Spectral Geometry probe. It analyzes local
function geometry via Jacobian singular-value spectra and representation
geometry via weighted PCA of routed hidden states. Using a controlled MLP-MoE
setting that permits exact Jacobian computation, we compare dense, Top-k, and
fully-soft routing architectures under matched capacity. Across random seeds,
we observe that MoE routing consistently reduces local sensitivity, with
expert-local Jacobians exhibiting smaller leading singular values and faster
spectral decay than dense baselines. At the same time, weighted PCA reveals
that expert-local representations distribute variance across a larger number of
principal directions, indicating higher effective rank under identical input
distributions. We further find that average expert Jacobians are nearly
orthogonal, suggesting a decomposition of the transformation into low-overlap
expert-specific subspaces rather than scaled variants of a shared map. We
analyze how routing sharpness modulates these effects, showing that Top-k
routing produces lower-rank, more concentrated expert-local structure, while
fully-soft routing yields broader, higher-rank representations. Together, these
results support a geometric interpretation of MoEs as soft partitionings of
function space that flatten local curvature while redistributing representation
variance.
\\ ( https://arxiv.org/abs/2601.11616 ,  857kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11618
Date: Sat, 10 Jan 2026 13:43:01 GMT   (62kb)

Title: Geometric Attention: A Regime-Explicit Operator Semantics for
  Transformer Attention
Authors: Luis Rosario Freytes
Categories: cs.LG cs.AI
Comments: 57 pages
\\
  Geometric Attention (GA) specifies an attention layer by four independent
inputs: a finite carrier (what indices are addressable), an evidence-kernel
rule (how masked proto-scores and a link induce nonnegative weights), a probe
family (which observables are treated as admissible), and an anchor/update rule
(which representative kernel is selected and how it is applied). Probe families
induce an operational equivalence relation on kernels and therefore a gauge;
anchors select representatives relative to that probe. Under a scalar
relational-work representation and a multiplicative compositionality law for
evidence, the admissible link family is exponential, yielding Gibbs weights;
with row anchoring this includes the softmax kernel family as a subregime.
After quotienting unary row/column score fields, the remaining interaction
component admits a canonical rank-r normal form (Eckart-Young/SVD); dot-product
score charts implement the corresponding low-rank interaction regime. Fixing
the carrier and extensionalizing the update yields the standard fixed-token
Transformer attention operator; allowing carrier updates yields
adaptive-carrier and staged-depth regimes. The operator language also supports
multihead/mixed kernels, plan-based anchors (e.g., entropic OT/Sinkhorn), and
unary operators (e.g., FFN-style fields) as explicit regime choices. This
separates invariant structure from modeling choice, enabling principled
comparison and extension of attention mechanisms, and attention-based
architectures.
\\ ( https://arxiv.org/abs/2601.11618 ,  62kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11619
Date: Sat, 10 Jan 2026 14:10:48 GMT   (537kb)

Title: NoiseFormer -- Noise Diffused Symmetric Attention Transformer
Authors: Phani Kumar, Nyshadham and Jyothendra Varma, Polisetty V R K and
  Aditya Rathore
Categories: cs.LG cs.AI
\\
  Transformer architecture has been very successful long runner in the field of
Deep Learning (DL) and Large Language Models (LLM) because of its powerful
attention-based learning and parallel-natured architecture. As the models grow
gigantic in terms of memory footprint, difficulties in fitting the model on a
device like a GPU or an AI accelerator give rise to the need for multiple
computing devices thereby escalating the computing cost. This increased
training/inference cost paved the way for efficient model size
reduction/parametric reduction deploying Sparse Attention techniques. In this
paper, we start analyzing one of the techniques of Sparse Attention called
Symmetric Dot-Product Attention (referred to as Symmetric Attention) and
propose a novel unified model architecture called Noise Diffused Symmetric
Attention Transformer to enhance the model's performance. While maintaining the
memory gains of Symmetric Attention, with minute overhead in terms of model
parameters and computational overhead, the proposed model brings in enhanced
performance in terms of accuracy and inference-time sampling. The proposed
model is validated upon GPT2 base model and the results reflect the performance
gains falling between plain Symmetric attention and GPT2 base model on a
variety of GLUE benchmark tasks in terms of accuracy, with significant model
size reduction with respect to the base model.
\\ ( https://arxiv.org/abs/2601.11619 ,  537kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11638
Date: Wed, 14 Jan 2026 15:20:54 GMT   (6458kb)

Title: Verifying Physics-Informed Neural Network Fidelity using Classical
  Fisher Information from Differentiable Dynamical System
Authors: Josafat Ribeiro Leal Filho and Ant\^onio Augusto Fr\"ohlich
Categories: cs.LG stat.ML
Comments: This paper has been submitted and is currently under review at IEEE
  Transactions on Neural Networks and Learning Systems (TNNLS)
\\
  Physics-Informed Neural Networks (PINNs) have emerged as a powerful tool for
solving differential equations and modeling physical systems by embedding
physical laws into the learning process. However, rigorously quantifying how
well a PINN captures the complete dynamical behavior of the system, beyond
simple trajectory prediction, remains a challenge. This paper proposes a novel
experimental framework to address this by employing Fisher information for
differentiable dynamical systems, denoted $g_F^C$. This Fisher information,
distinct from its statistical counterpart, measures inherent uncertainties in
deterministic systems, such as sensitivity to initial conditions, and is
related to the phase space curvature and the net stretching action of the state
space evolution. We hypothesize that if a PINN accurately learns the underlying
dynamics of a physical system, then the Fisher information landscape derived
from the PINN's learned equations of motion will closely match that of the
original analytical model. This match would signify that the PINN has achieved
comprehensive fidelity capturing not only the state evolution but also crucial
geometric and stability properties. We outline an experimental methodology
using the dynamical model of a car to compute and compare $g_F^C$ for both the
analytical model and a trained PINN. The comparison, based on the Jacobians of
the respective system dynamics, provides a quantitative measure of the PINN's
fidelity in representing the system's intricate dynamical characteristics.
\\ ( https://arxiv.org/abs/2601.11638 ,  6458kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11639
Date: Wed, 14 Jan 2026 15:26:01 GMT   (1432kb)

Title: Global Optimization By Gradient from Hierarchical Score-Matching Spaces
Authors: Ming Li
Categories: cs.LG
\\
  Gradient descent is the most commonly used optimization method, but limited
to local optimality, and confined to the field of continuous differentiable
problems with simple convex constraints. This work solve these limitations and
restrictions by unifying all optimization problems with various complex
constraints as a general hierarchical optimization objective without
constraints, which is optimized by gradient obtained through score matching. By
this way, global optimization by deterministic method using strict gradient is
achieved for the first time, and verified through simple-constructed and
complex-practical experiments. Even more importantly, it reveals the profound
connection between global optimization and diffusion based generative modeling.
\\ ( https://arxiv.org/abs/2601.11639 ,  1432kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11657
Date: Thu, 15 Jan 2026 20:23:41 GMT   (5278kb)

Title: Size is Not the Solution: Deformable Convolutions for Effective Physics
  Aware Deep Learning
Authors: Jack T. Beerman, Shobhan Roy, H.S. Udaykumar, and Stephen S. Baek
Categories: cs.LG cs.AI
\\
  Physics-aware deep learning (PADL) enables rapid prediction of complex
physical systems, yet current convolutional neural network (CNN) architectures
struggle with highly nonlinear flows. While scaling model size addresses
complexity in broader AI, this approach yields diminishing returns for physics
modeling. Drawing inspiration from Hybrid Lagrangian-Eulerian (HLE) numerical
methods, we introduce deformable physics-aware recurrent convolutions (D-PARC)
to overcome the rigidity of CNNs. Across Burgers' equation, Navier-Stokes, and
reactive flows, D-PARC achieves superior fidelity compared to substantially
larger architectures. Analysis reveals that kernels display anti-clustering
behavior, evolving into a learned "active filtration" strategy distinct from
traditional h- or p-adaptivity. Effective receptive field analysis confirms
that D-PARC autonomously concentrates resources in high-strain regions while
coarsening focus elsewhere, mirroring adaptive refinement in computational
mechanics. This demonstrates that physically intuitive architectural design can
outperform parameter scaling, establishing that strategic learning in lean
networks offers a more effective path forward for PADL than indiscriminate
network expansion.
\\ ( https://arxiv.org/abs/2601.11657 ,  5278kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11661
Date: Thu, 15 Jan 2026 21:39:09 GMT   (2177kb)

Title: Machine learning model for predicting surface wettability in
  laser-textured metal alloys
Authors: Mohammad Mohammadzadeh Sanandaji, Danial Ebrahimzadeh, Mohammad Ikram
  Haider, Yaser Mike Banad, Aleksandar Poleksic and Hongtao Ding
Categories: cs.LG cond-mat.mtrl-sci
Comments: This manuscript has 9 figures and contains 16 pages two column.
  submitted to journal of laser applications. Under review
\\
  Surface wettability, governed by both topography and chemistry, plays a
critical role in applications such as heat transfer, lubrication,
microfluidics, and surface coatings. In this study, we present a machine
learning (ML) framework capable of accurately predicting the wettability of
laser-textured metal alloys using experimentally derived morphological and
chemical features. Superhydrophilic and superhydrophobic surfaces were
fabricated on AA6061 and AISI 4130 alloys via nanosecond laser texturing
followed by chemical immersion treatments. Surface morphology was quantified
using the Laws texture energy method and profilometry, while surface chemistry
was characterized through X-ray photoelectron spectroscopy (XPS), extracting
features such as functional group polarity, molecular volume, and peak area
fraction. These features were used to train an ensemble neural network model
incorporating residual connections, batch normalization, and dropout
regularization. The model achieved high predictive accuracy (R2 = 0.942, RMSE =
13.896), outperforming previous approaches. Feature importance analysis
revealed that surface chemistry had the strongest influence on contact angle
prediction, with topographical features also contributing significantly. This
work demonstrates the potential of artificial intelligence to model and predict
wetting behavior by capturing the complex interplay of surface characteristics,
offering a data-driven pathway for designing tailored functional surfaces.
\\ ( https://arxiv.org/abs/2601.11661 ,  2177kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11663
Date: Thu, 15 Jan 2026 22:27:19 GMT   (16kb)

Title: Activation Sensitivity as a Unifying Principle for Post-Training
  Quantization
Authors: Bruce Changlong Xu
Categories: cs.LG cs.AI
\\
  Post-training quantization (PTQ) methods for large language models rely on
heuristics that implicitly estimate which weight channels most strongly
influence model behavior. Two dominant paradigms have emerged: activation-aware
methods such as AWQ prioritize channels with large activation magnitudes, while
second-order methods such as GPTQ allocate quantization error according to
input covariance structure. Despite strong empirical performance, these
approaches remain conceptually fragmented, and it is unclear what underlying
quantity they are approximating. In this work, we present a unified theoretical
framework for PTQ by formalizing activation sensitivity, defined as the
expected impact of channel-wise perturbations on the loss. Using a first-order
Taylor expansion, we show that sensitivity naturally arises as the squared norm
of gradient-weighted activations, yielding a principled measure of channel
importance that captures both activation magnitude and downstream error
propagation. Within this framework, AWQ and GPTQ can be interpreted as
complementary approximations that recover sensitivity under distinct
simplifying assumptions. We analyze the design space of sensitivity metrics,
connect gradient-based saliency, Fisher information, and Hessian-based
criteria, and clarify their relationships to classical pruning methods such as
Optimal Brain Damage and Optimal Brain Surgeon. Rather than proposing a new
quantization algorithm, this work provides a conceptual foundation for
understanding and comparing post-training quantization methods through the lens
of sensitivity.
\\ ( https://arxiv.org/abs/2601.11663 ,  16kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11667
Date: Fri, 16 Jan 2026 02:01:40 GMT   (673kb)

Title: Distill-then-Replace: Efficient Task-Specific Hybrid Attention Model
  Construction
Authors: Xiaojie Xia, Huigang Zhang, Chaoliang Zhong, Jun Sun, Yusuke Oishi
Categories: cs.LG cs.AI
\\
  Transformer architectures deliver state-of-the-art accuracy via dense
full-attention, but their quadratic time and memory complexity with respect to
sequence length limits practical deployment. Linear attention mechanisms offer
linear or near-linear scaling yet often incur performance degradation. Hybrid
models that integrate full and linear attention layers promise a balance
between efficiency and expressiveness, but face two major challenges: training
such hybrid models from scratch is computationally expensive, and manually
designing the optimal placement of attention types is highly nontrivial. We
address both issues by first transferring weights from the pretrained
full-attention modules to its linear attention counterparts through blockwise
local distillation, and second, introducing a greedy layer replacement strategy
that iteratively substitutes full attention blocks with linear ones while
monitoring validation performance on the target task. This yields a
task-specific hybrid model in a single efficient pass, without costly
re-training or neural architecture search, and can be applied to any pretrained
full-attention backbone for diverse downstream tasks.
\\ ( https://arxiv.org/abs/2601.11667 ,  673kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11669
Date: Fri, 16 Jan 2026 02:10:47 GMT   (361kb)

Title: IPEC: Test-Time Incremental Prototype Enhancement Classifier for
  Few-Shot Learning
Authors: Wenwen Liao, Hang Ruan, Jianbo Yu, Xiaofeng Yang, Qingchao Jiang,
  Xuefeng Yan
Categories: cs.LG cs.CV
\\
  Metric-based few-shot approaches have gained significant popularity due to
their relatively straightforward implementation, high interpret ability, and
computational efficiency. However, stemming from the batch-independence
assumption during testing, which prevents the model from leveraging valuable
knowledge accumulated from previous batches. To address these challenges, we
propose a novel test-time method called Incremental Prototype Enhancement
Classifier (IPEC), a test-time method that optimizes prototype estimation by
leveraging information from previous query samples. IPEC maintains a dynamic
auxiliary set by selectively incorporating query samples that are classified
with high confidence. To ensure sample quality, we design a robust
dual-filtering mechanism that assesses each query sample based on both global
prediction confidence and local discriminative ability. By aggregating this
auxiliary set with the support set in subsequent tasks, IPEC builds
progressively more stable and representative prototypes, effectively reducing
its reliance on the initial support set. We ground this approach in a Bayesian
interpretation, conceptualizing the support set as a prior and the auxiliary
set as a data-driven posterior, which in turn motivates the design of a
practical "warm-up and test" two-stage inference protocol. Extensive empirical
results validate the superior performance of our proposed method across
multiple few-shot classification tasks.
\\ ( https://arxiv.org/abs/2601.11669 ,  361kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11670
Date: Fri, 16 Jan 2026 02:51:59 GMT   (12549kb)

Title: A Confidence-Variance Theory for Pseudo-Label Selection in
  Semi-Supervised Learning
Authors: Jinshi Liu, Pan Liu
Categories: cs.LG cs.AI
\\
  Most pseudo-label selection strategies in semi-supervised learning rely on
fixed confidence thresholds, implicitly assuming that prediction confidence
reliably indicates correctness. In practice, deep networks are often
overconfident: high-confidence predictions can still be wrong, while
informative low-confidence samples near decision boundaries are discarded. This
paper introduces a Confidence-Variance (CoVar) theory framework that provides a
principled joint reliability criterion for pseudo-label selection. Starting
from the entropy minimization principle, we derive a reliability measure that
combines maximum confidence (MC) with residual-class variance (RCV), which
characterizes how probability mass is distributed over non-maximum classes. The
derivation shows that reliable pseudo-labels should have both high MC and low
RCV, and that the influence of RCV increases as confidence grows, thereby
correcting overconfident but unstable predictions. From this perspective, we
cast pseudo-label selection as a spectral relaxation problem that maximizes
separability in a confidence-variance feature space, and design a
threshold-free selection mechanism to distinguish high- from low-reliability
predictions. We integrate CoVar as a plug-in module into representative
semi-supervised semantic segmentation and image classification methods. Across
PASCAL VOC 2012, Cityscapes, CIFAR-10, and Mini-ImageNet with varying label
ratios and backbones, it consistently improves over strong baselines,
indicating that combining confidence with residual-class variance provides a
more reliable basis for pseudo-label selection than fixed confidence
thresholds. (Code:
https://github.com/ljs11528/CoVar_Pseudo_Label_Selection.git)
\\ ( https://arxiv.org/abs/2601.11670 ,  12549kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11686
Date: Fri, 16 Jan 2026 10:47:13 GMT   (832kb)

Title: Proof of Concept: Multi-Target Wildfire Risk Prediction and Large
  Language Model Synthesis
Authors: Nicolas Caron, Christophe Guyeux, Hassan Noura, Benjamin Aynes
Categories: cs.LG cs.AI
\\
  Current state-of-the-art approaches to wildfire risk assessment often
overlook operational needs, limiting their practical value for first responders
and firefighting services. Effective wildfire management requires a
multi-target analysis that captures the diverse dimensions of wildfire risk,
including meteorological danger, ignition activity, intervention complexity,
and resource mobilization, rather than relying on a single predictive
indicator. In this proof of concept, we propose the development of a hybrid
framework that combines predictive models for each risk dimension with large
language models (LLMs) to synthesize heterogeneous outputs into structured,
actionable reports.
\\ ( https://arxiv.org/abs/2601.11686 ,  832kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11719
Date: Fri, 16 Jan 2026 19:12:13 GMT   (6553kb)

Title: jBOT: Semantic Jet Representation Clustering Emerges from
  Self-Distillation
Authors: Ho Fung Tsoi, Dylan Rankin
Categories: cs.LG hep-ex
Comments: Under review
\\
  Self-supervised learning is a powerful pre-training method for learning
feature representations without labels, which often capture generic underlying
semantics from the data and can later be fine-tuned for downstream tasks. In
this work, we introduce jBOT, a pre-training method based on self-distillation
for jet data from the CERN Large Hadron Collider, which combines local
particle-level distillation with global jet-level distillation to learn jet
representations that support downstream tasks such as anomaly detection and
classification. We observe that pre-training on unlabeled jets leads to
emergent semantic class clustering in the representation space. The clustering
in the frozen embedding, when pre-trained on background jets only, enables
anomaly detection via simple distance-based metrics, and the learned embedding
can be fine-tuned for classification with improved performance compared to
supervised models trained from scratch.
\\ ( https://arxiv.org/abs/2601.11719 ,  6553kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11789
Date: Fri, 16 Jan 2026 21:32:48 GMT   (32712kb)

Title: Suspicious Alignment of SGD: A Fine-Grained Step Size Condition Analysis
Authors: Shenyang Deng, Boyao Liao, Zhuoli Ouyang, Tianyu Pang, Minhak Song,
  Yaoqing Yang
Categories: cs.LG
Comments: The 37th International Conference on Algorithmic Learning Theory
\\
  This paper explores the suspicious alignment phenomenon in stochastic
gradient descent (SGD) under ill-conditioned optimization, where the Hessian
spectrum splits into dominant and bulk subspaces. This phenomenon describes the
behavior of gradient alignment in SGD updates. Specifically, during the initial
phase of SGD updates, the alignment between the gradient and the dominant
subspace tends to decrease. Subsequently, it enters a rising phase and
eventually stabilizes in a high-alignment phase. The alignment is considered
``suspicious'' because, paradoxically, the projected gradient update along this
highly-aligned dominant subspace proves ineffective at reducing the loss. The
focus of this work is to give a fine-grained analysis in a high-dimensional
quadratic setup about how step size selection produces this phenomenon. Our
main contribution can be summarized as follows: We propose a step-size
condition revealing that in low-alignment regimes, an adaptive critical step
size $\eta_t^*$ separates alignment-decreasing ($\eta_t < \eta_t^*$) from
alignment-increasing ($\eta_t > \eta_t^*$) regimes, whereas in high-alignment
regimes, the alignment is self-correcting and decreases regardless of the step
size. We further show that under sufficient ill-conditioning, a step size
interval exists where projecting the SGD updates to the bulk space decreases
the loss while projecting them to the dominant space increases the loss, which
explains a recent empirical observation that projecting gradient updates to the
dominant subspace is ineffective. Finally, based on this adaptive step-size
theory, we prove that for a constant step size and large initialization, SGD
exhibits this distinct two-phase behavior: an initial alignment-decreasing
phase, followed by stabilization at high alignment.
\\ ( https://arxiv.org/abs/2601.11789 ,  32712kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11794
Date: Fri, 16 Jan 2026 21:40:39 GMT   (4020kb)

Title: Physics-Constrained Denoising Autoencoders for Data-Scarce Wildfire UAV
  Sensing
Authors: Abdelrahman Ramadan, Zahra Dorbeigi Namaghi, Emily Taylor, Lucas
  Edwards, Xan Giuliani, David S. McLagan, Sidney Givigi, Melissa Greeff
Categories: cs.LG cs.CV cs.RO
\\
  Wildfire monitoring requires high-resolution atmospheric measurements, yet
low-cost sensors on Unmanned Aerial Vehicles (UAVs) exhibit baseline drift,
cross-sensitivity, and response lag that corrupt concentration estimates.
Traditional deep learning denoising approaches demand large datasets
impractical to obtain from limited UAV flight campaigns. We present PC$^2$DAE,
a physics-informed denoising autoencoder that addresses data scarcity by
embedding physical constraints directly into the network architecture.
Non-negative concentration estimates are enforced via softplus activations and
physically plausible temporal smoothing, ensuring outputs are physically
admissible by construction rather than relying on loss function penalties. The
architecture employs hierarchical decoder heads for Black Carbon, Gas, and
CO$_2$ sensor families, with two variants: PC$^2$DAE-Lean (21k parameters) for
edge deployment and PC$^2$DAE-Wide (204k parameters) for offline processing. We
evaluate on 7,894 synchronized 1 Hz samples collected from UAV flights during
prescribed burns in Saskatchewan, Canada (approximately 2.2 hours of flight
data), two orders of magnitude below typical deep learning requirements.
PC$^2$DAE-Lean achieves 67.3\% smoothness improvement and 90.7\% high-frequency
noise reduction with zero physics violations. Five baselines (LSTM-AE, U-Net,
Transformer, CBDAE, DeSpaWN) produce 15--23\% negative outputs. The lean
variant outperforms wide (+5.6\% smoothness), suggesting reduced capacity with
strong inductive bias prevents overfitting in data-scarce regimes. Training
completes in under 65 seconds on consumer hardware.
\\ ( https://arxiv.org/abs/2601.11794 ,  4020kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11821
Date: Fri, 16 Jan 2026 22:57:24 GMT   (3205kb)

Title: Shapelets-Enriched Selective Forecasting using Time Series Foundation
  Models
Authors: Shivani Tomar, Seshu Tirupathi, Elizabeth Daly, Ivana Dusparic
Categories: cs.LG
Comments: Accepted by the AAAI-26 Workshop on Artificial Intelligence for Time
  Series Analysis (AI4TS)
\\
  Time series foundation models have recently gained a lot of attention due to
their ability to model complex time series data encompassing different domains
including traffic, energy, and weather. Although they exhibit strong average
zero-shot performance on forecasting tasks, their predictions on certain
critical regions of the data are not always reliable, limiting their usability
in real-world applications, especially when data exhibits unique trends. In
this paper, we propose a selective forecasting framework to identify these
critical segments of time series using shapelets. We learn shapelets using
shift-invariant dictionary learning on the validation split of the target
domain dataset. Utilizing distance-based similarity to these shapelets, we
facilitate the user to selectively discard unreliable predictions and be
informed of the model's realistic capabilities. Empirical results on diverse
benchmark time series datasets demonstrate that our approach leveraging both
zero-shot and full-shot fine-tuned models reduces the overall error by an
average of 22.17% for zero-shot and 22.62% for full-shot fine-tuned model.
Furthermore, our approach using zero-shot and full-shot fine-tuned models, also
outperforms its random selection counterparts by up to 21.41% and 21.43% on one
of the datasets.
\\ ( https://arxiv.org/abs/2601.11821 ,  3205kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11827
Date: Fri, 16 Jan 2026 23:13:21 GMT   (1653kb)

Title: MixFlow: Mixture-Conditioned Flow Matching for Out-of-Distribution
  Generalization
Authors: Andrea Rubbi, Amir Akbarnejad, Mohammad Vali Sanian, Aryan Yazdan
  Parast, Hesam Asadollahzadeh, Arian Amani, Naveed Akhtar, Sarah Cooper,
  Andrew Bassett, Pietro Li\`o, Lassi Paavolainen, Sattar Vakili, Mo Lotfollahi
Categories: cs.LG cs.CV eess.IV
\\
  Achieving robust generalization under distribution shift remains a central
challenge in conditional generative modeling, as existing conditional
flow-based methods often struggle to extrapolate beyond the training
conditions. We introduce MixFlow, a conditional flow-matching framework for
descriptor-controlled generation that directly targets this limitation by
jointly learning a descriptor-conditioned base distribution and a
descriptor-conditioned flow field via shortest-path flow matching. By modeling
the base distribution as a learnable, descriptor-dependent mixture, MixFlow
enables smooth interpolation and extrapolation to unseen conditions, leading to
substantially improved out-of-distribution generalization. We provide
analytical insights into the behavior of the proposed framework and empirically
demonstrate its effectiveness across multiple domains, including prediction of
responses to unseen perturbations in single-cell transcriptomic data and
high-content microscopy-based drug screening tasks. Across these diverse
settings, MixFlow consistently outperforms standard conditional flow-matching
baselines. Overall, MixFlow offers a simple yet powerful approach for achieving
robust, generalizable, and controllable generative modeling across
heterogeneous domains.
\\ ( https://arxiv.org/abs/2601.11827 ,  1653kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11864
Date: Sat, 17 Jan 2026 01:11:07 GMT   (566kb)

Title: AGGC: Adaptive Group Gradient Clipping for Stabilizing Large Language
  Model Training
Authors: Zhiyuan Li, Yuan Wu and Yi Chang
Categories: cs.LG cs.CL
Comments: 13 pages
\\
  To stabilize the training of Large Language Models (LLMs), gradient clipping
is a nearly ubiquitous heuristic used to alleviate exploding gradients.
However, traditional global norm clipping erroneously presupposes gradient
homogeneity across different functional modules, leading to an adverse
"spill-over" effect where volatile parameters force unnecessary scaling on
stable ones. To overcome this, we propose Adaptive Group-wise Gradient Clipping
(AGGC). AGGC partitions parameters into groups based on functional types and
regulates each according to its historical behavior using an Exponential Moving
Average (EMA). Specifically, it constructs an adaptive interval to
simultaneously mitigate gradient explosion and vanishing, while employing a
time-dependent scheduling mechanism to balance exploration and convergence.
Experiments on LLaMA 2-7B, Mistral-7B, and Gemma-7B models show that AGGC
consistently outperforms LoRA and frequently surpasses Full Fine-Tuning. On the
GSM8K benchmark, Mistral-7B fine-tuned with AGGC achieves an accuracy of
72.93%, exceeding LoRA's 69.5%. AGGC also effectively stabilizes Reinforcement
Learning with Verifiable Rewards (RLVR), enhancing the logic deduction of Qwen
2.5 and Llama 3.2 models. Experimental results demonstrate that AGGC
effectively addresses the limitations of traditional gradient clipping methods,
particularly in overcoming gradient heterogeneity, by utilizing a modular,
adaptive clipping strategy to stabilize the training process. Due to its
lightweight design, AGGC can be seamlessly integrated into existing
post-training pipelines with negligible overhead.
\\ ( https://arxiv.org/abs/2601.11864 ,  566kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11880
Date: Sat, 17 Jan 2026 02:27:56 GMT   (7368kb)

Title: TF-CoDiT: Conditional Time Series Synthesis with Diffusion Transformers
  for Treasury Futures
Authors: Yingxiao Zhang, Jiaxin Duan, Junfu Zhang, Ke Feng
Categories: cs.LG cs.AI
\\
  Diffusion Transformers (DiT) have achieved milestones in synthesizing
financial time-series data, such as stock prices and order flows. However,
their performance in synthesizing treasury futures data is still underexplored.
This work emphasizes the characteristics of treasury futures data, including
its low volume, market dependencies, and the grouped correlations among
multivariables. To overcome these challenges, we propose TF-CoDiT, the first
DiT framework for language-controlled treasury futures synthesis. To facilitate
low-data learning, TF-CoDiT adapts the standard DiT by transforming
multi-channel 1-D time series into Discrete Wavelet Transform (DWT) coefficient
matrices. A U-shape VAE is proposed to encode cross-channel dependencies
hierarchically into a latent variable and bridge the latent and DWT spaces
through decoding, thereby enabling latent diffusion generation. To derive
prompts that cover essential conditions, we introduce the Financial Market
Attribute Protocol (FinMAP) - a multi-level description system that
standardizes daily$/$periodical market dynamics by recognizing 17$/$23 economic
indicators from 7/8 perspectives. In our experiments, we gather four types of
treasury futures data covering the period from 2015 to 2025, and define data
synthesis tasks with durations ranging from one week to four months. Extensive
evaluations demonstrate that TF-CoDiT can produce highly authentic data with
errors at most 0.433 (MSE) and 0.453 (MAE) to the ground-truth. Further studies
evidence the robustness of TF-CoDiT across contracts and temporal horizons.
\\ ( https://arxiv.org/abs/2601.11880 ,  7368kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11883
Date: Sat, 17 Jan 2026 02:39:41 GMT   (73kb)

Title: Approximation Algorithm for Constrained $k$-Center Clustering: A Local
  Search Approach
Authors: Chaoqi Jia, Longkun Guo, Kewen Liao, Zhigang Lu, Chao Chen, Jason Xue
Categories: cs.LG
Comments: AAAI-26
\\
  Clustering is a long-standing research problem and a fundamental tool in AI
and data analysis. The traditional k-center problem, a fundamental theoretical
challenge in clustering, has a best possible approximation ratio of 2, and any
improvement to a ratio of 2 - {\epsilon} would imply P = NP. In this work, we
study the constrained k-center clustering problem, where instance-level
cannot-link (CL) and must-link (ML) constraints are incorporated as background
knowledge. Although general CL constraints significantly increase the hardness
of approximation, previous work has shown that disjoint CL sets permit
constant-factor approximations. However, whether local search can achieve such
a guarantee in this setting remains an open question. To this end, we propose a
novel local search framework based on a transformation to a dominating matching
set problem, achieving the best possible approximation ratio of 2. The
experimental results on both real-world and synthetic datasets demonstrate that
our algorithm outperforms baselines in solution quality.
\\ ( https://arxiv.org/abs/2601.11883 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11890
Date: Sat, 17 Jan 2026 03:08:34 GMT   (87kb)

Title: From Relative Entropy to Minimax: A Unified Framework for Coverage in
  MDPs
Authors: Xihe Gu, Urbashi Mitra, Tara Javidi
Categories: cs.LG
\\
  Targeted and deliberate exploration of state--action pairs is essential in
reward-free Markov Decision Problems (MDPs). More precisely, different
state-action pairs exhibit different degree of importance or difficulty which
must be actively and explicitly built into a controlled exploration strategy.
To this end, we propose a weighted and parameterized family of concave coverage
objectives, denoted by $U_\rho$, defined directly over state--action occupancy
measures. This family unifies several widely studied objectives within a single
framework, including divergence-based marginal matching, weighted average
coverage, and worst-case (minimax) coverage. While the concavity of $U_\rho$
captures the diminishing return associated with over-exploration, the simple
closed form of the gradient of $U_\rho$ enables an explicit control to
prioritize under-explored state--action pairs. Leveraging this structure, we
develop a gradient-based algorithm that actively steers the induced occupancy
toward a desired coverage pattern. Moreover, we show that as $\rho$ increases,
the resulting exploration strategy increasingly emphasizes the least-explored
state--action pairs, recovering worst-case coverage behavior in the limit.
\\ ( https://arxiv.org/abs/2601.11890 ,  87kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11895
Date: Sat, 17 Jan 2026 03:33:08 GMT   (1683kb)

Title: DevBench: A Realistic, Developer-Informed Benchmark for Code Generation
  Models
Authors: Pareesa Ameneh Golnari, Adarsh Kumarappan, Wen Wen, Xiaoyu Liu,
  Gabriel Ryan, Yuting Sun, Shengyu Fu, Elsie Nallipogu
Categories: cs.LG cs.AI cs.SE
\\
  DevBench is a telemetry-driven benchmark designed to evaluate Large Language
Models (LLMs) on realistic code completion tasks. It includes 1,800 evaluation
instances across six programming languages and six task categories derived from
real developer telemetry, such as API usage and code purpose understanding.
Unlike prior benchmarks, it emphasizes ecological validity, avoids training
data contamination, and enables detailed diagnostics. The evaluation combines
functional correctness, similarity-based metrics, and LLM-judge assessments
focused on usefulness and contextual relevance. 9 state-of-the-art models were
assessed, revealing differences in syntactic precision, semantic reasoning, and
practical utility. Our benchmark provides actionable insights to guide model
selection and improvement-detail that is often missing from other benchmarks
but is essential for both practical deployment and targeted model development.
\\ ( https://arxiv.org/abs/2601.11895 ,  1683kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11897
Date: Sat, 17 Jan 2026 03:49:50 GMT   (6703kb)

Title: Task-tailored Pre-processing: Fair Downstream Supervised Learning
Authors: Jinwon Sohn, Guang Lin, Qifan Song
Categories: cs.LG stat.ME stat.ML
\\
  Fairness-aware machine learning has recently attracted various communities to
mitigate discrimination against certain societal groups in data-driven tasks.
For fair supervised learning, particularly in pre-processing, there have been
two main categories: data fairness and task-tailored fairness. The former
directly finds an intermediate distribution among the groups, independent of
the type of the downstream model, so a learned downstream
classification/regression model returns similar predictive scores to
individuals inputting the same covariates irrespective of their sensitive
attributes. The latter explicitly takes the supervised learning task into
account when constructing the pre-processing map. In this work, we study
algorithmic fairness for supervised learning and argue that the data fairness
approaches impose overly strong regularization from the perspective of the HGR
correlation. This motivates us to devise a novel pre-processing approach
tailored to supervised learning. We account for the trade-off between fairness
and utility in obtaining the pre-processing map. Then we study the behavior of
arbitrary downstream supervised models learned on the transformed data to find
sufficient conditions to guarantee their fairness improvement and utility
preservation. To our knowledge, no prior work in the branch of task-tailored
methods has theoretically investigated downstream guarantees when using
pre-processed data. We further evaluate our framework through comparison
studies based on tabular and image data sets, showing the superiority of our
framework which preserves consistent trade-offs among multiple downstream
models compared to recent competing models. Particularly for computer vision
data, we see our method alters only necessary semantic features related to the
central machine learning task to achieve fairness.
\\ ( https://arxiv.org/abs/2601.11897 ,  6703kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11924
Date: Sat, 17 Jan 2026 06:13:52 GMT   (1093kb)

Title: Communication-Corruption Coupling and Verification in Cooperative
  Multi-Objective Bandits
Authors: Ming Shi
Categories: cs.LG
\\
  We study cooperative stochastic multi-armed bandits with vector-valued
rewards under adversarial corruption and limited verification. In each of $T$
rounds, each of $N$ agents selects an arm, the environment generates a clean
reward vector, and an adversary perturbs the observed feedback subject to a
global corruption budget $\Gamma$. Performance is measured by team regret under
a coordinate-wise nondecreasing, $L$-Lipschitz scalarization $\phi$, covering
linear, Chebyshev, and smooth monotone utilities. Our main contribution is a
communication-corruption coupling: we show that a fixed environment-side budget
$\Gamma$ can translate into an effective corruption level ranging from $\Gamma$
to $N\Gamma$, depending on whether agents share raw samples, sufficient
statistics, or only arm recommendations. We formalize this via a
protocol-induced multiplicity functional and prove regret bounds parameterized
by the resulting effective corruption. As corollaries, raw-sample sharing can
suffer an $N$-fold larger additive corruption penalty, whereas summary sharing
and recommendation-only sharing preserve an unamplified $O(\Gamma)$ term and
achieve centralized-rate team regret. We further establish
information-theoretic limits, including an unavoidable additive
$\Omega(\Gamma)$ penalty and a high-corruption regime $\Gamma=\Theta(NT)$ where
sublinear regret is impossible without clean information. Finally, we
characterize how a global budget $\nu$ of verified observations restores
learnability. That is, verification is necessary in the high-corruption regime,
and sufficient once it crosses the identification threshold, with certified
sharing enabling the team's regret to become independent of $\Gamma$.
\\ ( https://arxiv.org/abs/2601.11924 ,  1093kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11942
Date: Sat, 17 Jan 2026 07:32:18 GMT   (1428kb)

Title: Trainability-Oriented Hybrid Quantum Regression via Geometric
  Preconditioning and Curriculum Optimization
Authors: Qingyu Meng and Yangshuai Wang
Categories: cs.LG quant-ph
\\
  Quantum neural networks (QNNs) have attracted growing interest for scientific
machine learning, yet in regression settings they often suffer from limited
trainability under noisy gradients and ill-conditioned optimization. We propose
a hybrid quantum-classical regression framework designed to mitigate these
bottlenecks. Our model prepends a lightweight classical embedding that acts as
a learnable geometric preconditioner, reshaping the input representation to
better condition a downstream variational quantum circuit. Building on this
architecture, we introduce a curriculum optimization protocol that
progressively increases circuit depth and transitions from SPSA-based
stochastic exploration to Adam-based gradient fine-tuning. We evaluate the
approach on PDE-informed regression benchmarks and standard regression datasets
under a fixed training budget in a simulator setting. Empirically, the proposed
framework consistently improves over pure QNN baselines and yields more stable
convergence in data-limited regimes. We further observe reduced structured
errors that are visually correlated with oscillatory components on several
scientific benchmarks, suggesting that geometric preconditioning combined with
curriculum training is a practical approach for stabilizing quantum regression.
\\ ( https://arxiv.org/abs/2601.11942 ,  1428kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11953
Date: Sat, 17 Jan 2026 08:02:51 GMT   (7690kb)

Title: Controlling Underestimation Bias in Constrained Reinforcement Learning
  for Safe Exploration
Authors: Shiqing Gao, Jiaxin Ding, Luoyi Fu, Xinbing Wang
Categories: cs.LG
Comments: Published in the 42nd International Conference on Machine Learning
  (ICML 2025, Oral)
\\
  Constrained Reinforcement Learning (CRL) aims to maximize cumulative rewards
while satisfying constraints. However, existing CRL algorithms often encounter
significant constraint violations during training, limiting their applicability
in safety-critical scenarios. In this paper, we identify the underestimation of
the cost value function as a key factor contributing to these violations. To
address this issue, we propose the Memory-driven Intrinsic Cost Estimation
(MICE) method, which introduces intrinsic costs to mitigate underestimation and
control bias to promote safer exploration. Inspired by flashbulb memory, where
humans vividly recall dangerous experiences to avoid risks, MICE constructs a
memory module that stores previously explored unsafe states to identify
high-cost regions. The intrinsic cost is formulated as the pseudo-count of the
current state visiting these risk regions. Furthermore, we propose an
extrinsic-intrinsic cost value function that incorporates intrinsic costs and
adopts a bias correction strategy. Using this function, we formulate an
optimization objective within the trust region, along with corresponding
optimization methods. Theoretically, we provide convergence guarantees for the
proposed cost value function and establish the worst-case constraint violation
for the MICE update. Extensive experiments demonstrate that MICE significantly
reduces constraint violations while preserving policy performance comparable to
baselines.
\\ ( https://arxiv.org/abs/2601.11953 ,  7690kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11954
Date: Sat, 17 Jan 2026 08:03:09 GMT   (1451kb)

Title: Data-centric Prompt Tuning for Dynamic Graphs
Authors: Yufei Peng, Cheng Yang, Zhengjie Fan, Chuan Shi
Categories: cs.LG
Comments: CIKM 2025
\\
  Dynamic graphs have attracted increasing attention due to their ability to
model complex and evolving relationships in real-world scenarios. Traditional
approaches typically pre-train models using dynamic link prediction and
directly apply the resulting node temporal embeddings to specific downstream
tasks. However, the significant differences among downstream tasks often lead
to performance degradation, especially under few-shot settings. Prompt tuning
has emerged as an effective solution to this problem. Existing prompting
methods are often strongly coupled with specific model architectures or
pretraining tasks, which makes it difficult to adapt to recent or future model
designs. Moreover, their exclusive focus on modifying node or temporal features
while neglecting spatial structural information leads to limited expressiveness
and degraded performance. To address these limitations, we propose DDGPrompt, a
data-centric prompting framework designed to effectively refine pre-trained
node embeddings at the input data level, enabling better adaptability to
diverse downstream tasks. We first define a unified node expression feature
matrix that aggregates all relevant temporal and structural information of each
node, ensuring compatibility with a wide range of dynamic graph models. Then,
we introduce three prompt matrices (temporal bias, edge weight, and feature
mask) to adjust the feature matrix completely, achieving task-specific
adaptation of node embeddings. We evaluate DDGPrompt under a strict few-shot
setting on four public dynamic graph datasets. Experimental results demonstrate
that our method significantly outperforms traditional methods and prompting
approaches in scenarios with limited labels and cold-start conditions.
\\ ( https://arxiv.org/abs/2601.11954 ,  1451kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11960
Date: Sat, 17 Jan 2026 08:30:50 GMT   (10046kb)

Title: R$^2$PO: Decoupling Training Trajectories from Inference Responses for
  LLM Reasoning
Authors: Jingchu Wang, Bingbing Xu, Yige Yuan, Bin Xie, Xiaoqian Sun, Huawei
  Shen
Categories: cs.LG cs.AI cs.CL
\\
  Reinforcement learning has become a central paradigm for improving LLM
reasoning. However, existing methods use a single policy to produce both
inference responses and training optimization trajectories. The objective
conflict between generating stable inference responses and diverse training
trajectories leads to insufficient exploration, which harms reasoning
capability. In this paper, to address the problem, we propose R$^2$PO (Residual
Rollout Policy Optimization), which introduces a lightweight Residual
Rollout-Head atop the policy to decouple training trajectories from inference
responses, enabling controlled trajectory diversification during training while
keeping inference generation stable. Experiments across multiple benchmarks
show that our method consistently outperforms baselines, achieving average
accuracy gains of 3.1% on MATH-500 and 2.4% on APPS, while also reducing
formatting errors and mitigating length bias for stable optimization. Our code
is publicly available at https://github.com/RRPO-ARR/Code.
\\ ( https://arxiv.org/abs/2601.11960 ,  10046kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11977
Date: Sat, 17 Jan 2026 09:13:57 GMT   (180kb)

Title: One-Shot Price Forecasting with Covariate-Guided Experts under Privacy
  Constraints
Authors: Ren He (Tsinghua University), Yinliang Xu (Tsinghua University),
  Jinfeng Wang (Guangdong Power Grid Co.), Jeremy Watson (University of
  Canterbury), Jian Song (Tsinghua University)
Categories: cs.LG cs.AI
\\
  Forecasting in power systems often involves multivariate time series with
complex dependencies and strict privacy constraints across regions. Traditional
forecasting methods require significant expert knowledge and struggle to
generalize across diverse deployment scenarios. Recent advancements in
pre-trained time series models offer new opportunities, but their zero-shot
performance on domain-specific tasks remains limited. To address these
challenges, we propose a novel MoE Encoder module that augments pretrained
forecasting models by injecting a sparse mixture-of-experts layer between
tokenization and encoding. This design enables two key capabilities: (1) trans
forming multivariate forecasting into an expert-guided univariate task,
allowing the model to effectively capture inter-variable relations, and (2)
supporting localized training and lightweight parameter sharing in federated
settings where raw data cannot be exchanged. Extensive experiments on public
multivariate datasets demonstrate that MoE-Encoder significantly improves
forecasting accuracy compared to strong baselines. We further simulate
federated environments and show that transferring only MoE-Encoder parameters
allows efficient adaptation to new regions, with minimal performance
degradation. Our findings suggest that MoE-Encoder provides a scalable and
privacy-aware extension to foundation time series models.
\\ ( https://arxiv.org/abs/2601.11977 ,  180kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12008
Date: Sat, 17 Jan 2026 11:12:24 GMT   (6147kb)

Title: Extreme Value Policy Optimization for Safe Reinforcement Learning
Authors: Shiqing Gao, Yihang Zhou, Shuai Shao, Haoyu Luo, Yiheng Bing, Jiaxin
  Ding, Luoyi Fu, Xinbing Wang
Categories: cs.LG
Comments: Published in the 42nd International Conference on Machine Learning
  (ICML 2025)
\\
  Ensuring safety is a critical challenge in applying Reinforcement Learning
(RL) to real-world scenarios. Constrained Reinforcement Learning (CRL)
addresses this by maximizing returns under predefined constraints, typically
formulated as the expected cumulative cost. However, expectation-based
constraints overlook rare but high-impact extreme value events in the tail
distribution, such as black swan incidents, which can lead to severe constraint
violations. To address this issue, we propose the Extreme Value policy
Optimization (EVO) algorithm, leveraging Extreme Value Theory (EVT) to model
and exploit extreme reward and cost samples, reducing constraint violations.
EVO introduces an extreme quantile optimization objective to explicitly capture
extreme samples in the cost tail distribution. Additionally, we propose an
extreme prioritization mechanism during replay, amplifying the learning signal
from rare but high-impact extreme samples. Theoretically, we establish upper
bounds on expected constraint violations during policy updates, guaranteeing
strict constraint satisfaction at a zero-violation quantile level. Further, we
demonstrate that EVO achieves a lower probability of constraint violations than
expectation-based methods and exhibits lower variance than quantile regression
methods. Extensive experiments show that EVO significantly reduces constraint
violations during training while maintaining competitive policy performance
compared to baselines.
\\ ( https://arxiv.org/abs/2601.12008 ,  6147kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12011
Date: Sat, 17 Jan 2026 11:26:53 GMT   (3762kb)

Title: Why Loss Re-weighting Works If You Stop Early: Training Dynamics of
  Unconstrained Features
Authors: Yize Zhao, Christos Thrampoulidis
Categories: cs.LG
\\
  The application of loss reweighting in modern deep learning presents a
nuanced picture. While it fails to alter the terminal learning phase in
overparameterized deep neural networks (DNNs) trained on high-dimensional
datasets, empirical evidence consistently shows it offers significant benefits
early in training. To transparently demonstrate and analyze this phenomenon, we
introduce a small-scale model (SSM). This model is specifically designed to
abstract the inherent complexities of both the DNN architecture and the input
data, while maintaining key information about the structure of imbalance within
its spectral components. On the one hand, the SSM reveals how vanilla empirical
risk minimization preferentially learns to distinguish majority classes over
minorities early in training, consequently delaying minority learning. In stark
contrast, reweighting restores balanced learning dynamics, enabling the
simultaneous learning of features associated with both majorities and
minorities.
\\ ( https://arxiv.org/abs/2601.12011 ,  3762kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12083
Date: Sat, 17 Jan 2026 15:20:08 GMT   (5406kb)

Title: Learning to Factorize and Adapt: A Versatile Approach Toward Universal
  Spatio-Temporal Foundation Models
Authors: Siru Zhong, Junjie Qiu, Yangyu Wu, Yiqiu Liu, Yuanpeng He, Zhongwen
  Rao, Bin Yang, Chenjuan Guo, Hao Xu, Yuxuan Liang
Categories: cs.LG
Comments: This is an extended version of the paper presented at NeurIPS 2025.
  Code available at https://github.com/CityMind-Lab/FactoST
\\
  Spatio-Temporal (ST) Foundation Models (STFMs) promise cross-dataset
generalization, yet joint ST pretraining is computationally expensive and
grapples with the heterogeneity of domain-specific spatial patterns.
Substantially extending our preliminary conference version, we present
FactoST-v2, an enhanced factorized framework redesigned for full weight
transfer and arbitrary-length generalization. FactoST-v2 decouples universal
temporal learning from domain-specific spatial adaptation. The first stage
pretrains a minimalist encoder-only backbone using randomized sequence masking
to capture invariant temporal dynamics, enabling probabilistic quantile
prediction across variable horizons. The second stage employs a streamlined
adapter to rapidly inject spatial awareness via meta adaptive learning and
prompting. Comprehensive evaluations across diverse domains demonstrate that
FactoST-v2 achieves state-of-the-art accuracy with linear efficiency -
significantly outperforming existing foundation models in zero-shot and
few-shot scenarios while rivaling domain-specific expert baselines. This
factorized paradigm offers a practical, scalable path toward truly universal
STFMs. Code is available at https://github.com/CityMind-Lab/FactoST.
\\ ( https://arxiv.org/abs/2601.12083 ,  5406kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12091
Date: Sat, 17 Jan 2026 16:00:34 GMT   (2126kb)

Title: Mitigating Cultural Bias in LLMs via Multi-Agent Cultural Debate
Authors: Qian Tan, Lei Jiang, Yuting Zeng, Shuoyang Ding, Xiaohua Xu
Categories: cs.LG
Comments: 13 pages
\\
  Large language models (LLMs) exhibit systematic Western-centric bias, yet
whether prompting in non-Western languages (e.g., Chinese) can mitigate this
remains understudied. Answering this question requires rigorous evaluation and
effective mitigation, but existing approaches fall short on both fronts:
evaluation methods force outputs into predefined cultural categories without a
neutral option, while mitigation relies on expensive multi-cultural corpora or
agent frameworks that use functional roles (e.g., Planner--Critique) lacking
explicit cultural representation. To address these gaps, we introduce
CEBiasBench, a Chinese--English bilingual benchmark, and Multi-Agent Vote
(MAV), which enables explicit ``no bias'' judgments. Using this framework, we
find that Chinese prompting merely shifts bias toward East Asian perspectives
rather than eliminating it. To mitigate such persistent bias, we propose
Multi-Agent Cultural Debate (MACD), a training-free framework that assigns
agents distinct cultural personas and orchestrates deliberation via a "Seeking
Common Ground while Reserving Differences" strategy. Experiments demonstrate
that MACD achieves 57.6% average No Bias Rate evaluated by LLM-as-judge and
86.0% evaluated by MAV (vs. 47.6% and 69.0% baseline using GPT-4o as backbone)
on CEBiasBench and generalizes to the Arabic CAMeL benchmark, confirming that
explicit cultural representation in agent frameworks is essential for
cross-cultural fairness.
\\ ( https://arxiv.org/abs/2601.12091 ,  2126kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12093
Date: Sat, 17 Jan 2026 16:09:33 GMT   (7772kb)

Title: PTL-PINNs: Perturbation-Guided Transfer Learning with Physics- Informed
  Neural Networks for Nonlinear Systems
Authors: Duarte Alexandrino, Ben Moseley, Pavlos Protopapas
Categories: cs.LG
Comments: 51 pages, 14 figures, 7 tables
MSC-class: 68T07 (Primary) 37M05, 65L99 (Secondary)
ACM-class: I.6; J.2; I.2
\\
  Accurately and efficiently solving nonlinear differential equations is
crucial for modeling dynamic behavior across science and engineering.
Physics-Informed Neural Networks (PINNs) have emerged as a powerful solution
that embeds physical laws in training by enforcing equation residuals. However,
these struggle to model nonlinear dynamics, suffering from limited
generalization across problems and long training times. To address these
limitations, we propose a perturbation-guided transfer learning framework for
PINNs (PTL-PINN), which integrates perturbation theory with transfer learning
to efficiently solve nonlinear equations. Unlike gradient-based transfer
learning, PTL-PINNs solve an approximate linear perturbative system using
closed-form expressions, enabling rapid generalization with the time complexity
of matrix-vector multiplication. We show that PTL-PINNs achieve accuracy
comparable to various Runge-Kutta methods, with computational speeds up to one
order of magnitude faster. To benchmark performance, we solve a broad set of
problems, including nonlinear oscillators across various damping regimes, the
equilibrium-centered Lotka-Volterra system, the KPP-Fisher and the Wave
equation. Since perturbation theory sets the accuracy bound of PTL-PINNs, we
systematically evaluate its practical applicability. This work connects
long-standing perturbation methods with PINNs, demonstrating how perturbation
theory can guide foundational models to solve nonlinear systems with speeds
comparable to those of classical solvers.
\\ ( https://arxiv.org/abs/2601.12093 ,  7772kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12095
Date: Sat, 17 Jan 2026 16:25:52 GMT   (931kb)

Title: Neural Isomorphic Fields: A Transformer-based Algebraic Numerical
  Embedding
Authors: Hamidreza Sadeghi, Saeedeh Momtazi, Reza Safabakhsh
Categories: cs.LG cs.AI cs.CL
\\
  Neural network models often face challenges when processing very small or
very large numbers due to issues such as overflow, underflow, and unstable
output variations. To mitigate these problems, we propose using embedding
vectors for numbers instead of directly using their raw values. These
embeddings aim to retain essential algebraic properties while preventing
numerical instabilities. In this paper, we introduce, for the first time, a
fixed-length number embedding vector that preserves algebraic operations,
including addition, multiplication, and comparison, within the field of
rational numbers. We propose a novel Neural Isomorphic Field, a neural
abstraction of algebraic structures such as groups and fields. The elements of
this neural field are embedding vectors that maintain algebraic structure
during computations. Our experiments demonstrate that addition performs
exceptionally well, achieving over 95 percent accuracy on key algebraic tests
such as identity, closure, and associativity. In contrast, multiplication
exhibits challenges, with accuracy ranging from 53 percent to 73 percent across
various algebraic properties. These findings highlight the model's strengths in
preserving algebraic properties under addition while identifying avenues for
further improvement in handling multiplication.
\\ ( https://arxiv.org/abs/2601.12095 ,  931kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12124
Date: Sat, 17 Jan 2026 17:51:14 GMT   (398kb)

Title: SynQP: A Framework and Metrics for Evaluating the Quality and Privacy
  Risk of Synthetic Data
Authors: Bing Hu, Yixin Li, Asma Bahamyirou, Helen Chen
Categories: cs.LG cs.AI
Comments: 7 Pages, 22nd Annual International Conference on Privacy, Security,
  and Trust (PST2025), Fredericton, Canada
Journal-ref: 2025 22nd Annual International Conference on Privacy, Security,
  and Trust (PST)
DOI: 10.1109/PST65910.2025.11268831
\\
  The use of synthetic data in health applications raises privacy concerns, yet
the lack of open frameworks for privacy evaluations has slowed its adoption. A
major challenge is the absence of accessible benchmark datasets for evaluating
privacy risks, due to difficulties in acquiring sensitive data. To address
this, we introduce SynQP, an open framework for benchmarking privacy in
synthetic data generation (SDG) using simulated sensitive data, ensuring that
original data remains confidential. We also highlight the need for privacy
metrics that fairly account for the probabilistic nature of machine learning
models. As a demonstration, we use SynQP to benchmark CTGAN and propose a new
identity disclosure risk metric that offers a more accurate estimation of
privacy risks compared to existing approaches. Our work provides a critical
tool for improving the transparency and reliability of privacy evaluations,
enabling safer use of synthetic data in health-related applications. % In our
quality evaluations, non-private models achieved near-perfect machine-learning
efficacy \(\ge0.97\). Our privacy assessments (Table II) reveal that DP
consistently lowers both identity disclosure risk (SD-IDR) and
membership-inference attack risk (SD-MIA), with all DP-augmented models staying
below the 0.09 regulatory threshold. Code available at
https://github.com/CAN-SYNH/SynQP
\\ ( https://arxiv.org/abs/2601.12124 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12131
Date: Sat, 17 Jan 2026 18:18:11 GMT   (1110kb)

Title: SolarGPT-QA: A Domain-Adaptive Large Language Model for Educational
  Question Answering in Space Weather and Heliophysics
Authors: Santosh Chapagain, MohammadReza EskandariNasab, Onur Vural, Shah
  Muhammad Hamdi, Soukaina Filali Boubrahimi
Categories: cs.LG cs.HC
Comments: This is preliminary work towards a broader SolarGPT framework
\\
  Solar activity, including solar flares, coronal mass ejections (CMEs), and
geomagnetic storms, can significantly impact satellites, aviation, power grids,
data centers, and space missions. Extreme solar events can cause substantial
economic damage if not predicted in advance, highlighting the importance of
accurate forecasting and effective education in space science. Although large
language models (LLMs) perform well on general tasks, they often lack
domain-specific knowledge and pedagogical capability to clearly explain complex
space science concepts.
  We introduce SolarGPT-QA, a question answering system based on a
domain-adapted large language model built on the LLaMA-3 base model. The model
is trained using scientific literature and large-scale question-answer data
generated with GPT-4 and refined using Grok-3 in a student-friendly
storytelling style. Human pairwise evaluations show that SolarGPT-QA
outperforms general-purpose models in zero-shot settings and achieves
competitive performance compared to instruction-tuned models for educational
explanations in space weather and heliophysics. A small pilot student
comprehension study further suggests improved clarity and accessibility of the
generated explanations. Ablation experiments indicate that combining
domain-adaptive pretraining with pedagogical fine-tuning is important for
balancing scientific accuracy and educational effectiveness. This work
represents an initial step toward a broader SolarGPT framework for space
science education and forecasting.
\\ ( https://arxiv.org/abs/2601.12131 ,  1110kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12137
Date: Sat, 17 Jan 2026 18:49:25 GMT   (3859kb)

Title: EMoE: Eigenbasis-Guided Routing for Mixture-of-Experts
Authors: Anzhe Cheng, Shukai Duan, Shixuan Li, Chenzhong Yin, Mingxi Cheng,
  Shahin Nazarian, Paul Thompson, Paul Bogdan
Categories: cs.LG cs.CV
Comments: accepted by ICASSP2026
\\
  The relentless scaling of deep learning models has led to unsustainable
computational demands, positioning Mixture-of-Experts (MoE) architectures as a
promising path towards greater efficiency. However, MoE models are plagued by
two fundamental challenges: 1) a load imbalance problem known as the``rich get
richer" phenomenon, where a few experts are over-utilized, and 2) an expert
homogeneity problem, where experts learn redundant representations, negating
their purpose. Current solutions typically employ an auxiliary load-balancing
loss that, while mitigating imbalance, often exacerbates homogeneity by
enforcing uniform routing at the expense of specialization. To resolve this, we
introduce the Eigen-Mixture-of-Experts (EMoE), a novel architecture that
leverages a routing mechanism based on a learned orthonormal eigenbasis. EMoE
projects input tokens onto this shared eigenbasis and routes them based on
their alignment with the principal components of the feature space. This
principled, geometric partitioning of data intrinsically promotes both balanced
expert utilization and the development of diverse, specialized experts, all
without the need for a conflicting auxiliary loss function. Our code is
publicly available at https://github.com/Belis0811/EMoE.
\\ ( https://arxiv.org/abs/2601.12137 ,  3859kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12145
Date: Sat, 17 Jan 2026 19:41:23 GMT   (1399kb)

Title: Threshold Differential Attention for Sink-Free, Ultra-Sparse, and
  Non-Dispersive Language Modeling
Authors: Xingyue Huang, Xueying Ding, Mingxuan Ju, Yozen Liu, Neil Shah, Tong
  Zhao
Categories: cs.LG
\\
  Softmax attention struggles with long contexts due to structural limitations:
the strict sum-to-one constraint forces attention sinks on irrelevant tokens,
and probability mass disperses as sequence lengths increase. We tackle these
problems with Threshold Differential Attention (TDA), a sink-free attention
mechanism that achieves ultra-sparsity and improved robustness at longer
sequence lengths without the computational overhead of projection methods or
the performance degradation caused by noise accumulation of standard rectified
attention. TDA applies row-wise extreme-value thresholding with a
length-dependent gate, retaining only exceedances. Inspired by the differential
transformer, TDA also subtracts an inhibitory view to enhance expressivity.
Theoretically, we prove that TDA controls the expected number of spurious
survivors per row to $O(1)$ and that consensus spurious matches across
independent views vanish as context grows. Empirically, TDA produces $>99\%$
exact zeros and eliminates attention sinks while maintaining competitive
performance on standard and long-context benchmarks.
\\ ( https://arxiv.org/abs/2601.12145 ,  1399kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12178
Date: Sat, 17 Jan 2026 21:41:59 GMT   (267kb)

Title: Federated Learning for the Design of Parametric Insurance Indices under
  Heterogeneous Renewable Production Losses
Authors: Fallou Niakh
Categories: cs.LG stat.ML
\\
  We propose a federated learning framework for the calibration of parametric
insurance indices under heterogeneous renewable energy production losses.
Producers locally model their losses using Tweedie generalized linear models
and private data, while a common index is learned through federated
optimization without sharing raw observations. The approach accommodates
heterogeneity in variance and link functions and directly minimizes a global
deviance objective in a distributed setting. We implement and compare FedAvg,
FedProx and FedOpt, and benchmark them against an existing approximation-based
aggregation method. An empirical application to solar power production in
Germany shows that federated learning recovers comparable index coefficients
under moderate heterogeneity, while providing a more general and scalable
framework.
\\ ( https://arxiv.org/abs/2601.12178 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12212
Date: Sun, 18 Jan 2026 01:31:29 GMT   (193kb)

Title: Speculative Sampling with Reinforcement Learning
Authors: Chenan Wang, Daniel H. Shi, Haipeng Chen
Categories: cs.LG cs.AI
Comments: Accepted to AAAI 2026
\\
  Inference time latency has remained an open challenge for real world
applications of large language models (LLMs). State-of-the-art (SOTA)
speculative sampling (SpS) methods for LLMs, like EAGLE-3, use tree-based
drafting to explore multiple candidate continuations in parallel. However, the
hyperparameters controlling the tree structure are static, which limits
flexibility and efficiency across diverse contexts and domains. We introduce
Reinforcement learning for Speculative Sampling (Re-SpS), the first
reinforcement learning (RL)-based framework for draft tree hyperparameter
optimization. Re-SpS dynamically adjusts draft tree hyperparameters in
real-time, learning context-aware policies that maximize generation speed by
balancing speculative aggression with computational overhead. It leverages
efficient state representations from target model hidden states and introduces
multi-step action persistence for better context modeling. Evaluation results
across five diverse benchmarks demonstrate consistent improvements over the
SOTA method EAGLE-3, achieving up to 5.45$\times$ speedup over the backbone LLM
and up to 1.12$\times$ speedup compared to EAGLE-3 across five diverse
benchmarks, with no loss in output fidelity.
\\ ( https://arxiv.org/abs/2601.12212 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12213
Date: Sun, 18 Jan 2026 01:33:48 GMT   (2457kb)

Title: One-Sided Matrix Completion from Ultra-Sparse Samples
Authors: Hongyang R. Zhang, Zhenshuo Zhang, Huy L. Nguyen, Guanghui Lan
Categories: cs.LG math.OC stat.ML
Comments: 41 pages
Journal-ref: Trans. Mach. Learn. Res. 2026
\\
  Matrix completion is a classical problem that has received recurring interest
across a wide range of fields. In this paper, we revisit this problem in an
ultra-sparse sampling regime, where each entry of an unknown, $n\times d$
matrix $M$ (with $n \ge d$) is observed independently with probability $p = C /
d$, for a fixed integer $C \ge 2$. This setting is motivated by applications
involving large, sparse panel datasets, where the number of rows far exceeds
the number of columns. When each row contains only $C$ entries -- fewer than
the rank of $M$ -- accurate imputation of $M$ is impossible. Instead, we
estimate the row span of $M$ or the averaged second-moment matrix $T = M^{\top}
M / n$.
  The empirical second-moment matrix computed from observed entries exhibits
non-random and sparse missingness. We propose an unbiased estimator that
normalizes each nonzero entry of the second moment by its observed frequency,
followed by gradient descent to impute the missing entries of $T$. The
normalization divides a weighted sum of $n$ binomial random variables by the
total number of ones. We show that the estimator is unbiased for any $p$ and
enjoys low variance. When the row vectors of $M$ are drawn uniformly from a
rank-$r$ factor model satisfying an incoherence condition, we prove that if $n
\ge O({d r^5 \epsilon^{-2} C^{-2} \log d})$, any local minimum of the
gradient-descent objective is approximately global and recovers $T$ with error
at most $\epsilon^2$.
  Experiments on both synthetic and real-world data validate our approach. On
three MovieLens datasets, our algorithm reduces bias by $88\%$ relative to
baseline estimators. We also empirically validate the linear sampling
complexity of $n$ relative to $d$ on synthetic data. On an Amazon reviews
dataset with sparsity $10^{-7}$, our method reduces the recovery error of $T$
by $59\%$ and $M$ by $38\%$ compared to baseline methods.
\\ ( https://arxiv.org/abs/2601.12213 ,  2457kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12215
Date: Sun, 18 Jan 2026 01:34:47 GMT   (2489kb)

Title: Wavelet-Driven Masked Multiscale Reconstruction for PPG Foundation
  Models
Authors: Megha Thukral, Cyrus Tanade, Simon A. Lee, Juhyeon Lee, Hao Zhou, Keum
  San Chun, Migyeong Gwak, Viswam Nathan, Md Mahbubur Rahman, Li Zhu, Mehrab
  Bin Morshed, Subramaniam Venkatraman, Sharanya Arcot Desai
Categories: cs.LG cs.AI
\\
  Wearable foundation models have the potential to transform digital health by
learning transferable representations from large-scale biosignals collected in
everyday settings. While recent progress has been made in large-scale
pretraining, most approaches overlook the spectral structure of
photoplethysmography (PPG) signals, wherein physiological rhythms unfold across
multiple frequency bands. Motivated by the insight that many downstream
health-related tasks depend on multi-resolution features spanning fine-grained
waveform morphology to global rhythmic dynamics, we introduce Masked Multiscale
Reconstruction (MMR) for PPG representation learning - a self-supervised
pretraining framework that explicitly learns from hierarchical time-frequency
scales of PPG data. The pretraining task is designed to reconstruct randomly
masked out coefficients obtained from a wavelet-based multiresolution
decomposition of PPG signals, forcing the transformer encoder to integrate
information across temporal and spectral scales. We pretrain our model with MMR
using ~17 million unlabeled 10-second PPG segments from ~32,000 smartwatch
users. On 17 of 19 diverse health-related tasks, MMR trained on large-scale
wearable PPG data improves over or matches state-of-the-art open-source PPG
foundation models, time-series foundation models, and other self-supervised
baselines. Extensive analysis of our learned embeddings and systematic
ablations underscores the value of wavelet-based representations, showing that
they capture robust and physiologically-grounded features. Together, these
results highlight the potential of MMR as a step toward generalizable PPG
foundation models.
\\ ( https://arxiv.org/abs/2601.12215 ,  2489kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12227
Date: Sun, 18 Jan 2026 02:39:18 GMT   (164kb)

Title: Learning Longitudinal Health Representations from EHR and Wearable Data
Authors: Yuanyun Zhang, Han Zhou, Li Feng, Yilin Hong, Shi Li
Categories: cs.LG
\\
  Foundation models trained on electronic health records show strong
performance on many clinical prediction tasks but are limited by sparse and
irregular documentation. Wearable devices provide dense continuous
physiological signals but lack semantic grounding. Existing methods usually
model these data sources separately or combine them through late fusion. We
propose a multimodal foundation model that jointly represents electronic health
records and wearable data as a continuous time latent process. The model uses
modality specific encoders and a shared temporal backbone pretrained with self
supervised and cross modal objectives. This design produces representations
that are temporally coherent and clinically grounded. Across forecasting
physiological and risk modeling tasks the model outperforms strong electronic
health record only and wearable only baselines especially at long horizons and
under missing data. These results show that joint electronic health record and
wearable pretraining yields more faithful representations of longitudinal
health.
\\ ( https://arxiv.org/abs/2601.12227 ,  164kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12231
Date: Sun, 18 Jan 2026 02:49:48 GMT   (1136kb)

Title: Wavelet-Aware Anomaly Detection in Multi-Channel User Logs via Deviation
  Modulation and Resolution-Adaptive Attention
Authors: Kaichuan Kong, Dongjie Liu, Xiaobo Jin, Shijie Xu, Guanggang Geng
Categories: cs.LG cs.CR stat.CO
Comments: Accepted by ICASSP 2026. Copyright 2026 IEEE. Personal use of this
  material is permitted. Permission from IEEE must be obtained for all other
  uses, including reprinting/republishing this material for advertising or
  promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
\\
  Insider threat detection is a key challenge in enterprise security, relying
on user activity logs that capture rich and complex behavioral patterns. These
logs are often multi-channel, non-stationary, and anomalies are rare, making
anomaly detection challenging. To address these issues, we propose a novel
framework that integrates wavelet-aware modulation, multi-resolution wavelet
decomposition, and resolution-adaptive attention for robust anomaly detection.
Our approach first applies a deviation-aware modulation scheme to suppress
routine behaviors while amplifying anomalous deviations. Next, discrete wavelet
transform (DWT) decomposes the log signals into multi-resolution
representations, capturing both long-term trends and short-term anomalies.
Finally, a learnable attention mechanism dynamically reweights the most
discriminative frequency bands for detection. On the CERT r4.2 benchmark, our
approach consistently outperforms existing baselines in precision, recall, and
F1 score across various time granularities and scenarios.
\\ ( https://arxiv.org/abs/2601.12231 ,  1136kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12288
Date: Sun, 18 Jan 2026 07:02:13 GMT   (558kb)

Title: TimeGMM: Single-Pass Probabilistic Forecasting via Adaptive Gaussian
  Mixture Models with Reversible Normalization
Authors: Lei Liu, Tengyuan Liu, Hongwei Zhao, Jiahui Huang, Ruibo Guo, Bin Li
Categories: cs.LG cs.AI
\\
  Probabilistic time series forecasting is crucial for quantifying future
uncertainty, with significant applications in fields such as energy and
finance. However, existing methods often rely on computationally expensive
sampling or restrictive parametric assumptions to characterize future
distributions, which limits predictive performance and introduces
distributional mismatch. To address these challenges, this paper presents
TimeGMM, a novel probabilistic forecasting framework based on Gaussian Mixture
Models (GMM) that captures complex future distributions in a single forward
pass. A key component is GMM-adapted Reversible Instance Normalization (GRIN),
a novel module designed to dynamically adapt to temporal-probabilistic
distribution shifts. The framework integrates a dedicated Temporal Encoder
(TE-Module) with a Conditional Temporal-Probabilistic Decoder (CTPD-Module) to
jointly capture temporal dependencies and mixture distribution parameters.
Extensive experiments demonstrate that TimeGMM consistently outperforms
state-of-the-art methods, achieving maximum improvements of 22.48\% in CRPS and
21.23\% in NMAE.
\\ ( https://arxiv.org/abs/2601.12288 ,  558kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12296
Date: Sun, 18 Jan 2026 07:49:57 GMT   (75kb)

Title: Distribution Shift Is Key to Learning Invariant Prediction
Authors: Hong Zheng, Fei Teng
Categories: cs.LG
\\
  An interesting phenomenon arises: Empirical Risk Minimization (ERM) sometimes
outperforms methods specifically designed for out-of-distribution tasks. This
motivates an investigation into the reasons behind such behavior beyond
algorithmic design. In this study, we find that one such reason lies in the
distribution shift across training domains. A large degree of distribution
shift can lead to better performance even under ERM. Specifically, we derive
several theoretical and empirical findings demonstrating that distribution
shift plays a crucial role in model learning and benefits learning invariant
prediction. Firstly, the proposed upper bounds indicate that the degree of
distribution shift directly affects the prediction ability of the learned
models. If it is large, the models' ability can increase, approximating
invariant prediction models that make stable predictions under arbitrary known
or unseen domains; and vice versa. We also prove that, under certain data
conditions, ERM solutions can achieve performance comparable to that of
invariant prediction models. Secondly, the empirical validation results
demonstrated that the predictions of learned models approximate those of Oracle
or Optimal models, provided that the degree of distribution shift in the
training data increases.
\\ ( https://arxiv.org/abs/2601.12296 ,  75kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12305
Date: Sun, 18 Jan 2026 08:07:50 GMT   (1405kb)

Title: Machine Learning as a Service (MLaaS) Dataset Generator Framework for
  IoT Environments
Authors: Deepak Kanneganti, Sajib Mistry, Sheik Fattah, Joshua Boland, Aneesh
  Krishna
Categories: cs.LG
\\
  We propose a novel MLaaS Dataset Generator (MDG) framework that creates
configurable and reproducible datasets for evaluating Machine Learning as a
Service (MLaaS) selection and composition. MDG simulates realistic MLaaS
behaviour by training and evaluating diverse model families across multiple
real-world datasets and data distribution settings. It records detailed
functional attributes, quality of service metrics, and composition-specific
indicators, enabling systematic analysis of service performance and
cross-service behaviour. Using MDG, we generate more than ten thousand MLaaS
service instances and construct a large-scale benchmark dataset suitable for
downstream evaluation. We also implement a built-in composition mechanism that
models how services interact under varied Internet of Things conditions.
Experiments demonstrate that datasets generated by MDG enhance selection
accuracy and composition quality compared to existing baselines. MDG provides a
practical and extensible foundation for advancing data-driven research on MLaaS
selection and composition
\\ ( https://arxiv.org/abs/2601.12305 ,  1405kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12317
Date: Sun, 18 Jan 2026 09:00:03 GMT   (904kb)

Title: Explanova: Automatically Discover Data Insights in N \times M Table via
  XAI Combined LLM Workflow
Authors: Yiming Huang
Categories: cs.LG cs.AI
\\
  Automation in data analysis has been a long-time pursuit. Current agentic LLM
shows a promising solution towards it. Like DeepAnalyze, DataSage, and
Datawise. They are all powerful agentic frameworks for automatic fine-grained
analysis and are powered by LLM-based agentic tool calling ability. However,
what about powered by a preset AutoML-like workflow? If we traverse all
possible exploration, like Xn itself`s statistics, Xn1-Xn2 relationships, Xn to
all other, and finally explain? Our Explanova is such an attempt: Cheaper due
to a Local Small LLM.
\\ ( https://arxiv.org/abs/2601.12317 ,  904kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12322
Date: Sun, 18 Jan 2026 09:05:39 GMT   (10691kb)

Title: Ordered Local Momentum for Asynchronous Distributed Learning under
  Arbitrary Delays
Authors: Chang-Wei Shi, Shi-Shang Wang, Wu-Jun Li
Categories: cs.LG
\\
  Momentum SGD (MSGD) serves as a foundational optimizer in training deep
models due to momentum's key role in accelerating convergence and enhancing
generalization. Meanwhile, asynchronous distributed learning is crucial for
training large-scale deep models, especially when the computing capabilities of
the workers in the cluster are heterogeneous. To reduce communication
frequency, local updates are widely adopted in distributed learning. However,
how to implement asynchronous distributed MSGD with local updates remains
unexplored. To solve this problem, we propose a novel method, called
\underline{or}dered \underline{lo}cal \underline{mo}mentum (OrLoMo), for
asynchronous distributed learning. In OrLoMo, each worker runs MSGD locally.
Then the local momentum from each worker will be aggregated by the server in
order based on its global iteration index. To the best of our knowledge, OrLoMo
is the first method to implement asynchronous distributed MSGD with local
updates. We prove the convergence of OrLoMo for non-convex problems under
arbitrary delays. Experiments validate that OrLoMo can outperform its
synchronous counterpart and other asynchronous methods.
\\ ( https://arxiv.org/abs/2601.12322 ,  10691kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12330
Date: Sun, 18 Jan 2026 09:29:40 GMT   (3930kb)

Title: IceWatch: Forecasting Glacial Lake Outburst Floods (GLOFs) using
  Multimodal Deep Learning
Authors: Zuha Fatima, Muhammad Anser Sohaib, Muhammad Talha, Ayesha Kanwal,
  Sidra Sultana, Nazia Perwaiz
Categories: cs.LG cs.AI
\\
  Glacial Lake Outburst Floods (GLOFs) pose a serious threat in high mountain
regions. They are hazardous to communities, infrastructure, and ecosystems
further downstream. The classical methods of GLOF detection and prediction have
so far mainly relied on hydrological modeling, threshold-based lake monitoring,
and manual satellite image analysis. These approaches suffer from several
drawbacks: slow updates, reliance on manual labor, and losses in accuracy when
clouds interfere and/or lack on-site data. To tackle these challenges, we
present IceWatch: a novel deep learning framework for GLOF prediction that
incorporates both spatial and temporal perspectives. The vision component,
RiskFlow, of IceWatch deals with Sentinel-2 multispectral satellite imagery
using a CNN-based classifier and predicts GLOF events based on the spatial
patterns of snow, ice, and meltwater. Its tabular counterpart confirms this
prediction by considering physical dynamics. TerraFlow models glacier velocity
from NASA ITS_LIVE time series while TempFlow forecasts near-surface
temperature from MODIS LST records; both are trained on long-term observational
archives and integrated via harmonized preprocessing and synchronization to
enable multimodal, physics-informed GLOF prediction. Both together provide
cross-validation, which will improve the reliability and interpretability of
GLOF detection. This system ensures strong predictive performance, rapid data
processing for real-time use, and robustness to noise and missing information.
IceWatch paves the way for automatic, scalable GLOF warning systems. It also
holds potential for integration with diverse sensor inputs and global glacier
monitoring activities.
\\ ( https://arxiv.org/abs/2601.12330 ,  3930kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12341
Date: Sun, 18 Jan 2026 10:16:26 GMT   (1474kb)

Title: Time-Continuous Modeling for Temporal Affective Pattern Recognition in
  LLMs
Authors: Rezky Kam, Coddy N. Siswanto
Categories: cs.LG cs.AI cs.ET cs.HC cs.SY eess.SY
\\
  This paper introduces a dataset and conceptual framework for LLMs to mimic
real world emotional dynamics through time and in-context learning leveraging
physics-informed neural network, opening a possibility for interpretable
dialogue modeling.
\\ ( https://arxiv.org/abs/2601.12341 ,  1474kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12355
Date: Sun, 18 Jan 2026 11:26:31 GMT   (3841kb)

Title: LB-MCTS: Synergizing Large Language Models and Bayesian Optimization for
  Efficient CASH
Authors: Beicheng Xu, Weitong Qian, Lingching Tung, Yupeng Lu, Bin Cui
Categories: cs.LG
\\
  To lower the expertise barrier in machine learning, the AutoML community has
focused on the CASH problem, a fundamental challenge that automates the process
of algorithm selection and hyperparameter tuning. While traditional methods
like Bayesian Optimization (BO) struggle with cold-start issues, Large Language
Models (LLMs) can mitigate these via semantic priors. However, existing
LLM-based optimizers generalize poorly to the high-dimensional, structured CASH
space. We propose LB-MCTS, a framework synergizing LLMs and BO within a Monte
Carlo Tree Search structure. It maximizes LLM reasoning with Selective Tuning
Memory (STM) and explicit exploration-exploitation trade-off. It combines the
strengths of both paradigms by dynamically shifting from LLM-driven to
BO-driven proposals as data accumulates. Experiments on 104 AMLB datasets
demonstrate the superiority of LB-MCTS over the competitive baselines.
\\ ( https://arxiv.org/abs/2601.12355 ,  3841kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12362
Date: Sun, 18 Jan 2026 11:34:48 GMT   (2346kb)

Title: Machine Learning-Based Framework for Real Time Detection and Early
  Prediction of Control Valve Stiction in Industrial Control Systems
Authors: Natthapong Promsricha, Chotirawee Chatpattanasiri, Nuttavut
  Kerdgongsup, Stavroula Balabani
Categories: cs.LG physics.ins-det
\\
  Control valve stiction, a friction that prevents smooth valve movement, is a
common fault in industrial process systems that causes instability, equipment
wear, and higher maintenance costs. Many plants still operate with conventional
valves that lack real time monitoring, making early predictions challenging.
This study presents a machine learning (ML) framework for detecting and
predicting stiction using only routinely collected process signals: the
controller output (OP) from control systems and the process variable (PV), such
as flow rate. Three deep learning models were developed and compared: a
Convolutional Neural Network (CNN), a hybrid CNN with a Support Vector Machine
(CNN-SVM), and a Long Short-Term Memory (LSTM) network. To train these models,
a data-driven labeling method based on slope ratio analysis was applied to a
real oil and gas refinery dataset. The LSTM model achieved the highest accuracy
and was able to predict stiction up to four hours in advance. To the best of
the authors' knowledge, this is the first study to demonstrate ML based early
prediction of control valve stiction from real industry data. The proposed
framework can be integrated into existing control systems to support predictive
maintenance, reduce downtime, and avoid unnecessary hardware replacement.
\\ ( https://arxiv.org/abs/2601.12362 ,  2346kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12380
Date: Sun, 18 Jan 2026 12:18:10 GMT   (1974kb)

Title: Statistical-Neural Interaction Networks for Interpretable Mixed-Type
  Data Imputation
Authors: Ou Deng, Shoji Nishimura, Atsushi Ogihara, Qun Jin
Categories: cs.LG stat.ML
\\
  Real-world tabular databases routinely combine continuous measurements and
categorical records, yet missing entries are pervasive and can distort
downstream analysis. We propose Statistical-Neural Interaction (SNI), an
interpretable mixed-type imputation framework that couples correlation-derived
statistical priors with neural feature attention through a Controllable-Prior
Feature Attention (CPFA) module. CPFA learns head-wise prior-strength
coefficients $\{\lambda_h\}$ that softly regularize attention toward the prior
while allowing data-driven deviations when nonlinear patterns appear to be
present in the data. Beyond imputation, SNI aggregates attention maps into a
directed feature-dependency matrix that summarizes which variables the imputer
relied on, without requiring post-hoc explainers. We evaluate SNI against six
baselines (Mean/Mode, MICE, KNN, MissForest, GAIN, MIWAE) on six datasets
spanning ICU monitoring, population surveys, socio-economic statistics, and
engineering applications. Under MCAR/strict-MAR at 30\% missingness, SNI is
generally competitive on continuous metrics but is often outperformed by
accuracy-first baselines (MissForest, MIWAE) on categorical variables; in
return, it provides intrinsic dependency diagnostics and explicit
statistical-neural trade-off parameters. We additionally report MNAR stress
tests (with a mask-aware variant) and discuss computational cost, limitations
-- particularly for severely imbalanced categorical targets -- and deployment
scenarios where interpretability may justify the trade-off.
\\ ( https://arxiv.org/abs/2601.12380 ,  1974kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12401
Date: Sun, 18 Jan 2026 13:25:43 GMT   (18311kb)

Title: Beyond the Dirac Delta: Mitigating Diversity Collapse in Reinforcement
  Fine-Tuning for Versatile Image Generation
Authors: Jinmei Liu, Haoru Li, Zhenhong Sun, Chaofeng Chen, Yatao Bian, Bo
  Wang, Daoyi Dong, Chunlin Chen, Zhi Wang
Categories: cs.LG cs.AI
\\
  Reinforcement learning (RL) has emerged as a powerful paradigm for
fine-tuning large-scale generative models, such as diffusion and flow models,
to align with complex human preferences and user-specified tasks. A fundamental
limitation remains \textit{the curse of diversity collapse}, where the
objective formulation and optimization landscape inherently collapse the policy
to a Dirac delta distribution. To address this challenge, we propose
\textbf{DRIFT} (\textbf{D}ive\textbf{R}sity-\textbf{I}ncentivized Reinforcement
\textbf{F}ine-\textbf{T}uning for Versatile Image Generation), an innovative
framework that systematically incentivizes output diversity throughout the
on-policy fine-tuning process, reconciling strong task alignment with high
generation diversity to enhance versatility essential for applications that
demand diverse candidate generations. We approach the problem across three
representative perspectives: i) \textbf{sampling} a reward-concentrated subset
that filters out reward outliers to prevent premature collapse; ii)
\textbf{prompting} with stochastic variations to expand the conditioning space,
and iii) \textbf{optimization} of the intra-group diversity with a
potential-based reward shaping mechanism. Experimental results show that DRIFT
achieves superior Pareto dominance regarding task alignment and generation
diversity, yielding a $ 9.08\%\!\sim\! 43.46\%$ increase in diversity at
equivalent alignment levels and a $ 59.65\% \!\sim\! 65.86\%$ increase in
alignment at equivalent levels of diversity.
\\ ( https://arxiv.org/abs/2601.12401 ,  18311kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12405
Date: Sun, 18 Jan 2026 13:40:41 GMT   (478kb)

Title: Explainable Machine Learning for Pediatric Dental Risk Stratification
  Using Socio-Demographic Determinants
Authors: Manasi Kanade, Abhi Thakkar, Gabriela Fernandes
Categories: cs.LG cs.AI
\\
  Background: Pediatric dental disease remains one of the most prevalent and
inequitable chronic health conditions worldwide. Although strong
epidemiological evidence links oral health outcomes to socio-economic and
demographic determinants, most artificial intelligence (AI) applications in
dentistry rely on image-based diagnosis and black-box prediction models,
limiting transparency and ethical applicability in pediatric populations.
  Objective: This study aimed to develop and evaluate an explainable machine
learning framework for pediatric dental risk stratification that prioritizes
interpretability, calibration, and ethical deployment over maximal predictive
accuracy.
  Methods: A supervised machine learning model was trained using
population-level pediatric data including age, income-to-poverty ratio,
race/ethnicity, gender, and medical history. Model performance was assessed
using receiver operating characteristic (ROC) analysis and calibration curves.
Explainability was achieved using SHapley Additive exPlanations (SHAP) to
provide global and individual-level interpretation of predictions.
  Results: The model achieved modest discrimination (AUC = 0.61) with
conservative calibration, underestimating risk at higher probability levels.
SHAP analysis identified age and income-to-poverty ratio as the strongest
contributors to predicted risk, followed by race/ethnicity and gender.
  Conclusion: Explainable machine learning enables transparent,
prevention-oriented pediatric dental risk stratification and supports
population screening and equitable resource allocation rather than diagnostic
decision-making.
\\ ( https://arxiv.org/abs/2601.12405 ,  478kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12415
Date: Sun, 18 Jan 2026 13:57:44 GMT   (45kb)

Title: Orthogonalized Policy Optimization:Decoupling Sampling Geometry from
  Optimization Geometry in RLHF
Authors: Wang Zixian
Categories: cs.LG cs.AI
\\
  Recent alignment methods for large language models, including PPO, DPO, and
IPO, are often presented as distinct algorithms. In this work, we show that
many of these approaches implicitly conflate two fundamental and independent
design choices: (i) the sampling geometry, which determines which samples
dominate the gradient signal, and (ii) the optimization geometry, which
determines how deviations in value are penalized. We formalize this observation
by expressing alignment as the minimization of a generalized distance between
policy energy and target energy, parameterized by an alpha-divergence-based
sampling weight and a Bregman-divergence-based value metric. We demonstrate
that the commonly used KL divergence induces an exponential penalty on
unbounded value signals, leading to numerical instability and vanishing
gradients in high-confidence regimes. To address this issue, we propose
Orthogonalized Policy Optimization (OPO), a framework that explicitly decouples
sampling geometry from optimization geometry. By combining alpha-weighted
importance sampling with a chi-square-induced quadratic regularization in ratio
coordinates, OPO yields a simple and well-conditioned objective with linear
gradient dynamics. This formulation maintains stable optimization while
preserving peak-seeking behavior and avoids gradient saturation even when model
confidence is high. Our analysis positions OPO as a unifying perspective on
existing alignment methods and provides a principled foundation for robust
reasoning-oriented training.
\\ ( https://arxiv.org/abs/2601.12415 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12426
Date: Sun, 18 Jan 2026 14:14:47 GMT   (806kb)

Title: Graph Attention Networks with Physical Constraints for Anomaly Detection
Authors: Mohammadhossein Homaei, Iman Khazrak, Ruben Molano, Andres Caro, Mar
  Avila
Categories: cs.LG cs.CR
Comments: 7 Pages, 4 Figures, 5 Tables
\\
  Water distribution systems (WDSs) face increasing cyber-physical risks, which
make reliable anomaly detection essential. Many data-driven models ignore
network topology and are hard to interpret, while model-based ones depend
strongly on parameter accuracy. This work proposes a hydraulic-aware graph
attention network using normalized conservation law violations as features. It
combines mass and energy balance residuals with graph attention and
bidirectional LSTM to learn spatio-temporal patterns. A multi-scale module
aggregates detection scores from node to network level. On the BATADAL dataset,
it reaches $F1=0.979$, showing $3.3$pp gain and high robustness under $15\%$
parameter noise.
\\ ( https://arxiv.org/abs/2601.12426 ,  806kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12442
Date: Sun, 18 Jan 2026 14:57:35 GMT   (19kb)

Title: Constraint-Aware Neurosymbolic Uncertainty Quantification with Bayesian
  Deep Learning for Scientific Discovery
Authors: Shahnawaz Alam, Mohammed Mudassir Uddin, Mohammed Kaif Pasha
Categories: cs.LG cs.AI cs.CV
\\
  Scientific Artificial Intelligence (AI) applications require models that
deliver trustworthy uncertainty estimates while respecting domain constraints.
Existing uncertainty quantification methods lack mechanisms to incorporate
symbolic scientific knowledge, while neurosymbolic approaches operate
deterministically without principled uncertainty modeling. We introduce the
Constraint-Aware Neurosymbolic Uncertainty Framework (CANUF), unifying Bayesian
deep learning with differentiable symbolic reasoning. The architecture
comprises three components: automated constraint extraction from scientific
literature, probabilistic neural backbone with variational inference, and
differentiable constraint satisfaction layer ensuring physical consistency.
Experiments on Materials Project (140,000+ materials), QM9 molecular
properties, and climate benchmarks show CANUF reduces Expected Calibration
Error by 34.7% versus Bayesian neural networks while maintaining 99.2%
constraint satisfaction. Ablations reveal constraint-guided recalibration
contributes 18.3% performance gain, with constraint extraction achieving 91.4%
precision. CANUF provides the first end-to-end differentiable pipeline
simultaneously addressing uncertainty quantification, constraint satisfaction,
and interpretable explanations for scientific predictions.
\\ ( https://arxiv.org/abs/2601.12442 ,  19kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12467
Date: Sun, 18 Jan 2026 16:16:01 GMT   (61kb)

Title: Patch-Level Tokenization with CNN Encoders and Attention for Improved
  Transformer Time-Series Forecasting
Authors: Saurish Nagrath
Categories: cs.LG cs.AI
Comments: 6 pages, 2 figures, 3 tables
\\
  Transformer-based models have shown strong performance in time-series
forecasting by leveraging self-attention to model long-range temporal
dependencies. However, their effectiveness depends critically on the quality
and structure of input representations derived from raw multivariate
time-series data. This work proposes a two-stage forecasting framework that
explicitly separates local temporal representation learning from global
dependency modelling. In the first stage, a convolutional neural network (CNN)
operates on fixed-length temporal patches to extract short-range temporal
dynamics and non-linear feature interactions, producing compact patch-level
token embeddings. Token-level self-attention is subsequently applied during
representation learning to refine these embeddings by enabling interactions
across temporal patches. In the second stage, a Transformer encoder processes
the resulting token sequence to model inter-patch temporal dependencies and
generate per-patch forecasts. Experiments conducted on synthetic multivariate
time-series data with controlled static and dynamic factors demonstrate that
the proposed patch-based tokenization strategy achieves competitive forecasting
performance compared to convolutional and patch-based Transformer baselines.
The results highlight the importance of structured temporal representations and
show that decoupling local temporal encoding from global attention-based
modelling yields more effective and stable time-series forecasting.
\\ ( https://arxiv.org/abs/2601.12467 ,  61kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12502
Date: Sun, 18 Jan 2026 17:26:45 GMT   (102kb)

Title: Semidefinite Programming for Quantum Channel Learning
Authors: Mikhail Gennadievich Belov, Victor Victorovich Dubov, Vadim
  Konstantinovich Ivanov, Alexander Yurievich Maslov, Olga Vladimirovna
  Proshina, Vladislav Gennadievich Malyshkin
Categories: cs.LG cs.NA math.NA quant-ph
\\
  The problem of reconstructing a quantum channel from a sample of classical
data is considered. When the total fidelity can be represented as a ratio of
two quadratic forms (e.g., in the case of mapping a mixed state to a pure
state, projective operators, unitary learning, and others), Semidefinite
Programming (SDP) can be applied to solve the fidelity optimization problem
with respect to the Choi matrix. A remarkable feature of SDP is that the
optimization is convex, which allows the problem to be efficiently solved by a
variety of numerical algorithms. We have tested several commercially available
SDP solvers, all of which allowed for the reconstruction of quantum channels of
different forms. A notable feature is that the Kraus rank of the obtained
quantum channel typically comprises less than a few percent of its maximal
possible value. This suggests that a relatively small Kraus rank quantum
channel is typically sufficient to describe experimentally observed classical
data. The theory was also applied to the problem of reconstructing projective
operators from data. Finally, we discuss a classical computational model based
on quantum channel transformation, performed and calculated on a classical
computer, possibly hardware-optimized.
\\ ( https://arxiv.org/abs/2601.12502 ,  102kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12518
Date: Sun, 18 Jan 2026 18:05:23 GMT   (4161kb)

Title: Cooperative Multi-agent RL with Communication Constraints
Authors: Nuoya Xiong, Aarti Singh
Categories: cs.LG cs.AI stat.ML
Comments: 33 pages
\\
  Cooperative MARL often assumes frequent access to global information in a
data buffer, such as team rewards or other agents' actions, which is typically
unrealistic in decentralized MARL systems due to high communication costs. When
communication is limited, agents must rely on outdated information to estimate
gradients and update their policies. A common approach to handle missing data
is called importance sampling, in which we reweigh old data from a base policy
to estimate gradients for the current policy. However, it quickly becomes
unstable when the communication is limited (i.e. missing data probability is
high), so that the base policy in importance sampling is outdated. To address
this issue, we propose a technique called base policy prediction, which
utilizes old gradients to predict the policy update and collect samples for a
sequence of base policies, which reduces the gap between the base policy and
the current policy. This approach enables effective learning with significantly
fewer communication rounds, since the samples of predicted base policies could
be collected within one communication round. Theoretically, we show that our
algorithm converges to an $\varepsilon$-Nash equilibrium in potential games
with only $O(\varepsilon^{-3/4})$ communication rounds and $O(poly(\max_i
|A_i|)\varepsilon^{-11/4})$ samples, improving existing state-of-the-art
results in communication cost, as well as sample complexity without the
exponential dependence on the joint action space size. We also extend these
results to general Markov Cooperative Games to find an agent-wise local
maximum. Empirically, we test the base policy prediction algorithm in both
simulated games and MAPPO for complex environments.
\\ ( https://arxiv.org/abs/2601.12518 ,  4161kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12519
Date: Sun, 18 Jan 2026 18:09:26 GMT   (12139kb)

Title: Learning Relativistic Geodesics and Chaotic Dynamics via Stabilized
  Lagrangian Neural Networks
Authors: Abdullah Umut Hamzaogullari, Arkadas Ozakin
Categories: cs.LG
Comments: 21 pages
\\
  Lagrangian Neural Networks (LNNs) can learn arbitrary Lagrangians from
trajectory data, but their unusual optimization objective leads to significant
training instabilities that limit their application to complex systems. We
propose several improvements that address these fundamental challenges, namely,
a Hessian regularization scheme that penalizes unphysical signatures in the
Lagrangian's second derivatives with respect to velocities, preventing the
network from learning unstable dynamics, activation functions that are better
suited to the problem of learning Lagrangians, and a physics-aware coordinate
scaling that improves stability. We systematically evaluate these techniques
alongside previously proposed methods for improving stability. Our improved
architecture successfully trains on systems of unprecedented complexity,
including triple pendulums, and achieved 96.6\% lower validation loss value and
90.68\% better stability than baseline LNNs in double pendulum systems. With
the improved framework, we show that our LNNs can learn Lagrangians
representing geodesic motion in both non-relativistic and general relativistic
settings. To deal with the relativistic setting, we extended our regularization
to penalize violations of Lorentzian signatures, which allowed us to predict a
geodesic Lagrangian under AdS\textsubscript{4} spacetime metric directly from
trajectory data, which to our knowledge has not been done in the literature
before. This opens new possibilities for automated discovery of geometric
structures in physics, including extraction of spacetime metric tensor
components from geodesic trajectories. While our approach inherits some
limitations of the original LNN framework, particularly the requirement for
invertible Hessians, it significantly expands the practical applicability of
LNNs for scientific discovery tasks.
\\ ( https://arxiv.org/abs/2601.12519 ,  12139kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12525
Date: Sun, 18 Jan 2026 18:19:28 GMT   (72kb)

Title: Approximating splits for decision trees quickly in sparse data streams
Authors: Nikolaj Tatti
Categories: cs.LG cs.DS
Journal-ref: In Proceedings of the 2025 SIAM International Conference on Data
  Mining (SDM) (pp. 647-655) 2025
DOI: 10.1137/1.9781611978520.69
\\
  Decision trees are one of the most popular classifiers in the machine
learning literature. While the most common decision tree learning algorithms
treat data as a batch, numerous algorithms have been proposed to construct
decision trees from a data stream. A standard training strategy involves
augmenting the current tree by changing a leaf node into a split. Here we
typically maintain counters in each leaf which allow us to determine the
optimal split, and whether the split should be done. In this paper we focus on
how to speed up the search for the optimal split when dealing with sparse
binary features and a binary class. We focus on finding splits that have the
approximately optimal information gain or Gini index. In both cases finding the
optimal split can be done in $O(d)$ time, where $d$ is the number of features.
We propose an algorithm that yields $(1 + \alpha)$ approximation when using
conditional entropy in amortized $O(\alpha^{-1}(1 + m\log d) \log \log n)$
time, where $m$ is the number of 1s in a data point, and $n$ is the number of
data points. Similarly, for Gini index, we achieve $(1 + \alpha)$ approximation
in amortized $O(\alpha^{-1} + m \log d)$ time. Our approach is beneficial for
sparse data where $m \ll d$. In our experiments we find almost-optimal splits
efficiently, faster than the baseline, overperforming the theoretical
approximation guarantees.
\\ ( https://arxiv.org/abs/2601.12525 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12543
Date: Sun, 18 Jan 2026 19:15:29 GMT   (2386kb)

Title: Press Start to Charge: Videogaming the Online Centralized Charging
  Scheduling Problem
Authors: Alireza Ghahtarani, Martin Cousineau, Amir-massoud Farahmand, Jorge E.
  Mendoza
Categories: cs.LG
Comments: 41 pages
\\
  We study the online centralized charging scheduling problem (OCCSP). In this
problem, a central authority must decide, in real time, when to charge
dynamically arriving electric vehicles (EVs), subject to capacity limits, with
the objective of balancing load across a finite planning horizon. To solve the
problem, we first gamify it; that is, we model it as a game where charging
blocks are placed within temporal and capacity constraints on a grid. We design
heuristic policies, train learning agents with expert demonstrations, and
improve them using Dataset Aggregation (DAgger). From a theoretical standpoint,
we show that gamification reduces model complexity and yields tighter
generalization bounds than vector-based formulations. Experiments across
multiple EV arrival patterns confirm that gamified learning enhances load
balancing. In particular, the image-to-movement model trained with DAgger
consistently outperforms heuristic baselines, vector-based approaches, and
supervised learning agents, while also demonstrating robustness in sensitivity
analyses. These operational gains translate into tangible economic value. In a
real-world case study for the Greater Montr\'eal Area (Qu\'ebec, Canada) using
utility cost data, the proposed methods lower system costs by tens of millions
of dollars per year over the prevailing practice and show clear potential to
delay costly grid upgrades.
\\ ( https://arxiv.org/abs/2601.12543 ,  2386kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12557
Date: Sun, 18 Jan 2026 19:43:48 GMT   (2242kb)

Title: Life, Machine Learning, and the Search for Habitability: Predicting
  Biosignature Fluxes for the Habitable Worlds Observatory
Authors: Mark Moussa, Amber V. Young, Brianna Isola, Vasuda Trehan, Michael D.
  Himes, Nicholas Wogan, Giada Arney
Categories: cs.LG cs.AI cs.CV
Comments: 8 pages, 4 figures. Submitted and accepted in AAAI-26 (IAAI Emerging
  Applications track)
\\
  Future direct-imaging flagship missions, such as NASA's Habitable Worlds
Observatory (HWO), face critical decisions in prioritizing observations due to
extremely stringent time and resource constraints. In this paper, we introduce
two advanced machine-learning architectures tailored for predicting
biosignature species fluxes from exoplanetary reflected-light spectra: a
Bayesian Convolutional Neural Network (BCNN) and our novel model architecture,
the Spectral Query Adaptive Transformer (SQuAT). The BCNN robustly quantifies
both epistemic and aleatoric uncertainties, offering reliable predictions under
diverse observational conditions, whereas SQuAT employs query-driven attention
mechanisms to enhance interpretability by explicitly associating spectral
features with specific biosignature species. We demonstrate that both models
achieve comparably high predictive accuracy on an augmented dataset spanning a
wide range of exoplanetary conditions, while highlighting their distinct
advantages in uncertainty quantification and spectral interpretability. These
capabilities position our methods as promising tools for accelerating target
triage, optimizing observation schedules, and maximizing scientific return for
upcoming flagship missions such as HWO.
\\ ( https://arxiv.org/abs/2601.12557 ,  2242kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12598
Date: Sun, 18 Jan 2026 21:49:21 GMT   (249kb)

Title: Dissecting Linear Recurrent Models: How Different Gating Strategies
  Drive Selectivity and Generalization
Authors: Younes Bouhadjar, Maxime Fabre, Felix Schmidt, Emre Neftci
Categories: cs.LG cs.CL
Comments: 11 pages, 4 figures and 4 tables
\\
  Linear recurrent neural networks have emerged as efficient alternatives to
the original Transformer's softmax attention mechanism, thanks to their highly
parallelizable training and constant memory and computation requirements at
inference. Iterative refinements of these models have introduced an increasing
number of architectural mechanisms, leading to increased complexity and
computational costs. Nevertheless, systematic direct comparisons among these
models remain limited. Existing benchmark tasks are either too simplistic to
reveal substantial differences or excessively resource-intensive for
experimentation. In this work, we propose a refined taxonomy of linear
recurrent models and introduce SelectivBench, a set of lightweight and
customizable synthetic benchmark tasks for systematically evaluating sequence
models. SelectivBench specifically evaluates selectivity in sequence models at
small to medium scale, such as the capacity to focus on relevant inputs while
ignoring context-based distractors. It employs rule-based grammars to generate
sequences with adjustable complexity, incorporating irregular gaps that
intentionally violate transition rules. Evaluations of linear recurrent models
on SelectivBench reveal performance patterns consistent with results from
large-scale language tasks. Our analysis clarifies the roles of essential
architectural features: gating and rapid forgetting mechanisms facilitate
recall, in-state channel mixing is unnecessary for selectivity, but critical
for generalization, and softmax attention remains dominant due to its memory
capacity scaling with sequence length. Our benchmark enables targeted,
efficient exploration of linear recurrent models and provides a controlled
setting for studying behaviors observed in large-scale evaluations. Code is
available at https://github.com/symseqbench/selectivbench
\\ ( https://arxiv.org/abs/2601.12598 ,  249kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12604
Date: Sun, 18 Jan 2026 22:25:03 GMT   (633kb)

Title: Beyond Softmax and Entropy: Improving Convergence Guarantees of Policy
  Gradients by f-SoftArgmax Parameterization with Coupled Regularization
Authors: Safwan Labbi, Daniil Tiapkin, Paul Mangold, Eric Moulines
Categories: cs.LG
\\
  Policy gradient methods are known to be highly sensitive to the choice of
policy parameterization. In particular, the widely used softmax
parameterization can induce ill-conditioned optimization landscapes and lead to
exponentially slow convergence. Although this can be mitigated by
preconditioning, this solution is often computationally expensive. Instead, we
propose replacing the softmax with an alternative family of policy
parameterizations based on the generalized f-softargmax. We further advocate
coupling this parameterization with a regularizer induced by the same
f-divergence, which improves the optimization landscape and ensures that the
resulting regularized objective satisfies a Polyak-Lojasiewicz inequality.
Leveraging this structure, we establish the first explicit non-asymptotic
last-iterate convergence guarantees for stochastic policy gradient methods for
finite MDPs without any form of preconditioning. We also derive
sample-complexity bounds for the unregularized problem and show that f-PG, with
Tsallis divergences achieves polynomial sample complexity in contrast to the
exponential complexity incurred by the standard softmax parameterization.
\\ ( https://arxiv.org/abs/2601.12604 ,  633kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12612
Date: Sun, 18 Jan 2026 23:04:17 GMT   (505kb)

Title: What Trace Powers Reveal About Log-Determinants: Closed-Form Estimators,
  Certificates, and Failure Modes
Authors: Piyush Sao
Categories: cs.LG stat.ML
MSC-class: 65F40, 15A15, 60E05
ACM-class: G.1.3; G.3
\\
  Computing $\log\det(A)$ for large symmetric positive definite matrices arises
in Gaussian process inference and Bayesian model comparison. Standard methods
combine matrix-vector products with polynomial approximations. We study a
different model: access to trace powers $p_k = \tr(A^k)$, natural when matrix
powers are available.
  Classical moment-based approximations Taylor-expand $\log(\lambda)$ around
the arithmetic mean. This requires $|\lambda - \AM| < \AM$ and diverges when
$\kappa > 4$. We work instead with the moment-generating function $M(t) =
\E[X^t]$ for normalized eigenvalues $X = \lambda/\AM$. Since $M'(0) = \E[\log
X]$, the log-determinant becomes $\log\det(A) = n(\log \AM + M'(0))$ -- the
problem reduces to estimating a derivative at $t = 0$. Trace powers give $M(k)$
at positive integers, but interpolating $M(t)$ directly is ill-conditioned due
to exponential growth. The transform $K(t) = \log M(t)$ compresses this range.
Normalization by $\AM$ ensures $K(0) = K(1) = 0$. With these anchors fixed, we
interpolate $K$ through $m+1$ consecutive integers and differentiate to
estimate $K'(0)$. However, this local interpolation cannot capture arbitrary
spectral features.
  We prove a fundamental limit: no continuous estimator using finitely many
positive moments can be uniformly accurate over unbounded conditioning.
Positive moments downweight the spectral tail; $K'(0) = \E[\log X]$ is
tail-sensitive. This motivates guaranteed bounds. From the same traces we
derive upper bounds on $(\det A)^{1/n}$. Given a spectral floor $r \leq
\lambda_{\min}$, we obtain moment-constrained lower bounds, yielding a provable
interval for $\log\det(A)$. A gap diagnostic indicates when to trust the point
estimate and when to report bounds. All estimators and bounds cost $O(m)$,
independent of $n$. For $m \in \{4, \ldots, 8\}$, this is effectively constant
time.
\\ ( https://arxiv.org/abs/2601.12612 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12624
Date: Sun, 18 Jan 2026 23:31:12 GMT   (2263kb)

Title: Towards Robust Universal Perturbation Attacks: A Float-Coded,
  Penalty-Driven Evolutionary Approach
Authors: Shiqi Wang, Mahdi Khosravy, Neeraj Gupta, Olaf Witkowski
Categories: cs.LG cs.CV
\\
  Universal adversarial perturbations (UAPs) have garnered significant
attention due to their ability to undermine deep neural networks across
multiple inputs using a single noise pattern. Evolutionary algorithms offer a
promising approach to generating such perturbations due to their ability to
navigate non-convex, gradient-free landscapes. In this work, we introduce a
float-coded, penalty-driven single-objective evolutionary framework for UAP
generation that achieves lower visibility perturbations while enhancing attack
success rates. Our approach leverages continuous gene representations aligned
with contemporary deep learning scales, incorporates dynamic evolutionary
operators with adaptive scheduling, and utilizes a modular PyTorch
implementation for seamless integration with modern architectures.
Additionally, we ensure the universality of the generated perturbations by
testing across diverse models and by periodically switching batches to prevent
overfitting. Experimental results on the ImageNet dataset demonstrate that our
framework consistently produces perturbations with smaller norms, higher
misclassification effectiveness, and faster convergence compared to existing
evolutionary-based methods. These findings highlight the robustness and
scalability of our approach for universal adversarial attacks across various
deep learning architectures.
\\ ( https://arxiv.org/abs/2601.12624 ,  2263kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12637
Date: Mon, 19 Jan 2026 00:54:24 GMT   (446kb)

Title: Topology-Aware Multiscale Mixture of Experts for Efficient Molecular
  Property Prediction
Authors: Long D. Nguyen, Kelin Xia, Binh P. Nguyen
Categories: cs.LG cs.AI q-bio.QM
\\
  Many molecular properties depend on 3D geometry, where non-covalent
interactions, stereochemical effects, and medium- to long-range forces are
determined by spatial distances and angles that cannot be uniquely captured by
a 2D bond graph. Yet most 3D molecular graph neural networks still rely on
globally fixed neighborhood heuristics, typically defined by distance cutoffs
and maximum neighbor limits, to define local message-passing neighborhoods,
leading to rigid, data-agnostic interaction budgets. We propose Multiscale
Interaction Mixture of Experts (MI-MoE) to adapt interaction modeling across
geometric regimes. Our contributions are threefold: (1) we introduce a
distance-cutoff expert ensemble that explicitly captures short-, mid-, and
long-range interactions without committing to a single cutoff; (2) we design a
topological gating encoder that routes inputs to experts using filtration-based
descriptors, including persistent homology features, summarizing how
connectivity evolves across radii; and (3) we show that MI-MoE is a plug-in
module that consistently improves multiple strong 3D molecular backbones across
diverse molecular and polymer property prediction benchmark datasets, covering
both regression and classification tasks. These results highlight
topology-aware multiscale routing as an effective principle for 3D molecular
graph learning.
\\ ( https://arxiv.org/abs/2601.12637 ,  446kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12654
Date: Mon, 19 Jan 2026 02:01:18 GMT   (668kb)

Title: Explanation Multiplicity in SHAP: Characterization and Assessment
Authors: Hyunseung Hwang, Seungeun Lee, Lucas Rosenblatt, Julia Stoyanovich,
  Steven Euijong Whang
Categories: cs.LG cs.AI
\\
  Post-hoc explanations are widely used to justify, contest, and audit
automated decisions in high-stakes domains. SHAP, in particular, is often
treated as a reliable account of which features drove an individual prediction.
Yet SHAP explanations can vary substantially across repeated runs even when the
input, task, and trained model are held fixed. We term this phenomenon
explanation multiplicity: multiple internally valid but substantively different
explanations for the same decision. We present a methodology to characterize
multiplicity in feature-attribution explanations and to disentangle sources due
to model training/selection from stochasticity intrinsic to the explanation
pipeline. We further show that apparent stability depends on the metric:
magnitude-based distances can remain near zero while rank-based measures reveal
substantial churn in the identity and ordering of top features. To
contextualize observed disagreement, we derive randomized baseline values under
plausible null models. Across datasets, model classes, and confidence regimes,
we find explanation multiplicity is pervasive and persists even for
high-confidence predictions, highlighting the need for metrics and baselines
that match the intended use of explanations.
\\ ( https://arxiv.org/abs/2601.12654 ,  668kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12662
Date: Mon, 19 Jan 2026 02:18:45 GMT   (1345kb)

Title: Decentralized Learning Strategies for Estimation Error Minimization with
  Graph Neural Networks
Authors: Xingran Chen, Navid NaderiAlizadeh, Alejandro Ribeiro, Shirin Saeedi
  Bidokhti
Categories: cs.LG eess.SP
\\
  We address real-time sampling and estimation of autoregressive Markovian
sources in dynamic yet structurally similar multi-hop wireless networks. Each
node caches samples from others and communicates over wireless collision
channels, aiming to minimize time-average estimation error via decentralized
policies. Due to the high dimensionality of action spaces and complexity of
network topologies, deriving optimal policies analytically is intractable. To
address this, we propose a graphical multi-agent reinforcement learning
framework for policy optimization. Theoretically, we demonstrate that our
proposed policies are transferable, allowing a policy trained on one graph to
be effectively applied to structurally similar graphs. Numerical experiments
demonstrate that (i) our proposed policy outperforms state-of-the-art
baselines; (ii) the trained policies are transferable to larger networks, with
performance gains increasing with the number of agents; (iii) the graphical
training procedure withstands non-stationarity, even when using independent
learning techniques; and (iv) recurrence is pivotal in both independent
learning and centralized training and decentralized execution, and improves the
resilience to non-stationarity.
\\ ( https://arxiv.org/abs/2601.12662 ,  1345kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12680
Date: Mon, 19 Jan 2026 02:53:31 GMT   (111kb)

Title: MetaToolAgent: Towards Generalizable Tool Usage in LLMs through
  Meta-Learning
Authors: Zheng Fang, Wolfgang Mayer, Zeyu Zhang, Jian Wang, Hong-Yu Zhang,
  Wanli Li, Zaiwen Feng
Categories: cs.LG
\\
  Tool learning is increasingly important for large language models (LLMs) to
effectively coordinate and utilize a diverse set of tools in order to solve
complex real-world tasks. By selecting and integrating appropriate tools, LLMs
extend their capabilities beyond pure language understanding to perform
specialized functions. However, existing methods for tool selection often focus
on limited tool sets and struggle to generalize to novel tools encountered in
practical deployments. To address these challenges, we introduce a
comprehensive dataset spanning 7 domains, containing 155 tools and 9,377
question-answer pairs, which simulates realistic integration scenarios.
Additionally, we propose MetaToolAgent (MTA), a meta-learning approach designed
to improve cross-tool generalization. Experimental results show that MTA
significantly outperforms baseline methods on unseen tools, demonstrating its
promise for building flexible and scalable systems that require dynamic tool
coordination.
\\ ( https://arxiv.org/abs/2601.12680 ,  111kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12699
Date: Mon, 19 Jan 2026 03:45:08 GMT   (6487kb)

Title: Resource-Conscious RL Algorithms for Deep Brain Stimulation
Authors: Arkaprava Gupta, Nicholas Carter, William Zellers, Prateek Ganguli,
  Benedikt Dietrich, Vibhor Krishna, Parasara Sridhar Duggirala, Samarjit
  Chakraborty
Categories: cs.LG cs.SY eess.SY
\\
  Deep Brain Stimulation (DBS) has proven to be a promising treatment of
Parkinson's Disease (PD). DBS involves stimulating specific regions of the
brain's Basal Ganglia (BG) using electric impulses to alleviate symptoms of PD
such as tremors, rigidity, and bradykinesia. Although most clinical DBS
approaches today use a fixed frequency and amplitude, they suffer from side
effects (such as slurring of speech) and shortened battery life of the implant.
Reinforcement learning (RL) approaches have been used in recent research to
perform DBS in a more adaptive manner to improve overall patient outcome. These
RL algorithms are, however, too complex to be trained in vivo due to their long
convergence time and requirement of high computational resources.
  We propose a new Time & Threshold-Triggered Multi-Armed Bandit (T3P MAB) RL
approach for DBS that is more effective than existing algorithms. Further, our
T3P agent is lightweight enough to be deployed in the implant, unlike current
deep-RL strategies, and even forgoes the need for an offline training phase.
Additionally, most existing RL approaches have focused on modulating only
frequency or amplitude, and the possibility of tuning them together remains
greatly unexplored in the literature. Our RL agent can tune both frequency and
amplitude of DBS signals to the brain with better sample efficiency and
requires minimal time to converge. We implement an MAB agent for DBS for the
first time on hardware to report energy measurements and prove its suitability
for resource-constrained platforms. Our T3P MAB algorithm is deployed on a
variety of microcontroller unit (MCU) setups to show its efficiency in terms of
power consumption as opposed to other existing RL approaches used in recent
work.
\\ ( https://arxiv.org/abs/2601.12699 ,  6487kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12703
Date: Mon, 19 Jan 2026 04:01:17 GMT   (10281kb)

Title: Towards Spectroscopy: Susceptibility Clusters in Language Models
Authors: Andrew Gordon, Garrett Baker, George Wang, William Snell, Stan van
  Wingerden, Daniel Murfet
Categories: cs.LG
\\
  Spectroscopy infers the internal structure of physical systems by measuring
their response to perturbations. We apply this principle to neural networks:
perturbing the data distribution by upweighting a token $y$ in context $x$, we
measure the model's response via susceptibilities $\chi_{xy}$, which are
covariances between component-level observables and the perturbation computed
over a localized Gibbs posterior via stochastic gradient Langevin dynamics
(SGLD). Theoretically, we show that susceptibilities decompose as a sum over
modes of the data distribution, explaining why tokens that follow their
contexts "for similar reasons" cluster together in susceptibility space.
Empirically, we apply this methodology to Pythia-14M, developing a
conductance-based clustering algorithm that identifies 510 interpretable
clusters ranging from grammatical patterns to code structure to mathematical
notation. Comparing to sparse autoencoders, 50% of our clusters match SAE
features, validating that both methods recover similar structure.
\\ ( https://arxiv.org/abs/2601.12703 ,  10281kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12704
Date: Mon, 19 Jan 2026 04:05:03 GMT   (1970kb)

Title: Adaptively trained Physics-informed Radial Basis Function Neural
  Networks for Solving Multi-asset Option Pricing Problems
Authors: Yan Ma, Yumeng Ren
Categories: cs.LG
Comments: 30 pages,16 figures
\\
  The present study investigates the numerical solution of Black-Scholes
partial differential equation (PDE) for option valuation with multiple
underlying assets. We develop a physics-informed (PI) machine learning
algorithm based on a radial basis function neural network (RBFNN) that
concurrently optimizes the network architecture and predicts the target option
price. The physics-informed radial basis function neural network (PIRBFNN)
combines the strengths of the traditional radial basis function collocation
method and the physics-informed neural network machine learning approach to
effectively solve PDE problems in the financial context. By employing a PDE
residual-based technique to adaptively refine the distribution of hidden
neurons during the training process, the PIRBFNN facilitates accurate and
efficient handling of multidimensional option pricing models featuring
non-smooth payoff conditions. The validity of the proposed method is
demonstrated through a set of experiments encompassing a single-asset European
put option, a double-asset exchange option, and a four-asset basket call
option.
\\ ( https://arxiv.org/abs/2601.12704 ,  1970kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12706
Date: Mon, 19 Jan 2026 04:09:53 GMT   (357kb)

Title: Trend-Adjusted Time Series Models with an Application to Gold Price
  Forecasting
Authors: Sina Kazemdehbashi
Categories: cs.LG
\\
  Time series data play a critical role in various fields, including finance,
healthcare, marketing, and engineering. A wide range of techniques (from
classical statistical models to neural network-based approaches such as Long
Short-Term Memory (LSTM)) have been employed to address time series forecasting
challenges. In this paper, we reframe time series forecasting as a two-part
task: (1) predicting the trend (directional movement) of the time series at the
next time step, and (2) forecasting the quantitative value at the next time
step. The trend can be predicted using a binary classifier, while quantitative
values can be forecasted using models such as LSTM and Bidirectional Long
Short-Term Memory (Bi-LSTM). Building on this reframing, we propose the
Trend-Adjusted Time Series (TATS) model, which adjusts the forecasted values
based on the predicted trend provided by the binary classifier. We validate the
proposed approach through both theoretical analysis and empirical evaluation.
The TATS model is applied to a volatile financial time series (the daily gold
price) with the objective of forecasting the next days price. Experimental
results demonstrate that TATS consistently outperforms standard LSTM and
Bi-LSTM models by achieving significantly lower forecasting error. In addition,
our results indicate that commonly used metrics such as MSE and MAE are
insufficient for fully assessing time series model performance. Therefore, we
also incorporate trend detection accuracy, which measures how effectively a
model captures trends in a time series.
\\ ( https://arxiv.org/abs/2601.12706 ,  357kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12707
Date: Mon, 19 Jan 2026 04:12:51 GMT   (668kb)

Title: Decoding Rewards in Competitive Games: Inverse Game Theory with Entropy
  Regularization
Authors: Junyi Liao and Zihan Zhu and Ethan Fang and Zhuoran Yang and Vahid
  Tarokh
Categories: cs.LG stat.ML
Comments: Extended journal version of ICML 2025 paper. Submitted to Operations
  Research
ACM-class: I.2.6; F.2.1
\\
  Estimating the unknown reward functions driving agents' behaviors is of
central interest in inverse reinforcement learning and game theory. To tackle
this problem, we develop a unified framework for reward function recovery in
two-player zero-sum matrix games and Markov games with entropy regularization,
where we aim to reconstruct the underlying reward functions given observed
players' strategies and actions. This task is challenging due to the inherent
ambiguity of inverse problems, the non-uniqueness of feasible rewards, and
limited observational data coverage. To address these challenges, we establish
the reward function's identifiability using the quantal response equilibrium
(QRE) under linear assumptions. Building upon this theoretical foundation, we
propose a novel algorithm to learn reward functions from observed actions. Our
algorithm works in both static and dynamic settings and is adaptable to
incorporate different methods, such as Maximum Likelihood Estimation (MLE). We
provide strong theoretical guarantees for the reliability and sample efficiency
of our algorithm. Further, we conduct extensive numerical studies to
demonstrate the practical effectiveness of the proposed framework, offering new
insights into decision-making in competitive environments.
\\ ( https://arxiv.org/abs/2601.12707 ,  668kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12730
Date: Mon, 19 Jan 2026 05:20:46 GMT   (283kb)

Title: Distribution-Centric Policy Optimization Dominates
  Exploration-Exploitation Trade-off
Authors: Zhaochun Li, Chen Wang, Jionghao Bai, Shisheng Cui, Ge Lan, Zhou Zhao,
  Yue Wang
Categories: cs.LG
\\
  The exploration-exploitation (EE) trade-off is a central challenge in
reinforcement learning (RL) for large language models (LLMs). With Group
Relative Policy Optimization (GRPO), training tends to be exploitation driven:
entropy decreases monotonically, samples convergence, and exploration fades.
Most existing fixes are \textbf{sample-centric}: they seek or bonus rare
samples, assuming exploration comes from novel trajectories and tokens. These
heuristics depend on the "luck" of informative samples, lack principled control
of the policy, and often yield limited or inconsistent gains. In this work, we
are the first to introduce a \textbf{distribution-centric} perspective for RL,
in which exploration is always guided by a "better" target distribution, and
reveal that a policy's ability to resist entropy collapse is governed by the
distribution itself rather than individual samples. Building on this insight,
we propose Distribution-Centric Policy Optimization (DCPO), which reformulates
entropy regulation as distribution-level regularization. DCPO achieves
controllable entropy fully on-policy without sampling from external
distributions, enabling efficient exploration while maintaining training
stability. Across multiple models and seven benchmarks, DCPO improves over GRPO
by about 20\% on average. Overall, DCPO replaces sample-level heuristics with
distribution-level principles, offering a theoretically grounded and flexible
framework for controllable exploration and a stronger EE trade-off. The code is
available in https://github.com/597358816/DCPO.
\\ ( https://arxiv.org/abs/2601.12730 ,  283kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12745
Date: Mon, 19 Jan 2026 05:58:53 GMT   (1178kb)

Title: A Graph Prompt Fine-Tuning Method for WSN Spatio-Temporal Correlation
  Anomaly Detection
Authors: Miao Ye, Jing Cui, Yuan huang, Qian He, Yong Wang, Jiwen Zhang
Categories: cs.LG cs.AI
\\
  Anomaly detection of multi-temporal modal data in Wireless Sensor Network
(WSN) can provide an important guarantee for reliable network operation.
Existing anomaly detection methods in multi-temporal modal data scenarios have
the problems of insufficient extraction of spatio-temporal correlation
features, high cost of anomaly sample category annotation, and imbalance of
anomaly samples. In this paper, a graph neural network anomaly detection
backbone network incorporating spatio-temporal correlation features and a
multi-task self-supervised training strategy of "pre-training - graph prompting
- fine-tuning" are designed for the characteristics of WSN graph structure
data. First, the anomaly detection backbone network is designed by improving
the Mamba model based on a multi-scale strategy and inter-modal fusion method,
and combining it with a variational graph convolution module, which is capable
of fully extracting spatio-temporal correlation features in the multi-node,
multi-temporal modal scenarios of WSNs. Secondly, we design a three-subtask
learning "pre-training" method with no-negative comparative learning,
prediction, and reconstruction to learn generic features of WSN data samples
from unlabeled data, and design a "graph prompting-fine-tuning" mechanism to
guide the pre-trained self-supervised learning. The model is fine-tuned through
the "graph prompting-fine-tuning" mechanism to guide the pre-trained
self-supervised learning model to complete the parameter fine-tuning, thereby
reducing the training cost and enhancing the detection generalization
performance. The F1 metrics obtained from experiments on the public dataset and
the actual collected dataset are up to 91.30% and 92.31%, respectively, which
provides better detection performance and generalization ability than existing
methods designed by the method.
\\ ( https://arxiv.org/abs/2601.12745 ,  1178kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12751
Date: Mon, 19 Jan 2026 06:15:25 GMT   (509kb)

Title: A Boolean Function-Theoretic Framework for Expressivity in GNNs with
  Applications to Fair Graph Mining
Authors: Manjish Pal
Categories: cs.LG
\\
  We propose a novel expressivity framework for Graph Neural Networks (GNNs)
grounded in Boolean function theory, enabling a fine-grained analysis of their
ability to capture complex subpopulation structures. We introduce the notion of
\textit{Subpopulation Boolean Isomorphism} (SBI) as an invariant that strictly
subsumes existing expressivity measures such as Weisfeiler-Lehman (WL),
biconnectivity-based, and homomorphism-based frameworks. Our theoretical
results identify Fourier degree, circuit class (AC$^0$, NC$^1$), and influence
as key barriers to expressivity in fairness-aware GNNs. We design a
circuit-traversal-based fairness algorithm capable of handling subpopulations
defined by high-complexity Boolean functions, such as parity, which break
existing baselines. Experiments on real-world graphs show that our method
achieves low fairness gaps across intersectional groups where state-of-the-art
methods fail, providing the first principled treatment of GNN expressivity
tailored to fairness.
\\ ( https://arxiv.org/abs/2601.12751 ,  509kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12775
Date: Mon, 19 Jan 2026 07:11:08 GMT   (11848kb)

Title: Eddy-Resolving Global Ocean Forecasting with Multi-Scale Graph Neural
  Networks
Authors: Yuta Hirabayashi, Daisuke Matusoka, Konobu Kimura
Categories: cs.LG
\\
  Research on data-driven ocean models has progressed rapidly in recent years;
however, the application of these models to global eddy-resolving ocean
forecasting remains limited. The accurate representation of ocean dynamics
across a wide range of spatial scales remains a major challenge in such
applications. This study proposes a multi-scale graph neural network-based
ocean model for 10-day global forecasting that improves short-term prediction
skill and enhances the representation of multi-scale ocean variability. The
model employs an encoder-processor-decoder architecture and uses two spherical
meshes with different resolutions to better capture the multi-scale nature of
ocean dynamics. In addition, the model incorporates surface atmospheric
variables along with ocean state variables as node inputs to improve short-term
prediction accuracy by representing atmospheric forcing. Evaluation using
surface kinetic energy spectra and case studies shows that the model accurately
represents a broad range of spatial scales, while root mean square error
comparisons demonstrate improved skill in short-term predictions. These results
indicate that the proposed model delivers more accurate short-term forecasts
and improved representation of multi-scale ocean dynamics, thereby highlighting
its potential to advance data-driven, eddy-resolving global ocean forecasting.
\\ ( https://arxiv.org/abs/2601.12775 ,  11848kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12785
Date: Mon, 19 Jan 2026 07:32:00 GMT   (713kb)

Title: Distilling Time Series Foundation Models for Efficient Forecasting
Authors: Yuqi Li, Kuiye Ding, Chuanguang Yang, Szu-Yu Chen, Yingli Tian
Categories: cs.LG cs.AI
Comments: Accepted by ICASSP-2026
\\
  Time Series foundation models (TSFMs) deliver strong forecasting performance
through large-scale pretraining, but their large parameter sizes make
deployment costly. While knowledge distillation offers a natural and effective
approach for model compression, techniques developed for general machine
learning tasks are not directly applicable to time series forecasting due to
the unique characteristics. To address this, we present DistilTS, the first
distillation framework specifically designed for TSFMs. DistilTS addresses two
key challenges: (1) task difficulty discrepancy, specific to forecasting, where
uniform weighting makes optimization dominated by easier short-term horizons,
while long-term horizons receive weaker supervision; and (2) architecture
discrepancy, a general challenge in distillation, for which we design an
alignment mechanism in the time series forecasting. To overcome these issues,
DistilTS introduces horizon-weighted objectives to balance learning across
horizons, and a temporal alignment strategy that reduces architectural
mismatch, enabling compact models. Experiments on multiple benchmarks
demonstrate that DistilTS achieves forecasting performance comparable to
full-sized TSFMs, while reducing parameters by up to 1/150 and accelerating
inference by up to 6000x. Code is available at:
https://github.com/itsnotacie/DistilTS-ICASSP2026.
\\ ( https://arxiv.org/abs/2601.12785 ,  713kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12807
Date: Mon, 19 Jan 2026 08:10:53 GMT   (2230kb)

Title: Semi-supervised Instruction Tuning for Large Language Models on
  Text-Attributed Graphs
Authors: Zixing Song, Irwin King
Categories: cs.LG
\\
  The emergent reasoning capabilities of Large Language Models (LLMs) offer a
transformative paradigm for analyzing text-attributed graphs. While instruction
tuning is the prevailing method for adapting pre-trained LLMs to graph learning
tasks like node classification, it requires a substantial volume of annotated
(INSTRUCTION, OUTPUT) pairs deriving from labeled nodes. This requirement is
particularly prohibitive in the social domain, where obtaining expert labels
for sensitive or evolving content is costly and slow. Furthermore, standard
graph instruction tuning fails to exploit the vast amount of unlabeled nodes,
which contain latent correlations due to edge connections that are beneficial
for downstream predictions. To bridge this gap, we propose a novel
Semi-supervised Instruction Tuning pipeline for Graph Learning, named
SIT-Graph. Notably, SIT-Graph is model-agnostic and can be seamlessly
integrated into any graph instruction tuning method that utilizes LLMs as the
predictor. SIT-Graph operates via an iterative self-training process.
Initially, the model is fine-tuned using instruction pairs constructed solely
from the labeled nodes. Then it generates confidence-filtered pseudo-responses
for unlabeled nodes to strategically augment the dataset for the next round of
fine-tuning. Finally, this iterative refinement progressively aligns the LLM
with the underlying node correlations. Extensive experiments demonstrate that
when incorporated into state-of-the-art graph instruction tuning methods,
SIT-Graph significantly enhances their performance on text-attributed graph
benchmarks, achieving over 20% improvement under the low label ratio settings.
\\ ( https://arxiv.org/abs/2601.12807 ,  2230kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12816
Date: Mon, 19 Jan 2026 08:23:12 GMT   (105kb)

Title: Fisher-Orthogonal Projected Natural Gradient Descent for Continual
  Learning
Authors: Ishir Garg, Neel Kolhe, Andy Peng, Rohan Gopalam
Categories: cs.LG cs.AI
\\
  Continual learning aims to enable neural networks to acquire new knowledge on
sequential tasks. However, the key challenge in such settings is to learn new
tasks without catastrophically forgetting previously learned tasks. We propose
the Fisher-Orthogonal Projected Natural Gradient Descent (FOPNG) optimizer,
which enforces Fisher-orthogonal constraints on parameter updates to preserve
old task performance while learning new tasks. Unlike existing methods that
operate in Euclidean parameter space, FOPNG projects gradients onto the
Fisher-orthogonal complement of previous task gradients. This approach unifies
natural gradient descent with orthogonal gradient methods within an
information-geometric framework. The resulting update direction is invariant
under reparameterization, guarantees descent in the Fisher metric, and helps
preserve prior task outputs. We provide theoretical analysis establishing the
properties of the projected update, describe efficient and practical
implementations using the diagonal Fisher, and demonstrate strong results on
standard continual learning benchmarks such as Permuted-MNIST, Split-MNIST,
Rotated-MNIST, Split-CIFAR10, and Split-CIFAR100.
\\ ( https://arxiv.org/abs/2601.12816 ,  105kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12839
Date: Mon, 19 Jan 2026 08:51:50 GMT   (284kb)

Title: Knowledge-Integrated Representation Learning for Crypto Anomaly
  Detection under Extreme Label Scarcity; Relational Domain-Logic Integration
  with Retrieval-Grounded Context and Path-Level Explanations
Authors: Gyuyeon Na, Minjung Park, Soyoun Kim, Jungbin Shin, Sangmi Chai
Categories: cs.LG q-fin.RM
Comments: Gyuyeon Na, Minjung Park, Soyoun Kim contributed equally to this work
\\
  Detecting anomalous trajectories in decentralized crypto networks is
fundamentally challenged by extreme label scarcity and the adaptive evasion
strategies of illicit actors. While Graph Neural Networks (GNNs) effectively
capture local structural patterns, they struggle to internalize multi hop,
logic driven motifs such as fund dispersal and layering that characterize
sophisticated money laundering, limiting their forensic accountability under
regulations like the FATF Travel Rule. To address this limitation, we propose
Relational Domain Logic Integration (RDLI), a framework that embeds expert
derived heuristics as differentiable, logic aware latent signals within
representation learning. Unlike static rule based approaches, RDLI enables the
detection of complex transactional flows that evade standard message passing.
To further account for market volatility, we incorporate a Retrieval Grounded
Context (RGC) module that conditions anomaly scoring on regulatory and
macroeconomic context, mitigating false positives caused by benign regime
shifts. Under extreme label scarcity (0.01%), RDLI outperforms state of the art
GNN baselines by 28.9% in F1 score. A micro expert user study further confirms
that RDLI path level explanations significantly improve trustworthiness,
perceived usefulness, and clarity compared to existing methods, highlighting
the importance of integrating domain logic with contextual grounding for both
accuracy and explainability.
\\ ( https://arxiv.org/abs/2601.12839 ,  284kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12859
Date: Mon, 19 Jan 2026 09:16:17 GMT   (4153kb)

Title: Generating Cyclic Conformers with Flow Matching in Cremer-Pople
  Coordinates
Authors: Luca Schaufelberger, Aline Hartgers, Kjell Jorner
Categories: cs.LG physics.chem-ph
\\
  Cyclic molecules are ubiquitous across applications in chemistry and biology.
Their restricted conformational flexibility provides structural
pre-organization that is key to their function in drug discovery and catalysis.
However, reliably sampling the conformer ensembles of ring systems remains
challenging. Here, we introduce PuckerFlow, a generative machine learning model
that performs flow matching on the Cremer-Pople space, a low-dimensional
internal coordinate system capturing the relevant degrees of freedom of rings.
Our approach enables generation of valid closed rings by design and
demonstrates strong performance in generating conformers that are both diverse
and precise. We show that PuckerFlow outperforms other conformer generation
methods on nearly all quantitative metrics and illustrate the potential of
PuckerFlow for ring systems relevant to chemical applications, particularly in
catalysis and drug discovery. This work enables efficient and reliable
conformer generation of cyclic structures, paving the way towards modeling
structure-property relationships and the property-guided generation of rings
across a wide range of applications in chemistry and biology.
\\ ( https://arxiv.org/abs/2601.12859 ,  4153kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12879
Date: Mon, 19 Jan 2026 09:34:10 GMT   (18kb)

Title: Hierarchical Sparse Circuit Extraction from Billion-Parameter Language
  Models through Scalable Attribution Graph Decomposition
Authors: Mohammed Mudassir Uddin, Shahnawaz Alam, Mohammed Kaif Pasha
Categories: cs.LG cs.AI cs.CL
\\
  Mechanistic interpretability seeks to reverse-engineer neural network
computations into human-understandable algorithms, yet extracting sparse
computational circuits from billion-parameter language models remains
challenging due to exponential search complexity and pervasive polysemanticity.
The proposed Hierarchical Attribution Graph Decomposition (HAGD) framework
reduces circuit discovery complexity from O(2^n) exhaustive enumeration to
O(n^2 log n) through multi-resolution abstraction hierarchies and
differentiable circuit search. The methodology integrates cross-layer
transcoders for monosemantic feature extraction, graph neural network
meta-learning for topology prediction, and causal intervention protocols for
validation. Empirical evaluation spans GPT-2 variants, Llama-7B through
Llama-70B, and Pythia suite models across algorithmic tasks and natural
language benchmarks. On modular arithmetic tasks, the framework achieves up to
91% behavioral preservation ($\pm$2.3\% across runs) while maintaining
interpretable subgraph sizes. Cross-architecture transfer experiments suggest
that discovered circuits exhibit moderate structural similarity (averaging 67%)
across model families, indicating potential shared computational patterns.
These results provide preliminary foundations for interpretability at larger
model scales while identifying significant limitations in current attribution
methodologies that require future advances.
\\ ( https://arxiv.org/abs/2601.12879 ,  18kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12893
Date: Mon, 19 Jan 2026 09:46:54 GMT   (875kb)

Title: AdaNODEs: Test Time Adaptation for Time Series Forecasting Using Neural
  ODEs
Authors: Ting Dang, Soumyajit Chatterjee, Hong Jia, Yu Wu, Flora Salim, Fahim
  Kawsar
Categories: cs.LG cs.AI
Comments: Accepted by ICASSP 2026
\\
  Test time adaptation (TTA) has emerged as a promising solution to adapt
pre-trained models to new, unseen data distributions using unlabeled target
domain data. However, most TTA methods are designed for independent data, often
overlooking the time series data and rarely addressing forecasting tasks. This
paper presents AdaNODEs, an innovative source-free TTA method tailored
explicitly for time series forecasting. By leveraging Neural Ordinary
Differential Equations (NODEs), we propose a novel adaptation framework that
accommodates the unique characteristics of distribution shifts in time series
data. Moreover, we innovatively propose a new loss function to tackle TTA for
forecasting tasks. AdaNODEs only requires updating limited model parameters,
showing effectiveness in capturing temporal dependencies while avoiding
significant memory usage. Extensive experiments with one- and high-dimensional
data demonstrate that AdaNODEs offer relative improvements of 5.88\% and 28.4\%
over the SOTA baselines, especially demonstrating robustness across higher
severity distribution shifts.
\\ ( https://arxiv.org/abs/2601.12893 ,  875kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12900
Date: Mon, 19 Jan 2026 09:54:19 GMT   (424kb)

Title: Supervised Learning for the (s,S) Inventory Model with General
  Interarrival Demands and General Lead Times
Authors: Eliran Sherzer, Yonit Barron
Categories: cs.LG
\\
  The continuous-review (s,S) inventory model is a cornerstone of stochastic
inventory theory, yet its analysis becomes analytically intractable when
dealing with non-Markovian systems. In such systems, evaluating long-run
performance measures typically relies on costly simulation.
  This paper proposes a supervised learning framework via a neural network
model for approximating stationary performance measures of (s,S) inventory
systems with general distributions for the interarrival time between demands
and lead times under lost sales. Simulations are first used to generate
training labels, after which the neural network is trained. After training, the
neural network provides almost instantaneous predictions of various metrics of
the system, such as the stationary distribution of inventory levels, the
expected cycle time, and the probability of lost sales. We find that using a
small number of low-order moments of the distributions as input is sufficient
to train the neural networks and to accurately capture the steady-state
distribution. Extensive numerical experiments demonstrate high accuracy over a
wide range of system parameters. As such, it effectively replaces repeated and
costly simulation runs. Our framework is easily extendable to other inventory
models, offering an efficient and fast alternative for analyzing complex
stochastic systems.
\\ ( https://arxiv.org/abs/2601.12900 ,  424kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12903
Date: Mon, 19 Jan 2026 09:58:10 GMT   (4849kb)

Title: Deep Temporal Graph Clustering: A Comprehensive Benchmark and Datasets
Authors: Meng Liu, Ke Liang, Siwei Wang, Xingchen Hu, Sihang Zhou, Xinwang Liu
Categories: cs.LG
DOI: 10.1109/TPAMI.2025.3596609
\\
  Temporal Graph Clustering (TGC) is a new task with little attention, focusing
on node clustering in temporal graphs. Compared with existing static graph
clustering, it can find the balance between time requirement and space
requirement (Time-Space Balance) through the interaction sequence-based
batch-processing pattern. However, there are two major challenges that hinder
the development of TGC, i.e., inapplicable clustering techniques and
inapplicable datasets. To address these challenges, we propose a comprehensive
benchmark, called BenchTGC. Specially, we design a BenchTGC Framework to
illustrate the paradigm of temporal graph clustering and improve existing
clustering techniques to fit temporal graphs. In addition, we also discuss
problems with public temporal graph datasets and develop multiple datasets
suitable for TGC task, called BenchTGC Datasets. According to extensive
experiments, we not only verify the advantages of BenchTGC, but also
demonstrate the necessity and importance of TGC task. We wish to point out that
the dynamically changing and complex scenarios in real world are the foundation
of temporal graph clustering. The code and data is available at:
https://github.com/MGitHubL/BenchTGC.
\\ ( https://arxiv.org/abs/2601.12903 ,  4849kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12917
Date: Mon, 19 Jan 2026 10:17:08 GMT   (4306kb)

Title: CooperLLM: Cloud-Edge-End Cooperative Federated Fine-tuning for LLMs via
  ZOO-based Gradient Correction
Authors: He Sun, Jinrui Zhou, Li Li, Mingjun Xiao
Categories: cs.LG cs.DC
Comments: 14 pages, 9 figures, under review
ACM-class: F.2.2; I.2.7
\\
  Large Language Models (LLMs) perform well on many NLP tasks, but fine-tuning
them on resource-constrained mobile devices is challenging due to high memory
and computation costs, despite growing demands for privacy-preserving
personalization. Federated Learning (FL) enables local-data training, yet
existing methods either rely on memory-intensive backpropagation or use
zeroth-order optimization (ZOO), which avoids backward passes but suffers from
slow convergence and degraded accuracy. We propose CooperLLM, a cloud-assisted
edge-end cooperative federated fine-tuning framework that combines ZOO on
mobile devices with cloud-guided gradient rectification. Mobile clients perform
lightweight ZOO updates on private data, while the cloud fine-tunes on
auxiliary public data using backpropagation and injects guided perturbations to
rectify local updates, improving convergence and accuracy without violating
privacy. To address system bottlenecks, CooperLLM introduces pipeline
scheduling and adaptive compression to overlap computation and communication
and reduce memory usage. Experiments on multiple Transformer models and
datasets show that CooperLLM reduces on-device memory by up to $86.4\%$,
accelerates convergence by $8.8 \times$, and improves accuracy by up to 10
percentage points over state-of-the-art ZOO-based baselines.
\\ ( https://arxiv.org/abs/2601.12917 ,  4306kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12928
Date: Mon, 19 Jan 2026 10:30:52 GMT   (5039kb)

Title: An efficient heuristic for geometric analysis of cell deformations
Authors: Yaima Paz Soto, Silena Herold Garcia, Ximo Gual-Arnau, Antoni
  Jaume-i-Cap\'o, Manuel Gonz\'alez-Hidalgo
Categories: cs.LG q-bio.QM
Journal-ref: Soto, Y. P., Garcia, S. H., Gual-Arnau, X., Jaume-i-Cap\'o, A., &
  Gonz\'alez-Hidalgo, M. (2025). An efficient heuristic for geometric analysis
  of cell deformations. Computers in Biology and Medicine, 186, 109709
DOI: 10.1016/j.compbiomed.2025.109709
\\
  Sickle cell disease causes erythrocytes to become sickle-shaped, affecting
their movement in the bloodstream and reducing oxygen delivery. It has a high
global prevalence and places a significant burden on healthcare systems,
especially in resource-limited regions. Automated classification of sickle
cells in blood images is crucial, allowing the specialist to reduce the effort
required and avoid errors when quantifying the deformed cells and assessing the
severity of a crisis. Recent studies have proposed various erythrocyte
representation and classification methods. Since classification depends solely
on cell shape, a suitable approach models erythrocytes as closed planar curves
in shape space. This approach employs elastic distances between shapes, which
are invariant under rotations, translations, scaling, and reparameterizations,
ensuring consistent distance measurements regardless of the curves' position,
starting point, or traversal speed. While previous methods exploiting shape
space distances had achieved high accuracy, we refined the model by considering
the geometric characteristics of healthy and sickled erythrocytes. Our method
proposes (1) to employ a fixed parameterization based on the major axis of each
cell to compute distances and (2) to align each cell with two templates using
this parameterization before computing distances. Aligning shapes to templates
before distance computation, a concept successfully applied in areas such as
molecular dynamics, and using a fixed parameterization, instead of minimizing
distances across all possible parameterizations, simplifies calculations. This
strategy achieves 96.03\% accuracy rate in both supervised classification and
unsupervised clustering. Our method ensures efficient erythrocyte
classification, maintaining or improving accuracy over shape space models while
significantly reducing computational costs.
\\ ( https://arxiv.org/abs/2601.12928 ,  5039kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12931
Date: Mon, 19 Jan 2026 10:31:01 GMT   (278kb)

Title: Online Continual Learning for Time Series: a Natural Score-driven
  Approach
Authors: Edoardo Urettini, Daniele Atzeni, Ioanna-Yvonni Tsaknaki, Antonio
  Carta
Categories: cs.LG cs.AI stat.ML
\\
  Online continual learning (OCL) methods adapt to changing environments
without forgetting past knowledge. Similarly, online time series forecasting
(OTSF) is a real-world problem where data evolve in time and success depends on
both rapid adaptation and long-term memory. Indeed, time-varying and
regime-switching forecasting models have been extensively studied, offering a
strong justification for the use of OCL in these settings. Building on recent
work that applies OCL to OTSF, this paper aims to strengthen the theoretical
and practical connections between time series methods and OCL. First, we
reframe neural network optimization as a parameter filtering problem, showing
that natural gradient descent is a score-driven method and proving its
information-theoretic optimality. Then, we show that using a Student's t
likelihood in addition to natural gradient induces a bounded update, which
improves robustness to outliers. Finally, we introduce Natural Score-driven
Replay (NatSR), which combines our robust optimizer with a replay buffer and a
dynamic scale heuristic that improves fast adaptation at regime drifts.
Empirical results demonstrate that NatSR achieves stronger forecasting
performance than more complex state-of-the-art methods.
\\ ( https://arxiv.org/abs/2601.12931 ,  278kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12965
Date: Mon, 19 Jan 2026 11:23:21 GMT   (25kb)

Title: Deterministic Dynamics of Sampling Processes in Score-Based Diffusion
  Models with Multiplicative Noise Conditioning
Authors: Doheon Kim
Categories: cs.LG
\\
  Score-based diffusion models generate new samples by learning the score
function associated with a diffusion process. While the effectiveness of these
models can be theoretically explained using differential equations related to
the sampling process, previous work by Song and Ermon (2020) demonstrated that
neural networks using multiplicative noise conditioning can still generate
satisfactory samples. In this setup, the model is expressed as the product of
two functions: one depending on the spatial variable and the other on the noise
magnitude. This structure limits the model's ability to represent a more
general relationship between the spatial variable and the noise, indicating
that it cannot fully learn the correct score. Despite this limitation, the
models perform well in practice. In this work, we provide a theoretical
explanation for this phenomenon by studying the deterministic dynamics of the
associated differential equations, offering insight into how the model
operates.
\\ ( https://arxiv.org/abs/2601.12965 ,  25kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12971
Date: Mon, 19 Jan 2026 11:32:25 GMT   (14991kb)

Title: Architecture-Optimization Co-Design for Physics-Informed Neural Networks
  Via Attentive Representations and Conflict-Resolved Gradients
Authors: Pancheng Niu, Jun Guo, Qiaolin He, Yongming Chen, Yanchao Shi
Categories: cs.LG
\\
  Physics-Informed Neural Networks (PINNs) provide a learning-based framework
for solving partial differential equations (PDEs) by embedding governing
physical laws into neural network training. In practice, however, their
performance is often hindered by limited representational capacity and
optimization difficulties caused by competing physical constraints and
conflicting gradients. In this work, we study PINN training from a unified
architecture-optimization perspective. We first propose a layer-wise dynamic
attention mechanism to enhance representational flexibility, resulting in the
Layer-wise Dynamic Attention PINN (LDA-PINN). We then reformulate PINN training
as a multi-task learning problem and introduce a conflict-resolved gradient
update strategy to alleviate gradient interference, leading to the
Gradient-Conflict-Resolved PINN (GC-PINN). By integrating these two components,
we develop the Architecture-Conflict-Resolved PINN (ACR-PINN), which combines
attentive representations with conflict-aware optimization while preserving the
standard PINN loss formulation. Extensive experiments on benchmark PDEs,
including the Burgers, Helmholtz, Klein-Gordon, and lid-driven cavity flow
problems, demonstrate that ACR-PINN achieves faster convergence and
significantly lower relative $L_2$ and $L_\infty$ errors than standard PINNs.
These results highlight the effectiveness of architecture-optimization
co-design for improving the robustness and accuracy of PINN-based solvers.
\\ ( https://arxiv.org/abs/2601.12971 ,  14991kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12988
Date: Mon, 19 Jan 2026 12:07:51 GMT   (2977kb)

Title: PaperGuide: Making Small Language-Model Paper-Reading Agents More
  Efficient
Authors: Zijian Wang, Tiancheng Huang, Hanqi Li, Da Ma, Lu Chen, Kai Yu
Categories: cs.LG
Comments: 35 pages, 9 figures, 7 tables
\\
  The accelerating growth of the scientific literature makes it increasingly
difficult for researchers to track new advances through manual reading alone.
Recent progress in large language models (LLMs) has therefore spurred interest
in autonomous agents that can read scientific papers and extract task-relevant
information. However, most existing approaches rely either on heavily
engineered prompting or on a conventional SFT-RL training pipeline, both of
which often lead to excessive and low-yield exploration. Drawing inspiration
from cognitive science, we propose PaperCompass, a framework that mitigates
these issues by separating high-level planning from fine-grained execution.
PaperCompass first drafts an explicit plan that outlines the intended sequence
of actions, and then performs detailed reasoning to instantiate each step by
selecting the parameters for the corresponding function calls. To train such
behavior, we introduce Draft-and-Follow Policy Optimization (DFPO), a tailored
RL method that jointly optimizes both the draft plan and the final solution.
DFPO can be viewed as a lightweight form of hierarchical reinforcement
learning, aimed at narrowing the `knowing-doing' gap in LLMs. We provide a
theoretical analysis that establishes DFPO's favorable optimization properties,
supporting a stable and reliable training process. Experiments on paper-based
question answering (Paper-QA) benchmarks show that PaperCompass improves
efficiency over strong baselines without sacrificing performance, achieving
results comparable to much larger models.
\\ ( https://arxiv.org/abs/2601.12988 ,  2977kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13013
Date: Mon, 19 Jan 2026 12:47:31 GMT   (2123kb)

Title: HT-GNN: Hyper-Temporal Graph Neural Network for Customer Lifetime Value
  Prediction in Baidu Ads
Authors: Xiaohui Zhao, Xinjian Zhao, Jiahui Zhang, Guoyu Liu, Houzhi Wang, Shu
  Wu
Categories: cs.LG cs.AI
\\
  Lifetime value (LTV) prediction is crucial for news feed advertising,
enabling platforms to optimize bidding and budget allocation for long-term
revenue growth. However, it faces two major challenges: (1) demographic-based
targeting creates segment-specific LTV distributions with large value
variations across user groups; and (2) dynamic marketing strategies generate
irregular behavioral sequences where engagement patterns evolve rapidly. We
propose a Hyper-Temporal Graph Neural Network (HT-GNN), which jointly models
demographic heterogeneity and temporal dynamics through three key components:
(i) a hypergraph-supervised module capturing inter-segment relationships; (ii)
a transformer-based temporal encoder with adaptive weighting; and (iii) a
task-adaptive mixture-of-experts with dynamic prediction towers for
multi-horizon LTV forecasting. Experiments on \textit{Baidu Ads} with 15
million users demonstrate that HT-GNN consistently outperforms state-of-the-art
methods across all metrics and prediction horizons.
\\ ( https://arxiv.org/abs/2601.13013 ,  2123kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13020
Date: Mon, 19 Jan 2026 12:57:11 GMT   (1320kb)

Title: PASs-MoE: Mitigating Misaligned Co-drift among Router and Experts via
  Pathway Activation Subspaces for Continual Learning
Authors: Zhiyan Hou, Haiyun Guo, Haokai Ma, Yandu Sun, Yonghui Yang, Jinqiao
  Wang
Categories: cs.LG cs.AI
\\
  Continual instruction tuning (CIT) requires multimodal large language models
(MLLMs) to adapt to a stream of tasks without forgetting prior capabilities. A
common strategy is to isolate updates by routing inputs to different LoRA
experts. However, existing LoRA-based Mixture-of-Experts (MoE) methods often
jointly update the router and experts in an indiscriminate way, causing the
router's preferences to co-drift with experts' adaptation pathways and
gradually deviate from early-stage input-expert specialization. We term this
phenomenon Misaligned Co-drift, which blurs expert responsibilities and
exacerbates forgetting.To address this, we introduce the pathway activation
subspace (PASs), a LoRA-induced subspace that reflects which low-rank pathway
directions an input activates in each expert, providing a capability-aligned
coordinate system for routing and preservation. Based on PASs, we propose a
fixed-capacity PASs-based MoE-LoRA method with two components: PAS-guided
Reweighting, which calibrates routing using each expert's pathway activation
signals, and PAS-aware Rank Stabilization, which selectively stabilizes rank
directions important to previous tasks. Experiments on a CIT benchmark show
that our approach consistently outperforms a range of conventional continual
learning baselines and MoE-LoRA variants in both accuracy and anti-forgetting
without adding parameters. Our code will be released upon acceptance.
\\ ( https://arxiv.org/abs/2601.13020 ,  1320kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13021
Date: Mon, 19 Jan 2026 13:00:41 GMT   (1432kb)

Title: Enhancing Generalization in Sickle Cell Disease Diagnosis through
  Ensemble Methods and Feature Importance Analysis
Authors: Nata\v{s}a Petrovi\'c, Gabriel Moy\`a-Alcover, Antoni Jaume-i-Cap\'o,
  Jose Maria Buades Rubio
Categories: cs.LG
Journal-ref: Engineering Applications of Artificial Intelligence (2025), 142,
  109875
DOI: 10.1016/j.engappai.2024.109875
\\
  This work presents a novel approach for selecting the optimal ensemble-based
classification method and features with a primarly focus on achieving
generalization, based on the state-of-the-art, to provide diagnostic support
for Sickle Cell Disease using peripheral blood smear images of red blood cells.
We pre-processed and segmented the microscopic images to ensure the extraction
of high-quality features. To ensure the reliability of our proposed system, we
conducted an in-depth analysis of interpretability. Leveraging techniques
established in the literature, we extracted features from blood cells and
employed ensemble machine learning methods to classify their morphology.
Furthermore, we have devised a methodology to identify the most critical
features for classification, aimed at reducing complexity and training time and
enhancing interpretability in opaque models. Lastly, we validated our results
using a new dataset, where our model overperformed state-of-the-art models in
terms of generalization. The results of classifier ensembled of Random Forest
and Extra Trees classifier achieved an harmonic mean of precision and recall
(F1-score) of 90.71\% and a Sickle Cell Disease diagnosis support score
(SDS-score) of 93.33\%. These results demonstrate notable enhancement from
previous ones with Gradient Boosting classifier (F1-score 87.32\% and SDS-score
89.51\%). To foster scientific progress, we have made available the parameters
for each model, the implemented code library, and the confusion matrices with
the raw data.
\\ ( https://arxiv.org/abs/2601.13021 ,  1432kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13048
Date: Mon, 19 Jan 2026 13:39:42 GMT   (6129kb)

Title: Analysis of Long Range Dependency Understanding in State Space Models
Authors: Srividya Ravikumar, Abhinav Anand, Shweta Verma, Mira Mezini
Categories: cs.LG cs.AI
\\
  Although state-space models (SSMs) have demonstrated strong performance on
long-sequence benchmarks, most research has emphasized predictive accuracy
rather than interpretability. In this work, we present the first systematic
kernel interpretability study of the diagonalized state-space model (S4D)
trained on a real-world task (vulnerability detection in source code). Through
time and frequency domain analysis of the S4D kernel, we show that the
long-range modeling capability of S4D varies significantly under different
model architectures, affecting model performance. For instance, we show that
the depending on the architecture, S4D kernel can behave as low-pass, band-pass
or high-pass filter. The insights from our analysis can guide future work in
designing better S4D-based models.
\\ ( https://arxiv.org/abs/2601.13048 ,  6129kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13054
Date: Mon, 19 Jan 2026 13:43:28 GMT   (2231kb)

Title: TinyML-Enabled IoT for Sustainable Precision Irrigation
Authors: Kamogelo Taueatsoala, Caitlyn Daniels, Angelina J. Ramsunar, Petrus
  Bronkhorst, Absalom E. Ezugwu
Categories: cs.LG cs.AI
\\
  Small-scale farming communities are disproportionately affected by water
scarcity, erratic climate patterns, and a lack of access to advanced,
affordable agricultural technologies. To address these challenges, this paper
presents a novel, edge-first IoT framework that integrates Tiny Machine
Learning (TinyML) for intelligent, offline-capable precision irrigation. The
proposed four-layer architecture leverages low-cost hardware, an ESP32
microcontroller as an edge inference node, and a Raspberry Pi as a local edge
server to enable autonomous decision-making without cloud dependency. The
system utilizes capacitive soil moisture, temperature, humidity, pH, and
ambient light sensors for environmental monitoring. A rigorous comparative
analysis of ensemble models identified gradient boosting as superior, achieving
an R^2 score of 0.9973 and a Mean Absolute Percentage Error (MAPE) of 0.99%,
outperforming a random forest model (R^2 = 0.9916, MAPE = 1.81%). This
optimized model was converted and deployed as a lightweight TinyML inference
engine on the ESP32 and predicts irrigation needs with exceptional accuracy
(MAPE < 1%). Local communication is facilitated by an MQTT-based LAN protocol,
ensuring reliable operation in areas with limited or no internet connectivity.
Experimental validation in a controlled environment demonstrated a significant
reduction in water usage compared to traditional methods, while the system's
low-power design and offline functionality confirm its viability for
sustainable, scalable deployment in resource-constrained rural settings. This
work provides a practical, cost-effective blueprint for bridging the
technological divide in agriculture and enhancing water-use efficiency through
on-device artificial intelligence.
\\ ( https://arxiv.org/abs/2601.13054 ,  2231kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13075
Date: Mon, 19 Jan 2026 14:10:35 GMT   (1264kb)

Title: METIS: Mentoring Engine for Thoughtful Inquiry & Solutions
Authors: Abhinav Rajeev Kumar, Dhruv Trehan, Paras Chopra
Categories: cs.LG cs.AI
Comments: 12 pages, 5 figures, 4 tables
\\
  Many students lack access to expert research mentorship. We ask whether an AI
mentor can move undergraduates from an idea to a paper. We build METIS, a
tool-augmented, stage-aware assistant with literature search, curated
guidelines, methodology checks, and memory. We evaluate METIS against GPT-5 and
Claude Sonnet 4.5 across six writing stages using LLM-as-a-judge pairwise
preferences, student-persona rubrics, short multi-turn tutoring, and
evidence/compliance checks. On 90 single-turn prompts, LLM judges preferred
METIS to Claude Sonnet 4.5 in 71% and to GPT-5 in 54%. Student scores
(clarity/actionability/constraint-fit; 90 prompts x 3 judges) are higher across
stages. In multi-turn sessions (five scenarios/agent), METIS yields slightly
higher final quality than GPT-5. Gains concentrate in document-grounded stages
(D-F), consistent with stage-aware routing and groundings failure modes include
premature tool routing, shallow grounding, and occasional stage
misclassification.
\\ ( https://arxiv.org/abs/2601.13075 ,  1264kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13100
Date: Mon, 19 Jan 2026 14:39:40 GMT   (14kb)

Title: Recursive Meta-Distillation: An Axiomatic Framework for Iterative
  Knowledge Refinement
Authors: Aaron R. Flouro, Shawn P. Chadwick
Categories: cs.LG
MSC-class: 68T05, 60B10
ACM-class: I.2.6; F.2.2
\\
  Recent work in probability-domain knowledge distillation has established
axiomatic frameworks for temperature scaling, multi-teacher aggregation, and
bias-variance trade-offs in single-stage settings. However, the mathematical
behavior of recursive or multi-generation distillation remains poorly
understood, with prior approaches relying primarily on empirical heuristics. In
this work, we introduce an axiomatic and operator-theoretic framework for
recursive meta-distillation, formalizing iterative knowledge distillation as a
sequence of probability-distribution operators with explicit anchoring to base
teachers.
  We define structural axioms for valid meta-teacher construction and prove the
existence of non-trivial operator families satisfying these axioms without
specifying particular algorithms or loss functions. Under mild realizability
and convexity assumptions, we show that anchored recursive distillation induces
contraction in KL divergence, yielding geometric convergence to base teacher
distributions and a unique, globally attractive fixed point.
  The contribution is foundational rather than algorithmic: the framework
characterizes when recursive distillation is mathematically well-posed and
convergent rather than error-accumulating, independent of model architecture,
optimization details, or specific operator instantiations. These results
provide a theoretical basis for understanding stability, bias-variance
behavior, and failure modes in iterative and multi-teacher distillation under
capacity constraints.
\\ ( https://arxiv.org/abs/2601.13100 ,  14kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13143
Date: Mon, 19 Jan 2026 15:24:51 GMT   (499kb)

Title: FastAV: Efficient Token Pruning for Audio-Visual Large Language Model
  Inference
Authors: Chaeyoung Jung, Youngjoon Jang, Seungwoo Lee, Joon Son Chung
Categories: cs.LG
\\
  In this work, we present FastAV, the first token pruning framework tailored
for audio-visual large language models (AV-LLMs). While token pruning has been
actively explored in standard large language models (LLMs) and vision-language
models (LVLMs), its application to AV-LLMs has received little attention, even
though multimodal integration substantially increases their token demands. To
address this gap, we introduce a pruning strategy that utilizes attention
weights to identify tokens emphasized at different stages and estimates their
importance. Building on this analysis, FastAV applies a two-stage pruning
strategy: (1) global pruning in intermediate layers to remove broadly less
influential tokens, and (2) fine pruning in later layers considering the impact
on next token generation. Notably, our method does not rely on full attention
maps, which makes it fully compatible with efficient attention mechanisms such
as FlashAttention. Extensive experiments demonstrate that FastAV reduces FLOPs
by more than 40% on two representative AV-LLMs, while preserving or even
improving model performance.
\\ ( https://arxiv.org/abs/2601.13143 ,  499kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13160
Date: Mon, 19 Jan 2026 15:37:45 GMT   (15780kb)

Title: Training instability in deep learning follows low-dimensional dynamical
  principles
Authors: Zhipeng Zhang, Zhenjie Yao, Kai Li, Lei Yang
Categories: cs.LG cs.AI
\\
  Deep learning systems achieve remarkable empirical performance, yet the
stability of the training process itself remains poorly understood. Training
unfolds as a high-dimensional dynamical system in which small perturbations to
optimization, data, parameters, or learning signals can induce abrupt and
irreversible collapse, undermining reproducibility and scalability.
  We propose a unified dynamical perspective that characterizes training
stability as an intrinsic property of learning systems, organized along four
interacting dimensions: optimization, environmental/data, parametric, and
learning-signal stability. We operationalize this perspective through
controlled perturbation auditing of training trajectories, probing how learning
dynamics respond to structured disturbances without modifying learning
algorithms.
  Across reinforcement learning and large language model training, we identify
three recurring regularities: high final performance is frequently decoupled
from training stability; controlled stochasticity consistently buffers learning
dynamics across paradigms; and deviations in low-dimensional latent meta-states
systematically precede observable performance collapse. Together, these
findings establish training stability as a measurable and comparable dynamical
property of learning systems, providing a descriptive foundation for studying
learning dynamics beyond final performance outcomes.
\\ ( https://arxiv.org/abs/2601.13160 ,  15780kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13162
Date: Mon, 19 Jan 2026 15:39:24 GMT   (3300kb)

Title: NeuroShield: A Neuro-Symbolic Framework for Adversarial Robustness
Authors: Ali Shafiee Sarvestani, Jason Schmidt, Arman Roohi
Categories: cs.LG cs.ET
\\
  Adversarial vulnerability and lack of interpretability are critical
limitations of deep neural networks, especially in safety-sensitive settings
such as autonomous driving. We introduce \DesignII, a neuro-symbolic framework
that integrates symbolic rule supervision into neural networks to enhance both
adversarial robustness and explainability. Domain knowledge is encoded as
logical constraints over appearance attributes such as shape and color, and
enforced through semantic and symbolic logic losses applied during training.
Using the GTSRB dataset, we evaluate robustness against FGSM and PGD attacks at
a standard $\ell_\infty$ perturbation budget of $\varepsilon = 8/255$. Relative
to clean training, standard adversarial training provides modest improvements
in robustness ($\sim$10 percentage points). Conversely, our FGSM-Neuro-Symbolic
and PGD-Neuro-Symbolic models achieve substantially larger gains, improving
adversarial accuracy by 18.1\% and 17.35\% over their corresponding
adversarial-training baselines, representing roughly a three-fold larger
robustness gain than standard adversarial training provides when both are
measured relative to the same clean-training baseline, without reducing
clean-sample accuracy. Compared to transformer-based defenses such as LNL-MoEx,
which require heavy architectures and extensive data augmentation, our
PGD-Neuro-Symbolic variant attains comparable or superior robustness using a
ResNet18 backbone trained for 10 epochs. These results show that symbolic
reasoning offers an effective path to robust and interpretable AI.
\\ ( https://arxiv.org/abs/2601.13162 ,  3300kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13190
Date: Mon, 19 Jan 2026 16:12:41 GMT   (15137kb)

Title: LAViG-FLOW: Latent Autoregressive Video Generation for Fluid Flow
  Simulations
Authors: Vittoria De Pellegrini and Tariq Alkhalifah
Categories: cs.LG physics.flu-dyn
\\
  Modeling and forecasting subsurface multiphase fluid flow fields underpin
applications ranging from geological CO2 sequestration (GCS) operations to
geothermal production. This is essential for ensuring both operational
performance and long-term safety. While high fidelity multiphase simulators are
widely used for this purpose, they become prohibitively expensive once many
forward runs are required for inversion purposes and quantify uncertainty. To
tackle this challenge we propose LAViG-FLOW, a latent autoregressive video
generation diffusion framework that explicitly learns the coupled evolution of
saturation and pressure fields. Each state variable is compressed by a
dedicated 2D autoencoder, and a Video Diffusion Transformer (VDiT) models their
coupled distribution across time. We first train the model on a given time
horizon to learn their coupled relationship and then fine-tune it
autoregressively so it can extrapolate beyond the observed time window.
Evaluated on an open-source CO2 sequestration dataset, LAViG-FLOW generates
saturation and pressure fields that stay consistent across time while running
orders of magnitude faster than traditional numerical solvers.
\\ ( https://arxiv.org/abs/2601.13190 ,  15137kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13243
Date: Mon, 19 Jan 2026 17:23:45 GMT   (9432kb)

Title: A Comprehensive Evaluation of LLM Reasoning: From Single-Model to
  Multi-Agent Paradigms
Authors: Yapeng Li, Jiakuo Yu, Zhixin Liu, Xinnan Liu, Jing Yu, Songze Li,
  Tonghua Su
Categories: cs.LG
\\
  Large Language Models (LLMs) are increasingly deployed as reasoning systems,
where reasoning paradigms - such as Chain-of-Thought (CoT) and multi-agent
systems (MAS) - play a critical role, yet their relative effectiveness and
cost-accuracy trade-offs remain poorly understood. In this work, we conduct a
comprehensive and unified evaluation of reasoning paradigms, spanning direct
single-model generation, CoT-augmented single-model reasoning, and
representative MAS workflows, characterizing their reasoning performance across
a diverse suite of closed-form benchmarks. Beyond overall performance, we probe
role-specific capability demands in MAS using targeted role isolation analyses,
and analyze cost-accuracy trade-offs to identify which MAS workflows offer a
favorable balance between cost and accuracy, and which incur prohibitive
overhead for marginal gains. We further introduce MIMeBench, a new open-ended
benchmark that targets two foundational yet underexplored semantic capabilities
- semantic abstraction and contrastive discrimination - thereby providing an
alternative evaluation axis beyond closed-form accuracy and enabling
fine-grained assessment of semantic competence that is difficult to capture
with existing benchmarks. Our results show that increased structural complexity
does not consistently lead to improved reasoning performance, with its benefits
being highly dependent on the properties and suitability of the reasoning
paradigm itself. The codes are released at
https://gitcode.com/HIT1920/OpenLLMBench.
\\ ( https://arxiv.org/abs/2601.13243 ,  9432kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13244
Date: Mon, 19 Jan 2026 17:26:49 GMT   (933kb)

Title: Do Instruction-Tuned Models Always Perform Better Than Base Models?
  Evidence from Math and Domain-Shifted Benchmarks
Authors: Prateek Munjal, Clement Christophe, Ronnie Rajan, Praveenkumar Kanithi
Categories: cs.LG
\\
  Instruction finetuning is standard practice for improving LLM performance,
yet it remains unclear whether it enhances reasoning or merely induces
surface-level pattern matching. We investigate this by evaluating base and
instruction-tuned models on standard math benchmarks, structurally perturbed
variants, and domain-shifted tasks. Our analysis highlights two key (often
overlooked) limitations of instruction tuning. First, the performance advantage
is unstable and depends heavily on evaluation settings. In zero-shot CoT
settings on GSM8K, base models consistently outperform instruction-tuned
variants, with drops as high as 32.67\% (Llama3-70B). Instruction-tuned models
only match or exceed this performance when provided with few-shot exemplars,
suggesting a reliance on specific prompting patterns rather than intrinsic
reasoning. Second, tuning gains are brittle under distribution shift. Our
results show that base models surpass instruction-tuned variants on the
domain-specific MedCalc benchmark. Additionally, instruction-tuned models show
sharp declines on perturbed datasets, indicating sensitivity to prompt
structure over robust reasoning.
\\ ( https://arxiv.org/abs/2601.13244 ,  933kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13272
Date: Mon, 19 Jan 2026 18:17:25 GMT   (674kb)

Title: Multi-level Monte Carlo Dropout for Efficient Uncertainty Quantification
Authors: Aaron Pim, Tristan Pryer
Categories: cs.LG stat.CO stat.ML
Comments: 26 pages, 11 figures
\\
  We develop a multilevel Monte Carlo (MLMC) framework for uncertainty
quantification with Monte Carlo dropout. Treating dropout masks as a source of
epistemic randomness, we define a fidelity hierarchy by the number of
stochastic forward passes used to estimate predictive moments. We construct
coupled coarse--fine estimators by reusing dropout masks across fidelities,
yielding telescoping MLMC estimators for both predictive means and predictive
variances that remain unbiased for the corresponding dropout-induced quantities
while reducing sampling variance at fixed evaluation budget. We derive explicit
bias, variance and effective cost expressions, together with sample-allocation
rules across levels. Numerical experiments on forward and inverse PINNs--Uzawa
benchmarks confirm the predicted variance rates and demonstrate efficiency
gains over single-level MC-dropout at matched cost.
\\ ( https://arxiv.org/abs/2601.13272 ,  674kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13284
Date: Mon, 19 Jan 2026 18:31:31 GMT   (4422kb)

Title: Balancing Classification and Calibration Performance in Decision-Making
  LLMs via Calibration Aware Reinforcement Learning
Authors: Duygu Nur Yaldiz, Evangelia Spiliopoulou, Zheng Qi, Siddharth Varia,
  Srikanth Doss, Nikolaos Pappas
Categories: cs.LG
\\
  Large language models (LLMs) are increasingly deployed in decision-making
tasks, where not only accuracy but also reliable confidence estimates are
essential. Well-calibrated confidence enables downstream systems to decide when
to trust a model and when to defer to fallback mechanisms. In this work, we
conduct a systematic study of calibration in two widely used fine-tuning
paradigms: supervised fine-tuning (SFT) and reinforcement learning with
verifiable rewards (RLVR). We show that while RLVR improves task performance,
it produces extremely overconfident models, whereas SFT yields substantially
better calibration, even under distribution shift, though with smaller
performance gains. Through targeted experiments, we diagnose RLVR's failure,
showing that decision tokens act as extraction steps of the decision in
reasoning traces and do not carry confidence information, which prevents
reinforcement learning from surfacing calibrated alternatives. Based on this
insight, we propose a calibration-aware reinforcement learning formulation that
directly adjusts decision-token probabilities. Our method preserves RLVR's
accuracy level while mitigating overconfidence, reducing ECE scores up to 9
points.
\\ ( https://arxiv.org/abs/2601.13284 ,  4422kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13295
Date: Mon, 19 Jan 2026 18:48:37 GMT   (3647kb)

Title: CooperBench: Why Coding Agents Cannot be Your Teammates Yet
Authors: Arpandeep Khatua, Hao Zhu, Peter Tran, Arya Prabhudesai, Frederic
  Sadrieh, Johann K. Lieberwirth, Xinkai Yu, Yicheng Fu, Michael J. Ryan,
  Jiaxin Pei, Diyi Yang
Categories: cs.LG cs.AI cs.CL cs.MA cs.SI
Comments: https://cooperbench.com
\\
  Resolving team conflicts requires not only task-specific competence, but also
social intelligence to find common ground and build consensus. As AI agents
increasingly collaborate on complex work, they must develop coordination
capabilities to function as effective teammates. Yet we hypothesize that
current agents lack these capabilities. To test this, we introduce CooperBench,
a benchmark of over 600 collaborative coding tasks across 12 libraries in 4
programming languages. Each task assigns two agents different features that can
be implemented independently but may conflict without proper coordination.
Tasks are grounded in real open-source repositories with expert-written tests.
Evaluating state-of-the-art coding agents, we observe the curse of
coordination: agents achieve on average 30% lower success rates when working
together compared to performing both tasks individually. This contrasts sharply
with human teams, where adding teammates typically improves productivity. Our
analysis reveals three key issues: (1) communication channels become jammed
with vague, ill-timed, and inaccurate messages; (2) even with effective
communication, agents deviate from their commitments; and (3) agents often hold
incorrect expectations about others' plans and communication. Through
large-scale simulation, we also observe rare but interesting emergent
coordination behavior including role division, resource division, and
negotiation. Our research presents a novel benchmark for collaborative coding
and calls for a shift from pursuing individual agent capability to developing
social intelligence.
\\ ( https://arxiv.org/abs/2601.13295 ,  3647kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13303
Date: Mon, 19 Jan 2026 18:59:22 GMT   (20kb)

Title: Verifying Local Robustness of Pruned Safety-Critical Networks
Authors: Minh Le, Phuong Cao
Categories: cs.LG
\\
  Formal verification of Deep Neural Networks (DNNs) is essential for
safety-critical applications, ranging from surgical robotics to NASA JPL
autonomous systems. However, the computational cost of verifying large-scale
models remains a significant barrier to adoption. This paper investigates the
impact of pruning on formal local robustness certificates with different
ratios. Using the state-of-the-art $\alpha,\beta$-CROWN verifier, we evaluate
ResNet4 models across varying pruning ratios on MNIST and, more importantly, on
the NASA JPL Mars Frost Identification datasets. Our findings demonstrate a
non-linear relationship: light pruning (40%) in MNIST and heavy pruning
(70%-90%) in JPL improve verifiability, allowing models to outperform unpruned
baselines in proven $L_\infty$ robustness properties. This suggests that
reduced connectivity simplifies the search space for formal solvers and that
the optimal pruning ratio varies significantly between datasets. This research
highlights the complex nature of model compression, offering critical insights
into selecting the optimal pruning ratio for deploying efficient, yet formally
verified, DNNs in high-stakes environments where reliability is non-negotiable.
\\ ( https://arxiv.org/abs/2601.13303 ,  20kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13350
Date: Mon, 19 Jan 2026 19:38:59 GMT   (322kb)

Title: Beyond Mapping : Domain-Invariant Representations via Spectral Embedding
  of Optimal Transport Plans
Authors: Abdel Djalil Sad Saoud, Fred Maurice Ngol\`e Mboula, Hanane Slimani
Categories: cs.LG
Comments: 5 pages, 2 figures
\\
  Distributional shifts between training and inference time data remain a
central challenge in machine learning, often leading to poor performance. It
motivated the study of principled approaches for domain alignment, such as
optimal transport based unsupervised domain adaptation, that relies on
approximating Monge map using transport plans, which is sensitive to the
transport problem regularization strategy and hyperparameters, and might yield
biased domains alignment. In this work, we propose to interpret smoothed
transport plans as adjacency matrices of bipartite graphs connecting source to
target domain and derive domain-invariant samples' representations through
spectral embedding. We evaluate our approach on acoustic adaptation benchmarks
for music genre recognition, music-speech discrimination, as well as electrical
cable defect detection and classification tasks using time domain reflection in
different diagnosis settings, achieving overall strong performances.
\\ ( https://arxiv.org/abs/2601.13350 ,  322kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13357
Date: Mon, 19 Jan 2026 19:51:05 GMT   (112kb)

Title: On the Relation of State Space Models and Hidden Markov Models
Authors: Aydin Ghojogh, M.Hadi Sepanj, Benyamin Ghojogh
Categories: cs.LG cs.CL cs.SY eess.AS eess.SY
\\
  State Space Models (SSMs) and Hidden Markov Models (HMMs) are foundational
frameworks for modeling sequential data with latent variables and are widely
used in signal processing, control theory, and machine learning. Despite their
shared temporal structure, they differ fundamentally in the nature of their
latent states, probabilistic assumptions, inference procedures, and training
paradigms. Recently, deterministic state space models have re-emerged in
natural language processing through architectures such as S4 and Mamba, raising
new questions about the relationship between classical probabilistic SSMs,
HMMs, and modern neural sequence models.
  In this paper, we present a unified and systematic comparison of HMMs, linear
Gaussian state space models, Kalman filtering, and contemporary NLP state space
models. We analyze their formulations through the lens of probabilistic
graphical models, examine their inference algorithms -- including
forward-backward inference and Kalman filtering -- and contrast their learning
procedures via Expectation-Maximization and gradient-based optimization. By
highlighting both structural similarities and semantic differences, we clarify
when these models are equivalent, when they fundamentally diverge, and how
modern NLP SSMs relate to classical probabilistic models. Our analysis bridges
perspectives from control theory, probabilistic modeling, and modern deep
learning.
\\ ( https://arxiv.org/abs/2601.13357 ,  112kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13365
Date: Mon, 19 Jan 2026 20:01:59 GMT   (137kb)

Title: CausationEntropy: Pythonic Optimal Causation Entropy
Authors: Kevin Slote, Jeremie Fish, Erik Bollt
Categories: cs.LG physics.data-an
\\
  Optimal Causation Entropy (oCSE) is a robust causal network modeling
technique that reveals causal networks from dynamical systems and coupled
oscillators, distinguishing direct from indirect paths. CausationEntropy is a
Python package that implements oCSE and several of its significant
optimizations and methodological extensions. In this paper, we introduce the
version 1.1 release of CausationEntropy, which includes new synthetic data
generators, plotting tools, and several advanced information-theoretical causal
network discovery algorithms with criteria for estimating Gaussian, k-nearest
neighbors (kNN), geometric k-nearest neighbors (geometric-kNN), kernel density
(KDE) and Poisson entropic estimators. The package is easy to install from the
PyPi software repository, is thoroughly documented, supplemented with extensive
code examples, and is modularly structured to support future additions. The
entire codebase is released under the MIT license and is available on GitHub
and through PyPi Repository. We expect this package to serve as a benchmark
tool for causal discovery in complex dynamical systems.
\\ ( https://arxiv.org/abs/2601.13365 ,  137kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13398
Date: Mon, 19 Jan 2026 21:09:48 GMT   (3941kb)

Title: Can LLMs Compress (and Decompress)? Evaluating Code Understanding and
  Execution via Invertibility
Authors: Nickil Maveli, Antonio Vergari, Shay B. Cohen
Categories: cs.LG cs.AI cs.PL
Comments: 32 pages (preprint)
\\
  LLMs demonstrate strong performance on code benchmarks, yet round-trip code
execution reveals limitations in their ability to maintain consistent reasoning
across forward and backward execution. We present RoundTripCodeEval (RTCE), a
comprehensive benchmark consisting of four distinct code execution reasoning
tasks designed to rigorously test round-trip consistency. RTCE provides an
execution-free, exact-match evaluation of bijection fidelity, assessing whether
models preserve a consistent one-to-one mapping between encoding and decoding
operations across various algorithms and directions. We systematically evaluate
state-of-the-art Code-LLMs using zero-shot prompting, supervised fine-tuning on
execution traces, and self-reflection mechanisms. Each yields modest
improvements, but none closes the gap, indicating that current LLMs struggle
with true round-trip consistency, which demonstrates that they lack the
internal coherence required for trustworthy code reasoning. RTCE surfaces
several new and previously unmeasured insights that are not captured by
existing I/O-prediction, execution-reasoning, or round-trip natural-language
benchmarks. We will release the code and the dataset upon acceptance.
\\ ( https://arxiv.org/abs/2601.13398 ,  3941kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13422
Date: Mon, 19 Jan 2026 22:09:08 GMT   (777kb)

Title: TrustEnergy: A Unified Framework for Accurate and Reliable User-level
  Energy Usage Prediction
Authors: Dahai Yu, Rongchao Xu, Dingyi Zhuang, Yuheng Bu, Shenhao Wang, Guang
  Wang
Categories: cs.LG cs.AI
\\
  Energy usage prediction is important for various real-world applications,
including grid management, infrastructure planning, and disaster response.
Although a plethora of deep learning approaches have been proposed to perform
this task, most of them either overlook the essential spatial correlations
across households or fail to scale to individualized prediction, making them
less effective for accurate fine-grained user-level prediction. In addition,
due to the dynamic and uncertain nature of energy usage caused by various
factors such as extreme weather events, quantifying uncertainty for reliable
prediction is also significant, but it has not been fully explored in existing
work. In this paper, we propose a unified framework called TrustEnergy for
accurate and reliable user-level energy usage prediction. There are two key
technical components in TrustEnergy, (i) a Hierarchical Spatiotemporal
Representation module to efficiently capture both macro and micro energy usage
patterns with a novel memory-augmented spatiotemporal graph neural network, and
(ii) an innovative Sequential Conformalized Quantile Regression module to
dynamically adjust uncertainty bounds to ensure valid prediction intervals over
time, without making strong assumptions about the underlying data distribution.
We implement and evaluate our TrustEnergy framework by working with an
electricity provider in Florida, and the results show our TrustEnergy can
achieve a 5.4% increase in prediction accuracy and 5.7% improvement in
uncertainty quantification compared to state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2601.13422 ,  777kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13435
Date: Mon, 19 Jan 2026 22:41:31 GMT   (2761kb)

Title: A Learnable Wavelet Transformer for Long-Short Equity Trading and
  Risk-Adjusted Return Optimization
Authors: Shuozhe Li, Du Cheng, Leqi Liu
Categories: cs.LG cs.AI
\\
  Learning profitable intraday trading policies from financial time series is
challenging due to heavy noise, non-stationarity, and strong cross-sectional
dependence among related assets. We propose \emph{WaveLSFormer}, a learnable
wavelet-based long-short Transformer that jointly performs multi-scale
decomposition and return-oriented decision learning. Specifically, a learnable
wavelet front-end generates low-/high-frequency components via an end-to-end
trained filter bank, guided by spectral regularizers that encourage stable and
well-separated frequency bands. To fuse multi-scale information, we introduce a
low-guided high-frequency injection (LGHI) module that refines low-frequency
representations with high-frequency cues while controlling training stability.
The model outputs a portfolio of long/short positions that is rescaled to
satisfy a fixed risk budget, and is optimized directly with a trading objective
and risk-aware regularization. Extensive experiments on five years of hourly
data across six industry groups, evaluated over ten random seeds, demonstrate
that WaveLSFormer consistently outperforms MLP, LSTM and Transformer backbones,
with and without fixed discrete wavelet front-ends. On average in all
industries, WaveLSFormer achieves a cumulative overall strategy return of
$0.607 \pm 0.045$ and a Sharpe ratio of $2.157 \pm 0.166$, substantially
improving both profitability and risk-adjusted returns over the strongest
baselines.
\\ ( https://arxiv.org/abs/2601.13435 ,  2761kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13445
Date: Mon, 19 Jan 2026 23:02:33 GMT   (28563kb)

Title: BladeSDF : Unconditional and Conditional Generative Modeling of
  Representative Blade Geometries Using Signed Distance Functions
Authors: Ashish S. Nair, Sandipp Krishnan Ravi, Itzel Salgado, Changjie Sun,
  Sayan Ghosh, Liping Wang
Categories: cs.LG physics.comp-ph
\\
  Generative AI has emerged as a transformative paradigm in engineering design,
enabling automated synthesis and reconstruction of complex 3D geometries while
preserving feasibility and performance relevance. This paper introduces a
domain-specific implicit generative framework for turbine blade geometry using
DeepSDF, addressing critical gaps in performance-aware modeling and
manufacturable design generation. The proposed method leverages a continuous
signed distance function (SDF) representation to reconstruct and generate
smooth, watertight geometries with quantified accuracy. It establishes an
interpretable, near-Gaussian latent space that aligns with blade-relevant
parameters, such as taper and chord ratios, enabling controlled exploration and
unconditional synthesis through interpolation and Gaussian sampling. In
addition, a compact neural network maps engineering descriptors, such as
maximum directional strains, to latent codes, facilitating the generation of
performance-informed geometry. The framework achieves high reconstruction
fidelity, with surface distance errors concentrated within $1\%$ of the maximum
blade dimension, and demonstrates robust generalization to unseen designs. By
integrating constraints, objectives, and performance metrics, this approach
advances beyond traditional 2D-guided or unconstrained 3D pipelines, offering a
practical and interpretable solution for data-driven turbine blade modeling and
concept generation.
\\ ( https://arxiv.org/abs/2601.13445 ,  28563kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13448
Date: Mon, 19 Jan 2026 23:05:07 GMT   (3052kb)

Title: Fairness-informed Pareto Optimization : An Efficient Bilevel Framework
Authors: Sofiane Tanji and Samuel Vaiter and Yassine Laguel
Categories: cs.LG math.OC stat.ML
\\
  Despite their promise, fair machine learning methods often yield
Pareto-inefficient models, in which the performance of certain groups can be
improved without degrading that of others. This issue arises frequently in
traditional in-processing approaches such as fairness-through-regularization.
In contrast, existing Pareto-efficient approaches are biased towards a certain
perspective on fairness and fail to adapt to the broad range of fairness
metrics studied in the literature. In this paper, we present BADR, a simple
framework to recover the optimal Pareto-efficient model for any fairness
metric. Our framework recovers its models through a Bilevel Adaptive
Rescalarisation procedure. The lower level is a weighted empirical risk
minimization task where the weights are a convex combination of the groups,
while the upper level optimizes the chosen fairness objective. We equip our
framework with two novel large-scale, single-loop algorithms, BADR-GD and
BADR-SGD, and establish their convergence guarantees. We release badr, an
open-source Python toolbox implementing our framework for a variety of learning
tasks and fairness metrics. Finally, we conduct extensive numerical experiments
demonstrating the advantages of BADR over existing Pareto-efficient approaches
to fairness.
\\ ( https://arxiv.org/abs/2601.13448 ,  3052kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13456
Date: Mon, 19 Jan 2026 23:17:10 GMT   (714kb)

Title: Federated Learning Under Temporal Drift - Mitigating Catastrophic
  Forgetting via Experience Replay
Authors: Sahasra Kokkula, Daniel David, Aaditya Baruah
Categories: cs.LG cs.DC
Comments: 8 pages, 5 figures. Course project for Neural Networks & Deep
  Learning COMSW4776 course at Columbia University
\\
  Federated Learning struggles under temporal concept drift where client data
distributions shift over time. We demonstrate that standard FedAvg suffers
catastrophic forgetting under seasonal drift on Fashion-MNIST, with accuracy
dropping from 74% to 28%. We propose client-side experience replay, where each
client maintains a small buffer of past samples mixed with current data during
local training. This simple approach requires no changes to server aggregation.
Experiments show that a 50-sample-per-class buffer restores performance to
78-82%, effectively preventing forgetting. Our ablation study reveals a clear
memory-accuracy trade-off as buffer size increases.
\\ ( https://arxiv.org/abs/2601.13456 ,  714kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13463
Date: Mon, 19 Jan 2026 23:37:31 GMT   (7325kb)

Title: Quantum Qualifiers for Neural Network Model Selection in Hadronic
  Physics
Authors: Brandon B. Le and D. Keller
Categories: cs.LG hep-ph nucl-th quant-ph
Comments: 12 pages, 5 figures. Proceedings for the 26th International Symposium
  on Spin Physics (SPIN2025), September 21-26, 2025; Qingdao, Shandong, China
\\
  As quantum machine-learning architectures mature, a central challenge is no
longer their construction, but identifying the regimes in which they offer
practical advantages over classical approaches. In this work, we introduce a
framework for addressing this question in data-driven hadronic physics problems
by developing diagnostic tools - centered on a quantitative quantum qualifier -
that guide model selection between classical and quantum deep neural networks
based on intrinsic properties of the data. Using controlled classification and
regression studies, we show how relative model performance follows systematic
trends in complexity, noise, and dimensionality, and how these trends can be
distilled into a predictive criterion. We then demonstrate the utility of this
approach through an application to Compton form factor extraction from deeply
virtual Compton scattering, where the quantum qualifier identifies kinematic
regimes favorable to quantum models. Together, these results establish a
principled framework for deploying quantum machine-learning tools in precision
hadronic physics.
\\ ( https://arxiv.org/abs/2601.13463 ,  7325kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13474
Date: Tue, 20 Jan 2026 00:08:31 GMT   (2376kb)

Title: Preconditioning Benefits of Spectral Orthogonalization in Muon
Authors: Jianhao Ma, Yu Huang, Yuejie Chi, Yuxin Chen
Categories: cs.LG cs.AI math.OC stat.ML
\\
  The Muon optimizer, a matrix-structured algorithm that leverages spectral
orthogonalization of gradients, is a milestone in the pretraining of large
language models. However, the underlying mechanisms of Muon -- particularly the
role of gradient orthogonalization -- remain poorly understood, with very few
works providing end-to-end analyses that rigorously explain its advantages in
concrete applications. We take a step by studying the effectiveness of a
simplified variant of Muon through two case studies: matrix factorization, and
in-context learning of linear transformers. For both problems, we prove that
simplified Muon converges linearly with iteration complexities independent of
the relevant condition number, provably outperforming gradient descent and
Adam. Our analysis reveals that the Muon dynamics decouple into a collection of
independent scalar sequences in the spectral domain, each exhibiting similar
convergence behavior. Our theory formalizes the preconditioning effect induced
by spectral orthogonalization, offering insight into Muon's effectiveness in
these matrix optimization problems and potentially beyond.
\\ ( https://arxiv.org/abs/2601.13474 ,  2376kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13476
Date: Tue, 20 Jan 2026 00:17:54 GMT   (1388kb)

Title: A Unified Variational Imputation Framework for Electric Vehicle Charging
  Data Using Retrieval-Augmented Language Model
Authors: Jinhao Li and Hao Wang
Categories: cs.LG cs.AI
Comments: 15 pages
Journal-ref: IEEE Transactions on Smart Grid, 2026
DOI: 10.1109/TSG.2026.3656697
\\
  The reliability of data-driven applications in electric vehicle (EV)
infrastructure, such as charging demand forecasting, hinges on the availability
of complete, high-quality charging data. However, real-world EV datasets are
often plagued by missing records, and existing imputation methods are
ill-equipped for the complex, multimodal context of charging data, often
relying on a restrictive one-model-per-station paradigm that ignores valuable
inter-station correlations. To address these gaps, we develop a novel
PRobabilistic variational imputation framework that leverages the power of
large lAnguage models and retrIeval-augmented Memory (PRAIM). PRAIM employs a
pre-trained language model to encode heterogeneous data, spanning time-series
demand, calendar features, and geospatial context, into a unified, semantically
rich representation. This is dynamically fortified by retrieval-augmented
memory that retrieves relevant examples from the entire charging network,
enabling a single, unified imputation model empowered by variational neural
architecture to overcome data sparsity. Extensive experiments on four public
datasets demonstrate that PRAIM significantly outperforms established baselines
in both imputation accuracy and its ability to preserve the original data's
statistical distribution, leading to substantial improvements in downstream
forecasting performance.
\\ ( https://arxiv.org/abs/2601.13476 ,  1388kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13522
Date: Tue, 20 Jan 2026 02:18:20 GMT   (232kb)

Title: StoTAM: Stochastic Alternating Minimization for Tucker-Structured Tensor
  Sensing
Authors: Shuang Li
Categories: cs.LG math.OC
\\
  Low-rank tensor sensing is a fundamental problem with broad applications in
signal processing and machine learning. Among various tensor models,
low-Tucker-rank tensors are particularly attractive for capturing multi-mode
subspace structures in high-dimensional data. Existing recovery methods either
operate on the full tensor variable with expensive tensor projections, or adopt
factorized formulations that still rely on full-gradient computations, while
most stochastic factorized approaches are restricted to tensor decomposition
settings. In this work, we propose a stochastic alternating minimization
algorithm that operates directly on the core tensor and factor matrices under a
Tucker factorization. The proposed method avoids repeated tensor projections
and enables efficient mini-batch updates on low-dimensional tensor factors.
Numerical experiments on synthetic tensor sensing demonstrate that the proposed
algorithm exhibits favorable convergence behavior in wall-clock time compared
with representative stochastic tensor recovery baselines.
\\ ( https://arxiv.org/abs/2601.13522 ,  232kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13534
Date: Tue, 20 Jan 2026 02:45:03 GMT   (13683kb)

Title: MN-TSG:Continuous Time Series Generation with Irregular Observations
Authors: Xu Zhang, Junwei Deng, Chang Xu, Hao Li, Jiang Bian
Categories: cs.LG cs.AI
Comments: 34 pages
\\
  Time series generation (TSG) plays a critical role in a wide range of
domains, such as healthcare. However, most existing methods assume regularly
sampled observations and fixed output resolutions, which are often misaligned
with real-world scenarios where data are irregularly sampled and sparsely
observed. This mismatch is particularly problematic in applications such as
clinical monitoring, where irregular measurements must support downstream tasks
requiring continuous and high-resolution time series.
  Neural Controlled Differential Equations (NCDEs) have shown strong potential
for modeling irregular time series, yet they still face challenges in capturing
complex dynamic temporal patterns and supporting continuous TSG. To address
these limitations, we propose MN-TSG, a novel framework that explores
Mixture-of-Experts (MoE)-based NCDEs and integrates them with existing TSG
models for irregular and continuous generation tasks.
  The core of MN-TSG lies in a MoE-NCDE architecture with dynamically
parameterized expert functions and a decoupled design that facilitates more
effective optimization of MoE dynamics. Furthermore, we leverage existing TSG
models to learn the joint distribution over the mixture of experts and the
generated time series. This enables the framework not only to generate new
samples, but also to produce appropriate expert configurations tailored to each
sample, thereby supporting refined continuous TSG.
  Extensive experiments on ten public and synthetic datasets demonstrate the
effectiveness of MN-TSG, consistently outperforming strong TSG baselines on
both irregular-to-regular and irregular-to-continuous generation tasks.
\\ ( https://arxiv.org/abs/2601.13534 ,  13683kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13548
Date: Tue, 20 Jan 2026 03:15:27 GMT   (14351kb)

Title: Patterning: The Dual of Interpretability
Authors: George Wang, Daniel Murfet
Categories: cs.LG
\\
  Mechanistic interpretability aims to understand how neural networks
generalize beyond their training data by reverse-engineering their internal
structures. We introduce patterning as the dual problem: given a desired form
of generalization, determine what training data produces it. Our approach is
based on susceptibilities, which measure how posterior expectation values of
observables respond to infinitesimal shifts in the data distribution. Inverting
this linear response relationship yields the data intervention that steers the
model toward a target internal configuration. We demonstrate patterning in a
small language model, showing that re-weighting training data along principal
susceptibility directions can accelerate or delay the formation of structure,
such as the induction circuit. In a synthetic parentheses balancing task where
multiple algorithms achieve perfect training accuracy, we show that patterning
can select which algorithm the model learns by targeting the local learning
coefficient of each solution. These results establish that the same
mathematical framework used to read internal structure can be inverted to write
it.
\\ ( https://arxiv.org/abs/2601.13548 ,  14351kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13563
Date: Tue, 20 Jan 2026 03:39:33 GMT   (311kb)

Title: ButterflyMoE: Sub-Linear Ternary Experts via Structured Butterfly Orbits
Authors: Aryan Karmore
Categories: cs.LG cs.AI
\\
  Linear memory scaling stores $N$ independent expert weight matrices requiring
$\mathcal{O}(N \cdot d^2)$ memory, which exceeds edge devices memory budget.
Current compression methods like quantization, pruning and low-rank
factorization reduce constant factors but leave the scaling bottleneck
unresolved. We introduce ButterflyMoE, a method that treats experts not as
independent weight matrices but as geometric reorientations of a unified shared
quantized substrate. Diversity among experts arises from viewing different
angles of shared capacity, not from redundant storage. By applying learned
rotations to a shared ternary prototype, each expert yields $\mathcal{O}(d^2 +
N \cdot d \log d)$ memory -- sub-linear in the number of experts. The key
insight: training these rotations with quantization reduces activation outliers
and stabilizes extreme low bit training, where static methods collapse. Across
language modeling benchmarks, ButterflyMoE achieves 150 times memory reduction
at 256 experts with negligible accuracy loss. This allows 64 experts to fit on
4GB devices compared to standard MoE's 8 experts, showing geometric
parametrization breaks linear scaling.
\\ ( https://arxiv.org/abs/2601.13563 ,  311kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13564
Date: Tue, 20 Jan 2026 03:41:02 GMT   (14473kb)

Title: Multi-objective fluorescent molecule design with a data-physics
  dual-driven generative framework
Authors: Yanheng Li, Zhichen Pu, Lijiang Yang, Zehao Zhou, Yi Qin Gao
Categories: cs.LG cs.AI physics.chem-ph q-bio.BM
Comments: Total 43 pages: 32 pages Main Text + 11 pages SI
\\
  Designing fluorescent small molecules with tailored optical and
physicochemical properties requires navigating vast, underexplored chemical
space while satisfying multiple objectives and constraints. Conventional
generate-score-screen approaches become impractical under such realistic design
specifications, owing to their low search efficiency, unreliable
generalizability of machine-learning prediction, and the prohibitive cost of
quantum chemical calculation. Here we present LUMOS, a data-and-physics driven
framework for inverse design of fluorescent molecules. LUMOS couples generator
and predictor within a shared latent representation, enabling direct
specification-to-molecule design and efficient exploration. Moreover, LUMOS
combines neural networks with a fast time-dependent density functional theory
(TD-DFT) calculation workflow to build a suite of complementary predictors
spanning different trade-offs in speed, accuracy, and generalizability,
enabling reliable property prediction across diverse scenarios. Finally, LUMOS
employs a property-guided diffusion model integrated with multi-objective
evolutionary algorithms, enabling de novo design and molecular optimization
under multiple objectives and constraints. Across comprehensive benchmarks,
LUMOS consistently outperforms baseline models in terms of accuracy,
generalizability and physical plausibility for fluorescence property
prediction, and demonstrates superior performance in multi-objective scaffold-
and fragment-level molecular optimization. Further validation using TD-DFT and
molecular dynamics (MD) simulations demonstrates that LUMOS can generate valid
fluorophores that meet various target specifications. Overall, these results
establish LUMOS as a data-physics dual-driven framework for general fluorophore
inverse design.
\\ ( https://arxiv.org/abs/2601.13564 ,  14473kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13566
Date: Tue, 20 Jan 2026 03:50:02 GMT   (3145kb)

Title: Self-Improvement as Coherence Optimization: A Theoretical Account
Authors: Tianyi Qiu, Ahmed Hani Ismail, Zhonghao He, Shi Feng
Categories: cs.LG cs.AI cs.CL
Comments: 39 pages
\\
  Can language models improve their accuracy without external supervision?
Methods such as debate, bootstrap, and internal coherence maximization achieve
this surprising feat, even matching golden finetuning performance. Yet why they
work remains theoretically unclear. We show that they are all special cases of
coherence optimization: finding a context-to-behavior mapping that's most
compressible and jointly predictable. We prove that coherence optimization is
equivalent to description-length regularization, and that among all such
regularization schemes, it is optimal for semi-supervised learning when the
regularizer is derived from a pretrained model. Our theory, supported by
preliminary experiments, explains why feedback-free self-improvement works and
predicts when it should succeed or fail.
\\ ( https://arxiv.org/abs/2601.13566 ,  3145kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13569
Date: Tue, 20 Jan 2026 03:55:18 GMT   (794kb)

Title: DRGW: Learning Disentangled Representations for Robust Graph
  Watermarking
Authors: Jiasen Li, Yanwei Liu, Zhuoyi Shang, Xiaoyan Gu, Weiping Wang
Categories: cs.LG cs.CR
Comments: Published at The Web Conference 2026 (WWW '26)
\\
  Graph-structured data is foundational to numerous web applications, and
watermarking is crucial for protecting their intellectual property and ensuring
data provenance. Existing watermarking methods primarily operate on graph
structures or entangled graph representations, which compromise the
transparency and robustness of watermarks due to the information coupling in
representing graphs and uncontrollable discretization in transforming
continuous numerical representations into graph structures. This motivates us
to propose DRGW, the first graph watermarking framework that addresses these
issues through disentangled representation learning. Specifically, we design an
adversarially trained encoder that learns an invariant structural
representation against diverse perturbations and derives a statistically
independent watermark carrier, ensuring both robustness and transparency of
watermarks. Meanwhile, we devise a graph-aware invertible neural network to
provide a lossless channel for watermark embedding and extraction, guaranteeing
high detectability and transparency of watermarks. Additionally, we develop a
structure-aware editor that resolves the issue of latent modifications into
discrete graph edits, ensuring robustness against structural perturbations.
Experiments on diverse benchmark datasets demonstrate the superior
effectiveness of DRGW.
\\ ( https://arxiv.org/abs/2601.13569 ,  794kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13570
Date: Tue, 20 Jan 2026 03:56:06 GMT   (9171kb)

Title: GeoDynamics: A Geometric State-Space Neural Network for Understanding
  Brain Dynamics on Riemannian Manifolds
Authors: Tingting Dan and Jiaqi Ding and Guorong Wu
Categories: cs.LG cs.AI
Comments: Accepted to NeurIPS 2025
\\
  State-space models (SSMs) have become a cornerstone for unraveling brain
dynamics, revealing how latent neural states evolve over time and give rise to
observed signals. By combining the flexibility of deep learning with the
principled dynamical structure of SSMs, recent studies have achieved powerful
fits to functional neuroimaging data. However, most existing approaches still
view the brain as a set of loosely connected regions or impose oversimplified
network priors, falling short of a truly holistic and self-organized dynamical
system perspective. Brain functional connectivity (FC) at each time point
naturally forms a symmetric positive definite (SPD) matrix, which resides on a
curved Riemannian manifold rather than in Euclidean space. Capturing the
trajectories of these SPD matrices is key to understanding how coordinated
networks support cognition and behavior. To this end, we introduce GeoDynamics,
a geometric state-space neural network that tracks latent brain-state
trajectories directly on the high-dimensional SPD manifold. GeoDynamics embeds
each connectivity matrix into a manifold-aware recurrent framework, learning
smooth and geometry-respecting transitions that reveal task-driven state
changes and early markers of Alzheimer's disease, Parkinson's disease, and
autism. Beyond neuroscience, we validate GeoDynamics on human action
recognition benchmarks (UTKinect, Florence, HDM05), demonstrating its
scalability and robustness in modeling complex spatiotemporal dynamics across
diverse domains.
\\ ( https://arxiv.org/abs/2601.13570 ,  9171kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13572
Date: Tue, 20 Jan 2026 03:56:53 GMT   (1334kb)

Title: Behavior Knowledge Merge in Reinforced Agentic Models
Authors: Xiangchi Yuan, Dachuan Shi, Chunhui Zhang, Zheyuan Liu, Shenglong Yao,
  Soroush Vosoughi, Wenke Lee
Categories: cs.LG
\\
  Reinforcement learning (RL) is central to post-training, particularly for
agentic models that require specialized reasoning behaviors. In this setting,
model merging offers a practical mechanism for integrating multiple RL-trained
agents from different tasks into a single generalist model. However, existing
merging methods are designed for supervised fine-tuning (SFT), and they are
suboptimal to preserve task-specific capabilities on RL-trained agentic models.
The root is a task-vector mismatch between RL and SFT: on-policy RL induces
task vectors that are highly sparse and heterogeneous, whereas SFT-style
merging implicitly assumes dense and globally comparable task vectors. When
standard global averaging is applied under this mismatch, RL's non-overlapping
task vectors that encode critical task-specific behaviors are reduced and
parameter updates are diluted. To address this issue, we propose Reinforced
Agent Merging (RAM), a distribution-aware merging framework explicitly designed
for RL-trained agentic models. RAM disentangles shared and task-specific unique
parameter updates, averaging shared components while selectively preserving and
rescaling unique ones to counteract parameter update dilution. Experiments
across multiple agent domains and model architectures demonstrate that RAM not
only surpasses merging baselines, but also unlocks synergistic potential among
agents to achieve performance superior to that of specialized agents in their
domains.
\\ ( https://arxiv.org/abs/2601.13572 ,  1334kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13578
Date: Tue, 20 Jan 2026 04:05:13 GMT   (23893kb)

Title: FG-OrIU: Towards Better Forgetting via Feature-Gradient Orthogonality
  for Incremental Unlearning
Authors: Qian Feng, JiaHang Tu, Mintong Kang, Hanbin Zhao, Chao Zhang, Hui Qian
Categories: cs.LG cs.CV
Comments: This paper has been accepted by ICCV 2025. code:
  \url{https://github.com/RAIAN08/FG-OrIU}
\\
  Incremental unlearning (IU) is critical for pre-trained models to comply with
sequential data deletion requests, yet existing methods primarily suppress
parameters or confuse knowledge without explicit constraints on both feature
and gradient level, resulting in \textit{superficial forgetting} where residual
information remains recoverable. This incomplete forgetting risks security
breaches and disrupts retention balance, especially in IU scenarios. We propose
FG-OrIU (\textbf{F}eature-\textbf{G}radient \textbf{Or}thogonality for
\textbf{I}ncremental \textbf{U}nlearning), the first framework unifying
orthogonal constraints on both features and gradients level to achieve deep
forgetting, where the forgetting effect is irreversible. FG-OrIU decomposes
feature spaces via Singular Value Decomposition (SVD), separating forgetting
and remaining class features into distinct subspaces. It then enforces dual
constraints: feature orthogonal projection on both forgetting and remaining
classes, while gradient orthogonal projection prevents the reintroduction of
forgotten knowledge and disruption to remaining classes during updates.
Additionally, dynamic subspace adaptation merges newly forgetting subspaces and
contracts remaining subspaces, ensuring a stable balance between removal and
retention across sequential unlearning tasks. Extensive experiments demonstrate
the effectiveness of our method.
\\ ( https://arxiv.org/abs/2601.13578 ,  23893kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13580
Date: Tue, 20 Jan 2026 04:10:57 GMT   (653kb)

Title: Neural Organ Transplantation (NOT): Checkpoint-Based Modular Adaptation
  for Transformer Models
Authors: Ahmad Al-Zuraiqi
Categories: cs.LG cs.AI
Comments: 27 pages, 8 figures, 16 tables. Decoder-only transformers (124M-20B
  parameters). Complete experimental results and reproducibility details in
  appendices. Code and checkpoints:
  https://github.com/zuraiqi/neural-organ-transplant
\\
  We introduce Neural Organ Transplantation (NOT), a modular adaptation
framework that enables trained transformer layers to function as reusable
transferable checkpoints for domain adaptation. Unlike conventional fine-tuning
approaches that tightly couple trained parameters to specific model instances
and training data, NOT extracts contiguous layer subsets ("donor organs") from
pre-trained models, trains them independently on domain-specific data, and
saves them as standalone checkpoint files that can be transplanted into
compatible recipient models without access to the original training data.
Through experiments on three decoder-only transformer architectures spanning
124M to 20B parameters (GPT-2, TinyLlama, and GPT-OSS), we demonstrate that
donor transplantation substantially outperforms existing adaptation methods,
achieving an order-of-magnitude improvement in perplexity over LoRA while
training significantly faster. The method exhibits position dependence, with
early insertion positions yielding optimal results. Cross-domain transfer at
billion-parameter scale reveals unexpected regularization benefits. These
findings demonstrate that transformer middle layers can support efficient
modular transfer for decoder-only architectures, enabling privacy-preserving
expertise sharing through checkpoint distribution. We note that this approach
is currently limited to decoder-only models; preliminary experiments on
encoder-based architectures show reduced effectiveness.
\\ ( https://arxiv.org/abs/2601.13580 ,  653kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13592
Date: Tue, 20 Jan 2026 04:45:45 GMT   (1910kb)

Title: Machine learning based radiative parameterization scheme and its
  performance in operational reforecast experiments
Authors: Hao Jing, Sa Xiao, Haoyu Li, Huadong Xiao, Wei Xue
Categories: cs.LG cs.AI
\\
  Radiation is typically the most time-consuming physical process in numerical
models. One solution is to use machine learning methods to simulate the
radiation process to improve computational efficiency. From an operational
standpoint, this study investigates critical limitations inherent to hybrid
forecasting frameworks that embed deep neural networks into numerical
prediction models, with a specific focus on two fundamental bottlenecks:
coupling compatibility and long-term integration stability. A residual
convolutional neural network is employed to approximate the Rapid Radiative
Transfer Model for General Circulation Models (RRTMG) within the global
operational system of China Meteorological Administration. We adopted an
offline training and online coupling approach. First, a comprehensive dataset
is generated through model simulations, encompassing all atmospheric columns
both with and without cloud cover. To ensure the stability of the hybrid model,
the dataset is enhanced via experience replay, and additional output
constraints based on physical significance are imposed. Meanwhile, a
LibTorch-based coupling method is utilized, which is more suitable for
real-time operational computations. The hybrid model is capable of performing
ten-day integrated forecasts as required. A two-month operational reforecast
experiment demonstrates that the machine learning emulator achieves accuracy
comparable to that of the traditional physical scheme, while accelerating the
computation speed by approximately eightfold.
\\ ( https://arxiv.org/abs/2601.13592 ,  1910kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13599
Date: Tue, 20 Jan 2026 05:00:26 GMT   (144kb)

Title: Diffusion In Diffusion: Breaking the Autoregressive Bottleneck in Block
  Diffusion Models
Authors: Linrui Ma, Yufei Cui, Kai Han, Yunhe Wang
Categories: cs.LG cs.AI
Comments: Work In Progress
\\
  Block diffusion language models, operating as semi-autoregressive paradigms,
combine the strengths of both autoregressive and diffusion paradigms. However,
their strict unidirectional block dependencies introduce irreversibility and
sacrifice the global planning capabilities for which diffusion models are
renowned. In order to address these issues, we propose Diffusion in Diffusion,
a draft-then-refine framework designed to overcome the irreversibility and
myopia problems inherent in block diffusion models. Our approach first employs
block diffusion to generate rapid drafts using small blocks, then refines these
drafts through global bidirectional diffusion with a larger bidirectional
receptive field. We utilise snapshot confidence remasking to identify the most
critical tokens that require modification, and apply mix-scale training to
expand the block diffusion model's global capabilities. Empirical results
demonstrate that our approach sets a new benchmark for discrete diffusion
models on the OpenWebText dataset. Using just 26% of the fine-tuning budget of
baseline models, we reduce generative perplexity from 25.7 to 21.9,
significantly narrowing the performance gap with autoregressive models.
\\ ( https://arxiv.org/abs/2601.13599 ,  144kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13608
Date: Tue, 20 Jan 2026 05:18:09 GMT   (4528kb)

Title: Fisher-Informed Parameterwise Aggregation for Federated Learning with
  Heterogeneous Data
Authors: Zhipeng Chang and Ting He and Wenrui Hao
Categories: cs.LG
\\
  Federated learning aggregates model updates from distributed clients, but
standard first order methods such as FedAvg apply the same scalar weight to all
parameters from each client. Under non-IID data, these uniformly weighted
updates can be strongly misaligned across clients, causing client drift and
degrading the global model. Here we propose Fisher-Informed Parameterwise
Aggregation (FIPA), a second-order aggregation method that replaces
client-level scalar weights with parameter-specific Fisher Information Matrix
(FIM) weights, enabling true parameter-level scaling that captures how each
client's data uniquely influences different parameters. With low-rank
approximation, FIPA remains communication- and computation-efficient. Across
nonlinear function regression, PDE learning, and image classification, FIPA
consistently improves over averaging-based aggregation, and can be effectively
combined with state-of-the-art client-side optimization algorithms to further
improve image classification accuracy. These results highlight the benefits of
FIPA for federated learning under heterogeneous data distributions.
\\ ( https://arxiv.org/abs/2601.13608 ,  4528kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13645
Date: Tue, 20 Jan 2026 06:27:34 GMT   (362kb)

Title: Quadratic Upper Bound for Boosting Robustness
Authors: Euijin You, Hyang-Won Lee
Categories: cs.LG cs.AI cs.CV
Comments: Accepted at ICML 2025. Published in PMLR 267:72656-72676
Journal-ref: Proceedings of the 42nd International Conference on Machine
  Learning (ICML 2025), Proceedings of Machine Learning Research (PMLR), vol.
  267, pp. 72656-72676, 2025
\\
  Fast adversarial training (FAT) aims to enhance the robustness of models
against adversarial attacks with reduced training time, however, FAT often
suffers from compromised robustness due to insufficient exploration of
adversarial space. In this paper, we develop a loss function to mitigate the
problem of degraded robustness under FAT. Specifically, we derive a quadratic
upper bound (QUB) on the adversarial training (AT) loss function and propose to
utilize the bound with existing FAT methods. Our experimental results show that
applying QUB loss to the existing methods yields significant improvement of
robustness. Furthermore, using various metrics, we demonstrate that this
improvement is likely to result from the smoothened loss landscape of the
resulting model.
\\ ( https://arxiv.org/abs/2601.13645 ,  362kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13653
Date: Tue, 20 Jan 2026 06:39:10 GMT   (1173kb)

Title: TimeART: Towards Agentic Time Series Reasoning via Tool-Augmentation
Authors: Xingjian Wu, Junkai Lu, Zhengyu Li, Xiangfei Qiu, Jilin Hu, Chenjuan
  Guo, Christian S. Jensen, Bin Yang
Categories: cs.LG
\\
  Time series data widely exist in real-world cyber-physical systems. Though
analyzing and interpreting them contributes to significant values, e.g,
disaster prediction and financial risk control, current workflows mainly rely
on human data scientists, which requires significant labor costs and lacks
automation. To tackle this, we introduce TimeART, a framework fusing the
analytical capability of strong out-of-the-box tools and the reasoning
capability of Large Language Models (LLMs), which serves as a fully agentic
data scientist for Time Series Question Answering (TSQA). To teach the
LLM-based Time Series Reasoning Models (TSRMs) strategic tool-use, we also
collect a 100k expert trajectory corpus called TimeToolBench. To enhance TSRMs'
generalization capability, we then devise a four-stage training strategy, which
boosts TSRMs through learning from their own early experiences and
self-reflections. Experimentally, we train an 8B TSRM on TimeToolBench and
equip it with the TimeART framework, and it achieves consistent
state-of-the-art performance on multiple TSQA tasks, which pioneers a novel
approach towards agentic time series reasoning.
\\ ( https://arxiv.org/abs/2601.13653 ,  1173kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13676
Date: Tue, 20 Jan 2026 07:25:32 GMT   (5342kb)

Title: Autoregressive deep learning for real-time simulation of soft tissue
  dynamics during virtual neurosurgery
Authors: Fabian Greifeneder, Wolfgang Fenz, Benedikt Alkin, Johannes
  Brandstetter, Michael Giretzlehner, Philipp Moser
Categories: cs.LG
\\
  Accurate simulation of brain deformation is a key component for developing
realistic, interactive neurosurgical simulators, as complex nonlinear
deformations must be captured to ensure realistic tool-tissue interactions.
However, traditional numerical solvers often fall short in meeting real-time
performance requirements. To overcome this, we introduce a deep learning-based
surrogate model that efficiently simulates transient brain deformation caused
by continuous interactions between surgical instruments and the virtual brain
geometry. Building on Universal Physics Transformers, our approach operates
directly on large-scale mesh data and is trained on an extensive dataset
generated from nonlinear finite element simulations, covering a broad spectrum
of temporal instrument-tissue interaction scenarios. To reduce the accumulation
of errors in autoregressive inference, we propose a stochastic teacher forcing
strategy applied during model training. Specifically, training consists of
short stochastic rollouts in which the proportion of ground truth inputs is
gradually decreased in favor of model-generated predictions. Our results show
that the proposed surrogate model achieves accurate and efficient predictions
across a range of transient brain deformation scenarios, scaling to meshes with
up to 150,000 nodes. The introduced stochastic teacher forcing technique
substantially improves long-term rollout stability, reducing the maximum
prediction error from 6.7 mm to 3.5 mm. We further integrate the trained
surrogate model into an interactive neurosurgical simulation environment,
achieving runtimes below 10 ms per simulation step on consumer-grade inference
hardware. Our proposed deep learning framework enables rapid, smooth and
accurate biomechanical simulations of dynamic brain tissue deformation, laying
the foundation for realistic surgical training environments.
\\ ( https://arxiv.org/abs/2601.13676 ,  5342kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13698
Date: Tue, 20 Jan 2026 07:51:48 GMT   (1491kb)

Title: Does Privacy Always Harm Fairness? Data-Dependent Trade-offs via
  Chernoff Information Neural Estimation
Authors: Arjun Nichani, Hsiang Hsu, Chun-Fu (Richard) Chen, Haewon Jeong
Categories: cs.LG cs.AI cs.IT math.IT stat.ML
\\
  Fairness and privacy are two vital pillars of trustworthy machine learning.
Despite extensive research on these individual topics, the relationship between
fairness and privacy has received significantly less attention. In this paper,
we utilize the information-theoretic measure Chernoff Information to highlight
the data-dependent nature of the relationship among the triad of fairness,
privacy, and accuracy. We first define Noisy Chernoff Difference, a tool that
allows us to analyze the relationship among the triad simultaneously. We then
show that for synthetic data, this value behaves in 3 distinct ways (depending
on the distribution of the data). We highlight the data distributions involved
in these cases and explore their fairness and privacy implications.
Additionally, we show that Noisy Chernoff Difference acts as a proxy for the
steepness of the fairness-accuracy curves. Finally, we propose a method for
estimating Chernoff Information on data from unknown distributions and utilize
this framework to examine the triad dynamic on real datasets. This work builds
towards a unified understanding of the fairness-privacy-accuracy relationship
and highlights its data-dependent nature.
\\ ( https://arxiv.org/abs/2601.13698 ,  1491kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13710
Date: Tue, 20 Jan 2026 08:07:58 GMT   (6065kb)

Title: Who Should Have Surgery? A Comparative Study of GenAI vs Supervised ML
  for CRS Surgical Outcome Prediction
Authors: Sayeed Shafayet Chowdhury, Snehasis Mukhopadhyay, Shiaofen Fang, and
  Vijay R. Ramakrishnan
Categories: cs.LG cs.AI cs.CL
\\
  Artificial intelligence has reshaped medical imaging, yet the use of AI on
clinical data for prospective decision support remains limited. We study
pre-operative prediction of clinically meaningful improvement in chronic
rhinosinusitis (CRS), defining success as a more than 8.9-point reduction in
SNOT-22 at 6 months (MCID). In a prospectively collected cohort where all
patients underwent surgery, we ask whether models using only pre-operative
clinical data could have identified those who would have poor outcomes, i.e.
those who should have avoided surgery. We benchmark supervised ML (logistic
regression, tree ensembles, and an in-house MLP) against generative AI
(ChatGPT, Claude, Gemini, Perplexity), giving each the same structured inputs
and constraining outputs to binary recommendations with confidence. Our best ML
model (MLP) achieves 85 % accuracy with superior calibration and decision-curve
net benefit. GenAI models underperform on discrimination and calibration across
zero-shot setting. Notably, GenAI justifications align with clinician
heuristics and the MLP's feature importance, repeatedly highlighting baseline
SNOT-22, CT/endoscopy severity, polyp phenotype, and physchology/pain
comorbidities. We provide a reproducible tabular-to-GenAI evaluation protocol
and subgroup analyses. Findings support an ML-first, GenAI- augmented workflow:
deploy calibrated ML for primary triage of surgical candidacy, with GenAI as an
explainer to enhance transparency and shared decision-making.
\\ ( https://arxiv.org/abs/2601.13710 ,  6065kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13748
Date: Tue, 20 Jan 2026 09:03:49 GMT   (12156kb)

Title: EEG-Titans: Long-Horizon Seizure Forecasting via Dual-Branch Attention
  and Neural Memory
Authors: Tien-Dat Pham, Xuan-The Tran
Categories: cs.LG cs.HC
\\
  Accurate epileptic seizure prediction from electroencephalography (EEG)
remains challenging because pre-ictal dynamics may span long time horizons
while clinically relevant signatures can be subtle and transient. Many deep
learning models face a persistent trade-off between capturing local
spatiotemporal patterns and maintaining informative long-range context when
operating on ultralong sequences. We propose EEG-Titans, a dualbranch
architecture that incorporates a modern neural memory mechanism for
long-context modeling. The model combines sliding-window attention to capture
short-term anomalies with a recurrent memory pathway that summarizes slower,
progressive trends over time. On the CHB-MIT scalp EEG dataset, evaluated under
a chronological holdout protocol, EEG-Titans achieves 99.46% average
segment-level sensitivity across 18 subjects. We further analyze safety-first
operating points on artifact-prone recordings and show that a hierarchical
context strategy extending the receptive field for high-noise subjects can
markedly reduce false alarms (down to 0.00 FPR/h in an extreme outlier) without
sacrificing sensitivity. These results indicate that memory-augmented
long-context modeling can provide robust seizure forecasting under clinically
constrained evaluation
\\ ( https://arxiv.org/abs/2601.13748 ,  12156kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13768
Date: Tue, 20 Jan 2026 09:23:10 GMT   (1664kb)

Title: vLinear: A Powerful Linear Model for Multivariate Time Series
  Forecasting
Authors: Wenzhen Yue, Ruohao Guo, Ji Shi, Zihan Hao, Shiyu Hu, Xianghua Ying
Categories: cs.LG cs.AI
\\
  In this paper, we present \textbf{vLinear}, an effective yet efficient
\textbf{linear}-based multivariate time series forecaster featuring two
components: the \textbf{v}ecTrans module and the WFMLoss objective. Many
state-of-the-art forecasters rely on self-attention or its variants to capture
multivariate correlations, typically incurring $\mathcal{O}(N^2)$ computational
complexity with respect to the number of variates $N$. To address this, we
propose vecTrans, a lightweight module that utilizes a learnable vector to
model multivariate correlations, reducing the complexity to $\mathcal{O}(N)$.
Notably, vecTrans can be seamlessly integrated into Transformer-based
forecasters, delivering up to 5$\times$ inference speedups and consistent
performance gains. Furthermore, we introduce WFMLoss (Weighted Flow Matching
Loss) as the objective. In contrast to typical \textbf{velocity-oriented} flow
matching objectives, we demonstrate that a \textbf{final-series-oriented}
formulation yields significantly superior forecasting accuracy. WFMLoss also
incorporates path- and horizon-weighted strategies to focus learning on more
reliable paths and horizons. Empirically, vLinear achieves state-of-the-art
performance across 22 benchmarks and 124 forecasting settings. Moreover,
WFMLoss serves as an effective plug-and-play objective, consistently improving
existing forecasters. The code is available at
https://anonymous.4open.science/r/vLinear.
\\ ( https://arxiv.org/abs/2601.13768 ,  1664kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13776
Date: Tue, 20 Jan 2026 09:33:14 GMT   (971kb,D)

Title: Orthogonium : A Unified, Efficient Library of Orthogonal and 1-Lipschitz
  Building Blocks
Authors: Thibaut Boissin (IRIT-MISFIT), Franck Mamalet, Valentin Lafargue
  (ANITI, IMT), Mathieu Serrurier (IRIT-MISFIT)
Categories: cs.LG stat.ML
Journal-ref: ICML 2025 Workshop on Championing Open- source Development in
  Machine Learning (CODEML '25), Jul 2025, Vancouver, France
\\
  Orthogonal and 1-Lipschitz neural network layers are essential building
blocks in robust deep learning architectures, crucial for certified adversarial
robustness, stable generative models, and reliable recurrent networks. Despite
significant advancements, existing implementations remain fragmented, limited,
and computationally demanding. To address these issues, we introduce
Orthogonium , a unified, efficient, and comprehensive PyTorch library providing
orthogonal and 1-Lipschitz layers. Orthogonium provides access to standard
convolution features-including support for strides, dilation, grouping, and
transposed-while maintaining strict mathematical guarantees. Its optimized
implementations reduce overhead on large scale benchmarks such as ImageNet.
Moreover, rigorous testing within the library has uncovered critical errors in
existing implementations, emphasizing the importance of standardized and
reliable tools. Orthogonium thus significantly lowers adoption barriers,
enabling scalable experimentation and integration across diverse applications
requiring orthogonality and robust Lipschitz constraints. Orthogonium is
available at https://github.com/deel-ai/orthogonium.
\\ ( https://arxiv.org/abs/2601.13776 ,  971kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13780
Date: Tue, 20 Jan 2026 09:37:53 GMT   (359kb)

Title: Principled Latent Diffusion for Graphs via Laplacian Autoencoders
Authors: Antoine Siraudin, Christopher Morris
Categories: cs.LG
Comments: Preprint, under review
\\
  Graph diffusion models achieve state-of-the-art performance in graph
generation but suffer from quadratic complexity in the number of nodes -- and
much of their capacity is wasted modeling the absence of edges in sparse
graphs. Inspired by latent diffusion in other modalities, a natural idea is to
compress graphs into a low-dimensional latent space and perform diffusion
there. However, unlike images or text, graph generation requires nearly
lossless reconstruction, as even a single error in decoding an adjacency matrix
can render the entire sample invalid. This challenge has remained largely
unaddressed. We propose LG-Flow, a latent graph diffusion framework that
directly overcomes these obstacles. A permutation-equivariant autoencoder maps
each node into a fixed-dimensional embedding from which the full adjacency is
provably recoverable, enabling near-lossless reconstruction for both undirected
graphs and DAGs. The dimensionality of this latent representation scales
linearly with the number of nodes, eliminating the quadratic bottleneck and
making it feasible to train larger and more expressive models. In this latent
space, we train a Diffusion Transformer with flow matching, enabling efficient
and expressive graph generation. Our approach achieves competitive results
against state-of-the-art graph diffusion models, while achieving up to
$1000\times$ speed-up.
\\ ( https://arxiv.org/abs/2601.13780 ,  359kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13793
Date: Tue, 20 Jan 2026 09:51:35 GMT   (158kb)

Title: PAtt: A Pattern Attention Network for ETA Prediction Using Historical
  Speed Profiles
Authors: ByeoungDo Kim, JunYeop Na, Kyungwook Tak, JunTae Kim, DongHyeon Kim,
  Duckky Kim
Categories: cs.LG
Comments: 7 pages, 3 figures, ITSC 2025, to be published
\\
  In this paper, we propose an ETA model (Estimated Time of Arrival) that
leverages an attention mechanism over historical road speed patterns. As
autonomous driving and intelligent transportation systems become increasingly
prevalent, the need for accurate and reliable ETA estimation has grown, playing
a vital role in navigation, mobility planning, and traffic management. However,
predicting ETA remains a challenging task due to the dynamic and complex nature
of traffic flow. Traditional methods often combine real-time and historical
traffic data in simplistic ways, or rely on complex rule-based computations.
While recent deep learning models have shown potential, they often require high
computational costs and do not effectively capture the spatio-temporal patterns
crucial for ETA prediction. ETA prediction inherently involves spatio-temporal
causality, and our proposed model addresses this by leveraging attention
mechanisms to extract and utilize temporal features accumulated at each
spatio-temporal point along a route. This architecture enables efficient and
accurate ETA estimation while keeping the model lightweight and scalable. We
validate our approach using real-world driving datasets and demonstrate that
our approach outperforms existing baselines by effectively integrating road
characteristics, real-time traffic conditions, and historical speed patterns in
a task-aware manner.
\\ ( https://arxiv.org/abs/2601.13793 ,  158kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13824
Date: Tue, 20 Jan 2026 10:33:19 GMT   (1131kb)

Title: ELSA: Efficient LLM-Centric Split Aggregation for Privacy-Aware
  Hierarchical Federated Learning over Resource-Constrained Edge Networks
Authors: Xiaohong Yang, Tong Xie, Minghui Liwang, Chikai Shang, Yang Lu,
  Zhenzhen Jiao, Liqun Fu, Seyyedali Hosseinalipour
Categories: cs.LG
Comments: 11 pages, 16 figures
\\
  Training large language models (LLMs) at the network edge faces fundamental
challenges arising from device resource constraints, severe data heterogeneity,
and heightened privacy risks. To address these, we propose ELSA (Efficient
LLM-centric Split Aggregation), a novel framework that systematically
integrates split learning (SL) and hierarchical federated learning (HFL) for
distributed LLM fine-tuning over resource-constrained edge networks. ELSA
introduces three key innovations. First, it employs a task-agnostic,
behavior-aware client clustering mechanism that constructs semantic
fingerprints using public probe inputs and symmetric KL divergence, further
enhanced by prediction-consistency-based trust scoring and latency-aware edge
assignment to jointly address data heterogeneity, client unreliability, and
communication constraints. Second, it splits the LLM into three parts across
clients and edge servers, with the cloud used only for adapter aggregation,
enabling an effective balance between on-device computation cost and global
convergence stability. Third, it incorporates a lightweight communication
scheme based on computational sketches combined with semantic subspace
orthogonal perturbation (SS-OP) to reduce communication overhead while
mitigating privacy leakage during model exchanges. Experiments across diverse
NLP tasks demonstrate that ELSA consistently outperforms state-of-the-art
methods in terms of adaptability, convergence behavior, and robustness,
establishing a scalable and privacy-aware solution for edge-side LLM
fine-tuning under resource constraints.
\\ ( https://arxiv.org/abs/2601.13824 ,  1131kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13844
Date: Tue, 20 Jan 2026 10:58:14 GMT   (191kb)

Title: Optimal L2 Regularization in High-dimensional Continual Linear
  Regression
Authors: Gilad Karpel, Edward Moroshko, Ran Levinstein, Ron Meir, Daniel
  Soudry, Itay Evron
Categories: cs.LG
Comments: Accepted to ALT 2026
\\
  We study generalization in an overparameterized continual linear regression
setting, where a model is trained with L2 (isotropic) regularization across a
sequence of tasks. We derive a closed-form expression for the expected
generalization loss in the high-dimensional regime that holds for arbitrary
linear teachers. We demonstrate that isotropic regularization mitigates label
noise under both single-teacher and multiple i.i.d. teacher settings, whereas
prior work accommodating multiple teachers either did not employ regularization
or used memory-demanding methods. Furthermore, we prove that the optimal fixed
regularization strength scales nearly linearly with the number of tasks $T$,
specifically as $T/\ln T$. To our knowledge, this is the first such result in
theoretical continual learning. Finally, we validate our theoretical findings
through experiments on linear regression and neural networks, illustrating how
this scaling law affects generalization and offering a practical recipe for the
design of continual learning systems.
\\ ( https://arxiv.org/abs/2601.13844 ,  191kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13851
Date: Tue, 20 Jan 2026 11:02:54 GMT   (1421kb)

Title: Inverting Self-Organizing Maps: A Unified Activation-Based Framework
Authors: Alessandro Londei, Matteo Benati, Denise Lanzieri, Vittorio Loreto
Categories: cs.LG stat.ML
\\
  Self-Organizing Maps provide topology-preserving projections of
high-dimensional data and have been widely used for visualization, clustering,
and vector quantization. In this work, we show that the activation pattern of a
SOM - the squared distances to its prototypes - can be inverted to recover the
exact input under mild geometric conditions. This follows from a classical fact
in Euclidean distance geometry: a point in $D$ dimensions is uniquely
determined by its distances to $D{+}1$ affinely independent references. We
derive the corresponding linear system and characterize the conditions under
which the inversion is well-posed. Building upon this mechanism, we introduce
the Manifold-Aware Unified SOM Inversion and Control (MUSIC) update rule, which
enables controlled, semantically meaningful trajectories in latent space. MUSIC
modifies squared distances to selected prototypes while preserving others,
resulting in a deterministic geometric flow aligned with the SOM's
piecewise-linear structure. Tikhonov regularization stabilizes the update rule
and ensures smooth motion on high-dimensional datasets. Unlike variational or
probabilistic generative models, MUSIC does not rely on sampling, latent
priors, or encoder-decoder architectures. If no perturbation is applied,
inversion recovers the exact input; when a target cluster or prototype is
specified, MUSIC produces coherent semantic variations while remaining on the
data manifold. This leads to a new perspective on data augmentation and
controllable latent exploration based solely on prototype geometry. We validate
the approach using synthetic Gaussian mixtures, the MNIST and the Faces in the
Wild dataset. Across all settings, MUSIC produces smooth, interpretable
trajectories that reveal the underlying geometry of the learned manifold,
illustrating the advantages of SOM-based inversion over unsupervised
clustering.
\\ ( https://arxiv.org/abs/2601.13851 ,  1421kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13892
Date: Tue, 20 Jan 2026 12:10:13 GMT   (4963kb)

Title: Multi-Objective Hierarchical Optimization with Large Language Models
Authors: Andrej Schwanke, Lyubomir Ivanov, David Salinas, Frank Hutter, Arber
  Zela
Categories: cs.LG
Comments: 23 pages, 21 figures, 9 tables
\\
  Despite their widespread adoption in various domains, especially due to their
powerful reasoning capabilities, Large Language Models (LLMs) are not the
off-the-shelf choice to drive multi-objective optimization yet. Conventional
strategies rank high in benchmarks due to their intrinsic capabilities to
handle numerical inputs and careful modelling choices that balance exploration
and Pareto-front exploitation, as well as handle multiple (conflicting)
objectives. In this paper, we close this gap by leveraging LLMs as surrogate
models and candidate samplers inside a structured hierarchical search strategy.
By adaptively partitioning the input space into disjoint hyperrectangular
regions and ranking them with a composite score function, we restrict the
generative process of the LLM to specific, high-potential sub-spaces, hence
making the problem easier to solve as the LLM doesn't have to reason about the
global structure of the problem, but only locally instead. We show that under
standard regularity assumptions, our algorithm generates candidate solutions
that converge to the true Pareto set in Hausdorff distance. Empirically, it
consistently outperforms the global LLM-based multi-objective optimizer and is
on par with standard evolutionary and Bayesian optimization algorithm on
synthetic and real-world benchmarks.
\\ ( https://arxiv.org/abs/2601.13892 ,  4963kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13897
Date: Tue, 20 Jan 2026 12:26:38 GMT   (2232kb)

Title: TractRLFusion: A GPT-Based Multi-Critic Policy Fusion Framework for
  Fiber Tractography
Authors: Ankita Joshi, Ashutosh Sharma, Anoushkrit Goel, Ranjeet Ranjan Jha,
  Chirag Ahuja, Arnav Bhavsar, Aditya Nigam
Categories: cs.LG cs.AI
Comments: Accepted at 23rd IEEE International Symposium on Biomedical Imaging
  (ISBI), 2026
\\
  Tractography plays a pivotal role in the non-invasive reconstruction of white
matter fiber pathways, providing vital information on brain connectivity and
supporting precise neurosurgical planning. Although traditional methods relied
mainly on classical deterministic and probabilistic approaches, recent progress
has benefited from supervised deep learning (DL) and deep reinforcement
learning (DRL) to improve tract reconstruction. A persistent challenge in
tractography is accurately reconstructing white matter tracts while minimizing
spurious connections. To address this, we propose TractRLFusion, a novel
GPT-based policy fusion framework that integrates multiple RL policies through
a data-driven fusion strategy. Our method employs a two-stage training data
selection process for effective policy fusion, followed by a multi-critic
fine-tuning phase to enhance robustness and generalization. Experiments on HCP,
ISMRM, and TractoInferno datasets demonstrate that TractRLFusion outperforms
individual RL policies as well as state-of-the-art classical and DRL methods in
accuracy and anatomical reliability.
\\ ( https://arxiv.org/abs/2601.13897 ,  2232kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13953
Date: Tue, 20 Jan 2026 13:26:52 GMT   (188kb)

Title: Differentiable Logic Synthesis: Spectral Coefficient Selection via
  Sinkhorn-Constrained Composition
Authors: Gorgi Pavlov
Categories: cs.LG cs.AR cs.LO
Comments: 35 pages, 22 figures. Code available at
  https://github.com/gogipav14/spectral-llm
ACM-class: I.2.6; F.2.2; C.1.3
\\
  Learning precise Boolean logic via gradient descent remains challenging:
neural networks typically converge to "fuzzy" approximations that degrade under
quantization. We introduce Hierarchical Spectral Composition, a differentiable
architecture that selects spectral coefficients from a frozen Boolean Fourier
basis and composes them via Sinkhorn-constrained routing with column-sign
modulation. Our approach draws on recent insights from Manifold-Constrained
Hyper-Connections (mHC), which demonstrated that projecting routing matrices
onto the Birkhoff polytope preserves identity mappings and stabilizes
large-scale training. We adapt this framework to logic synthesis, adding
column-sign modulation to enable Boolean negation -- a capability absent in
standard doubly stochastic routing.
  We validate our approach across four phases of increasing complexity: (1) For
n=2 (16 Boolean operations over 4-dim basis), gradient descent achieves 100%
accuracy with zero routing drift and zero-loss quantization to ternary masks.
(2) For n=3 (10 three-variable operations), gradient descent achieves 76%
accuracy, but exhaustive enumeration over 3^8 = 6561 configurations proves that
optimal ternary masks exist for all operations (100% accuracy, 39% sparsity).
(3) For n=4 (10 four-variable operations over 16-dim basis), spectral synthesis
-- combining exact Walsh-Hadamard coefficients, ternary quantization, and MCMC
refinement with parallel tempering -- achieves 100% accuracy on all operations.
This progression establishes (a) that ternary polynomial threshold
representations exist for all tested functions, and (b) that finding them
requires methods beyond pure gradient descent as dimensionality grows. All
operations enable single-cycle combinational logic inference at 10,959 MOps/s
on GPU, demonstrating viability for hardware-efficient neuro-symbolic logic
synthesis.
\\ ( https://arxiv.org/abs/2601.13953 ,  188kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13964
Date: Tue, 20 Jan 2026 13:38:01 GMT   (2437kb)

Title: RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised
  EEG Representation Learning
Authors: Cheol-Hui Lee, Hwa-Yeon Lee, Dong-Joo Kim
Categories: cs.LG cs.AI
\\
  The quality of data augmentation serves as a critical determinant for the
performance of contrastive learning in EEG tasks. Although this paradigm is
promising for utilizing unlabeled data, static or random augmentation
strategies often fail to preserve intrinsic information due to the
non-stationarity of EEG signals where statistical properties change over time.
To address this, we propose RL-BioAug, a framework that leverages a
label-efficient reinforcement learning (RL) agent to autonomously determine
optimal augmentation policies. While utilizing only a minimal fraction (10\%)
of labeled data to guide the agent's policy, our method enables the encoder to
learn robust representations in a strictly self-supervised manner. Experimental
results demonstrate that RL-BioAug significantly outperforms the random
selection strategy, achieving substantial improvements of 9.69\% and 8.80\% in
Macro-F1 score on the Sleep-EDFX and CHB-MIT datasets, respectively. Notably,
this agent mainly chose optimal strategies for each task -- for example, Time
Masking with a 62\% probability for sleep stage classification and Crop \&
Resize with a 77\% probability for seizure detection. Our framework suggests
its potential to replace conventional heuristic-based augmentations and
establish a new autonomous paradigm for data augmentation. The source code is
available at
\href{https://github.com/dlcjfgmlnasa/RL-BioAug}{https://github.com/dlcjfgmlnasa/RL-BioAug}.
\\ ( https://arxiv.org/abs/2601.13964 ,  2437kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13989
Date: Tue, 20 Jan 2026 14:03:28 GMT   (2359kb)

Title: A universal linearized subspace refinement framework for neural networks
Authors: Wenbo Cao, Weiwei Zhang
Categories: cs.LG
\\
  Neural networks are predominantly trained using gradient-based methods, yet
in many applications their final predictions remain far from the accuracy
attainable within the model's expressive capacity. We introduce Linearized
Subspace Refinement (LSR), a general and architecture-agnostic framework that
exploits the Jacobian-induced linear residual model at a fixed trained network
state. By solving a reduced direct least-squares problem within this subspace,
LSR computes a subspace-optimal solution of the linearized residual model,
yielding a refined linear predictor with substantially improved accuracy over
standard gradient-trained solutions, without modifying network architectures,
loss formulations, or training procedures. Across supervised function
approximation, data-driven operator learning, and physics-informed operator
fine-tuning, we show that gradient-based training often fails to access this
attainable accuracy, even when local linearization yields a convex problem.
This observation indicates that loss-induced numerical ill-conditioning, rather
than nonconvexity or model expressivity, can constitute a dominant practical
bottleneck. In contrast, one-shot LSR systematically exposes accuracy levels
not fully exploited by gradient-based training, frequently achieving
order-of-magnitude error reductions. For operator-constrained problems with
composite loss structures, we further introduce Iterative LSR, which alternates
one-shot LSR with supervised nonlinear alignment, transforming ill-conditioned
residual minimization into numerically benign fitting steps and yielding
accelerated convergence and improved accuracy. By bridging nonlinear neural
representations with reduced-order linear solvers at fixed linearization
points, LSR provides a numerically grounded and broadly applicable refinement
framework for supervised learning, operator learning, and scientific computing.
\\ ( https://arxiv.org/abs/2601.13989 ,  2359kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14022
Date: Tue, 20 Jan 2026 14:43:21 GMT   (7916kb)

Title: Credible CO2 Comparisons: A Machine Learning Approach to Vehicle
  Powertrain Assessment
Authors: Rodrigo Pereira David, Luciano Araujo Dourado Filho, Daniel Marques da
  Silva and Jo\~ao Alfredo Cal-Braz
Categories: cs.LG cs.AI
\\
  Decarbonizing road transport requires consistent and transparent methods for
comparing CO2 emissions across vehicle technologies. This paper proposes a
machine learning-based framework for like-for-like operational assessment of
internal combustion engine vehicles (ICEVs) and electric vehicles (EVs) under
identical, real-world driving conditions. The approach isolates
technology-specific effects by holding the observed speed profile and
environmental context fixed, enabling direct comparison of powertrain
performance. Recurrent neural network models are trained independently for each
domain to learn the mapping from contextual driving variables (speed,
acceleration, temperature) to internal actuation variables (torque, throttle)
and instantaneous CO2-equivalent emission rates. This structure allows the
construction of counterfactual scenarios that answer: What emissions would an
EV have generated if it had followed the same driving profile as an ICEV? By
aligning both vehicle types on a unified instantaneous emissions metric, the
framework enables fair and reproducible evaluation of powertrain technologies.
It offers a scalable foundation for credible, data-driven assessments of
vehicle carbon performance under real-world operating conditions.
\\ ( https://arxiv.org/abs/2601.14022 ,  7916kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14026
Date: Tue, 20 Jan 2026 14:48:45 GMT   (18kb)

Title: Universal Approximation Theorem for Input-Connected Multilayer
  Perceptrons
Authors: Vugar Ismailov
Categories: cs.LG cs.NE math.FA
Comments: 18 pages, 2 figures, 31 references
\\
  We introduce the Input-Connected Multilayer Perceptron (IC-MLP), a
feedforward neural network architecture in which each hidden neuron receives,
in addition to the outputs of the preceding layer, a direct affine connection
from the raw input. We first study this architecture in the univariate setting
and give an explicit and systematic description of IC-MLPs with an arbitrary
finite number of hidden layers, including iterated formulas for the network
functions. In this setting, we prove a universal approximation theorem showing
that deep IC-MLPs can approximate any continuous function on a closed interval
of the real line if and only if the activation function is nonlinear. We then
extend the analysis to vector-valued inputs and establish a corresponding
universal approximation theorem for continuous functions on compact subsets of
$\mathbb{R}^n$.
\\ ( https://arxiv.org/abs/2601.14026 ,  18kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14033
Date: Tue, 20 Jan 2026 14:53:39 GMT   (200kb)

Title: PAC-Private Responses with Adversarial Composition
Authors: Xiaochen Zhu, Mayuri Sridhar, Srinivas Devadas
Categories: cs.LG cs.CR
Comments: 16 pages, 3 figures
\\
  Modern machine learning models are increasingly deployed behind APIs. This
renders standard weight-privatization methods (e.g. DP-SGD) unnecessarily noisy
at the cost of utility. While model weights may vary significantly across
training datasets, model responses to specific inputs are much lower
dimensional and more stable. This motivates enforcing privacy guarantees
directly on model outputs.
  We approach this under PAC privacy, which provides instance-based privacy
guarantees for arbitrary black-box functions by controlling mutual information
(MI). Importantly, PAC privacy explicitly rewards output stability with reduced
noise levels. However, a central challenge remains: response privacy requires
composing a large number of adaptively chosen, potentially adversarial queries
issued by untrusted users, where existing composition results on PAC privacy
are inadequate. We introduce a new algorithm that achieves adversarial
composition via adaptive noise calibration and prove that mutual information
guarantees accumulate linearly under adaptive and adversarial querying.
  Experiments across tabular, vision, and NLP tasks show that our method
achieves high utility at extremely small per-query privacy budgets. On
CIFAR-10, we achieve 87.79% accuracy with a per-step MI budget of $2^{-32}$.
This enables serving one million queries while provably bounding membership
inference attack (MIA) success rates to 51.08% -- the same guarantee of $(0.04,
10^{-5})$-DP. Furthermore, we show that private responses can be used to label
public data to distill a publishable privacy-preserving model; using an
ImageNet subset as a public dataset, our model distilled from 210,000 responses
achieves 91.86% accuracy on CIFAR-10 with MIA success upper-bounded by 50.49%,
which is comparable to $(0.02,10^{-5})$-DP.
\\ ( https://arxiv.org/abs/2601.14033 ,  200kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14053
Date: Tue, 20 Jan 2026 15:06:19 GMT   (395kb)

Title: LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling
  Walls to Agentic AI Systems
Authors: Badri N. Patro, Vijay S. Agneeswaran
Categories: cs.LG cs.AI cs.CV cs.MA eess.IV
\\
  The field of artificial intelligence has undergone a revolution from
foundational Transformer architectures to reasoning-capable systems approaching
human-level performance. We present LLMOrbit, a comprehensive circular taxonomy
navigating the landscape of large language models spanning 2019-2025. This
survey examines over 50 models across 15 organizations through eight
interconnected orbital dimensions, documenting architectural innovations,
training methodologies, and efficiency patterns defining modern LLMs,
generative AI, and agentic systems. We identify three critical crises: (1) data
scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M
to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase),
establishing the scaling wall limiting brute-force approaches. Our analysis
reveals six paradigms breaking this wall: (1) test-time compute (o1,
DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2)
quantization (4-8x compression), (3) distributed edge computing (10x cost
reduction), (4) model merging, (5) efficient training (ORPO reduces memory
50%), and (6) small specialized models (Phi-4 14B matches larger models). Three
paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute
substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution
(MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache
compression enables GPT-4-level performance at <$0.30/M tokens), and (3)
democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We
provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution
from passive generation to tool-using agents (ReAct, RAG, multi-agent systems),
and analyze post-training innovations.
\\ ( https://arxiv.org/abs/2601.14053 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14092
Date: Tue, 20 Jan 2026 15:55:11 GMT   (750kb)

Title: Optimizing Energy and Data Collection in UAV-aided IoT Networks using
  Attention-based Multi-Objective Reinforcement Learning
Authors: Babacar Toure, Dimitrios Tsilimantos, Omid Esrafilian, and Marios
  Kountouris
Categories: cs.LG cs.NI
\\
  Due to their adaptability and mobility, Unmanned Aerial Vehicles (UAVs) are
becoming increasingly essential for wireless network services, particularly for
data harvesting tasks. In this context, Artificial Intelligence (AI)-based
approaches have gained significant attention for addressing UAV path planning
tasks in large and complex environments, bridging the gap with real-world
deployments. However, many existing algorithms suffer from limited training
data, which hampers their performance in highly dynamic environments. Moreover,
they often overlook the inherently multi-objective nature of the task, treating
it in an overly simplistic manner. To address these limitations, we propose an
attention-based Multi-Objective Reinforcement Learning (MORL) architecture that
explicitly handles the trade-off between data collection and energy consumption
in urban environments, even without prior knowledge of wireless channel
conditions. Our method develops a single model capable of adapting to varying
trade-off preferences and dynamic scenario parameters without the need for
fine-tuning or retraining. Extensive simulations show that our approach
achieves substantial improvements in performance, model compactness, sample
efficiency, and most importantly, generalization to previously unseen
scenarios, outperforming existing RL solutions.
\\ ( https://arxiv.org/abs/2601.14092 ,  750kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14099
Date: Tue, 20 Jan 2026 15:58:51 GMT   (5805kb)

Title: Causal feature selection framework for stable soft sensor modeling based
  on time-delayed cross mapping
Authors: Shi-Shun Chen, Xiao-Yang Li, Enrico Zio
Categories: cs.LG cs.AI
Journal-ref: Advanced Engineering Informatics 2026, 71, 104337
DOI: 10.1016/j.aei.2026.104337
\\
  Soft sensor modeling plays a crucial role in process monitoring. Causal
feature selection can enhance the performance of soft sensor models in
industrial applications. However, existing methods ignore two critical
characteristics of industrial processes. Firstly, causal relationships between
variables always involve time delays, whereas most causal feature selection
methods investigate causal relationships in the same time dimension. Secondly,
variables in industrial processes are often interdependent, which contradicts
the decorrelation assumption of traditional causal inference methods.
Consequently, soft sensor models based on existing causal feature selection
approaches often lack sufficient accuracy and stability. To overcome these
challenges, this paper proposes a causal feature selection framework based on
time-delayed cross mapping. Time-delayed cross mapping employs state space
reconstruction to effectively handle interdependent variables in causality
analysis, and considers varying causal strength across time delay. Time-delayed
convergent cross mapping (TDCCM) is introduced for total causal inference, and
time-delayed partial cross mapping (TDPCM) is developed for direct causal
inference. Then, in order to achieve automatic feature selection, an objective
feature selection strategy is presented. The causal threshold is automatically
determined based on the model performance on the validation set, and the causal
features are then selected. Two real-world case studies show that TDCCM
achieves the highest average performance, while TDPCM improves soft sensor
stability and performance in the worst scenario. The code is publicly available
at https://github.com/dirge1/TDPCM.
\\ ( https://arxiv.org/abs/2601.14099 ,  5805kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14115
Date: Tue, 20 Jan 2026 16:09:05 GMT   (2138kb)

Title: Riemannian Liquid Spatio-Temporal Graph Network
Authors: Liangsi Lu, Jingchao Wang, Zhaorong Dai, Hanqian Liu, Yang Shi
Categories: cs.LG cs.AI
Comments: This paper has been accepted to The Web Conference 2026
\\
  Liquid Time-Constant networks (LTCs), a type of continuous-time graph neural
network, excel at modeling irregularly-sampled dynamics but are fundamentally
confined to Euclidean space. This limitation introduces significant geometric
distortion when representing real-world graphs with inherent non-Euclidean
structures (e.g., hierarchies and cycles), degrading representation quality. To
overcome this limitation, we introduce the Riemannian Liquid Spatio-Temporal
Graph Network (RLSTG), a framework that unifies continuous-time liquid dynamics
with the geometric inductive biases of Riemannian manifolds. RLSTG models graph
evolution through an Ordinary Differential Equation (ODE) formulated directly
on a curved manifold, enabling it to faithfully capture the intrinsic geometry
of both structurally static and dynamic spatio-temporal graphs. Moreover, we
provide rigorous theoretical guarantees for RLSTG, extending stability theorems
of LTCs to the Riemannian domain and quantifying its expressive power via state
trajectory analysis. Extensive experiments on real-world benchmarks demonstrate
that, by combining advanced temporal dynamics with a Riemannian spatial
representation, RLSTG achieves superior performance on graphs with complex
structures. Project Page: https://rlstg.github.io
\\ ( https://arxiv.org/abs/2601.14115 ,  2138kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14173
Date: Tue, 20 Jan 2026 17:25:47 GMT   (64kb)

Title: Penalizing Localized Dirichlet Energies in Low Rank Tensor Products
Authors: Paris A. Karakasis, Nicholas D. Sidiropoulos
Categories: cs.LG stat.ML
Comments: 19 pages
\\
  We study low-rank tensor-product B-spline (TPBS) models for regression tasks
and investigate Dirichlet energy as a measure of smoothness. We show that TPBS
models admit a closed-form expression for the Dirichlet energy, and reveal
scenarios where perfect interpolation is possible with exponentially small
Dirichlet energy. This renders global Dirichlet energy-based regularization
ineffective. To address this limitation, we propose a novel regularization
strategy based on local Dirichlet energies defined on small hypercubes centered
at the training points. Leveraging pretrained TPBS models, we also introduce
two estimators for inference from incomplete samples. Comparative experiments
with neural networks demonstrate that TPBS models outperform neural networks in
the overfitting regime for most datasets, and maintain competitive performance
otherwise. Overall, TPBS models exhibit greater robustness to overfitting and
consistently benefit from regularization, while neural networks are more
sensitive to overfitting and less effective in leveraging regularization.
\\ ( https://arxiv.org/abs/2601.14173 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14175
Date: Tue, 20 Jan 2026 17:27:03 GMT   (146kb)

Title: A model of errors in transformers
Authors: Suvrat Raju and Praneeth Netrapalli
Categories: cs.LG cs.AI cs.CL hep-th
Comments: 8+17pages
ACM-class: I.2.7; I.2.6
\\
  We study the error rate of LLMs on tasks like arithmetic that require a
deterministic output, and repetitive processing of tokens drawn from a small
set of alternatives. We argue that incorrect predictions arise when small
errors in the attention mechanism accumulate to cross a threshold, and use this
insight to derive a quantitative two-parameter relationship between the
accuracy and the complexity of the task. The two parameters vary with the
prompt and the model; they can be interpreted in terms of an elementary noise
rate, and the number of plausible erroneous tokens that can be predicted. Our
analysis is inspired by an ``effective field theory'' perspective: the LLM's
many raw parameters can be reorganized into just two parameters that govern the
error rate. We perform extensive empirical tests, using Gemini 2.5 Flash,
Gemini 2.5 Pro and DeepSeek R1, and find excellent agreement between the
predicted and observed accuracy for a variety of tasks, although we also
identify deviations in some cases. Our model provides an alternative to
suggestions that errors made by LLMs on long repetitive tasks indicate the
``collapse of reasoning'', or an inability to express ``compositional''
functions. Finally, we show how to construct prompts to reduce the error rate.
\\ ( https://arxiv.org/abs/2601.14175 ,  146kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14196
Date: Tue, 20 Jan 2026 18:00:42 GMT   (269kb)

Title: Differentiated Pickup Point Offering for Emission Reduction in Last-Mile
  Delivery
Authors: Albina Galiullina, Wouter van Heeswijk, Tom van Woensel
Categories: cs.LG
\\
  Pickup points are widely recognized as a sustainable alternative to home
delivery, as consolidating orders at pickup locations can shorten delivery
routes and improve first-attempt success rates. However, these benefits may be
negated when customers drive to pick up their orders. This study proposes a
Differentiated Pickup Point Offering (DPO) policy that aims to jointly reduce
emissions from delivery truck routes and customer travel. Under DPO, each
arriving customer is offered a single recommended pickup point, rather than an
unrestricted choice among all locations, while retaining the option of home
delivery. We study this problem in a dynamic and stochastic setting, where the
pickup point offered to each customer depends on previously realized customer
locations and delivery choices. To design effective DPO policies, we adopt a
reinforcement learning-based approach that accounts for spatial relationships
between customers and pickup points and their implications for future route
consolidation. Computational experiments show that differentiated pickup point
offerings can substantially reduce total carbon emissions. The proposed
policies reduce total emissions by up to 9% relative to home-only delivery and
by 2% on average compared with alternative policies, including unrestricted
pickup point choice and nearest pickup point assignment. Differentiated
offerings are particularly effective in dense urban settings with many pickup
points and short inter-location distances. Moreover, explicitly accounting for
the dynamic nature of customer arrivals and choices is especially important
when customers are less inclined to choose pickup point delivery over home
delivery.
\\ ( https://arxiv.org/abs/2601.14196 ,  269kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14209
Date: Tue, 20 Jan 2026 18:15:38 GMT   (1600kb)

Title: InT: Self-Proposed Interventions Enable Credit Assignment in LLM
  Reasoning
Authors: Matthew Y. R. Yang, Hao Bai, Ian Wu, Gene Yang, Amrith Setlur and
  Aviral Kumar
Categories: cs.LG cs.AI cs.CL
\\
  Outcome-reward reinforcement learning (RL) has proven effective at improving
the reasoning capabilities of large language models (LLMs). However, standard
RL assigns credit only at the level of the final answer, penalizing entire
reasoning traces when the outcome is incorrect and uniformly reinforcing all
steps when it is correct. As a result, correct intermediate steps may be
discouraged in failed traces, while spurious steps may be reinforced in
successful ones. We refer to this failure mode as the problem of credit
assignment. While a natural remedy is to train a process reward model,
accurately optimizing such models to identify corrective reasoning steps
remains challenging. We introduce Intervention Training (InT), a training
paradigm in which the model performs fine-grained credit assignment on its own
reasoning traces by proposing short, targeted corrections that steer
trajectories toward higher reward. Using reference solutions commonly available
in mathematical reasoning datasets and exploiting the fact that verifying a
model-generated solution is easier than generating a correct one from scratch,
the model identifies the first error in its reasoning and proposes a
single-step intervention to redirect the trajectory toward the correct
solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout
up to the point of error concatenated with the intervention, localizing error
to the specific step that caused failure. We show that the resulting model
serves as a far better initialization for RL training. After running InT and
subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a
4B-parameter base model on IMO-AnswerBench, outperforming larger open-source
models such as gpt-oss-20b.
\\ ( https://arxiv.org/abs/2601.14209 ,  1600kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14228
Date: Tue, 20 Jan 2026 18:41:44 GMT   (1110kb)

Title: Attention-Based Offline Reinforcement Learning and Clustering for
  Interpretable Sepsis Treatment
Authors: Punit Kumar, Vaibhav Saran, Divyesh Patel, Nitin Kulkarni, Alina
  Vereshchaka
Categories: cs.LG
Comments: 8 pages, 6 figures, Conference: IEEE International Conference on
  Machine Learning and Applications 2025 (ICMLA 2025):
  https://www.icmla-conference.org/icmla25/
\\
  Sepsis remains one of the leading causes of mortality in intensive care
units, where timely and accurate treatment decisions can significantly impact
patient outcomes. In this work, we propose an interpretable decision support
framework. Our system integrates four core components: (1) a clustering-based
stratification module that categorizes patients into low, intermediate, and
high-risk groups upon ICU admission, using clustering with statistical
validation; (2) a synthetic data augmentation pipeline leveraging variational
autoencoders (VAE) and diffusion models to enrich underrepresented trajectories
such as fluid or vasopressor administration; (3) an offline reinforcement
learning (RL) agent trained using Advantage Weighted Regression (AWR) with a
lightweight attention encoder and supported by an ensemble models for
conservative, safety-aware treatment recommendations; and (4) a rationale
generation module powered by a multi-modal large language model (LLM), which
produces natural-language justifications grounded in clinical context and
retrieved expert knowledge. Evaluated on the MIMIC-III and eICU datasets, our
approach achieves high treatment accuracy while providing clinicians with
interpretable and robust policy recommendations.
\\ ( https://arxiv.org/abs/2601.14228 ,  1110kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14232
Date: Tue, 20 Jan 2026 18:44:28 GMT   (6737kb)

Title: KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for
  Reinforcement Learning
Authors: Egor Cherepanov, Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I.
  Panov
Categories: cs.LG cs.AI cs.CV
Comments: 38 pages, 44 figures, 3 tables
\\
  Pixel-based reinforcement learning agents often fail under purely visual
distribution shift even when latent dynamics and rewards are unchanged, but
existing benchmarks entangle multiple sources of shift and hinder systematic
analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the
observation process into independently controllable visual axes while keeping
the underlying control problem fixed. By construction, varying a visual axis
affects performance only through the induced state-conditional action
distribution of a pixel policy, providing a clean abstraction for visual
generalization. Building on this environment, we define KAGE-Bench, a benchmark
of six known-axis suites comprising 34 train-evaluation configuration pairs
that isolate individual visual shifts. Using a standard PPO-CNN baseline, we
observe strong axis-dependent failures, with background and photometric shifts
often collapsing success, while agent-appearance shifts are comparatively
benign. Several shifts preserve forward motion while breaking task completion,
showing that return alone can obscure generalization failures. Finally, the
fully vectorized JAX implementation enables up to 33M environment steps per
second on a single GPU, enabling fast and reproducible sweeps over visual
factors. Code: https://avanturist322.github.io/KAGEBench/.
\\ ( https://arxiv.org/abs/2601.14232 ,  6737kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14234
Date: Tue, 20 Jan 2026 18:45:34 GMT   (1469kb)

Title: Q-learning with Adjoint Matching
Authors: Qiyang Li, Sergey Levine
Categories: cs.LG cs.AI cs.RO stat.ML
Comments: 32 pages, 8 figures, 7 tables
\\
  We propose Q-learning with Adjoint Matching (QAM), a novel TD-based
reinforcement learning (RL) algorithm that tackles a long-standing challenge in
continuous-action RL: efficient optimization of an expressive diffusion or
flow-matching policy with respect to a parameterized Q-function. Effective
optimization requires exploiting the first-order information of the critic, but
it is challenging to do so for flow or diffusion policies because direct
gradient-based optimization via backpropagation through their multi-step
denoising process is numerically unstable. Existing methods work around this
either by only using the value and discarding the gradient information, or by
relying on approximations that sacrifice policy expressivity or bias the
learned policy. QAM sidesteps both of these challenges by leveraging adjoint
matching, a recently proposed technique in generative modeling, which
transforms the critic's action gradient to form a step-wise objective function
that is free from unstable backpropagation, while providing an unbiased,
expressive policy at the optimum. Combined with temporal-difference backup for
critic learning, QAM consistently outperforms prior approaches on hard, sparse
reward tasks in both offline and offline-to-online RL.
\\ ( https://arxiv.org/abs/2601.14234 ,  1469kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14238
Date: Tue, 20 Jan 2026 18:50:12 GMT   (29511kb)

Title: Spatiotemporal Wildfire Prediction and Reinforcement Learning for
  Helitack Suppression
Authors: Shaurya Mathur, Shreyas Bellary Manjunath, Nitin Kulkarni, Alina
  Vereshchaka
Categories: cs.LG
Comments: 6 pages, 5 figures (two of them in tables), Conference: IEEE
  International Conference on Machine Learning and Applications 2025 (ICMLA
  2025): https://www.icmla-conference.org/icmla25/
\\
  Wildfires are growing in frequency and intensity, devastating ecosystems and
communities while causing billions of dollars in suppression costs and economic
damage annually in the U.S. Traditional wildfire management is mostly reactive,
addressing fires only after they are detected. We introduce
\textit{FireCastRL}, a proactive artificial intelligence (AI) framework that
combines wildfire forecasting with intelligent suppression strategies. Our
framework first uses a deep spatiotemporal model to predict wildfire ignition.
For high-risk predictions, we deploy a pre-trained reinforcement learning (RL)
agent to execute real-time suppression tactics with helitack units inside a
physics-informed 3D simulation. The framework generates a threat assessment
report to help emergency responders optimize resource allocation and planning.
In addition, we are publicly releasing a large-scale, spatiotemporal dataset
containing $\mathbf{9.5}$ million samples of environmental variables for
wildfire prediction. Our work demonstrates how deep learning and RL can be
combined to support both forecasting and tactical wildfire response. More
details can be found at https://sites.google.com/view/firecastrl.
\\ ( https://arxiv.org/abs/2601.14238 ,  29511kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14243
Date: Tue, 20 Jan 2026 18:54:31 GMT   (2065kb)

Title: Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified
  Training and Rollout Precision Flow
Authors: Haocheng Xi, Charlie Ruan, Peiyuan Liao, Yujun Lin, Han Cai, Yilong
  Zhao, Shuo Yang, Kurt Keutzer, Song Han, Ligeng Zhu
Categories: cs.LG cs.CL
Comments: 11 pages, 6 figures, 4 tables
\\
  Reinforcement learning (RL) is essential for enhancing the complex reasoning
capabilities of large language models (LLMs). However, existing RL training
pipelines are computationally inefficient and resource-intensive, with the
rollout phase accounting for over 70% of total training time. Quantized RL
training, particularly using FP8 precision, offers a promising approach to
mitigating this bottleneck. A commonly adopted strategy applies FP8 precision
during rollout while retaining BF16 precision for training. In this work, we
present the first comprehensive study of FP8 RL training and demonstrate that
the widely used BF16-training + FP8-rollout strategy suffers from severe
training instability and catastrophic accuracy collapse under long-horizon
rollouts and challenging tasks. Our analysis shows that these failures stem
from the off-policy nature of the approach, which introduces substantial
numerical mismatch between training and inference. Motivated by these
observations, we propose Jet-RL, an FP8 RL training framework that enables
robust and stable RL optimization. The key idea is to adopt a unified FP8
precision flow for both training and rollout, thereby minimizing numerical
discrepancies and eliminating the need for inefficient inter-step calibration.
Extensive experiments validate the effectiveness of Jet-RL: our method achieves
up to 33% speedup in the rollout phase, up to 41% speedup in the training
phase, and a 16% end-to-end speedup over BF16 training, while maintaining
stable convergence across all settings and incurring negligible accuracy
degradation.
\\ ( https://arxiv.org/abs/2601.14243 ,  2065kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11647
Date: Thu, 15 Jan 2026 05:19:46 GMT   (646kb)

Title: Reinforcement Learning for Dynamic Workflow Optimization in CI/CD
  Pipelines
Authors: Aniket Abhishek Soni, Milan Parikh, Rashi Nimesh Kumar Dhenia, Jubin
  Abhishek Soni, Ayush Raj Jha, and Sneja Mitinbhai Shah
Categories: cs.SE cs.AI cs.LG
Comments: Accepted and presented at CICN 2025 (International Conference on
  Computational Intelligence and Communication Networks). 7 pages, 5 figures
ACM-class: D.2.8; I.2.6; D.2.2
\\
  Continuous Integration and Continuous Deployment (CI/CD) pipelines are
central to modern software delivery, yet their static workflows often introduce
inefficiencies as systems scale. This paper proposes a reinforcement learning
(RL) based approach to dynamically optimize CI/CD pipeline workflows. The
pipeline is modeled as a Markov Decision Process, and an RL agent is trained to
make runtime decisions such as selecting full, partial, or no test execution in
order to maximize throughput while minimizing testing overhead.
  A configurable CI/CD simulation environment is developed to evaluate the
approach across build, test, and deploy stages. Experimental results show that
the RL optimized pipeline achieves up to a 30 percent improvement in throughput
and approximately a 25 percent reduction in test execution time compared to
static baselines, while maintaining a defect miss rate below 5 percent. The
agent learns to selectively skip or abbreviate tests for low risk commits,
accelerating feedback cycles without significantly increasing failure risk.
  These results demonstrate the potential of reinforcement learning to enable
adaptive and intelligent DevOps workflows, providing a practical pathway toward
more efficient, resilient, and sustainable CI/CD automation.
\\ ( https://arxiv.org/abs/2601.11647 ,  646kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11655
Date: Thu, 15 Jan 2026 18:55:03 GMT   (7208kb)

Title: Advances and Frontiers of LLM-based Issue Resolution in Software
  Engineering: A Comprehensive Survey
Authors: Caihua Li, Lianghong Guo, Yanlin Wang, Daya Guo, Wei Tao, Zhenyu Shan,
  Mingwei Liu, Jiachi Chen, Haoyu Song, Duyu Tang, Hongyu Zhang, Zibin Zheng
Categories: cs.SE cs.CL
Comments: 26 pages, 4 figures, 5 tables
ACM-class: D.2.0; I.2.7
\\
  Issue resolution, a complex Software Engineering (SWE) task integral to
real-world development, has emerged as a compelling challenge for artificial
intelligence. The establishment of benchmarks like SWE-bench revealed this task
as profoundly difficult for large language models, thereby significantly
accelerating the evolution of autonomous coding agents. This paper presents a
systematic survey of this emerging domain. We begin by examining data
construction pipelines, covering automated collection and synthesis approaches.
We then provide a comprehensive analysis of methodologies, spanning
training-free frameworks with their modular components to training-based
techniques, including supervised fine-tuning and reinforcement learning.
Subsequently, we discuss critical analyses of data quality and agent behavior,
alongside practical applications. Finally, we identify key challenges and
outline promising directions for future research. An open-source repository is
maintained at https://github.com/DeepSoftwareAnalytics/Awesome-Issue-Resolution
to serve as a dynamic resource in this field.
\\ ( https://arxiv.org/abs/2601.11655 ,  7208kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11659
Date: Thu, 15 Jan 2026 21:06:38 GMT   (22kb)

Title: The Llama 4 Herd: Architecture, Training, Evaluation, and Deployment
  Notes
Authors: Aaron Adcock, Aayushi Srivastava, Abhimanyu Dubey, Abhinav Jauhri,
  Abhinav Pande, Abhinav Pandey, Abhinav Sharma, Abhishek Kadian, Abhishek
  Kumawat, Adam Kelsey, Adam Stelle, Adeel Cheema, Adela Kabiljo, Adina Katz,
  Adithya Gangidi, Aditya Tayade, Adolfo Victoria, Adrian Samatan Alastuey,
  Adrien Conrath, Afroz Mohiuddin, Ahmed Sharif, Ahnaf Siddiqui, Ahuva
  Goldstand, Aijung Li, Aidan Boyd, Aidin Kazemi Daliri, Aisha Iqbal, Ajay
  Menon, Ajit Mathews, Akhil Mathur, Akshat Agarwal, Alan Schelten, Alana
  Shine, Alejandro Castillejo Mu\~noz, Aleksei Guliaev, Alex Radovic, Alex
  Song, Alex Vaughan, Alexander Simeonov, Alexandre Rezende, Alexandre Rezende,
  Alexei Baevski, Alexey Roubaud, Allen Ma, Alvin Lee, Alyssa Pereira, Aman
  Ahmed, Aman Shankar, Amanda Kallet, Amar Budhiraja, Ameya Khandekar, Amine
  Benhalloum, et al. (1271 additional authors not shown)
Categories: cs.SE cs.LG
Comments: 15 pages
MSC-class: 68T01
\\
  This document consolidates publicly reported technical details about Metas
Llama 4 model family. It summarizes (i) released variants (Scout and Maverick)
and the broader herd context including the previewed Behemoth teacher model,
(ii) architectural characteristics beyond a high-level MoE description covering
routed/shared-expert structure, early-fusion multimodality, and long-context
design elements reported for Scout (iRoPE and length generalization
strategies), (iii) training disclosures spanning pre-training, mid-training for
long-context extension, and post-training methodology (lightweight SFT, online
RL, and lightweight DPO) as described in release materials, (iv)
developer-reported benchmark results for both base and instruction-tuned
checkpoints, and (v) practical deployment constraints observed across major
serving environments, including provider-specific context limits and
quantization packaging. The manuscript also summarizes licensing obligations
relevant to redistribution and derivative naming, and reviews publicly
described safeguards and evaluation practices. The goal is to provide a compact
technical reference for researchers and practitioners who need precise,
source-backed facts about Llama 4.
\\ ( https://arxiv.org/abs/2601.11659 ,  22kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11672
Date: Fri, 16 Jan 2026 03:40:28 GMT   (1581kb)

Title: From Everything-is-a-File to Files-Are-All-You-Need: How Unix Philosophy
  Informs the Design of Agentic AI Systems
Authors: Deepak Babu Piskala
Categories: cs.SE cs.SY eess.SY
ACM-class: I.2.11; D.2.9
\\
  A core abstraction in early Unix systems was the principle that 'everything
is a file', enabling heterogeneous devices and kernel resources to be
manipulated via uniform read/write interfaces. This paper explores how an
analogous unification is emerging in contemporary agentic AI. We trace the
evolution from Unix to DevOps, Infrastructure-as-Code, and finally autonomous
software agents, highlighting how file-like abstractions and code-based
specifications collapse diverse resources into consistent, composable
interfaces. The resulting perspective suggests that adopting file- and
code-centric interaction models may enable agentic systems that are more
maintainable, auditable, and operationally robust.
\\ ( https://arxiv.org/abs/2601.11672 ,  1581kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11687
Date: Fri, 16 Jan 2026 11:32:20 GMT   (15kb)

Title: Semantic Caching and Intent-Driven Context Optimization for Multi-Agent
  Natural Language to Code Systems
Authors: Harmohit Singh
Categories: cs.SE cs.AI
\\
  We present a production-optimized multi-agent system designed to translate
natural language queries into executable Python code for structured data
analytics. Unlike systems that rely on expensive frontier models, our approach
achieves high accuracy and cost efficiency through three key innovations: (1) a
semantic caching system with LLM-based equivalence detection and structured
adaptation hints that provides cache hit rates of 67% on production queries;
(2) a dual-threshold decision mechanism that separates exact-match retrieval
from reference-guided generation; and (3) an intent-driven dynamic prompt
assembly system that reduces token consumption by 40-60% through table-aware
context filtering. The system has been deployed in production for enterprise
inventory management, processing over 10,000 queries with an average latency of
8.2 seconds and 94.3% semantic accuracy. We describe the architecture, present
empirical results from production deployment, and discuss practical
considerations for deploying LLM-based analytics systems at scale.
\\ ( https://arxiv.org/abs/2601.11687 ,  15kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11688
Date: Fri, 16 Jan 2026 11:50:18 GMT   (709kb)

Title: SpecMap: Hierarchical LLM Agent for Datasheet-to-Code Traceability Link
  Recovery in Systems Engineering
Authors: Vedant Nipane, Pulkit Agrawal, Amit Singh
Categories: cs.SE cs.AI
\\
  Establishing precise traceability between embedded systems datasheets and
their corresponding code implementations remains a fundamental challenge in
systems engineering, particularly for low-level software where manual mapping
between specification documents and large code repositories is infeasible.
Existing Traceability Link Recovery approaches primarily rely on lexical
similarity and information retrieval techniques, which struggle to capture the
semantic, structural, and symbol level relationships prevalent in embedded
systems software. We present a hierarchical datasheet-to-code mapping
methodology that employs large language models for semantic analysis while
explicitly structuring the traceability process across multiple abstraction
levels. Rather than performing direct specification-to-code matching, the
proposed approach progressively narrows the search space through
repository-level structure inference, file-level relevance estimation, and
fine-grained symbollevel alignment. The method extends beyond function-centric
mapping by explicitly covering macros, structs, constants, configuration
parameters, and register definitions commonly found in systems-level C/C++
codebases. We evaluate the approach on multiple open-source embedded systems
repositories using manually curated datasheet-to-code ground truth.
Experimental results show substantial improvements over traditional
information-retrieval-based baselines, achieving up to 73.3% file mapping
accuracy. We significantly reduce computational overhead, lowering total LLM
token consumption by 84% and end-to-end runtime by approximately 80%. This
methodology supports automated analysis of large embedded software systems and
enables downstream applications such as training data generation for
systems-aware machine learning models, standards compliance verification, and
large-scale specification coverage analysis.
\\ ( https://arxiv.org/abs/2601.11688 ,  709kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11693
Date: Fri, 16 Jan 2026 16:27:32 GMT   (122kb)

Title: Technical Lag as Latent Technical Debt: A Rapid Review
Authors: Shane K. Panter, Nasir U. Eisty
Categories: cs.SE
Comments: Accepted to: TechDebt 2026 - International Conference on Technical
  Debt April 12--15, 2026 Rio de Janeiro, Brazil
\\
  Context: Technical lag accumulates when software systems fail to keep pace
with technological advancements, leading to a deterioration in software
quality. Objective: This paper aims to consolidate existing research on
technical lag, clarify definitions, explore its detection and quantification
methods, examine underlying causes and consequences, review current management
practices, and lay out a vision as an indicator of passively accumulated
technical debt. Method: We conducted a Rapid Review with snowballing to select
the appropriate peer-reviewed studies. We leveraged the ACM Digital Library,
IEEE Xplore, Scopus, and Springer as our primary source databases. Results:
Technical lag accumulates passively, often unnoticed due to inadequate
detection metrics and tools. It negatively impacts software quality through
outdated dependencies, obsolete APIs, unsupported platforms, and aging
infrastructure. Strategies to manage technical lag primarily involve automated
dependency updates, continuous integration processes, and regular auditing.
Conclusions: Enhancing and extending the current standardized metrics,
detection methods, and empirical studies to use technical lag as an indication
of accumulated latent debt can greatly improve the process of maintaining large
codebases that are heavily dependent on external packages. We have identified
the research gaps and outlined a future vision for researchers and
practitioners to explore.
\\ ( https://arxiv.org/abs/2601.11693 ,  122kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11783
Date: Fri, 16 Jan 2026 21:15:13 GMT   (110kb)

Title: The Stability Trap: Evaluating the Reliability of LLM-Based Instruction
  Adherence Auditing
Authors: Murtuza N. Shergadwala
Categories: cs.SE cs.CL
\\
  The enterprise governance of Generative AI (GenAI) in regulated sectors, such
as Human Resources (HR), demands scalable yet reproducible auditing mechanisms.
While Large Language Model (LLM)-as-a-Judge approaches offer scalability, their
reliability in evaluating adherence of different types of system instructions
remains unverified. This study asks: To what extent does the instruction type
of an Application Under Test (AUT) influence the stability of judge
evaluations? To address this, we introduce the Scoped Instruction Decomposition
Framework to classify AUT instructions into Objective and Subjective types,
isolating the factors that drive judge instability. We applied this framework
to two representative HR GenAI applications, evaluating the stability of four
judge architectures over variable runs. Our results reveal a ``Stability Trap''
characterized by a divergence between Verdict Stability and Reasoning
Stability. While judges achieved near-perfect verdict agreement ($>99\%$) for
both objective and subjective evaluations, their accompanying justification
traces diverged significantly. Objective instructions requiring quantitative
analysis, such as word counting, exhibited reasoning stability as low as
$\approx19\%$, driven by variances in numeric justifications. Similarly,
reasoning stability for subjective instructions varied widely ($35\%$--$83\%$)
based on evidence granularity, with feature-specific checks failing to
reproduce consistent rationale. Conversely, objective instructions focusing on
discrete entity extraction achieved high reasoning stability ($>90\%$). These
findings demonstrate that high verdict stability can mask fragile reasoning.
Thus, we suggest that auditors scope automated evaluation protocols strictly:
delegate all deterministically verifiable logic to code, while reserving LLM
judges for complex semantic evaluation.
\\ ( https://arxiv.org/abs/2601.11783 ,  110kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11835
Date: Fri, 16 Jan 2026 23:51:31 GMT   (1018kb)

Title: Changes in Coding Behavior and Performance Since the Introduction of
  LLMs
Authors: Yufan Zhang, Jaromir Savelka, Seth Goldstein, and Michael Conway
Categories: cs.SE
DOI: 10.1145/3785022.3785075
\\
  The widespread availability of large language models (LLMs) has changed how
students engage with coding and problem-solving. While these tools may increase
student productivity, they also make it more difficult for instructors to
assess students' learning and effort. In this quasi-longitudinal study, we
analyze five years of student source code submissions in a graduate-level cloud
computing course, focusing on an assignment that remained unchanged and
examining students' behavior during the period spanning five semesters before
the release of ChatGPT and five semesters after.
  Student coding behavior has changed significantly since Fall 2022. The length
of their final submissions increased. Between consecutive submissions, average
edit distances increased while average score improvement decreased, suggesting
that both student productivity and learning have decreased after ChatGPT's
release. Additionally, there are statistically significant correlations between
these behavioral changes and their overall performance. Although we cannot
definitively attribute them to LLM misuse, they are consistent with our
hypothesis that some students are over-reliant on LLMs, which is negatively
affecting their learning outcomes. Our findings raise an alarm around the first
generation of graduates in the age of LLMs, calling upon both educators and
employers to reflect on their evaluation methods for genuine expertise and
productivity.
\\ ( https://arxiv.org/abs/2601.11835 ,  1018kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11836
Date: Fri, 16 Jan 2026 23:51:31 GMT   (159kb)

Title: Trace Validation of Unmodified Concurrent Systems with OmniLink
Authors: Finn Hackett, Evan Wrench, Peter Macko, A. Jesse Jiryu Davis, Yuanhao
  Wei, Ivan Beschastnikh
Categories: cs.SE
MSC-class: 68Q60
ACM-class: D.2.2; D.2.4; D.2.5
\\
  Concurrent systems are notoriously difficult to validate: subtle bugs may
only manifest under rare thread interleavings, and existing tools often require
intrusive instrumentation or unrealistic execution models. We present OmniLink,
a new methodology for validating concurrent implementations against high-level
specifications in TLA+. Unlike prior TLA+ based approaches which use a
technique called trace validation, OmniLink treats system events as black boxes
with a timebox in which they occurred and a meaning in TLA+, solving for a
logical total order of actions. Unlike prior approaches based on
linearizability checking, which already solves for total orders of actions with
timeboxes, OmniLink uses a flexible specification language, and offers a
different linearizability checking method based on off-the-shelf model
checking. OmniLink offers different features compared existing linearizability
checking tools, and we show that it outperforms the state of the art on large
scale validation tasks.
  Our evaluation validates WiredTiger, a state-of-the-art industrial database
storage layer, as well as Balanced Augmented Tree (BAT), a state-of-the art
lock-free data structure from the research community, and ConcurrentQueue, a
popular lock-free queue featuring aggressive performance optimizations. We use
OmniLink to improve WiredTiger's existing TLA+ model, as well as develop new
TLA+ models that closely match the behavior of the modeled systems, including
non-linearizable behaviors. OmniLink is able to find known bugs injected into
the systems under test, as well as help discover two previously unknown bugs (1
in BAT, 1 in ConcurrentQueue), which we have confirmed with the authors of
those systems.
\\ ( https://arxiv.org/abs/2601.11836 ,  159kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11868
Date: Sat, 17 Jan 2026 01:29:30 GMT   (17342kb)

Title: Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command
  Line Interfaces
Authors: Mike A. Merrill, Alexander G. Shaw, Nicholas Carlini, Boxuan Li, Harsh
  Raj, Ivan Bercovich, Lin Shi, Jeong Yeon Shin, Thomas Walshe, E. Kelly
  Buchanan, Junhong Shen, Guanghao Ye, Haowei Lin, Jason Poulos, Maoyu Wang,
  Marianna Nezhurina, Jenia Jitsev, Di Lu, Orfeas Menis Mastromichalakis,
  Zhiwei Xu, Zizhao Chen, Yue Liu, Robert Zhang, Leon Liangyu Chen, Anurag
  Kashyap, Jan-Lucas Uslu, Jeffrey Li, Jianbo Wu, Minghao Yan, Song Bian,
  Vedang Sharma, Ke Sun, Steven Dillmann, Akshay Anand, Andrew Lanpouthakoun,
  Bardia Koopah, Changran Hu, Etash Guha, Gabriel H. S. Dreiman, Jiacheng Zhu,
  Karl Krauth, Li Zhong, Niklas Muennighoff, Robert Amanfu, Shangyin Tan,
  Shreyas Pimpalgaonkar, Tushar Aggarwal, Xiangning Lin, Xin Lan, Xuandong
  Zhao, Yiqing Liang, Yuanli Wang, Zilong Wang, Changzhi Zhou, David Heineman,
  Hange Liu, Harsh Trivedi, John Yang, Junhong Lin, Manish Shetty, Michael
  Yang, Nabil Omi, Negin Raoof, Shanda Li, Terry Yue Zhuo, Wuwei Lin, Yiwei
  Dai, Yuxin Wang, Wenhao Chai, Shang Zhou, Dariush Wahdany, Ziyu She, Jiaming
  Hu, Zhikang Dong, Yuxuan Zhu, Sasha Cui, Ahson Saiyed, Arinbj\"orn
  Kolbeinsson, Jesse Hu, Christopher Michael Rytting, Ryan Marten, Yixin Wang,
  Alex Dimakis, Andy Konwinski, Ludwig Schmidt
Categories: cs.SE cs.AI
\\
  AI agents may soon become capable of autonomously completing valuable,
long-horizon tasks in diverse domains. Current benchmarks either do not measure
real-world tasks, or are not sufficiently difficult to meaningfully measure
frontier models. To this end, we present Terminal-Bench 2.0: a carefully
curated hard benchmark composed of 89 tasks in computer terminal environments
inspired by problems from real workflows. Each task features a unique
environment, human-written solution, and comprehensive tests for verification.
We show that frontier models and agents score less than 65\% on the benchmark
and conduct an error analysis to identify areas for model and agent
improvement. We publish the dataset and evaluation harness to assist developers
and researchers in future work at https://www.tbench.ai/ .
\\ ( https://arxiv.org/abs/2601.11868 ,  17342kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11926
Date: Sat, 17 Jan 2026 06:15:05 GMT   (611kb)

Title: Harmonica: A Self-Adaptation Exemplar for Sustainable MLOps
Authors: Ananya Halgatti, Shaunak Biswas, Hiya Bhatt, Srinivasan Rakhunathan,
  Karthik Vaidhyanathan
Categories: cs.SE cs.LG
Comments: This paper has been accepted to SEAMS 2026 Artifact Track
\\
  Machine learning enabled systems (MLS) often operate in settings where they
regularly encounter uncertainties arising from changes in their surrounding
environment. Without structured oversight, such changes can degrade model
behavior, increase operational cost, and reduce the usefulness of deployed
systems. Although Machine Learning Operations (MLOps) streamlines the lifecycle
of ML models, it provides limited support for addressing runtime uncertainties
that influence the longer term sustainability of MLS. To support continued
viability, these systems need a mechanism that detects when execution drifts
outside acceptable bounds and adjusts system behavior in response. Despite the
growing interest in sustainable and self-adaptive MLS, there has been limited
work towards exemplars that allow researchers to study these challenges in
MLOps pipelines. This paper presents Harmonica, a self-adaptation exemplar
built on the HarmonE approach, designed to enable the sustainable operation of
such pipelines. Harmonica introduces structured adaptive control through MAPE-K
loop, separating high-level adaptation policy from low-level tactic execution.
It continuously monitors sustainability metrics, evaluates them against dynamic
adaptation boundaries, and automatically triggers architectural tactics when
thresholds are violated. We demonstrate the tool through case studies in time
series regression and computer vision, examining its ability to improve system
stability and reduce manual intervention. The results show that Harmonica
offers a practical and reusable foundation for enabling adaptive behavior in
MLS that rely on MLOps pipelines for sustained operation.
\\ ( https://arxiv.org/abs/2601.11926 ,  611kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11972
Date: Sat, 17 Jan 2026 09:08:11 GMT   (453kb)

Title: Enhancing Fuzz Testing Efficiency through Automated Fuzz Target
  Generation
Authors: Chi Thien Tran
Categories: cs.SE cs.CR
Comments: 4 tables, 4 figures, 7 pages
Journal-ref: Programming and Computer Software, vol. 51, no. 2, 2025
DOI: 10.1134/S0361768825700227
\\
  Fuzzing continues to be the most effective method for identifying security
vulnerabilities in software. In the context of fuzz testing, the fuzzer
supplies varied inputs to fuzz targets, which are designed to comprehensively
exercise critical sections of the client code. Various studies have focused on
optimizing and developing advanced fuzzers, such as AFL++, libFuzzer,
Honggfuzz, syzkaller, ISP-Fuzzer, which have substantially enhanced
vulnerability detection in widely used software and libraries. Nevertheless,
achieving greater coverage necessitates improvements in both the quality and
quantity of fuzz targets. In large-scale software projects and libraries --
characterized by numerous user defined functions and data types -- manual
creation of fuzz targets is both labor-intensive and time-consuming. This
challenge underscores the need for automated techniques not only to generate
fuzz targets but also to streamline the execution and analysis of their
results. In this paper, we introduce an approach to improving fuzz target
generation through static analysis of library source code. The proposed method
encompasses several key aspects: it analyzes source code structures to
accurately construct function calls and generate fuzz targets; it maps fuzzer
input data to the corresponding function parameters; it synthesizes compilation
information for the fuzz targets; and it automatically collects and analyzes
execution results. Our findings are demonstrated through the application of
this approach to the generation of fuzz targets for C/C++ libraries.
\\ ( https://arxiv.org/abs/2601.11972 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12146
Date: Sat, 17 Jan 2026 19:43:06 GMT   (3241kb)

Title: From LLMs to Agents in Programming: The Impact of Providing an LLM with
  a Compiler
Authors: Viktor Kjellberg, Miroslaw Staron, Farnaz Fotrousi
Categories: cs.SE
\\
  Large Language Models have demonstrated a remarkable capability in natural
language and program generation and software development. However, the source
code generated by the LLMs does not always meet quality requirements and may
fail to compile. Therefore, many studies evolve into agents that can reason
about the problem before generating the source code for the solution. The goal
of this paper is to study the degree to which such agents benefit from access
to software development tools, in our case, a \texttt{gcc} compiler. We conduct
a computational experiment on the RosettaCode dataset, on 699 programming tasks
in C. We evaluate how the integration with a compiler shifts the role of the
language model from a passive generator to an active agent capable of
iteratively developing runnable programs based on feedback from the compiler.
We evaluated 16 language models with sizes ranging from small (135 million) to
medium (3 billion) and large (70 billion). Our results show that access to a
compiler improved the compilation success by 5.3 to 79.4 percentage units in
compilation without affecting the semantics of the generated program. Syntax
errors dropped by 75\%, and errors related to undefined references dropped by
87\% for the tasks where the agents outperformed the baselines. We also
observed that in some cases, smaller models with a compiler outperform larger
models with a compiler. We conclude that it is essential for LLMs to have
access to software engineering tools to enhance their performance and reduce
the need for large models in software engineering, such as reducing our energy
footprint.
\\ ( https://arxiv.org/abs/2601.12146 ,  3241kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12148
Date: Sat, 17 Jan 2026 19:43:22 GMT   (572kb)

Title: Many Hands Make Light Work: An LLM-based Multi-Agent System for
  Detecting Malicious PyPI Packages
Authors: Muhammad Umar Zeshan, Motunrayo Ibiyo, Claudio Di Sipio, Phuong T.
  Nguyen, Davide Di Ruscio
Categories: cs.SE
Comments: The paper has been peer-reviewed and accepted for publication to the
  Journal of Systems and Software
  (https://www.sciencedirect.com/journal/journal-of-systems-and-software)
\\
  Malicious code in open-source repositories such as PyPI poses a growing
threat to software supply chains. Traditional rule-based tools often overlook
the semantic patterns in source code that are crucial for identifying
adversarial components. Large language models (LLMs) show promise for software
analysis, yet their use in interpretable and modular security pipelines remains
limited. This paper presents LAMPS, a multi-agent system that employs
collaborative LLMs to detect malicious PyPI packages. The system consists of
four role-specific agents for package retrieval, file extraction,
classification, and verdict aggregation, coordinated through the CrewAI
framework. A prototype combines a fine-tuned CodeBERT model for classification
with LLaMA-3 agents for contextual reasoning. LAMPS has been evaluated on two
complementary datasets: D1, a balanced collection of 6,000 setup.py files, and
D2, a realistic multi-file dataset with 1,296 files and natural class
imbalance. On D1, LAMPS achieves 97.7% accuracy, surpassing MPHunter--one of
the state-of-the-art approaches. On D2, it reaches 99.5% accuracy and 99.5%
balanced accuracy, outperforming RAG-based approaches and fine-tuned
single-agent baselines. McNemar's test confirmed these improvements as highly
significant. The results demonstrate the feasibility of distributed LLM
reasoning for malicious code detection and highlight the benefits of modular
multi-agent designs in software supply chain security.
\\ ( https://arxiv.org/abs/2601.12148 ,  572kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12186
Date: Sat, 17 Jan 2026 22:30:45 GMT   (2564kb)

Title: Aletheia: What Makes RLVR For Code Verifiers Tick?
Authors: Vatsal Venkatkrishna, Indraneil Paul, Iryna Gurevych
Categories: cs.SE cs.AI
Comments: 8 pages, 6 figures
\\
  Multi-domain thinking verifiers trained via Reinforcement Learning from
Verifiable Rewards (RLVR) are a prominent fixture of the Large Language Model
(LLM) post-training pipeline, owing to their ability to robustly rate and
rerank model outputs. However, the adoption of such verifiers towards code
generation has been comparatively sparse, with execution feedback constituting
the dominant signal. Nonetheless, code verifiers remain valuable toward judging
model outputs in scenarios where execution feedback is hard to obtain and are a
potentially powerful addition to the code generation post-training toolbox. To
this end, we create and open-source Aletheia, a controlled testbed that enables
execution-grounded evaluation of code verifiers' robustness across disparate
policy models and covariate shifts. We examine components of the RLVR-based
verifier training recipe widely credited for its success: (1) intermediate
thinking traces, (2) learning from negative samples, and (3) on-policy
training. While experiments show the optimality of RLVR, we uncover important
opportunities to simplify the recipe. Particularly, despite code verification
exhibiting positive training- and inference-time scaling, on-policy learning
stands out as the key component at small verifier sizes, and thinking-based
training emerges as the most important component at larger scales.
\\ ( https://arxiv.org/abs/2601.12186 ,  2564kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12262
Date: Sun, 18 Jan 2026 04:58:15 GMT   (2854kb)

Title: Environment-Aware Code Generation: How far are We?
Authors: Tongtong Wu, Rongyi Chen, Wenjie Du, Suyu Ma, Guilin Qi, Zhenchang
  Xing, Shahram Khadivi, Ramesh Periyathambi, Gholamreza Haffari
Categories: cs.SE cs.CL
Comments: ICSE 2026
\\
  Recent progress in large language models (LLMs) has improved code generation,
but most evaluations still test isolated, small-scale code (e.g., a single
function) under default or unspecified software environments. As a result, it
is unclear whether LLMs can reliably generate executable code tailored to a
user's specific environment. We present the first systematic study of
Environment-Aware Code Generation (EACG), where generated code must be
functionally correct and directly executable under arbitrary software
configurations. To enable realistic evaluation, we introduce VersiBCB, a
benchmark that is multi-package, execution-verified, and deprecation-aware,
capturing complex and evolving environments that prior datasets often overlook.
Using VersiBCB, we investigate three complementary adaptation axes: data,
parameters, and cache, and develop representative strategies for each. Our
results show that current LLMs struggle with environment-specific code
generation, while our adaptations improve environment compatibility and
executability. These findings highlight key challenges and opportunities for
deploying LLMs in practical software engineering workflows.
\\ ( https://arxiv.org/abs/2601.12262 ,  2854kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12273
Date: Sun, 18 Jan 2026 06:07:42 GMT   (208kb)

Title: Leveraging Mutation Analysis for LLM-based Repair of Quantum Programs
Authors: Chihiro Yoshida, Yuta Ishimoto, Olivier Nourry, Masanari Kondo, Makoto
  Matsushita, Yasutaka Kamei, Yoshiki Higo
Categories: cs.SE
Comments: 6 pages, Accepted at SANER-ERA 2026
\\
  In recent years, Automated Program Repair (APR) techniques specifically
designed for quantum programs have been proposed. However, existing approaches
often suffer from low repair success rates or poor understandability of the
generated patches. In this study, we construct a framework in which a large
language model (LLM) generates code repairs along with a natural language
explanation of the applied repairs. To investigate how the contextual
information included in prompts influences APR performance for quantum
programs, we design four prompt configurations with different combinations of
static information, dynamic information, and mutation analysis results.
Mutation analysis evaluates how small changes to specific parts of a program
affect its execution results and provides more detailed dynamic information
than simple execution outputs such as stack traces. Our experimental results
show that mutation analysis can provide valuable contextual information for
LLM-based APR of quantum programs, improving repair success rates (achieving
94.4% in our experiment) and in some cases also improving the quality of
generated explanations. Our findings point toward new directions for developing
APR techniques for quantum programs that enhance both reliability and
explainability.
\\ ( https://arxiv.org/abs/2601.12273 ,  208kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12274
Date: Sun, 18 Jan 2026 06:09:18 GMT   (374kb)

Title: Hybrid Concolic Testing with Large Language Models for Guided Path
  Exploration
Authors: Mahdi Eslamimehr
Categories: cs.SE
Comments: 12 pages, 2 Figures, 2 Tables
\\
  Concolic testing, a powerful hybrid software testing technique, has
historically been plagued by fundamental limitations such as path explosion and
the high cost of constraint solving, which hinder its practical application in
large-scale, real-world software systems. This paper introduces a novel
algorithmic framework that synergistically integrates concolic execution with
Large Language Models (LLMs) to overcome these challenges. Our hybrid approach
leverages the semantic reasoning capabilities of LLMs to guide path
exploration, prioritize interesting execution paths, and assist in constraint
solving. We formally define the system architecture and algorithms that
constitute this new paradigm. Through a series of experiments on both synthetic
and real-world Fintech applications, we demonstrate that our approach
significantly outperforms traditional concolic testing, random testing, and
genetic algorithm-based methods in terms of branch coverage, path coverage, and
time-to-coverage. The results indicate that by combining the strengths of both
concolic execution and LLMs, our method achieves a more efficient and effective
exploration of the program state space, leading to improved bug detection
capabilities.
\\ ( https://arxiv.org/abs/2601.12274 ,  374kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12327
Date: Sun, 18 Jan 2026 09:20:21 GMT   (14kb)

Title: The Expert Validation Framework (EVF): Enabling Domain Expert Control in
  AI Engineering
Authors: Lucas Gren and Felix Dobslaw
Categories: cs.SE cs.AI
Journal-ref: CAIN2026: 5th International Conference on AI Engineering -
  Software Engineering for AI
\\
  Generative AI (GenAI) systems promise to transform knowledge work by
automating a range of tasks, yet their deployment in enterprise settings
remains hindered by the lack of systematic quality assurance mechanisms. We
present an Expert Validation Framework that places domain experts at the center
of building software with GenAI components, enabling them to maintain
authoritative control over system behavior through structured specification,
testing, validation, and continuous monitoring processes. Our framework
addresses the critical gap between AI capabilities and organizational trust by
establishing a rigorous, expert-driven methodology for ensuring quality across
diverse GenAI applications. Through a four-stage implementation process
encompassing specification, system creation, validation, and production
monitoring, the framework enables organizations to leverage GenAI capabilities
while maintaining expert oversight and quality standards.
\\ ( https://arxiv.org/abs/2601.12327 ,  14kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12360
Date: Sun, 18 Jan 2026 11:33:45 GMT   (424kb)

Title: Discovering 100+ Compiler Defects in 72 Hours via LLM-Driven Semantic
  Logic Recomposition
Authors: Xinabang He, Yuanwei Chen, Hao Wu, Jikang Zhang, Zicheng Wang, Ligeng
  Chen, Junjie Peng, Haiyang Wei, Yi Qian, Tiantai Zhang, Linzhang Wang, Bing
  Mao
Categories: cs.SE
\\
  Compilers constitute the foundational root-of-trust in software supply
chains; however, their immense complexity inevitably conceals critical defects.
Recent research has attempted to leverage historical bugs to design new
mutation operators or fine-tune models to increase program diversity for
compiler fuzzing.We observe, however, that bugs manifest primarily based on the
semantics of input programs rather than their syntax. Unfortunately, current
approaches, whether relying on syntactic mutation or general Large Language
Model (LLM) fine-tuning, struggle to preserve the specific semantics found in
the logic of bug-triggering programs. Consequently, these critical semantic
triggers are often lost, resulting in a limitation of the diversity of
generated programs.
  To explicitly reuse such semantics, we propose FeatureFuzz, a compiler fuzzer
that combines features to generate programs. We define a feature as a decoupled
primitive that encapsulates a natural language description of a bug-prone
invariant, such as an out-of-bounds array access, alongside a concrete code
witness of its realization. FeatureFuzz operates via a three-stage workflow: it
first extracts features from historical bug reports, synthesizes coherent
groups of features, and finally instantiates these groups into valid programs
for compiler fuzzing.
  We evaluated FeatureFuzz on GCC and LLVM. Over 24-hour campaigns, FeatureFuzz
uncovered 167 unique crashes, which is 2.78x more than the second-best fuzzer.
Furthermore, through a 72-hour fuzzing campaign, FeatureFuzz identified 106
bugs in GCC and LLVM, 76 of which have already been confirmed by compiler
developers, validating the approach's ability to stress-test modern compilers
effectively.
\\ ( https://arxiv.org/abs/2601.12360 ,  424kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12448
Date: Sun, 18 Jan 2026 15:07:16 GMT   (578kb)

Title: Evaluating Large Language Models for Time Series Anomaly Detection in
  Aerospace Software
Authors: Yang Liu, Yixing Luo, Xiaofeng Li, Xiaogang Dong, Bin Gu, Zhi Jin
Categories: cs.SE
Comments: This paper has been accepted by ASE 2025
\\
  Time series anomaly detection (TSAD) is essential for ensuring the safety and
reliability of aerospace software systems. Although large language models
(LLMs) provide a promising training-free alternative to unsupervised
approaches, their effectiveness in aerospace settings remains under-examined
because of complex telemetry, misaligned evaluation metrics, and the absence of
domain knowledge. To address this gap, we introduce ATSADBench, the first
benchmark for aerospace TSAD. ATSADBench comprises nine tasks that combine
three pattern-wise anomaly types, univariate and multivariate signals, and both
in-loop and out-of-loop feedback scenarios, yielding 108,000 data points. Using
this benchmark, we systematically evaluate state-of-the-art open-source LLMs
under two paradigms: Direct, which labels anomalies within sliding windows, and
Prediction-Based, which detects anomalies from prediction errors. To reflect
operational needs, we reformulate evaluation at the window level and propose
three user-oriented metrics: Alarm Accuracy (AA), Alarm Latency (AL), and Alarm
Contiguity (AC), which quantify alarm correctness, timeliness, and credibility.
We further examine two enhancement strategies, few-shot learning and
retrieval-augmented generation (RAG), to inject domain knowledge. The
evaluation results show that (1) LLMs perform well on univariate tasks but
struggle with multivariate telemetry, (2) their AA and AC on multivariate tasks
approach random guessing, (3) few-shot learning provides modest gains whereas
RAG offers no significant improvement, and (4) in practice LLMs can detect true
anomaly onsets yet sometimes raise false alarms, which few-shot prompting
mitigates but RAG exacerbates. These findings offer guidance for future
LLM-based TSAD in aerospace software.
\\ ( https://arxiv.org/abs/2601.12448 ,  578kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12522
Date: Sun, 18 Jan 2026 18:12:21 GMT   (1210kb)

Title: Improved Bug Localization with AI Agents Leveraging Hypothesis and
  Dynamic Cognition
Authors: Asif Mohammed Samir, Mohammad Masudur Rahman
Categories: cs.SE cs.AI cs.IR cs.LG cs.MA
Comments: 13 pages, 7 tables, 5 figures
\\
  Software bugs cost technology providers (e.g., AT&T) billions annually and
cause developers to spend roughly 50% of their time on bug resolution.
Traditional methods for bug localization often analyze the suspiciousness of
code components (e.g., methods, documents) in isolation, overlooking their
connections with other components in the codebase. Recent advances in Large
Language Models (LLMs) and agentic AI techniques have shown strong potential
for code understanding, but still lack causal reasoning during code exploration
and struggle to manage growing context effectively, limiting their capability.
In this paper, we present a novel agentic technique for bug localization --
CogniGent -- that overcomes the limitations above by leveraging multiple AI
agents capable of causal reasoning, call-graph-based root cause analysis and
context engineering. It emulates developers-inspired debugging practices
(a.k.a., dynamic cognitive debugging) and conducts hypothesis testing to
support bug localization. We evaluate CogniGent on a curated dataset of 591 bug
reports using three widely adopted performance metrics and compare it against
six established baselines from the literature. Experimental results show that
our technique consistently outperformed existing traditional and LLM-based
techniques, achieving MAP improvements of 23.33-38.57% at the document and
method levels. Similar gains were observed in MRR, with increases of
25.14-53.74% at both granularity levels. Statistical significance tests also
confirm the superiority of our technique. By addressing the reasoning,
dependency, and context limitations, CogniGent advances the state of bug
localization, bridging human-like cognition with agentic automation for
improved performance.
\\ ( https://arxiv.org/abs/2601.12522 ,  1210kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12559
Date: Sun, 18 Jan 2026 19:48:02 GMT   (2339kb)

Title: Automated Tool Support for Category-Partition Testing: Design Decisions,
  UI and Examples of Use
Authors: Yvan Labiche
Categories: cs.SE
\\
  Category-Partition is a functional testing technique that is based on the
idea that the input domain of the system under test can be divided into
sub-domains, with the assumption that inputs that belong to the same sub-domain
trigger a similar behaviour and that therefore it is sufficient to select one
input from each sub-domain. Category-Partition proceeds in several steps, from
the identification of so-called categories and choices, possibly constrained,
which are subsequently used to form test frames, i.e., combinations of choices,
and eventually test cases. This paper reports on an ongoing attempt to automate
as many of those steps as possible, with graphical-user interface tool support.
Specifically, the user interface allows the user to specify parameters as well
as so-called environment variables, further specify categories and choices with
optional constraints. Choices are provided with precise specifications with
operations specific to their types (e.g., Boolean, Integer, Real, String).
Then, the tool automates the construction of test frames, which are
combinations of choices, according to alternative selection criteria, and the
identification of input values for parameters and environment variables for
these test frames, thereby producing test cases. The paper illustrates the
capabilities of the tool with the use of nine different case studies.
\\ ( https://arxiv.org/abs/2601.12559 ,  2339kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12735
Date: Mon, 19 Jan 2026 05:36:53 GMT   (205kb)

Title: OpenAI for OpenAPI: Automated generation of REST API specification via
  LLMs
Authors: Hao Chen, Yunchun Li, Chen Chen, Fengxu Lin, Wei Li
Categories: cs.SE
\\
  REST APIs, based on the REpresentational State Transfer (REST) architecture,
are the primary type of Web API. The OpenAPI Specification (OAS) serves as the
de facto standard for describing REST APIs and is crucial for multiple software
engineering tasks. However, developers face challenges in writing and
maintaining OAS. Although static analysis shows potential for OAS generation,
it is limited to specific programming languages and development frameworks. The
powerful code understanding capabilities of LLMs offer new opportunities for
OAS generation, yet they are constrained by context limitations and
hallucinations. To address these challenges, we propose the OpenAI OpenAPI
Project Scanner (OOPS), the first technology-agnostic LLM-based static analysis
method for OAS generation, requiring fewer technology-specific rules and less
human expert intervention. OOPS is implemented as an LLM agent workflow
comprising two key steps: endpoint method extraction and OAS generation. By
constructing an API dependency graph, it establishes necessary file
associations to address LLMs' context limitations. Through multi-stage
generation and self-refine, it mitigates both syntactic and semantic
hallucinations during OAS generation. We evaluated OOPS on 12 real-world REST
APIs spanning 5 programming languages and 8 development frameworks.
Experimental results demonstrate that OOPS accurately generates high-quality
OAS for REST APIs implemented with diverse technologies, achieving an average
F1-score exceeding 98% for endpoint method inference, 97% for both request
parameter and response inference, and 92% for parameter constraint inference.
The input tokens average below 5.6K with a maximum of 16.2K, while the output
tokens average below 0.9K with a maximum of 7.7K.
\\ ( https://arxiv.org/abs/2601.12735 ,  205kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12762
Date: Mon, 19 Jan 2026 06:46:33 GMT   (526kb)

Title: Teaching LLMs to Learn Tool Trialing and Execution through Environment
  Interaction
Authors: Xingjie Gao, Pengcheng Huang, Zhenghao Liu, Yukun Yan, Shuo Wang,
  Zulong Chen, Chen Qian, Ge Yu, Yu Gu
Categories: cs.SE cs.AI
\\
  Equipping Large Language Models (LLMs) with external tools enables them to
solve complex real-world problems. However, the robustness of existing methods
remains a critical challenge when confronting novel or evolving tools. Existing
trajectory-centric paradigms primarily rely on memorizing static solution paths
during training, which limits the ability of LLMs to generalize tool usage to
newly introduced or previously unseen tools. In this paper, we propose
ToolMaster, a framework that shifts tool use from imitating golden tool-calling
trajectories to actively learning tool usage through interaction with the
environment. To optimize LLMs for tool planning and invocation, ToolMaster
adopts a trial-and-execution paradigm, which trains LLMs to first imitate
teacher-generated trajectories containing explicit tool trials and
self-correction, followed by reinforcement learning to coordinate the trial and
execution phases jointly. This process enables agents to autonomously explore
correct tool usage by actively interacting with environments and forming
experiential knowledge that benefits tool execution. Experimental results
demonstrate that ToolMaster significantly outperforms existing baselines in
terms of generalization and robustness across unseen or unfamiliar tools. All
code and data are available at https://github.com/NEUIR/ToolMaster.
\\ ( https://arxiv.org/abs/2601.12762 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12811
Date: Mon, 19 Jan 2026 08:16:43 GMT   (173kb)

Title: Docker Does Not Guarantee Reproducibility
Authors: Julien Malka, Stefano Zacchiroli, Th\'eo Zimmermann
Categories: cs.SE
\\
  The reproducibility of software environments is a critical concern in modern
software engineering, with ramifications ranging from the effectiveness of
collaboration workflows to software supply chain security and scientific
reproducibility. Containerization technologies like Docker address this problem
by encapsulating software environments into shareable filesystem snapshots
known as images. While Docker is frequently cited in the literature as a tool
that enables reproducibility in theory, the extent of its guarantees and
limitations in practice remains under-explored.
  In this work, we address this gap through two complementary approaches.
First, we conduct a systematic literature review to examine how Docker is
framed in scientific discourse on reproducibility and to identify documented
best practices for writing Dockerfiles enabling reproducible image building.
Then, we perform a large-scale empirical study of 5298 Docker builds collected
from GitHub workflows. By rebuilding these images and comparing the results
with their historical counterparts, we assess the real reproducibility of
Docker images and evaluate the effectiveness of the best practices identified
in the literature.
\\ ( https://arxiv.org/abs/2601.12811 ,  173kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12845
Date: Mon, 19 Jan 2026 08:56:43 GMT   (971kb)

Title: Automatic Generation of Formal Specification and Verification
  Annotations Using LLMs and Test Oracles
Authors: Jo\~ao Pascoal Faria, Emanuel Trigo, Vinicius Honorato, Rui Abreu
Categories: cs.SE
\\
  Recent verification tools aim to make formal verification more accessible to
software engineers by automating most of the verification process. However,
annotating conventional programs with the formal specification and verification
constructs (preconditions, postconditions, loop invariants, auxiliary
predicates and functions and proof helpers) required to prove their correctness
still demands significant manual effort and expertise. This paper investigates
how LLMs can automatically generate such annotations for programs written in
Dafny, a verification-aware programming language, starting from conventional
code accompanied by natural language specifications (in comments) and test
code. In experiments on 110 Dafny programs, a multimodel approach combining
Claude Opus 4.5 and GPT-5.2 generated correct annotations for 98.2% of the
programs within at most 8 repair iterations, using verifier feedback. A
logistic regression analysis shows that proof-helper annotations contribute
disproportionately to problem difficulty for current LLMs. Assertions in the
test cases served as static oracles to automatically validate the generated
pre/postconditions. We also compare generated and manual solutions and present
an extension for Visual Studio Code to incorporate automatic generation into
the IDE, with encouraging usability feedback.
\\ ( https://arxiv.org/abs/2601.12845 ,  971kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12890
Date: Mon, 19 Jan 2026 09:42:00 GMT   (3851kb)

Title: Efficient Code Analysis via Graph-Guided Large Language Models
Authors: Hang Gao, Tao Peng, Baoquan Cui, Hong Huang, Fengge Wu, Junsuo Zhao,
  Jian Zhang
Categories: cs.SE
\\
  Malicious behavior is often hidden in small, easily overlooked code
fragments, especially within large and complex codebases. The cross-file
dependencies of these fragments make it difficult for even powerful large
language models (LLMs) to detect them reliably. We propose a graph-centric
attention acquisition pipeline that enhances LLMs' ability to localize
malicious behavior. The approach parses a project into a code graph, uses an
LLM to encode nodes with semantic and structural signals, and trains a Graph
Neural Network (GNN) under sparse supervision. The GNN performs an initial
detection, and through backtracking of its predictions, identifies key code
sections that are most likely to contain malicious behavior. These influential
regions are then used to guide the LLM's attention for in-depth analysis. This
strategy significantly reduces interference from irrelevant context while
maintaining low annotation costs. Extensive experiments show that the method
consistently outperforms existing methods on multiple public and self-built
datasets, highlighting its potential for practical deployment in software
security scenarios.
\\ ( https://arxiv.org/abs/2601.12890 ,  3851kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12927
Date: Mon, 19 Jan 2026 10:30:46 GMT   (372kb)

Title: A Benchmark for Language Models in Real-World System Building
Authors: Weilin Jin, Chenyu Zhao, Zeshun Huang, Chaoyun Zhang, Qingwei Lin,
  Chetan Bansal, Saravan Rajmohan, Shenglin Zhang, Yongqian Sun, Dan Pei, Yifan
  Wu, Tong Jia, Ying Li, Zhonghai Wu, Minghua Ma
Categories: cs.SE
\\
  During migration across instruction set architectures (ISAs), software
package build repair is a critical task for ensuring the reliability of
software deployment and the stability of modern operating systems. While Large
Language Models (LLMs) have shown promise in tackling this challenge, prior
work has primarily focused on single instruction set architecture (ISA) and
homogeneous programming languages. To address this limitation, we introduce a
new benchmark designed for software package build repair across diverse
architectures and languages. Comprising 268 real-world software package build
failures, the benchmark provides a standardized evaluation pipeline. We
evaluate six state-of-the-art LLMs on the benchmark, and the results show that
cross-ISA software package repair remains difficult and requires further
advances. By systematically exposing this challenge, the benchmark establishes
a foundation for advancing future methods aimed at improving software
portability and bridging architectural gaps.
\\ ( https://arxiv.org/abs/2601.12927 ,  372kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12951
Date: Mon, 19 Jan 2026 10:58:24 GMT   (37kb)

Title: Beyond Accuracy: Characterizing Code Comprehension Capabilities in
  (Large) Language Models
Authors: Felix M\"achtle, Jan-Niclas Serr, Nils Loose, Thomas Eisenbarth
Categories: cs.SE cs.AI
Comments: Published in the Proceedings of DeepTest 2026
\\
  Large Language Models (LLMs) are increasingly integrated into software
engineering workflows, yet current benchmarks provide only coarse performance
summaries that obscure the diverse capabilities and limitations of these
models. This paper investigates whether LLMs' code-comprehension performance
aligns with traditional human-centric software metrics or instead reflects
distinct, non-human regularities. We introduce a diagnostic framework that
reframes code understanding as a binary input-output consistency task, enabling
the evaluation of classification and generative models. Using a large-scale
dataset, we correlate model performance with traditional, human-centric
complexity metrics, such as lexical size, control-flow complexity, and abstract
syntax tree structure. Our analyses reveal minimal correlation between
human-defined metrics and LLM success (AUROC 0.63), while shadow models achieve
substantially higher predictive performance (AUROC 0.86), capturing complex,
partially predictable patterns beyond traditional software measures. These
findings suggest that LLM comprehension reflects model-specific regularities
only partially accessible through either human-designed or learned features,
emphasizing the need for benchmark methodologies that move beyond aggregate
accuracy and toward instance-level diagnostics, while acknowledging fundamental
limits in predicting correct outcomes.
\\ ( https://arxiv.org/abs/2601.12951 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13007
Date: Mon, 19 Jan 2026 12:39:05 GMT   (1364kb)

Title: ArchAgent: Scalable Legacy Software Architecture Recovery with LLMs
Authors: Rusheng Pan, Bingcheng Mao, Tianyi Ma, Zhenhua Ling
Categories: cs.SE cs.AI
Comments: to be published in ICASSP 2026
\\
  Recovering accurate architecture from large-scale legacy software is hindered
by architectural drift, missing relations, and the limited context of Large
Language Models (LLMs). We present ArchAgent, a scalable agent-based framework
that combines static analysis, adaptive code segmentation, and LLM-powered
synthesis to reconstruct multiview, business-aligned architectures from
cross-repository codebases. ArchAgent introduces scalable diagram generation
with contextual pruning and integrates cross-repository data to identify
business-critical modules. Evaluations of typical large-scale GitHub projects
show significant improvements over existing benchmarks. An ablation study
confirms that dependency context improves the accuracy of generated
architectures of production-level repositories, and a real-world case study
demonstrates effective recovery of critical business logics from legacy
projects. The dataset is available at
https://github.com/panrusheng/arch-eval-benchmark.
\\ ( https://arxiv.org/abs/2601.13007 ,  1364kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13015
Date: Mon, 19 Jan 2026 12:49:39 GMT   (482kb)

Title: MeltRTL: Multi-Expert LLMs with Inference-time Intervention for RTL Code
  Generation
Authors: Nowfel Mashnoor, Mohammad Akyash, Hadi Kamali, Kimia Azar
Categories: cs.SE
\\
  The automated generation of hardware register-transfer level (RTL) code with
large language models (LLMs) shows promise, yet current solutions struggle to
produce syntactically and functionally correct code for complex digital
designs. This paper introduces MeltRTL, a novel framework that integrates
multi-expert attention with inference-time intervention (ITI) to significantly
improve LLM-based RTL code generation accuracy without retraining the base
model. MeltRTL introduces three key innovations: (1) A multi-expert attention
architecture that dynamically routes design specifications to specialized
expert networks, enabling targeted reasoning across various hardware
categories; (2) An inference-time intervention mechanism that employs
non-linear probes to detect and correct hardware-specific inaccuracies during
generation; and (3) An efficient intervention framework that selectively
operates on expert-specific attention heads with minimal computational
overhead. We evaluate MeltRTL on the VerilogEval benchmark, achieving 96%
synthesizability and 60% functional correctness, compared to the base LLM's
85.3% and 45.3%, respectively. These improvements are obtained entirely at
inference time, with only 27% computational overhead and no model fine-tuning,
making MeltRTL immediately deployable on existing pre-trained LLMs. Ablation
studies further show the complementary benefits of multi-expert architecture
and ITI, highlighting their synergistic effects when combined.
\\ ( https://arxiv.org/abs/2601.13015 ,  482kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13097
Date: Mon, 19 Jan 2026 14:37:50 GMT   (118kb)

Title: RM -RF: Reward Model for Run-Free Unit Test Evaluation
Authors: Elena Bruches, Daniil Grebenkin, Mikhail Klementev, Vadim Alperovich,
  Roman Derunets, Dari Baturova, Georgy Mkrtchyan, Oleg Sedukhin, Ivan
  Bondarenko, Nikolay Bushkov and Stanislav Moiseev
Categories: cs.SE cs.LG
Comments: This paper has been accepted for publication at the 33rd IEEE
  International Conference on Software Analysis, Evolution and Reengineering
  (SANER 2026)
\\
  We present RM-RF, a lightweight reward model for run-free evaluation of
automatically generated unit tests. Instead of repeatedly compiling and
executing candidate tests, RM-RF predicts - from source and test code alone -
three execution-derived signals: (1) whether the augmented test suite compiles
and runs successfully, (2) whether the generated test cases increase code
coverage, and (3) whether the generated test cases improve the mutation kill
rate. To train and evaluate RM-RF we assemble a multilingual dataset (Java,
Python, Go) of focal files, test files, and candidate test additions labeled by
an execution-based pipeline, and we release an associated dataset and
methodology for comparative evaluation. We tested multiple model families and
tuning regimes (zero-shot, full fine-tuning, and PEFT via LoRA), achieving an
average F1 of 0.69 across the three targets. Compared to conventional
compile-and-run instruments, RM-RF provides substantially lower latency and
infrastructure cost while delivering competitive predictive fidelity, enabling
fast, scalable feedback for large-scale test generation and RL-based code
optimization.
\\ ( https://arxiv.org/abs/2601.13097 ,  118kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13118
Date: Mon, 19 Jan 2026 15:01:42 GMT   (590kb)

Title: Guidelines to Prompt Large Language Models for Code Generation: An
  Empirical Characterization
Authors: Alessandro Midolo, Alessandro Giagnorio, Fiorella Zampetti, Rosalia
  Tufano, Gabriele Bavota, Massimiliano Di Penta
Categories: cs.SE
\\
  Large Language Models (LLMs) are nowadays extensively used for various types
of software engineering tasks, primarily code generation. Previous research has
shown how suitable prompt engineering could help developers in improving their
code generation prompts. However, so far, there do not exist specific
guidelines driving developers towards writing suitable prompts for code
generation. In this work, we derive and evaluate development-specific prompt
optimization guidelines. First, we use an iterative, test-driven approach to
automatically refine code generation prompts, and we analyze the outcome of
this process to identify prompt improvement items that lead to test passes. We
use such elements to elicit 10 guidelines for prompt improvement, related to
better specifying I/O, pre-post conditions, providing examples, various types
of details, or clarifying ambiguities. We conduct an assessment with 50
practitioners, who report their usage of the elicited prompt improvement
patterns, as well as their perceived usefulness, which does not always
correspond to the actual usage before knowing our guidelines. Our results lead
to implications not only for practitioners and educators, but also for those
aimed at creating better LLM-aided software development tools.
\\ ( https://arxiv.org/abs/2601.13118 ,  590kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13134
Date: Mon, 19 Jan 2026 15:20:18 GMT   (25kb)

Title: Earth Embeddings as Products: Taxonomy, Ecosystem, and Standardized
  Access
Authors: Heng Fang, Adam J. Stewart, Isaac Corley, Xiao Xiang Zhu, Hossein
  Azizpour
Categories: cs.SE cs.CV
\\
  Geospatial Foundation Models (GFMs) provide powerful representations, but
high compute costs hinder their widespread use. Pre-computed embedding data
products offer a practical "frozen" alternative, yet they currently exist in a
fragmented ecosystem of incompatible formats and resolutions. This lack of
standardization creates an engineering bottleneck that prevents meaningful
model comparison and reproducibility. We formalize this landscape through a
three-layer taxonomy: Data, Tools, and Value. We survey existing products to
identify interoperability barriers. To bridge this gap, we extend TorchGeo with
a unified API that standardizes the loading and querying of diverse embedding
products. By treating embeddings as first-class geospatial datasets, we
decouple downstream analysis from model-specific engineering, providing a
roadmap for more transparent and accessible Earth observation workflows.
\\ ( https://arxiv.org/abs/2601.13134 ,  25kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13139
Date: Mon, 19 Jan 2026 15:22:37 GMT   (544kb)

Title: From Human to Machine Refactoring: Assessing GPT-4's Impact on Python
  Class Quality and Readability
Authors: Alessandro Midolo, Emiliano Tramontana, Massimiliano Di Penta
Categories: cs.SE
\\
  Refactoring is a software engineering practice that aims to improve code
quality without altering program behavior. Although automated refactoring tools
have been extensively studied, their practical applicability remains limited.
Recent advances in Large Language Models (LLMs) have introduced new
opportunities for automated code refactoring. The evaluation of such an
LLM-driven approach, however, leaves unanswered questions about its effects on
code quality. In this paper, we present a comprehensive empirical study on
LLM-driven refactoring using GPT-4o, applied to 100 Python classes from the
ClassEval benchmark. Unlike prior work, our study explores a wide range of
class-level refactorings inspired by Fowler's catalog and evaluates their
effects from three complementary perspectives: (i) behavioral correctness,
verified through unit tests; (ii) code quality, assessed via Pylint, Flake8,
and SonarCloud; and (iii) readability, measured using a state-of-the-art
readability tool. Our findings show that GPT-4o generally produces
behavior-preserving refactorings that reduce code smells and improve quality
metrics, albeit at the cost of decreased readability. Our results provide new
evidence on the capabilities and limitations of LLMs in automated software
refactoring, highlighting directions for integrating LLMs into practical
refactoring workflows.
\\ ( https://arxiv.org/abs/2601.13139 ,  544kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13240
Date: Mon, 19 Jan 2026 17:20:16 GMT   (818kb)

Title: KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in
  Software Development?
Authors: Xue Jiang, Jiaru Qian, Xianjie Shi, Chenjie Li, Hao Zhu, Ziyu Wang,
  Jielun Zhang, Zheyu Zhao, Kechi Zhang, Jia Li, Wenpin Jiao, Zhi Jin, Ge Li,
  Yihong Dong
Categories: cs.SE cs.AI cs.CL cs.LG
\\
  Large language models (LLMs) excel at general programming but struggle with
domain-specific software development, necessitating domain specialization
methods for LLMs to learn and utilize domain knowledge and data. However,
existing domain-specific code benchmarks cannot evaluate the effectiveness of
domain specialization methods, which focus on assessing what knowledge LLMs
possess rather than how they acquire and apply new knowledge, lacking explicit
knowledge corpora for developing domain specialization methods. To this end, we
present KOCO-BENCH, a novel benchmark designed for evaluating domain
specialization methods in real-world software development. KOCO-BENCH contains
6 emerging domains with 11 software frameworks and 25 projects, featuring
curated knowledge corpora alongside multi-granularity evaluation tasks
including domain code generation (from function-level to project-level with
rigorous test suites) and domain knowledge understanding (via multiple-choice
Q&A). Unlike previous benchmarks that only provide test sets for direct
evaluation, KOCO-BENCH requires acquiring and applying diverse domain knowledge
(APIs, rules, constraints, etc.) from knowledge corpora to solve evaluation
tasks. Our evaluations reveal that KOCO-BENCH poses significant challenges to
state-of-the-art LLMs. Even with domain specialization methods (e.g., SFT, RAG,
kNN-LM) applied, improvements remain marginal. Best-performing coding agent,
Claude Code, achieves only 34.2%, highlighting the urgent need for more
effective domain specialization methods. We release KOCO-BENCH, evaluation
code, and baselines to advance further research at
https://github.com/jiangxxxue/KOCO-bench.
\\ ( https://arxiv.org/abs/2601.13240 ,  818kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13334
Date: Mon, 19 Jan 2026 19:13:40 GMT   (1419kb)

Title: SEER: Spectral Entropy Encoding of Roles for Context-Aware
  Attention-Based Design Pattern Detection
Authors: Tarik Houichime, Younes El Amrani
Categories: cs.SE
\\
  This paper presents SEER, an upgraded version of our prior method Context Is
All You Need for detecting Gang of Four (GoF) design patterns from source code.
The earlier approach modeled code as attention-ready sequences that blended
lightweight structure with behavioral context; however, it lacked explicit role
disambiguation within classes and treated call edges uniformly. SEER addresses
these limitations with two principled additions: (i) a spectral-entropy role
encoder that derives per-member role embeddings from the Laplacian spectrum of
each class's interaction graph, and (ii) a time-weighted calling context that
assigns empirically calibrated duration priors to method categories (e.g.,
constructors, getters/setters, static calls, virtual dispatch, cloning).
Together, these components sharpen the model's notion of "who does what" and
"how much it matters," while remaining portable across languages with minimal
adaptation and fully compatible with Transformer-based sequence encoders.
Importantly, SEER does not "force" a win by capacity or data; it nudges the
classifier, steering attention toward role-consistent and temporally calibrated
signals that matter most. We evaluate SEER on PyDesignNet (1,832 files, 35,000
sequences, 23 GoF patterns) and observe consistent gains over our previous
system: macro-F1 increases from 92.47% to 93.20% and accuracy from 92.52% to
93.98%, with macro-precision 93.98% and macro-recall 92.52%. Beyond aggregate
metrics, SEER reduces false positives by nearly 20%, a decisive improvement
that strengthens its robustness and practical reliability. Moreover, SEER
yields interpretable, symbol-level attributions aligned with canonical roles,
exhibits robustness under small graph perturbations, and shows stable
calibration.
\\ ( https://arxiv.org/abs/2601.13334 ,  1419kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13345
Date: Mon, 19 Jan 2026 19:30:25 GMT   (1017kb)

Title: FlipFlop: A Static Analysis-based Energy Optimization Framework for GPU
  Kernels
Authors: Saurabhsingh Rajput, Alexander Brandt, Vadim Elisseev, Tushar Sharma
Categories: cs.SE cs.PF
\\
  Artificial Intelligence (AI) applications, such as Large Language Models, are
primarily driven and executed by Graphics Processing Units (GPUs). These GPU
programs (kernels) consume substantial amounts of energy, yet software
developers often lack the hardware expertise and ad hoc knowledge required to
optimize for power efficiency. We propose FlipFlop, a framework using static
code analysis to predict energy consumption and recommend Pareto-optimal thread
block configurations considering both power consumption and execution time. Our
framework requires no runtime execution and analyzes PTX code, a low-level
instruction set for CUDA-enabled GPUs. It is validated across a diverse set of
GPUs and kernels, including multi-head attention, convolution, and matrix
multiplication. FlipFlop achieves 83% accuracy in identifying locally optimal
energy-efficient configurations, while also minimizing developer effort by
reducing the optimization search space by 93.4%. For multi-head attention
kernels, it yields up to 79% energy savings and 106% throughput gains relative
to NVIDIA's occupancy heuristic. By integrating static analysis with real-time
monitoring and providing explainable optimization guidance, FlipFlop empowers
developers to create sustainable, high-performance GPU software which minimizes
environmental and computational costs.
\\ ( https://arxiv.org/abs/2601.13345 ,  1017kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13384
Date: Mon, 19 Jan 2026 20:33:53 GMT   (662kb)

Title: From Completion to Editing: Unlocking Context-Aware Code Infilling via
  Search-and-Replace Instruction Tuning
Authors: Jiajun Zhang, Zeyu Cui, Jiaxi Yang, Lei Zhang, Yuheng Jing, Zeyao Ma,
  Tianyi Bai, Zilei Wang, Qiang Liu, Liang Wang, Binyuan Hui, Junyang Lin
Categories: cs.SE cs.CL
\\
  The dominant Fill-in-the-Middle (FIM) paradigm for code completion is
constrained by its rigid inability to correct contextual errors and reliance on
unaligned, insecure Base models. While Chat LLMs offer safety and Agentic
workflows provide flexibility, they suffer from performance degradation and
prohibitive latency, respectively. To resolve this dilemma, we propose
Search-and-Replace Infilling (SRI), a framework that internalizes the agentic
verification-and-editing mechanism into a unified, single-pass inference
process. By structurally grounding edits via an explicit search phase, SRI
harmonizes completion tasks with the instruction-following priors of Chat LLMs,
extending the paradigm from static infilling to dynamic context-aware editing.
We synthesize a high-quality dataset, SRI-200K, and fine-tune the SRI-Coder
series. Extensive evaluations demonstrate that with minimal data (20k samples),
SRI-Coder enables Chat models to surpass the completion performance of their
Base counterparts. Crucially, unlike FIM-style tuning, SRI preserves general
coding competencies and maintains inference latency comparable to standard FIM.
We empower the entire Qwen3-Coder series with SRI, encouraging the developer
community to leverage this framework for advanced auto-completion and assisted
development.
\\ ( https://arxiv.org/abs/2601.13384 ,  662kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13460
Date: Mon, 19 Jan 2026 23:34:43 GMT   (469kb)

Title: A Tool for Automatically Cataloguing and Selecting Pre-Trained Models
  and Datasets for Software Engineering
Authors: Alexandra Gonz\'alez, Oscar Cerezo, Xavier Franch, Silverio
  Mart\'inez-Fern\'andez
Categories: cs.SE
\\
  The rapid growth of machine learning assets has made it increasingly
difficult for software engineers to identify models and datasets that match
their specific needs. Browsing large registries, such as Hugging Face, is
time-consuming, error-prone, and rarely tailored to Software Engineering (SE)
tasks. We present MLAssetSelection, a web application that automatically
extracts SE assets and supports four key functionalities: (i) a configurable
leaderboard for ranking models across multiple benchmarks and metrics; (ii)
requirements-based selection of models and datasets; (iii) real-time automated
updates through scheduled jobs that keep asset information current; and (iv)
user-centric features including login, personalized asset lists, and
configurable alert notifications. A demonstration video is available at
https://youtu.be/t6CJ6P9asV4.
\\ ( https://arxiv.org/abs/2601.13460 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13466
Date: Mon, 19 Jan 2026 23:41:18 GMT   (1393kb)

Title: Governance Matters: Lessons from Restructuring the data.table OSS
  Project
Authors: Pedro Oliveira, Doris Amoakohene, Toby Hocking, Marco Gerosa, Igor
  Steinmacher
Categories: cs.SE
Journal-ref: Pedro Oliveira, Doris Amoakohene, Toby Hocking, Marco Gerosa and
  Igor Steinmacher, "Governance Matters: Lessons From Restructuring the
  Data.Table OSS Project," ICSME 2025
DOI: 10.1109/ICSME64153.2025.00067
\\
  Open source software (OSS) forms the backbone of industrial data workflows
and enterprise systems. However, many OSS projects face operational risks due
to informal or centralized governance. This paper presents a practical case
study of data.table, a high-performance R package widely adopted in production
analytics pipelines, which underwent a community-led governance reform to
address scalability and sustainability concerns. Before the reform, data.table
faced a growing backlog of unresolved issues and open pull requests, unclear
contributor pathways, and bottlenecks caused by reliance on a single core
maintainer. In response, the community initiated a redesign of its governance
structure. In this paper, we evaluated the impact of this transition through a
mixed-methods approach, combining a contributor survey (n=17) with mining
project repository data. Our results show that following the reform, the
project experienced a 200% increase in new contributor recruitment, a drop in
pull request resolution time from over 700 days to under a week, and a 3x
increase in contributor retention. Community sentiment improved around
transparency, onboarding, and project momentum, though concerns around fairness
and conflict resolution remain. This case study provides practical guidance for
maintainers, companies, and foundations seeking to enhance OSS governance.
\\ ( https://arxiv.org/abs/2601.13466 ,  1393kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13597
Date: Tue, 20 Jan 2026 04:51:56 GMT   (72kb)

Title: AI IDEs or Autonomous Agents? Measuring the Impact of Coding Agents on
  Software Development
Authors: Shyam Agarwal, Hao He, Bogdan Vasilescu
Categories: cs.SE
\\
  Large language model (LLM)-based coding agents increasingly act as autonomous
contributors that generate and merge pull requests, yet their real-world
effects on software projects are unclear, especially relative to widely adopted
IDE-based AI assistants. We present a longitudinal causal study of agent
adoption in open-source repositories using staggered difference-in-differences
with matched controls. Using the AIDev dataset, we define adoption as the first
agent-generated pull request and analyze monthly repository-level outcomes
spanning development velocity (commits, lines added) and software quality
(static-analysis warnings, cognitive complexity, duplication, and comment
density). Results show large, front-loaded velocity gains only when agents are
the first observable AI tool in a project; repositories with prior AI IDE usage
experience minimal or short-lived throughput benefits. In contrast, quality
risks are persistent across settings, with static-analysis warnings and
cognitive complexity rising roughly 18% and 35%, indicating sustained
agent-induced complexity debt even when velocity advantages fade. These
heterogeneous effects suggest diminishing returns to AI assistance and
highlight the need for quality safeguards, provenance tracking, and selective
deployment of autonomous agents. Our findings establish an empirical basis for
understanding how agentic and IDE-based tools interact, and motivate research
on balancing acceleration with maintainability in AI-integrated development
workflows.
\\ ( https://arxiv.org/abs/2601.13597 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13655
Date: Tue, 20 Jan 2026 06:42:56 GMT   (990kb)

Title: Why Does the LLM Stop Computing: An Empirical Study of User-Reported
  Failures in Open-Source LLMs
Authors: Guangba Yu, Zirui Wang, Yujie Huang, Renyi Zhong, Yuedong Zhong, Yilun
  Wang, Michael R. Lyu
Categories: cs.SE cs.AI cs.DC
\\
  The democratization of open-source Large Language Models (LLMs) allows users
to fine-tune and deploy models on local infrastructure but exposes them to a
First Mile deployment landscape. Unlike black-box API consumption, the
reliability of user-managed orchestration remains a critical blind spot. To
bridge this gap, we conduct the first large-scale empirical study of 705
real-world failures from the open-source DeepSeek, Llama, and Qwen ecosystems.
  Our analysis reveals a paradigm shift: white-box orchestration relocates the
reliability bottleneck from model algorithmic defects to the systemic fragility
of the deployment stack. We identify three key phenomena: (1) Diagnostic
Divergence: runtime crashes distinctively signal infrastructure friction,
whereas incorrect functionality serves as a signature for internal tokenizer
defects. (2) Systemic Homogeneity: Root causes converge across divergent
series, confirming reliability barriers are inherent to the shared ecosystem
rather than specific architectures. (3) Lifecycle Escalation: Barriers escalate
from intrinsic configuration struggles during fine-tuning to compounded
environmental incompatibilities during inference. Supported by our publicly
available dataset, these insights provide actionable guidance for enhancing the
reliability of the LLM landscape.
\\ ( https://arxiv.org/abs/2601.13655 ,  990kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13682
Date: Tue, 20 Jan 2026 07:32:44 GMT   (8009kb)

Title: CodeContests-O: Powering LLMs via Feedback-Driven Iterative Test Case
  Generation
Authors: Jianfeng Cai, Jinhua Zhu, Ruopei Sun, Kangwen Zhao, Dongyun Xue,
  Mingxiao Feng, Wengang Zhou, Houqiang Li
Categories: cs.SE cs.PL
\\
  The rise of reasoning models necessitates large-scale verifiable data, for
which programming tasks serve as an ideal source. However, while competitive
programming platforms provide abundant problems and solutions, high-quality
test cases for verification remain scarce. Existing approaches attempt to
synthesize test cases using Large Language Models (LLMs), but rely solely on
the model's intrinsic generation capabilities without external feedback,
frequently resulting in insufficiently diverse cases. To address this
limitation, we propose a $\textbf{Feedback-Driven Iterative Framework}$ for
comprehensive test case construction. Specifically, our method leverages the
LLM to generate initial test cases, executes them against known correct and
incorrect solutions, and utilizes the failed results as feedback to guide the
LLM in refining the test cases toward high fidelity and discriminability. We
then apply this method to the CodeContests dataset to construct an optimized
high-quality derivative, $\textbf{CodeContests-O}$. Evaluating against the
entire pool of solutions ($1.1 \times 10^7$ in total), our dataset achieves an
average True Positive Rate (TPR) of $89.37\%$ and True Negative Rate (TNR) of
$90.89\%$, significantly outperforming the CodeContests and CodeContests+ by
margins of $4.32\%$ and $9.37\%$, respectively. Furthermore, fine-tuning the
Qwen2.5-7B model on CodeContests-O results in a $9.52\%$ improvement on
LiveCodeBench (Pass@1). Experiments demonstrate the effectiveness of our
framework and the quality of CodeContests-O. To support reproducibility and
facilitate future research, we release the
$\href{https://github.com/cai-jianfeng/CodeContests-O}{code}$ and
$\href{https://huggingface.co/datasets/caijanfeng/CodeContests-O}{dataset}$.
\\ ( https://arxiv.org/abs/2601.13682 ,  8009kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13713
Date: Tue, 20 Jan 2026 08:10:56 GMT   (464kb)

Title: SWE-Tester: Training Open-Source LLMs for Issue Reproduction in
  Real-World Repositories
Authors: Aditya Bharat Soni, Rajat Ghosh, Vaishnavi Bhargava, Valerie Chen,
  Debojyoti Dutta
Categories: cs.SE cs.LG
\\
  Software testing is crucial for ensuring the correctness and reliability of
software systems. Automated generation of issue reproduction tests from natural
language issue descriptions enhances developer productivity by simplifying root
cause analysis, promotes test-driven development -- "test first, write code
later", and can be used for improving the effectiveness of automated issue
resolution systems like coding agents. Existing methods proposed for this task
predominantly rely on closed-source LLMs, with limited exploration of open
models. To address this, we propose SWE-Tester -- a novel pipeline for training
open-source LLMs to generate issue reproduction tests. First, we curate a
high-quality training dataset of 41K instances from 2.6K open-source GitHub
repositories and use it to train LLMs of varying sizes and families. The
fine-tuned models achieve absolute improvements of up to 10\% in success rate
and 21\% in change coverage on SWT-Bench Verified. Further analysis shows
consistent improvements with increased inference-time compute, more data, and
larger models. These results highlight the effectiveness of our framework for
advancing open-source LLMs in this domain.
\\ ( https://arxiv.org/abs/2601.13713 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13743
Date: Tue, 20 Jan 2026 08:57:20 GMT   (994kb)

Title: Counterexample Classification against Signal Temporal Logic
  Specifications
Authors: Zhenya Zhang, Parv Kapoor, Jie An, Eunsuk Kang
Categories: cs.SE
\\
  Signal Temporal Logic (STL) has been widely adopted as a specification
language for specifying desirable behaviors of hybrid systems. By monitoring a
given STL specification, we can detect the executions that violate it, which
are often referred to as counterexamples. In practice, these counterexamples
may arise from different causes and thus are relevant to different system
defects. To effectively address this, we need a proper criterion for
classifying these counterexamples, by which we can comprehend the possible
violation patterns and the distributions of these counterexamples with respect
to the patterns. In this paper, we propose a classification criterion by using
parametric signal temporal logic (PSTL) to represent each class. Due to this
formalism, identifying the classes of a counterexample requires finding proper
parameter values of PSTL that enable a class to include the counterexample. To
improve the efficiency of class identification, we further derive an inclusion
relation between different classes, and then propose a binary search-like
approach over it that significantly prunes the classes needed to query. We
implement a prototype tool and experimentally evaluate its effectiveness on two
widely-studied systems.
\\ ( https://arxiv.org/abs/2601.13743 ,  994kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13754
Date: Tue, 20 Jan 2026 09:09:53 GMT   (59kb)

Title: On Autopilot? An Empirical Study of Human-AI Teaming and Review
  Practices in Open Source
Authors: Haoyu Gao, Peerachai Banyongrakkul, Hao Guan, Mansooreh Zahedi,
  Christoph Treude
Categories: cs.SE
Comments: accepted as MSR short paper
\\
  Large Language Models (LLMs) increasingly automate software engineering
tasks. While recent studies highlight the accelerated adoption of ``AI as a
teammate'' in Open Source Software (OSS), developer interaction patterns remain
under-explored. In this work, we investigated project-level guidelines and
developers' interactions with AI-assisted pull requests (PRs) by expanding the
AIDev dataset to include finer-grained contributor code ownership and a
comparative baseline of human-created PRs. We found that over 67.5\% of
AI-co-authored PRs originate from contributors without prior code ownership.
Despite this, the majority of repositories lack guidelines for AI-coding agent
usage. Notably, we observed a distinct interaction pattern: AI-co-authored PRs
are merged significantly faster with minimal feedback. In contrast to
human-created PRs where non-owner developers receive the most feedback,
AI-co-authored PRs from non-owners receive the least, with approximately 80\%
merged without any explicit review. Finally, we discuss implications for
developers and researchers.
\\ ( https://arxiv.org/abs/2601.13754 ,  59kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13772
Date: Tue, 20 Jan 2026 09:31:14 GMT   (285kb)

Title: A Blockchain-Oriented Software Engineering Architecture for Carbon
  Credit Certification Systems
Authors: Matteo Vaccargiu, Azmat Ullah, Pierluigi Gallo
Categories: cs.SE cs.DC cs.SI cs.SY eess.SY
Comments: 2026 IEEE International Conference on Software Analysis, Evolution
  and Reengineering - Companion (SANER-C) 9th International Workshop on
  Blockchain Oriented Software Engineering March 17-20, 2026 Limassol, Cyprus
\\
  Carbon credit systems have emerged as a policy tool to incentivize emission
reductions and support the transition to clean energy. Reliable carbon-credit
certification depends on mechanisms that connect actual, measured
renewable-energy production to verifiable emission-reduction records. Although
blockchain and IoT technologies have been applied to emission monitoring and
trading, existing work offers limited support for certification processes,
particularly for small and medium-scale renewable installations. This paper
introduces a blockchain-based carbon-credit certification architecture,
demonstrated through a 100 kWp photovoltaic case study, that integrates
real-time IoT data collection, edge-level aggregation, and secure on-chain
storage on a permissioned blockchain with smart contracts. Unlike approaches
focused on trading mechanisms, the proposed system aligns with European
legislation and voluntary carbon-market standards, clarifying the practical
requirements and constraints that apply to photovoltaic operators. The
resulting architecture provides a structured pathway for generating verifiable
carbon-credit records and supporting third-party verification.
\\ ( https://arxiv.org/abs/2601.13772 ,  285kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13894
Date: Tue, 20 Jan 2026 12:19:34 GMT   (1578kb)

Title: Multi-Location Software Model Completion
Authors: Alisa Welter, Christof Tinnes, Sven Apel
Categories: cs.SE
Comments: Accepted at the 48th IEEE/ACM International Conference on Software
  Engineering (ICSE 2026) - Research Track
\\
  In model-driven engineering and beyond, software models are key development
artifacts. In practice, they often grow to substantial size and complexity,
undergoing thousands of modifications over time due to evolution, refactoring,
and maintenance. The rise of AI has sparked interest in how software modeling
activities can be automated. Recently, LLM-based approaches for software model
completion have been proposed, however, the state of the art supports only
single-location model completion by predicting changes at a specific location.
Going beyond, we aim to bridge the gap toward handling coordinated changes that
span multiple locations across large, complex models. Specifically, we propose
a novel global embedding-based next focus predictor, NextFocus, which is
capable of multi-location model completion for the first time. The predictor
consists of a neural network with an attention mechanism that is trained on
historical software model evolution data. Starting from an existing change, it
predicts further model elements to change, potentially spanning multiple parts
of the model. We evaluate our approach on multi-location model changes that
have actually been performed by developers in real-world projects. NextFocus
achieves promising results for multi-location model completion, even when
changes are heavily spread across the model. It achieves an average Precision@k
score of 0.98 for $k \leq 10$, significantly outperforming the three baseline
approaches.
\\ ( https://arxiv.org/abs/2601.13894 ,  1578kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13933
Date: Tue, 20 Jan 2026 13:09:16 GMT   (379kb)

Title: VulnResolver: A Hybrid Agent Framework for LLM-Based Automated
  Vulnerability Issue Resolution
Authors: Mingming Zhang, Xu Wang, Jian Zhang, Xiangxin Meng, Jiayi Zhang,
  Chunming Hu
Categories: cs.SE
\\
  As software systems grow in complexity, security vulnerabilities have become
increasingly prevalent, posing serious risks and economic costs. Although
automated detection tools such as fuzzers have advanced considerably, effective
resolution still often depends on human expertise. Existing automated
vulnerability repair (AVR) methods rely heavily on manually provided
annotations (e.g., fault locations or CWE labels), which are often difficult
and time-consuming to obtain, while overlooking the rich, naturally embedded
semantic context found in issue reports from developers.
  In this paper, we present VulnResolver, the first LLM-based hybrid agent
framework for automated vulnerability issue resolution. VulnResolver unites the
adaptability of autonomous agents with the stability of workflow-guided repair
through two specialized agents. The Context Pre-Collection Agent (CPCAgent)
adaptively explores the repository to gather dependency and contextual
information, while the Safety Property Analysis Agent (SPAAgent) generates and
validates the safety properties violated by vulnerabilities. Together, these
agents produce structured analyses that enrich the original issue reports,
enabling more accurate vulnerability localization and patch generation.
  Evaluations on the SEC-bench benchmark show that VulnResolver resolves 75% of
issues on SEC-bench Lite, achieving the best resolution performance. On
SEC-bench Full, VulnResolver also significantly outperforms the strongest
baseline, the agent-based OpenHands, confirming its effectiveness. Overall,
VulnResolver delivers an adaptive and security-aware framework that advances
end-to-end automated vulnerability issue resolution through workflow stability
and the specialized agents' capabilities in contextual reasoning and
property-based analysis.
\\ ( https://arxiv.org/abs/2601.13933 ,  379kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13943
Date: Tue, 20 Jan 2026 13:19:20 GMT   (2420kb)

Title: RepoGenesis: Benchmarking End-to-End Microservice Generation from Readme
  to Repository
Authors: Zhiyuan Peng, Xin Yin, Pu Zhao, Fangkai Yang, Lu Wang, Ran Jia, Xu
  Chen, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang
Categories: cs.SE
\\
  Large language models and agents have achieved remarkable progress in code
generation. However, existing benchmarks focus on isolated function/class-level
generation (e.g., ClassEval) or modifications to existing codebases (e.g.,
SWE-Bench), neglecting complete microservice repository generation that
reflects real-world 0-to-1 development workflows. To bridge this gap, we
introduce RepoGenesis, the first multilingual benchmark for repository-level
end-to-end web microservice generation, comprising 106 repositories (60 Python,
46 Java) across 18 domains and 11 frameworks, with 1,258 API endpoints and
2,335 test cases verified through a "review-rebuttal" quality assurance
process. We evaluate open-source agents (e.g., DeepCode) and commercial IDEs
(e.g., Cursor) using Pass@1, API Coverage (AC), and Deployment Success Rate
(DSR). Results reveal that despite high AC (up to 73.91%) and DSR (up to 100%),
the best-performing system achieves only 23.67% Pass@1 on Python and 21.45% on
Java, exposing deficiencies in architectural coherence, dependency management,
and cross-file consistency. Notably, GenesisAgent-8B, fine-tuned on RepoGenesis
(train), achieves performance comparable to GPT-5 mini, demonstrating the
quality of RepoGenesis for advancing microservice generation. We release our
benchmark at https://github.com/pzy2000/RepoGenesis.
\\ ( https://arxiv.org/abs/2601.13943 ,  2420kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13996
Date: Tue, 20 Jan 2026 14:09:24 GMT   (9kb)

Title: Software Testing in the Quantum World
Authors: Rui Abreu, Shaukat Ali, Paolo Arcaini, Jose Campos, Michael Felderer,
  Claude Gravel, Fuyuki Ishikawa, Stefan Klikovits, Andriy Miranskyy, Mohammad
  Mousavi, Masaomi Yamaguchi, Lei Zhang, Jianjun Zhao, Anila Mjeda
Categories: cs.SE
\\
  Quantum computing offers significant speedups for simulating physical,
chemical, and biological systems, and for optimization and machine learning. As
quantum software grows in complexity, the classical simulation of quantum
computers, which has long been essential for quality assurance, becomes
infeasible. This shift requires new quality-assurance methods that operate
directly on real quantum computers. This paper presents the key challenges in
testing large-scale quantum software and offers software engineering
perspectives for addressing them.
\\ ( https://arxiv.org/abs/2601.13996 ,  9kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14034
Date: Tue, 20 Jan 2026 14:54:58 GMT   (85kb)

Title: Analyzing the Availability of E-Mail Addresses for PyPI Libraries
Authors: Alexandros Tsakpinis, Alexander Pretschner
Categories: cs.SE
Comments: 6 pages, 4 figures
\\
  Open Source Software (OSS) libraries form the backbone of modern software
systems, yet their long-term sustainability often depends on maintainers being
reachable for support, coordination, and security reporting. In this paper, we
empirically analyze the availability of contact information - specifically
e-mail addresses - across 686,034 Python libraries on the Python Package Index
(PyPI) and their associated GitHub repositories. We examine how and where
maintainers provide this information, assess its validity, and explore coverage
across individual libraries and their dependency chains. Our findings show that
81.6% of libraries include at least one valid e-mail address, with PyPI serving
as the primary source (79.5%). When analyzing dependency chains, we observe
that up to 97.8% of direct and 97.7% of transitive dependencies provide valid
contact information. At the same time, we identify over 698,000 invalid
entries, primarily due to missing fields. These results demonstrate strong
maintainer reachability across the ecosystem, while highlighting opportunities
for improvement - such as offering clearer guidance to maintainers during the
packaging process and introducing opt-in validation mechanisms for existing
e-mail addresses.
\\ ( https://arxiv.org/abs/2601.14034 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14081
Date: Tue, 20 Jan 2026 15:41:06 GMT   (8200kb)

Title: Feature-Aware Test Generation for Deep Learning Models
Authors: Xingcheng Chen, Oliver Weissl, Andrea Stocco
Categories: cs.SE
\\
  As deep learning models are widely used in software systems, test generation
plays a crucial role in assessing the quality of such models before deployment.
To date, the most advanced test generators rely on generative AI to synthesize
inputs; however, these approaches remain limited in providing semantic insight
into the causes of misbehaviours and in offering fine-grained semantic
controllability over the generated inputs. In this paper, we introduce Detect,
a feature-aware test generation framework for vision-based deep learning (DL)
models that systematically generates inputs by perturbing disentangled semantic
attributes within the latent space. Detect perturbs individual latent features
in a controlled way and observes how these changes affect the model's output.
Through this process, it identifies which features lead to behavior shifts and
uses a vision-language model for semantic attribution. By distinguishing
between task-relevant and irrelevant features, Detect applies feature-aware
perturbations targeted for both generalization and robustness. Empirical
results across image classification and detection tasks show that Detect
generates high-quality test cases with fine-grained control, reveals distinct
shortcut behaviors across model architectures (convolutional and
transformer-based), and bugs that are not captured by accuracy metrics.
Specifically, Detect outperforms a state-of-the-art test generator in decision
boundary discovery and a leading spurious feature localization method in
identifying robustness failures. Our findings show that fully fine-tuned
convolutional models are prone to overfitting on localized cues, such as
co-occurring visual traits, while weakly supervised transformers tend to rely
on global features, such as environmental variances. These findings highlight
the value of interpretable and feature-aware testing in improving DL model
reliability.
\\ ( https://arxiv.org/abs/2601.14081 ,  8200kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14131
Date: Tue, 20 Jan 2026 16:30:02 GMT   (80kb)

Title: Practitioner Views on Mobile App Accessibility: Practices and Challenges
Authors: Amila Indika, Rick Kazman, Anthony Peruma
Categories: cs.SE
Comments: The 48th IEEE/ACM International Conference on Software Engineering -
  Research Track
DOI: 10.1145/3744916.3787791
\\
  As mobile applications (apps) become ubiquitous in everyday life, it is
crucial for developers to prioritize accessibility for users with diverse
abilities. While previous research has identified widespread accessibility
issues and raised awareness of developer challenges, there remains a lack of
cross-platform, globally representative insights into how practitioners
approach accessibility in practice. This paper presents findings from a
mixed-methods survey of 110 mobile app developers across 43 countries,
examining how platform ecosystems (iOS vs. Android) and developer experience
shape accessibility practices. Results show that while developers recognize the
importance of accessibility, they often rely on platform-specific guidelines
and typically perform compliance testing late in the development process.
Developers primarily implement text-focused features while struggling with API
limitations and organizational constraints. Through systematic cross-platform
comparison, we identify novel platform-specific barriers and demonstrate how
accessibility practices differ across developer experience levels. Our findings
offer new insights into the challenges of achieving accessibility in practice
and provide actionable steps for various stakeholders to promote more
consistent and inclusive app development.
\\ ( https://arxiv.org/abs/2601.14131 ,  80kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14132
Date: Tue, 20 Jan 2026 16:30:05 GMT   (77kb)

Title: Toward self-coding information systems
Authors: Rodrigo Falc\~ao, Frank Elberzhager, Karthik Vaidhyanathan
Categories: cs.SE
Comments: Accepted for ICSE 2026 Track "Software Architecture BoF"
\\
  In this extended abstract, we propose a novel research topic in the field of
agentic AI, which we refer to as self-coding information systems. These systems
will be able to dynamically adapt their structure or behavior by evaluating
potential adaptation decisions, generate source code, test, and (re)deploy
their source code autonomously, at runtime, reducing the time to market of new
features. Here we motivate the topic, provide a formal definition of
self-coding information systems, discuss some expected impacts of the new
technology, and indicate potential research directions.
\\ ( https://arxiv.org/abs/2601.14132 ,  77kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14163
Date: Tue, 20 Jan 2026 17:13:42 GMT   (2447kb)

Title: An Empirical Study on Remote Code Execution in Machine Learning Model
  Hosting Ecosystems
Authors: Mohammed Latif Siddiq, Tanzim Hossain Romel, Natalie Sekerak, Beatrice
  Casey, Joanna C. S. Santos
Categories: cs.SE cs.CR
\\
  Model-sharing platforms, such as Hugging Face, ModelScope, and OpenCSG, have
become central to modern machine learning development, enabling developers to
share, load, and fine-tune pre-trained models with minimal effort. However, the
flexibility of these ecosystems introduces a critical security concern: the
execution of untrusted code during model loading (i.e., via trust_remote_code
or trust_repo). In this work, we conduct the first large-scale empirical study
of custom model loading practices across five major model-sharing platforms to
assess their prevalence, associated risks, and developer perceptions. We first
quantify the frequency with which models require custom code to function and
identify those that execute arbitrary Python files during loading. We then
apply three complementary static analysis tools: Bandit, CodeQL, and Semgrep,
to detect security smells and potential vulnerabilities, categorizing our
findings by CWE identifiers to provide a standardized risk taxonomy. We also
use YARA to identify malicious patterns and payload signatures. In parallel, we
systematically analyze the documentation, API design, and safety mechanisms of
each platform to understand their mitigation strategies and enforcement levels.
Finally, we conduct a qualitative analysis of over 600 developer discussions
from GitHub, Hugging Face, and PyTorch Hub forums, as well as Stack Overflow,
to capture community concerns and misconceptions regarding security and
usability. Our findings reveal widespread reliance on unsafe defaults, uneven
security enforcement across platforms, and persistent confusion among
developers about the implications of executing remote code. We conclude with
actionable recommendations for designing safer model-sharing infrastructures
and striking a balance between usability and security in future AI ecosystems.
\\ ( https://arxiv.org/abs/2601.14163 ,  2447kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2601.11526 (*cross-listing*)
Date: Fri, 14 Nov 2025 05:08:53 GMT   (1074kb)

Title: Chatsparent: An Interactive System for Detecting and Mitigating
  Cognitive Fatigue in LLMs
Authors: Riju Marwah, Vishal Pallagani, Ritvik Garimella, Amit Sheth
Categories: cs.HC cs.AI
Comments: Accepted to AAAI 2026 Demonstration Track
\\
  LLMs are increasingly being deployed as chatbots, but today's interfaces
offer little to no friction: users interact through seamless conversations that
conceal when the model is drifting, hallucinating or failing. This lack of
transparency fosters blind trust, even as models produce unstable or repetitive
outputs. We introduce an interactive demo that surfaces and mitigates cognitive
fatigue, a failure mode where LLMs gradually lose coherence during
auto-regressive generation. Our system, Chatsparent, instruments real-time,
token-level signals of fatigue, including attention-to-prompt decay, embedding
drift, and entropy collapse, and visualizes them as a unified fatigue index.
When fatigue thresholds are crossed, the interface allows users to activate
lightweight interventions such as attention resets, entropy-regularized
decoding, and self-reflection checkpoints. The demo streams live text and
fatigue signals, allowing users to observe when fatigue arises, how it affects
output quality, and how interventions restore stability. By turning passive
chatbot interaction into an interactive diagnostic experience, our system
empowers users to better understand LLM behavior while improving reliability at
inference time.
\\ ( https://arxiv.org/abs/2601.11526 ,  1074kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11527 (*cross-listing*)
Date: Fri, 14 Nov 2025 05:19:14 GMT   (532kb)

Title: Do LLMs Give Good Romantic Relationship Advice? A Study on User
  Satisfaction and Attitude Change
Authors: Niva Manchanda, Akshata Kishore Moharir, Isabel Michel, Ratna Kandala
Categories: cs.HC cs.AI cs.CY
Comments: Accepted at the 39th Conference on Neural Information Processing
  Systems (NeurIPS 2025) First Workshop on LLM Persona Modeling
\\
  Large Language Models (LLMs) are increasingly being used to provide support
and advice in personal domains such as romantic relationships, yet little is
known about user perceptions of this type of advice. This study investigated
how people evaluate advice on LLM-generated romantic relationships.
Participants rated advice satisfaction, model reliability, and helpfulness, and
completed pre- and post-measures of their general attitudes toward LLMs.
Overall, the results showed participants' high satisfaction with LLM-generated
advice. Greater satisfaction was, in turn, strongly and positively associated
with their perceptions of the models' reliability and helpfulness. Importantly,
participants' attitudes toward LLMs improved significantly after exposure to
the advice, suggesting that supportive and contextually relevant advice can
enhance users' trust and openness toward these AI systems.
\\ ( https://arxiv.org/abs/2601.11527 ,  532kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11528 (*cross-listing*)
Date: Sun, 16 Nov 2025 16:49:09 GMT   (658kb)

Title: Knowledge Graph Construction for Stock Markets with LLM-Based
  Explainable Reasoning
Authors: Cheonsol Lee, Youngsang Jeong, Jeongyeol Shin, Huiju Kim, Jidong Kim
Categories: cs.DB cs.AI
Comments: 6 pages, 3 figures, CIKM 2025 Workshop - Advances in Financial AI:
  Innovations, Risk, and Responsibility in the Era of LLMs
\\
  The stock market is inherently complex, with interdependent relationships
among companies, sectors, and financial indicators. Traditional research has
largely focused on time-series forecasting and single-company analysis, relying
on numerical data for stock price prediction. While such approaches can provide
short-term insights, they are limited in capturing relational patterns,
competitive dynamics, and explainable investment reasoning. To address these
limitations, we propose a knowledge graph schema specifically designed for the
stock market, modeling companies, sectors, stock indicators, financial
statements, and inter-company relationships. By integrating this schema with
large language models (LLMs), our approach enables multi-hop reasoning and
relational queries, producing explainable and in-depth answers to complex
financial questions. Figure1 illustrates the system pipeline, detailing the
flow from data collection and graph construction to LLM-based query processing
and answer generation. We validate the proposed framework through practical
case studies on Korean listed companies, demonstrating its capability to
extract insights that are difficult or impossible to obtain from traditional
database queries alone. The results highlight the potential of combining
knowledge graphs with LLMs for advanced investment analysis and decision
support.
\\ ( https://arxiv.org/abs/2601.11528 ,  658kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11529 (*cross-listing*)
Date: Tue, 18 Nov 2025 03:03:19 GMT   (510kb)

Title: SNAP: A Plan-Driven Framework for Controllable Interactive Narrative
  Generation
Authors: Geonwoo Bang, DongMyung Kim, Hayoung Oh
Categories: cs.HC cs.AI
Comments: 5 pages, 3 figures
\\
  Large Language Models (LLMs) hold great potential for web-based interactive
applications, including browser games, online education, and digital
storytelling platforms. However, LLM-based conversational agents suffer from
spatiotemporal distortions when responding to variant user inputs, failing to
maintain consistency with provided scenarios. We propose SNAP (Story and
Narrative-based Agent with Planning), a framework that structures narratives
into Cells with explicit Plans to prevent narrative drift in web environments.
By confining context within each Cell and employing detailed plans that specify
spatiotemporal settings, character actions, and plot developments, SNAP enables
coherent and scenario-consistent dialogues while adapting to diverse user
responses. Via automated and human evaluations, we validate SNAP's superiority
in narrative controllability, demonstrating effective scenario consistency
despite variant user inputs in web-based interactive storytelling.
\\ ( https://arxiv.org/abs/2601.11529 ,  510kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11530 (*cross-listing*)
Date: Tue, 18 Nov 2025 19:16:42 GMT   (975kb)

Title: AI for Proactive Mental Health: A Multi-Institutional, Longitudinal,
  Randomized Controlled Trial
Authors: Julie Y.A. Cachia, Xuan Zhao, John Hunter, Delancey Wu, Eta Lin,
  Julian De Freitas
Categories: cs.HC cs.AI cs.CY
\\
  Young adults today face unprecedented mental health challenges, yet many
hesitate to seek support due to barriers such as accessibility, stigma, and
time constraints. Bite-sized well-being interventions offer a promising
solution to preventing mental distress before it escalates to clinical levels,
but have not yet been delivered through personalized, interactive, and scalable
technology. We conducted the first multi-institutional, longitudinal,
preregistered randomized controlled trial of a generative AI-powered mobile app
("Flourish") designed to address this gap. Over six weeks in Fall 2024, 486
undergraduate students from three U.S. institutions were randomized to receive
app access or waitlist control. Participants in the treatment condition
reported significantly greater positive affect, resilience, and social
well-being (i.e., increased belonging, closeness to community, and reduced
loneliness) and were buffered against declines in mindfulness and flourishing.
These findings suggest that, with purposeful and ethical design, generative AI
can deliver proactive, population-level well-being interventions that produce
measurable benefits.
\\ ( https://arxiv.org/abs/2601.11530 ,  975kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11531 (*cross-listing*)
Date: Thu, 20 Nov 2025 06:17:07 GMT   (8376kb)

Title: NOVAID: Natural-language Observability Visualization Assistant for ITOps
  Dashboard Widget Generation
Authors: Pratik Mishra, Caner G\"oz\"ub\"uy\"uk, Seema Nagar, Prateeti
  Mohapatra, Raya Wittich, Arthur de Magalhaes
Categories: cs.HC cs.AI cs.SE
Comments: 15 pages, 6 figures, accepted IAAI 26
\\
  Manual creation of IT monitoring dashboard widgets is slow, error-prone, and
a barrier for both novice and expert users. We present NOVAID, an interactive
chatbot that leverages Large Language Models (LLMs) to generate IT monitoring
widgets directly from natural language queries. Unlike general natural
language-to-visualization tools, NOVAID addresses IT operations-specific
challenges: specialized widget types like SLO charts, dynamic API-driven data
retrieval, and complex contextual filters. The system combines a domain-aware
semantic parser, fuzzy entity matching, and schema completion to produce
standardized widget JSON specifications. An interactive clarification loop
ensures accuracy in underspecified queries. On a curated dataset of 271
realistic queries, NOVAID achieves promising accuracy (up to 94.10% in metric
extraction) across multiple LLMs. A user study with IT engineers yielded a
System Usability Scale score of 74.2 for NOVAID, indicating good usability. By
bridging natural language intent with operational dashboards, NOVAID
demonstrates clear potential and a path for deployment in enterprise ITOps
monitoring platforms.
\\ ( https://arxiv.org/abs/2601.11531 ,  8376kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11532 (*cross-listing*)
Date: Thu, 20 Nov 2025 15:06:14 GMT   (14600kb)

Title: "Jutters"
Authors: Meike Driessen, Selina Khan, Gon\c{c}alo Marcelino
Categories: cs.HC cs.AI cs.CV
Comments: Accepted at 39th Conference on Neural Information Processing Systems
  (NeurIPS 2025) Creative AI Track
\\
  This project explores how we engage with AI-generated content through the
lens of the jutter: Dutch coastal foragers who comb the shoreline after storms,
gathering and repurposing what the sea leaves behind. Reflecting how our lives
are increasingly shaped by AI-generated media, we create a beach-like
installation that blends real shoreline debris with AI-transformed images and
videos. Visitors are invited to explore this space as contemporary jutters,
deciding what to keep and what to discard. In doing so, the project reimagines
AI-imagery as material for reflection, encouraging a more discerning engagement
with the content that drifts through our feeds. A video preview of the
installation can be found at https://www.youtube.com/watch?v=L6319Ii7MT8.
\\ ( https://arxiv.org/abs/2601.11532 ,  14600kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11533 (*cross-listing*)
Date: Fri, 21 Nov 2025 10:09:20 GMT   (655kb)

Title: Artificial Intelligence as a Training Tool in Clinical Psychology: A
  Comparison of Text-Based and Avatar Simulations
Authors: V. El Sawah, A. Bhardwaj, A. Pryke-Hobbes, D. Gamaleldin, C. S. Ang,
  A. K. Martin
Categories: cs.HC cs.AI cs.CY
Comments: 38 pages, 2 figures
\\
  Clinical psychology students frequently report feeling underprepared for the
interpersonal demands of therapeutic work, highlighting the need for accessible
opportunities to practise core counselling skills before seeing real clients.
Advances in artificial intelligence (AI) now enable simulated interaction
partners that may support early skills development. This study examined
postgraduate clinical psychology students' perceptions of two AI-based
simulations: a text-based chatbot (ChatGPT) and a voice-based avatar (HeyGen).
Twenty-four students completed two brief cognitive-behavioural role-plays
(counterbalanced), one with each tool, and provided both quantitative ratings
and qualitative feedback on perceived usefulness, skill application,
responsiveness and engagement, and perceived skill improvement. Both AI tools
were evaluated positively across dimensions. However, the avatar was rated
significantly higher than the chatbot for perceived usefulness, skill
application, and perceived skill improvement, and qualitative comments
highlighted the added value of voice-based interaction for conveying social and
emotional cues. These findings suggest that AI-driven simulation may supplement
early-stage clinical skills training, with voice-based avatars offering
additional benefits. Future work should test whether such simulated
interactions translate to objective improvements in real therapeutic
performance.
\\ ( https://arxiv.org/abs/2601.11533 ,  655kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11534 (*cross-listing*)
Date: Fri, 21 Nov 2025 18:25:26 GMT   (17983kb)

Title: Modular AI-Powered Interviewer with Dynamic Question Generation and
  Expertise Profiling
Authors: Aisvarya Adeseye, Jouni Isoaho, Seppo Virtanen, Mohammad Tahir
Categories: cs.HC cs.AI
Comments: Accepted and Waiting to be published in conference AIR-RES'25 (
  http://www.american-cse.org/air-res2025 )
\\
  Automated interviewers and chatbots are common in research, recruitment,
customer service, and education. Many existing systems use fixed question
lists, strict rules, and limited personalization, leading to repeated
conversations that cause low engagement. Therefore, these tools are not
effective for complex qualitative research, which requires flexibility, context
awareness, and ethical sensitivity. Consequently, there is a need for a more
adaptive and context-aware interviewing system. To address this, an AI-powered
interviewer that dynamically generates questions that are contextually
appropriate and expertise aligned is presented in this study. The interviewer
is built on a locally hosted large language model (LLM) that generates coherent
dialogue while preserving data privacy. The interviewer profiles the
participants' expertise in real time to generate knowledge-appropriate
questions, well-articulated responses, and smooth transition messages similar
to human-like interviews. To implement these functionalities, a modular prompt
engineering pipeline was designed to ensure that the interview conversation
remains scalable, adaptive, and semantically rich. To evaluate the AI-powered
interviewer, it was tested with various participants, and it achieved high
satisfaction (mean 4.45) and engagement (mean 4.33). The proposed interviewer
is a scalable, privacy-conscious solution that advances AI-assisted qualitative
data collection.
\\ ( https://arxiv.org/abs/2601.11534 ,  17983kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11535 (*cross-listing*)
Date: Sat, 22 Nov 2025 22:49:40 GMT   (19196kb)

Title: Augmented Assembly: Object Recognition and Hand Tracking for Adaptive
  Assembly Instructions in Augmented Reality
Authors: Alexander Htet Kyaw, Haotian Ma, Sasa Zivkovic, Jenny Sabin
Categories: cs.HC cs.AI
Comments: Submitted to the Association for Computing Machinery (ACM) Conference
  on Tangible, Embedded, and Embodied Interaction (TEI'26)
ACM-class: H.5.2; H.5.1; I.4.8; I.2.6
\\
  Recent advances in augmented reality (AR) have enabled interactive systems
that assist users in physical assembly tasks. In this paper, we present an
AR-assisted assembly workflow that leverages object recognition and hand
tracking to (1) identify custom components, (2) display step-by-step
instructions, (3) detect assembly deviations, and (4) dynamically update the
instructions based on users' hands-on interactions with physical parts. Using
object recognition, the system detects and localizes components in real time to
create a digital twin of the workspace. For each assembly step, it overlays
bounding boxes in AR to indicate both the current position and the target
placement of relevant components, while hand-tracking data verifies whether the
user interacts with the correct part. Rather than enforcing a fixed sequence,
the system highlights potential assembly errors and interprets user deviations
as opportunities for iteration and creative exploration. A case study with LEGO
blocks and custom 3D-printed components demonstrates how the system links
digital instructions to physical assembly, eliminating the need for manual
searching, sorting, or labeling of parts.
\\ ( https://arxiv.org/abs/2601.11535 ,  19196kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11541 (*cross-listing*)
Date: Mon, 1 Dec 2025 22:51:54 GMT   (74kb)

Title: A Comparative Study of Technical Writing Feedback Quality: Evaluating
  LLMs, SLMs, and Humans in Computer Science Topics
Authors: Suqing Liu and Bogdan Simion and Christopher Eaton and Michael Liut
Categories: cs.HC cs.AI cs.CY
\\
  Feedback is a critical component of the learning process, particularly in
computer science education. This study investigates the quality of feedback
generated by Large Language Models (LLMs), Small Language Models (SLMs),
compared with human feedback, in three computer science course with technical
writing components: an introductory computer science course (CS2), a third-year
advanced systems course (operating systems), and a third-year writing course (a
topics course on artificial intelligence). Using a mixed-methods approach which
integrates quantitative Likert-scale questions with qualitative commentary, we
analyze the student perspective on feedback quality, evaluated based on
multiple criteria, including readability, detail, specificity, actionability,
helpfulness, and overall quality. The analysis reveals that in the larger
upper-year operating systems course ($N=80$), SLMs and LLMs are perceived to
deliver clear, actionable, and well-structured feedback, while humans provide
more contextually nuanced guidance. As for the high-enrollment CS2 course
($N=176$) showed the same preference for the AI tools' clarity and breadth, but
students noted that AI feedback sometimes lacked the concise,
straight-to-the-point, guidance offered by humans. Conversely, in the smaller
upper-year technical writing course on AI topics ($N=7$), all students
preferred feedback from the course instructor, who was able to provide clear,
specific, and personalized feedback, compared to the more general and less
targeted AI-based feedback. We also highlight the scalability of AI-based
feedback by focusing on its effectiveness at large scale. Our findings
underscore the potential of hybrid approaches that combine AI and human
feedback to achieve efficient and high-quality feedback at scale.
\\ ( https://arxiv.org/abs/2601.11541 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11544 (*cross-listing*)
Date: Tue, 2 Dec 2025 13:50:03 GMT   (281kb)

Title: Medication counseling with large language models: balancing flexibility
  and rigidity
Authors: Joar Sabel, Mattias Wingren, Andreas Lundell, S\"oren Andersson, Sara
  Rosenberg, Susanne H\"agglund, Linda Estman, Malin Andtfolk
Categories: cs.HC cs.AI cs.CL
Comments: Accepted for 2025 IEEE International Conference on Agentic AI (ICA).
  14 pages, 2 figures
\\
  The introduction of large language models (LLMs) has greatly enhanced the
capabilities of software agents. Instead of relying on rule-based interactions,
agents can now interact in flexible ways akin to humans. However, this
flexibility quickly becomes a problem in fields where errors can be disastrous,
such as in a pharmacy context, but the opposite also holds true; a system that
is too inflexible will also lead to errors, as it can become too rigid to
handle situations that are not accounted for. Work using LLMs in a pharmacy
context have adopted a wide scope, accounting for many different medications in
brief interactions -- our strategy is the opposite: focus on a more narrow and
long task. This not only enables a greater understanding of the task at hand,
but also provides insight into what challenges are present in an interaction of
longer nature. The main challenge, however, remains the same for a narrow and
wide system: it needs to strike a balance between adherence to conversational
requirements and flexibility. In an effort to strike such a balance, we present
a prototype system meant to provide medication counseling while juggling these
two extremes. We also cover our design in constructing such a system, with a
focus on methods aiming to fulfill conversation requirements, reduce
hallucinations and promote high-quality responses. The methods used have the
potential to increase the determinism of the system, while simultaneously not
removing the dynamic conversational abilities granted by the usage of LLMs.
However, a great deal of work remains ahead, and the development of this kind
of system needs to involve continuous testing and a human-in-the-loop. It
should also be evaluated outside of commonly used benchmarks for LLMs, as these
do not adequately capture the complexities of this kind of conversational
system.
\\ ( https://arxiv.org/abs/2601.11544 ,  281kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11560 (*cross-listing*)
Date: Tue, 23 Dec 2025 14:34:38 GMT   (1731kb)

Title: DeepEvidence: Empowering Biomedical Discovery with Deep Knowledge Graph
  Research
Authors: Zifeng Wang, Zheng Chen, Ziwei Yang, Xuan Wang, Qiao Jin, Yifan Peng,
  Zhiyong Lu, Jimeng Sun
Categories: cs.IR cs.AI cs.LG
\\
  Biomedical knowledge graphs (KGs) encode vast, heterogeneous information
spanning literature, genes, pathways, drugs, diseases, and clinical trials, but
leveraging them collectively for scientific discovery remains difficult. Their
structural differences, continual evolution, and limited cross-resource
alignment require substantial manual integration, limiting the depth and scale
of knowledge exploration. We introduce DeepEvidence, an AI-agent framework
designed to perform Deep Research across various heterogeneous biomedical KGs.
Unlike generic Deep Research systems that rely primarily on internet-scale
text, DeepEvidence incorporates specialized knowledge-graph tooling and
coordinated exploration strategies to systematically bridge heterogeneous
resources. At its core is an orchestrator that directs two complementary
agents: Breadth-First ReSearch (BFRS) for broad, multi-graph entity search, and
Depth-First ReSearch (DFRS) for multi-hop, evidence-focused reasoning. An
internal, incrementally built evidence graph provides a structured record of
retrieved entities, relations, and supporting evidence. To operate at scale,
DeepEvidence includes unified interfaces for querying diverse biomedical APIs
and an execution sandbox that enables programmatic data retrieval, extraction,
and analysis. Across established deep-reasoning benchmarks and four key stages
of the biomedical discovery lifecycle: drug discovery, pre-clinical
experimentation, clinical trial development, and evidence-based medicine,
DeepEvidence demonstrates substantial gains in systematic exploration and
evidence synthesis. These results highlight the potential of
knowledge-graph-driven Deep Research to accelerate biomedical discovery.
\\ ( https://arxiv.org/abs/2601.11560 ,  1731kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11564 (*cross-listing*)
Date: Thu, 25 Dec 2025 08:37:57 GMT   (697kb)

Title: Context Discipline and Performance Correlation: Analyzing LLM
  Performance and Quality Degradation Under Varying Context Lengths
Authors: Ahilan Ayyachamy Nadar Ponnusamy and Karthic Chandran and M Maruf
  Hossain
Categories: cs.CL cs.AI
Comments: 22 pages, 6 figures
\\
  The scaling trend in Large Language Models (LLMs) has prioritized increasing
the maximum context window to facilitate complex, long-form reasoning and
document analysis. However, managing this expanded context introduces severe
computational overhead. This paper investigates the critical trade-off between
system performance and model quality when dense transformer
architectures--specifically Llama-3.1-70B and Qwen1.5-14B--are exposed to large
volumes of irrelevant and distracting context. The research identifies a
non-linear performance degradation tied to the growth of the Key-Value (KV)
cache. Furthermore, an extended analysis of the Mixture-of-Experts (MoE)
architecture reveals unique behavioral anomalies at varying context scales,
suggesting that architectural benefits may be masked by infrastructure
bottlenecks at high token volumes.
\\ ( https://arxiv.org/abs/2601.11564 ,  697kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11565 (*cross-listing*)
Date: Thu, 25 Dec 2025 13:41:53 GMT   (805kb)

Title: Compass-Embedding v4: Robust Contrastive Learning for Multilingual
  E-commerce Embeddings
Authors: Pakorn Ueareeworakul, Shuman Liu, Jinghao Feng, Ling Hu, Zhantang Shi,
  Chengqi Sun, Liang Yao, Panyi Ouyang, Haibo Zhang and Anxiang Zeng
Categories: cs.CL cs.AI
\\
  As global e-commerce rapidly expands into emerging markets, the lack of
high-quality semantic representations for low-resource languages has become a
decisive bottleneck for retrieval, recommendation, and search systems. In this
work, we present Compass-Embedding v4, a high-efficiency multilingual embedding
framework specifically optimized for Southeast Asian (SEA) e-commerce
scenarios, where data scarcity, noisy supervision, and strict production
constraints jointly challenge representation learning. Compass-Embedding v4
addresses three core challenges. First, large-batch contrastive training under
mixed task supervision introduces systematic false negatives that degrade
semantic alignment. We propose Class-Aware Masking (CAM), a lightweight
modification to the InfoNCE objective that suppresses invalid in-batch
negatives and improves semantic discrimination without altering training
efficiency. Second, low-resource SEA languages suffer from limited and uneven
data coverage. We construct a diversified training corpus through
context-grounded synthetic data generation, cross-lingual translation, and
structured e-commerce data construction, enabling robust multilingual and
domain-specific learning. Third, production deployment requires high-throughput
inference while preserving embedding quality. We combine robustness-driven
large-batch training with spherical model merging to mitigate catastrophic
forgetting, and optimize inference via vLLM and FP8 quantization. Extensive
evaluations across multilingual benchmarks and proprietary e-commerce tasks
show that Compass-Embedding v4 achieves state-of-the-art performance on major
SEA languages, significantly outperforming general-purpose embedding models in
domain-specific retrieval and classification, while maintaining competitive
performance on high-resource languages.
\\ ( https://arxiv.org/abs/2601.11565 ,  805kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11567 (*cross-listing*)
Date: Fri, 26 Dec 2025 14:30:53 GMT   (163kb)

Title: Measuring Stability Beyond Accuracy in Small Open-Source Medical Large
  Language Models for Pediatric Endocrinology
Authors: Vanessa D'Amario, Randy Daniel, Alessandro Zanetti, Dhruv Edamadaka,
  Nitya Alaparthy, Joshua Tarkoff
Categories: cs.CL cs.AI
Comments: 20 pages, 11 figures, accepted at 47 workshop Reproducible Artificial
  Intelligence (AAAI 2026, Singapore, January 27, 2026)
\\
  Small open-source medical large language models (LLMs) offer promising
opportunities for low-resource deployment and broader accessibility. However,
their evaluation is often limited to accuracy on medical multiple choice
question (MCQ) benchmarks, and lacks evaluation of consistency, robustness, or
reasoning behavior. We use MCQ coupled to human evaluation and clinical review
to assess six small open-source medical LLMs (HuatuoGPT-o1 (Chen 2024),
Diabetica-7B, Diabetica-o1 (Wei 2024), Meditron3-8B (Sallinen2025), MedFound-7B
(Liu 2025), and ClinicaGPT-base-zh (Wang 2023)) in pediatric endocrinology. In
deterministic settings, we examine the effect of prompt variation on models'
output and self-assessment bias. In stochastic settings, we evaluate output
variability and investigate the relationship between consistency and
correctness. HuatuoGPT-o1-8B achieved the highest performance. The results show
that high consistency across the model response is not an indicator of
correctness, although HuatuoGPT-o1-8B showed the highest consistency rate. When
tasked with selecting correct reasoning, both HuatuoGPT-o1-8B and Diabetica-o1
exhibit self-assessment bias and dependency on the order of the candidate
explanations. Expert review of incorrect reasoning rationales identified a mix
of clinically acceptable responses and clinical oversight. We further show that
system-level perturbations, such as differences in CUDA builds, can yield
statistically significant shifts in model output despite stable accuracy. This
work demonstrates that small, semantically negligible prompt perturbations lead
to divergent outputs, raising concerns about reproducibility of LLM-based
evaluations and highlights the output variability under different stochastic
regimes, emphasizing the need of a broader diagnostic framework to understand
potential pitfalls in real-world clinical decision support scenarios.
\\ ( https://arxiv.org/abs/2601.11567 ,  163kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11575 (*cross-listing*)
Date: Tue, 30 Dec 2025 11:53:49 GMT   (41636kb)

Title: Concept Attractors in LLMs and their Applications
Authors: Sotirios Panagiotis Chytas, Vikas Singh
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) often map semantically related prompts to
similar internal representations at specific layers, even when their surface
forms differ widely. We show that this behavior can be explained through
Iterated Function Systems (IFS), where layers act as contractive mappings
toward concept-specific Attractors. We leverage this insight and develop
simple, training-free methods that operate directly on these Attractors to
solve a wide range of practical tasks, including language translation,
hallucination reduction, guardrailing, and synthetic data generation. Despite
their simplicity, these Attractor-based interventions match or exceed
specialized baselines, offering an efficient alternative to heavy fine-tuning,
generalizable in scenarios where baselines underperform.
\\ ( https://arxiv.org/abs/2601.11575 ,  41636kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11578 (*cross-listing*)
Date: Tue, 30 Dec 2025 18:12:52 GMT   (144kb)

Title: LimAgents: Multi-Agent LLMs for Generating Research Limitations
Authors: Ibrahim Al Azher, Zhishuai Guo, Hamed Alhoori
Categories: cs.CL cs.AI
Comments: 18 Pages, 9 figures
\\
  Identifying and articulating limitations is essential for transparent and
rigorous scientific research. However, zero-shot large language models (LLMs)
approach often produce superficial or general limitation statements (e.g.,
dataset bias or generalizability). They usually repeat limitations reported by
authors without looking at deeper methodological issues and contextual gaps.
This problem is made worse because many authors disclose only partial or
trivial limitations. We propose LimAgents, a multi-agent LLM framework for
generating substantive limitations. LimAgents integrates OpenReview comments
and author-stated limitations to provide stronger ground truth. It also uses
cited and citing papers to capture broader contextual weaknesses. In this
setup, different agents have specific roles as sequential role: some extract
explicit limitations, others analyze methodological gaps, some simulate the
viewpoint of a peer reviewer, and a citation agent places the work within the
larger body of literature. A Judge agent refines their outputs, and a Master
agent consolidates them into a clear set. This structure allows for systematic
identification of explicit, implicit, peer review-focused, and
literature-informed limitations. Moreover, traditional NLP metrics like BLEU,
ROUGE, and cosine similarity rely heavily on n-gram or embedding overlap. They
often overlook semantically similar limitations. To address this, we introduce
a pointwise evaluation protocol that uses an LLM-as-a-Judge to measure coverage
more accurately. Experiments show that LimAgents substantially improve
performance. The RAG + multi-agent GPT-4o mini configuration achieves a +15.51%
coverage gain over zero-shot baselines, while the Llama 3 8B multi-agent setup
yields a +4.41% improvement.
\\ ( https://arxiv.org/abs/2601.11578 ,  144kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11579 (*cross-listing*)
Date: Tue, 30 Dec 2025 18:35:15 GMT   (171kb)

Title: Bielik 11B v3: Multilingual Large Language Model for European Languages
Authors: Krzysztof Ociepa, {\L}ukasz Flis, Remigiusz Kinas, Krzysztof Wr\'obel,
  Adrian Gwo\'zdziej
Categories: cs.CL cs.AI
ACM-class: I.2.7
\\
  We present Bielik 11B v3, a state-of-the-art language model highly optimized
for the Polish language, while also maintaining strong capabilities in other
European languages. This model extends the Mistral 7B v0.2 architecture, scaled
to 11B parameters via depth up-scaling. Its development involved a
comprehensive four-stage training pipeline: continuous pre-training, supervised
fine-tuning (SFT), Direct Preference Optimization (DPO), and reinforcement
learning.
  Comprehensive evaluations demonstrate that Bielik 11B v3 achieves exceptional
performance. It significantly surpasses other specialized Polish language
models and outperforms many larger models (with 2-6 times more parameters) on a
wide range of tasks, from basic linguistic understanding to complex reasoning.
  The model's parameter efficiency, combined with extensive quantization
options, allows for effective deployment across diverse hardware
configurations. Bielik 11B v3 not only advances AI capabilities for the Polish
language but also establishes a new benchmark for developing
resource-efficient, high-performance models for less-represented languages.
\\ ( https://arxiv.org/abs/2601.11579 ,  171kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11580 (*cross-listing*)
Date: Wed, 31 Dec 2025 20:31:36 GMT   (10188kb)

Title: Speculative Decoding: Performance or Illusion?
Authors: Xiaoxuan Liu, Jiaxiang Yu, Jongseok Park, Ion Stoica, Alvin Cheung
Categories: cs.CL cs.AI
\\
  Speculative decoding (SD) has become a popular technique to accelerate Large
Language Model (LLM) inference, yet its real-world effectiveness remains
unclear as prior evaluations rely on research prototypes and unrealistically
small batch sizes. We present, to our knowledge, the first systematic study of
SD on a production-grade and widely deployed inference engine (vLLM), covering
multiple SD variants ($n$-gram, EAGLE/EAGLE-3, Draft-Model, Multi-Token
Prediction) across diverse workloads, model scales, and batch sizes. We analyze
key factors governing SD performance, and quantify a theoretical upper bound on
SD speedup. Our results show that verification by the target model dominates
the execution, while acceptance length varies markedly across output token
positions, requests, and datasets. Comparing measured performance with
theoretical bounds reveals substantial gaps between observed and theoretical
upper bounds, and we leverage this observation to highlight new research
opportunities that our study opens up in improving SD.
\\ ( https://arxiv.org/abs/2601.11580 ,  10188kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11581 (*cross-listing*)
Date: Thu, 1 Jan 2026 08:39:07 GMT   (54kb)

Title: Enhancing the QA Model through a Multi-domain Debiasing Framework
Authors: Yuefeng Wang, ChangJae Lee
Categories: cs.CL cs.AI
Comments: 5 pages, 7 tables
\\
  Question-answering (QA) models have advanced significantly in machine reading
comprehension but often exhibit biases that hinder their performance,
particularly with complex queries in adversarial conditions. This study
evaluates the ELECTRA-small model on the Stanford Question Answering Dataset
(SQuAD) v1.1 and adversarial datasets AddSent and AddOneSent. By identifying
errors related to lexical bias, numerical reasoning, and entity recognition, we
develop a multi-domain debiasing framework incorporating knowledge
distillation, debiasing techniques, and domain expansion. Our results
demonstrate up to 2.6 percentage point improvements in Exact Match (EM) and F1
scores across all test sets, with gains in adversarial contexts. These findings
highlight the potential of targeted bias mitigation strategies to enhance the
robustness and reliability of natural language understanding systems.
\\ ( https://arxiv.org/abs/2601.11581 ,  54kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11582 (*cross-listing*)
Date: Thu, 1 Jan 2026 16:25:16 GMT   (72kb)

Title: Overview of the SciHigh Track at FIRE 2025: Research Highlight
  Generation from Scientific Papers
Authors: Tohida Rehman, Debarshi Kumar Sanyal, Samiran Chattopadhyay
Categories: cs.CY cs.AI cs.CL
Comments: 7 pages, 2 tables
\\
  `SciHigh: Research Highlight Generation from Scientific Papers' focuses on
the task of automatically generating concise, informative, and meaningful
bullet-point highlights directly from scientific abstracts. The goal of this
task is to evaluate how effectively computational models can generate
highlights that capture the key contributions, findings, and novelty of a paper
in a concise form. Highlights help readers grasp essential ideas quickly and
are often easier to read and understand than longer paragraphs, especially on
mobile devices. The track uses the MixSub dataset \cite{10172215}, which
provides pairs of abstracts and corresponding author-written highlights.
  In this inaugural edition of the track, 12 teams participated, exploring
various approaches, including pre-trained language models, to generate
highlights from this scientific dataset. All submissions were evaluated using
established metrics such as ROUGE, METEOR, and BERTScore to measure both
alignment with author-written highlights and overall informativeness. Teams
were ranked based on ROUGE-L scores. The findings suggest that automatically
generated highlights can reduce reading effort, accelerate literature reviews,
and enhance metadata for digital libraries and academic search platforms.
SciHigh provides a dedicated benchmark for advancing methods aimed at concise
and accurate highlight generation from scientific writing.
\\ ( https://arxiv.org/abs/2601.11582 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11583 (*cross-listing*)
Date: Thu, 1 Jan 2026 17:26:54 GMT   (1521kb)

Title: Bit-politeia: An AI Agent Community in Blockchain
Authors: Xing Yang
Categories: cs.CY cs.AI cs.MA
\\
  Current resource allocation paradigms, particularly in academic evaluation,
are constrained by inherent limitations such as the Matthew Effect, reward
hacking driven by Goodhart's Law, and the trade-off between efficiency and
fairness. To address these challenges, this paper proposes "Bit-politeia", an
AI agent community on blockchain designed to construct a fair, efficient, and
sustainable resource allocation system. In this virtual community, residents
interact via AI agents serving as their exclusive proxies, which are optimized
for impartiality and value alignment. The community adopts a "clustered
grouping + hierarchical architecture" that integrates democratic centralism to
balance decision-making efficiency and trust mechanisms. Agents engage through
casual chat and deliberative interactions to evaluate research outputs and
distribute a virtual currency as rewards. This incentive mechanism aims to
achieve incentive compatibility through consensus-driven evaluation, while
blockchain technology ensures immutable records of all transactions and
reputation data. By leveraging AI for objective assessment and decentralized
verification, Bit-politeia minimizes human bias and mitigates resource
centralization issues found in traditional peer review. The proposed framework
provides a novel pathway for optimizing scientific innovation through a fair
and automated resource configuration process.
\\ ( https://arxiv.org/abs/2601.11583 ,  1521kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11586 (*cross-listing*)
Date: Sat, 3 Jan 2026 00:17:03 GMT   (1258kb)

Title: Let Me Try Again: Examining Replay Behavior by Tracing Students' Latent
  Problem-Solving Pathways
Authors: Shan Zhang, Siddhartha Pradhan, Ji-Eun Lee, Ashish Gurung, Anthony F.
  Botelho
Categories: cs.CY cs.AI cs.LG
Comments: 16 pages, 7 figures, LAK2026
\\
  Prior research has shown that students' problem-solving pathways in
game-based learning environments reflect their conceptual understanding,
procedural knowledge, and flexibility. Replay behaviors, in particular, may
indicate productive struggle or broader exploration, which in turn foster
deeper learning. However, little is known about how these pathways unfold
sequentially across problems or how the timing of replays and other
problem-solving strategies relates to proximal and distal learning outcomes.
This study addresses these gaps using Markov Chains and Hidden Markov Models
(HMMs) on log data from 777 seventh graders playing the game-based learning
platform of From Here to There!. Results show that within problem sequences,
students often persisted in states or engaged in immediate replay after
successful completions, while across problems, strong self-transitions
indicated stable strategic pathways. Four latent states emerged from HMMs:
Incomplete-dominant, Optimal-ending, Replay, and Mixed. Regression analyses
revealed that engagement in replay-dominant and optimal-ending states predicted
higher conceptual knowledge, flexibility, and performance compared with the
Incomplete-dominant state. Immediate replay consistently supported learning
outcomes, whereas delayed replay was weakly or negatively associated in
relation to Non-Replay. These findings suggest that replay in digital learning
is not uniformly beneficial but depends on timing, with immediate replay
supporting flexibility and more productive exploration.
\\ ( https://arxiv.org/abs/2601.11586 ,  1258kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11589 (*cross-listing*)
Date: Sun, 4 Jan 2026 18:14:24 GMT   (2257kb)

Title: PLA-Serve: A Prefill-Length-Aware LLM Serving System
Authors: Jianshu She, Zonghang Li, Hongchao Du, Shangyu Wu, Wenhao Zheng, Eric
  Xing, Zhengzhong Liu, Huaxiu Yao, Jason Xue, Qirong Ho
Categories: cs.DC cs.AI
Comments: 12 pages
\\
  PLA-Serve identifies and disaggregates requests with different prompt lengths
in LLM serving to reduce TTFT latency. While recent systems have decoupled the
prefill and decode stages to improve throughput, they still rely on unified
scheduling policies that fail to adapt to heterogeneous workload
characteristics. We observe that prompt-length variations lead to distinct
performance bottlenecks, motivating an adaptive scheduling strategy. PLA-Serve
disaggregates multi-turn long-prefill requests from short-prefill ones and
introduces a length-aware smart batching mechanism for short-prefill workloads.
It adopts a dual-queue design that supports temporal disaggregation on a single
prefill instance or spatial disaggregation across multiple instances. For
short-prefill batches, a batch waiting window and CUDA Graph-based clustering
mitigate interference from heterogeneous computation, reducing batching delay
and lowering average latency. In real multi-turn workloads, PLA-Serve reduces
prefill latency by over 30% compared to vanilla SGLang under
prefill**--**decode disaggregation, and further decreases SLO violations by 28%
in multi-instance deployments with vanilla data-parallel configuration.
Compared to the SGLang router with load balancing, it further lowers SLO
violations by 12% in multi-GPU settings. Under high concurrency and
mixed-request scenarios, PLA-Serve improves request throughput by 35% serving
Qwen2.5-32B model for prefill instance, demonstrating its effectiveness in
optimizing heterogeneous LLM serving workloads.
\\ ( https://arxiv.org/abs/2601.11589 ,  2257kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11590 (*cross-listing*)
Date: Mon, 5 Jan 2026 03:17:15 GMT   (6991kb)

Title: EPD-Serve: A Flexible Multimodal EPD Disaggregation Inference Serving
  System On Ascend
Authors: Fan Bai, Pai Peng, Zhengzhi Tang, Zhe Wang, Gong Chen, Xiang Lu, Yinuo
  Li, Huan Lin, Weizhe Lin, Yaoyuan Wang, Xiaosong Li
Categories: cs.DC cs.AI
\\
  With the widespread adoption of large multimodal models, efficient inference
across text, image, audio, and video modalities has become critical. However,
existing multimodal inference systems typically employ monolithic architectures
that tightly couple the Encode, Prefill, and Decode stages on homogeneous
hardware, neglecting the heterogeneous computational characteristics of each
stage. This design leads to inefficient resource utilization and limited system
throughput. To address these issues, we propose EPD-Serve, a stage-level
disaggregated inference serving system for multimodal models. EPD-Serve
decouples the inference pipeline into independent Encode, Prefill, and Decode
stages, enabling logical isolation and flexible co-located deployment through
dynamic orchestration. Leveraging the Ascend interconnect topology, EPD-Serve
introduces asynchronous feature prefetching between Encode and Prefill stages
and a hierarchical grouped KV cache transmission mechanism between Prefill and
Decode stages to improve cross-node communication efficiency. In addition,
EPD-Serve incorporates multi-route scheduling, instance-level load balancing,
and multi-stage hardware co-location with spatial multiplexing to better
support diverse multimodal workloads. Comprehensive experiments on multimodal
understanding models demonstrate that, under high-concurrency scenarios,
EPD-Serve improves end-to-end throughput by 57.37-69.48% compared to
PD-disaggregated deployment, while satisfying strict SLO constraints, including
TTFT below 2000 ms and TPOT below 50 ms. These results highlight the
effectiveness of stage-level disaggregation for optimizing multimodal large
model inference systems.
\\ ( https://arxiv.org/abs/2601.11590 ,  6991kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11598 (*cross-listing*)
Date: Wed, 7 Jan 2026 21:17:53 GMT   (553kb)

Title: Toward Youth-Centered Privacy-by-Design in Smart Devices: A Systematic
  Review
Authors: Molly Campbell, Mohamad Sheikho Al Jasem and Ajay Kumar Shrestha
Categories: cs.CY cs.AI cs.HC
Comments: To appear in the IEEE CCWC 2026 proceedings
\\
  This literature review evaluates privacy-by-design frameworks, tools, and
policies intended to protect youth in AI-enabled smart devices using a
PRISMA-guided workflow. Sources from major academic and grey-literature
repositories from the past decade were screened. The search identified 2,216
records; after deduplication and screening, 645 articles underwent eligibility
assessment, and 122 were included for analysis. The corpus was organized along
three thematic categories: technical solutions, policy/regulatory measures, and
education/awareness strategies. Findings reveal that while technical
interventions such as on-device processing, federated learning, and lightweight
encryption significantly reduce data exposure, their adoption remains limited.
Policy frameworks, including the EU's GDPR, the UK Age-Appropriate Design Code,
and Canada's PIPEDA, provide important baselines but are hindered by gaps in
enforcement and age-appropriate design obligations, while educational
initiatives are rarely integrated systematically into curricula. Overall, the
corpus skews toward technical solutions (67%) relative to policy (21%) and
education (12%), indicating an implementation gap outside the technical domain.
To address these challenges, we recommend a multi-stakeholder model in which
policymakers, manufacturers, and educators co-develop inclusive, transparent,
and context-sensitive privacy ecosystems. This work advances discourse on youth
data protection by offering empirically grounded insights and actionable
recommendations for the design of ethical, privacy-preserving AI systems
tailored to young users.
\\ ( https://arxiv.org/abs/2601.11598 ,  553kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11608 (*cross-listing*)
Date: Fri, 9 Jan 2026 02:28:14 GMT   (41kb)

Title: Hardware-Aware Reformulation of Convolutions for Efficient Execution on
  Specialized AI Hardware: A Case Study on NVIDIA Tensor Cores
Authors: Ganesh Bikshandi
Categories: cs.DC cs.AI
\\
  Convolutional Neural Networks (CNNs) are central to modern AI, but their
performance is often limited by hardware constraints. NVIDIA Tensor Cores, for
instance, require input channels to be multiples of 8 and sometimes 512 for
efficient execution. {\em oneDNN} framework for CPU imposes such a requirement
for the blocked format. Traditional approaches address such alignment issue
using zero-padding, which can be inefficient. In this work, we present a
first-step, hardware-aware reformulation of CNN computations using rewrite
rules, restructuring the underlying math to satisfy hardware alignment entirely
{\bf post-training} without modifying network weights. While our current
implementation focuses on a single transformation for Tensor Cores, this
approach is generalizable, laying the foundation to explore additional
transformations for CPU and accelerators. This study represents an initial step
toward {\em semantic tuning}, a systematic, hardware-aware optimization
strategy for efficient deployment of CNN models on specialized AI hardware.
\\ ( https://arxiv.org/abs/2601.11608 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11610 (*cross-listing*)
Date: Fri, 9 Jan 2026 06:29:55 GMT   (1564kb)

Title: Multifaceted Scenario-Aware Hypergraph Learning for Next POI
  Recommendation
Authors: Yuxi Lin, Yongkang Li, Jie Xing, Zipei Fan
Categories: cs.SI cs.AI
ACM-class: I.2.0
\\
  Among the diverse services provided by Location-Based Social Networks
(LBSNs), Next Point-of-Interest (POI) recommendation plays a crucial role in
inferring user preferences from historical check-in trajectories. However,
existing sequential and graph-based methods frequently neglect significant
mobility variations across distinct contextual scenarios (e.g., tourists versus
locals). This oversight results in suboptimal performance due to two
fundamental limitations: the inability to capture scenario-specific features
and the failure to resolve inherent inter-scenario conflicts. To overcome these
limitations, we propose the Multifaceted Scenario-Aware Hypergraph Learning
method (MSAHG), a framework that adopts a scenario-splitting paradigm for next
POI recommendation.
  Our main contributions are:
  (1) Construction of scenario-specific, multi-view disentangled
sub-hypergraphs to capture distinct mobility patterns;
  (2) A parameter-splitting mechanism to adaptively resolve conflicting
optimization directions across scenarios while preserving generalization
capability.
  Extensive experiments on three real-world datasets demonstrate that MSAHG
consistently outperforms five state-of-the-art methods across diverse
scenarios, confirming its effectiveness in multi-scenario POI recommendation.
\\ ( https://arxiv.org/abs/2601.11610 ,  1564kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11643 (*cross-listing*)
Date: Wed, 14 Jan 2026 17:47:24 GMT   (141kb)

Title: Syllabic Agglutinative Tokenizations for Indonesian LLM: A Study from
  Gasing Literacy Learning System
Authors: H. Situngkir, A.B. Lumbantobing, Y. Surya
Categories: cs.CY cs.AI
Comments: 12 pages, 1 figures
Report-no: WP-1-2026
MSC-class: 68T50
ACM-class: I.2.7; E.4; F.2.2
Journal-ref: BFI Working Paper Series 2026
\\
  This paper presents a novel syllable-based tokenization approach for
Indonesian large language models, inspired by the Gasing Literacy Learning
System's pedagogical methodology. Drawing on information-theoretic principles,
we develop a tokenization framework that segments Indonesian text at syllable
boundaries before applying byte-pair encoding, creating a vocabulary that
aligns with the language's morphophonological structure. Our approach first
identifies high-frequency syllables through rule-based segmentation, then
constructs a compact vocabulary of 3,500 tokens that preserves meaningful
linguistic units while maintaining coverage through character-level fallback.
Empirical evaluation on Indonesian Wikipedia and folklore corpora from
Indonesian Culture Digital Library (PDBI) demonstrates substantial improvements
over conventional tokenization methods: the syllable-based approach achieves
R\'enyi efficiency of 0.74 compared to 0.50-0.64 for pretrained multilingual
tokenizers, while maintaining higher average token lengths (3.67 characters
versus 2.72 for GPT-2) despite using a vocabulary an order of magnitude
smaller. These gains emerge from the method's ability to internalize
character-level dependencies within syllable units, reducing the computational
burden on language models while respecting Indonesian's agglutinative
morphology. We call the LLM built upon this principle, TOBA LLM (Tokenisasi
Optimum Berbasis Aglutinasi), the convergence of human literacy pedagogy with
computational optimization principles offers a promising paradigm for
developing linguistically-informed tokenization strategies, particularly for
morphologically rich and underrepresented languages in natural language
processing.
\\ ( https://arxiv.org/abs/2601.11643 ,  141kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11644 (*cross-listing*)
Date: Wed, 14 Jan 2026 22:00:28 GMT   (16481kb)

Title: Predicting When to Trust Vision-Language Models for Spatial Reasoning
Authors: Muhammad Imran and Yugyung Lee
Categories: cs.CV cs.AI
Comments: 9 pages, 5 figures, 6 tables
MSC-class: 68T45
ACM-class: I.2.10; I.2.7; I.5.1; H.5.1
\\
  Vision-Language Models (VLMs) demonstrate impressive capabilities across
multimodal tasks, yet exhibit systematic spatial reasoning failures, achieving
only 49% (CLIP) to 54% (BLIP-2) accuracy on basic directional relationships.
For safe deployment in robotics and autonomous systems, we need to predict when
to trust VLM spatial predictions rather than accepting all outputs. We propose
a vision-based confidence estimation framework that validates VLM predictions
through independent geometric verification using object detection. Unlike
text-based approaches relying on self-assessment, our method fuses four signals
via gradient boosting: geometric alignment between VLM claims and coordinates,
spatial ambiguity from overlap, detection quality, and VLM internal
uncertainty. We achieve 0.674 AUROC on BLIP-2 (34.0% improvement over
text-based baselines) and 0.583 AUROC on CLIP (16.1% improvement), generalizing
across generative and classification architectures. Our framework enables
selective prediction: at 60% target accuracy, we achieve 61.9% coverage versus
27.6% baseline (2.2x improvement) on BLIP-2. Feature analysis reveals
vision-based signals contribute 87.4% of model importance versus 12.7% from VLM
confidence, validating that external geometric verification outperforms
self-assessment. We demonstrate reliable scene graph construction where
confidence-based pruning improves precision from 52.1% to 78.3% while retaining
68.2% of edges.
\\ ( https://arxiv.org/abs/2601.11644 ,  16481kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11650 (*cross-listing*)
Date: Thu, 15 Jan 2026 12:18:45 GMT   (275kb)

Title: Large Language Model Agent for User-friendly Chemical Process
  Simulations
Authors: Jingkang Liang, Niklas Groll, G\"urkan Sin
Categories: physics.chem-ph cs.AI
\\
  Modern process simulators enable detailed process design, simulation, and
optimization; however, constructing and interpreting simulations is
time-consuming and requires expert knowledge. This limits early exploration by
inexperienced users. To address this, a large language model (LLM) agent is
integrated with AVEVA Process Simulation (APS) via Model Context Protocol
(MCP), allowing natural language interaction with rigorous process simulations.
An MCP server toolset enables the LLM to communicate programmatically with APS
using Python, allowing it to execute complex simulation tasks from
plain-language instructions. Two water-methanol separation case studies assess
the framework across different task complexities and interaction modes. The
first shows the agent autonomously analyzing flowsheets, finding improvement
opportunities, and iteratively optimizing, extracting data, and presenting
results clearly. The framework benefits both educational purposes, by
translating technical concepts and demonstrating workflows, and experienced
practitioners by automating data extraction, speeding routine tasks, and
supporting brainstorming. The second case study assesses autonomous flowsheet
synthesis through both a step-by-step dialogue and a single prompt,
demonstrating its potential for novices and experts alike. The step-by-step
mode gives reliable, guided construction suitable for educational contexts; the
single-prompt mode constructs fast baseline flowsheets for later refinement.
While current limitations such as oversimplification, calculation errors, and
technical hiccups mean expert oversight is still needed, the framework's
capabilities in analysis, optimization, and guided construction suggest
LLM-based agents can become valuable collaborators.
\\ ( https://arxiv.org/abs/2601.11650 ,  275kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11651 (*cross-listing*)
Date: Thu, 15 Jan 2026 15:23:38 GMT   (42645kb)

Title: Aesthetics as Structural Harm: Algorithmic Lookism Across Text-to-Image
  Generation and Classification
Authors: Miriam Doh, Aditya Gulati, Corina Canali, Nuria Oliver
Categories: cs.CV cs.AI cs.CY
Comments: 22 pages, 15 figures
\\
  This paper examines algorithmic lookism-the systematic preferential treatment
based on physical appearance-in text-to-image (T2I) generative AI and a
downstream gender classification task. Through the analysis of 26,400 synthetic
faces created with Stable Diffusion 2.1 and 3.5 Medium, we demonstrate how
generative AI models systematically associate facial attractiveness with
positive attributes and vice-versa, mirroring socially constructed biases
rather than evidence-based correlations. Furthermore, we find significant
gender bias in three gender classification algorithms depending on the
attributes of the input faces. Our findings reveal three critical harms: (1)
the systematic encoding of attractiveness-positive attribute associations in
T2I models; (2) gender disparities in classification systems, where women's
faces, particularly those generated with negative attributes, suffer
substantially higher misclassification rates than men's; and (3) intensifying
aesthetic constraints in newer models through age homogenization, gendered
exposure patterns, and geographic reductionism. These convergent patterns
reveal algorithmic lookism as systematic infrastructure operating across AI
vision systems, compounding existing inequalities through both representation
and recognition.
  Disclaimer: This work includes visual and textual content that reflects
stereotypical associations between physical appearance and socially constructed
attributes, including gender, race, and traits associated with social
desirability. Any such associations found in this study emerge from the biases
embedded in generative AI systems-not from empirical truths or the authors'
views.
\\ ( https://arxiv.org/abs/2601.11651 ,  42645kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11652 (*cross-listing*)
Date: Thu, 15 Jan 2026 16:46:01 GMT   (1681kb)

Title: WISP: Waste- and Interference-Suppressed Distributed Speculative LLM
  Serving at the Edge via Dynamic Drafting and SLO-Aware Batching
Authors: Xiangchen Li, Jiakun Fan, Qingyuan Wang, Dimitrios Spatharakis, Saeid
  Ghafouri, Hans Vandierendonck, Deepu John, Bo Ji, Ali R. Butt, Dimitrios S.
  Nikolopoulos
Categories: cs.DC cs.AI
Comments: 28 Pages, 11 Figures, 12 Tables
ACM-class: C.2.4; D.4.8; D.4.1; F.2.2; I.2.7; D.4.7
\\
  As Large Language Models (LLMs) become increasingly accessible to end users,
an ever-growing number of inference requests are initiated from edge devices
and computed on centralized GPU clusters. However, the resulting exponential
growth in computation workload is placing significant strain on data centers,
while edge devices remain largely underutilized, leading to imbalanced
workloads and resource inefficiency across the network. Integrating edge
devices into the LLM inference process via speculative decoding helps balance
the workload between the edge and the cloud, while maintaining lossless
prediction accuracy. In this paper, we identify and formalize two critical
bottlenecks that limit the efficiency and scalability of distributed
speculative LLM serving: Wasted Drafting Time and Verification Interference. To
address these challenges, we propose WISP, an efficient and SLO-aware
distributed LLM inference system that consists of an intelligent speculation
controller, a verification time estimator, and a verification batch scheduler.
These components collaboratively enhance drafting efficiency and optimize
verification request scheduling on the server. Extensive numerical results show
that WISP improves system capacity by up to 2.1x and 4.1x, and increases system
goodput by up to 1.94x and 3.7x, compared to centralized serving and SLED,
respectively.
\\ ( https://arxiv.org/abs/2601.11652 ,  1681kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11658 (*cross-listing*)
Date: Thu, 15 Jan 2026 20:43:44 GMT   (786kb)

Title: Towards AGI A Pragmatic Approach Towards Self Evolving Agent
Authors: Indrajit Kar, Sammy Zonunpuia, Zonunfeli Ralte
Categories: cs.CL cs.AI
\\
  Large Language Model (LLM) based agents are powerful yet fundamentally static
after deployment, lacking the ability to autonomously expand capabilities,
generate new tools, or evolve their reasoning. This work introduces a
hierarchical self-evolving multi-agent framework that integrates a Base LLM, an
operational SLM agent, a Code-Generation LLM, and a Teacher-LLM to enable
continuous adaptation. The workflow begins with the agent attempting a task
using reasoning and existing tools; if unsuccessful, it escalates to tool
synthesis through the Code-Gen LLM, and when failures persist, it triggers an
evolution phase using Curriculum Learning (CL), Reward-Based Learning (RL), or
Genetic Algorithm (GA) evolution. Using the TaskCraft dataset rich in
hierarchical tasks, tool-use traces, and difficulty scaling we evaluate these
paradigms. CL delivers fast recovery and strong generalization, RL excels on
high-difficulty tasks, and GA offers high behavioral diversity. Across all
settings, evolved agents outperform their originals, demonstrating robust,
autonomous, self-improving agentic evolution.
\\ ( https://arxiv.org/abs/2601.11658 ,  786kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11664 (*cross-listing*)
Date: Thu, 15 Jan 2026 23:32:37 GMT   (469kb)

Title: Serverless AI Security: Attack Surface Analysis and Runtime Protection
  Mechanisms for FaaS-Based Machine Learning
Authors: Chetan Pathade, Vinod Dhimam, Sheheryar Ahmad, Ilsa Lareb
Categories: cs.CR cs.AI
Comments: 17 Pages, 2 Figures, 4 Tables
\\
  Serverless computing has achieved widespread adoption, with over 70% of AWS
organizations using serverless solutions [1]. Meanwhile, machine learning
inference workloads increasingly migrate to Function-as-a-Service (FaaS)
platforms for their scalability and cost-efficiency [2], [3], [4]. However,
this convergence introduces critical security challenges, with recent reports
showing a 220% increase in AI/ML vulnerabilities [5] and serverless computing's
fragmented architecture raises new security concerns distinct from traditional
cloud deployments [6], [7]. This paper presents the first comprehensive
security analysis of machine learning workloads in serverless environments. We
systematically characterize the attack surface across five categories:
function-level vulnerabilities (cold start exploitation, dependency poisoning),
model-specific threats (API-based extraction, adversarial inputs),
infrastructure attacks (cross-function contamination, privilege escalation),
supply chain risks (malicious layers, backdoored libraries), and IAM complexity
(ephemeral nature, serverless functions). Through empirical assessments across
AWS Lambda, Azure Functions, and Google Cloud Functions, we demonstrate
real-world attack scenarios and quantify their security impact. We propose
Serverless AI Shield (SAS), a multi-layered defense framework providing
pre-deployment validation, runtime monitoring, and post-execution forensics.
Our evaluation shows SAS achieves 94% detection rates while maintaining
performance overhead below 9% for inference latency. We release an open-source
security toolkit to enable practitioners to assess and harden their serverless
AI deployments, advancing the field toward more resilient cloud-native machine
learning systems.
\\ ( https://arxiv.org/abs/2601.11664 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11666 (*cross-listing*)
Date: Fri, 16 Jan 2026 01:18:02 GMT   (4491kb)

Title: MATEX: Multi-scale Attention and Text-guided Explainability of Medical
  Vision-Language Models
Authors: Muhammad Imran, Chi Lee, and Yugyung Lee
Categories: cs.CV cs.AI
Comments: 12 pages, 3 figures, 1 table
MSC-class: 68T45, 68U10, 92C55
ACM-class: I.2.10; I.4.8; H.2.8; J.3
\\
  We introduce MATEX (Multi-scale Attention and Text-guided Explainability), a
novel framework that advances interpretability in medical vision-language
models by incorporating anatomically informed spatial reasoning. MATEX
synergistically combines multi-layer attention rollout, text-guided spatial
priors, and layer consistency analysis to produce precise, stable, and
clinically meaningful gradient attribution maps. By addressing key limitations
of prior methods, such as spatial imprecision, lack of anatomical grounding,
and limited attention granularity, MATEX enables more faithful and
interpretable model explanations. Evaluated on the MS-CXR dataset, MATEX
outperforms the state-of-the-art M2IB approach in both spatial precision and
alignment with expert-annotated findings. These results highlight MATEX's
potential to enhance trust and transparency in radiological AI applications.
\\ ( https://arxiv.org/abs/2601.11666 ,  4491kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11674 (*cross-listing*)
Date: Fri, 16 Jan 2026 05:38:48 GMT   (1083kb)

Title: Pigment Network Detection and Classification in Dermoscopic Images Using
  Directional Imaging Algorithms and Convolutional Neural Networks
Authors: M. A. Rasel, Sameem Abdul Kareem, Unaizah Obaidellah
Categories: eess.IV cs.AI cs.CV
Journal-ref: Biomedical Signal Processing and Control (2024), 106883
DOI: 10.1016/j.bspc.2024.106883
\\
  Early diagnosis of melanoma, which can save thousands of lives, relies
heavily on the analysis of dermoscopic images. One crucial diagnostic criterion
is the identification of unusual pigment network (PN). However, distinguishing
between regular (typical) and irregular (atypical) PN is challenging. This
study aims to automate the PN detection process using a directional imaging
algorithm and classify PN types using machine learning classifiers. The
directional imaging algorithm incorporates Principal Component Analysis (PCA),
contrast enhancement, filtering, and noise reduction. Applied to the PH2
dataset, this algorithm achieved a 96% success rate, which increased to 100%
after pixel intensity adjustments. We created a new dataset containing only PN
images from these results. We then employed two classifiers, Convolutional
Neural Network (CNN) and Bag of Features (BoF), to categorize PN into atypical
and typical classes. Given the limited dataset of 200 images, a simple and
effective CNN was designed, featuring two convolutional layers and two batch
normalization layers. The proposed CNN achieved 90% accuracy, 90% sensitivity,
and 89% specificity. When compared to state-of-the-art methods, our CNN
demonstrated superior performance. Our study highlights the potential of the
proposed CNN model for effective PN classification, suggesting future research
should focus on expanding datasets and incorporating additional dermatological
features to further enhance melanoma diagnosis.
\\ ( https://arxiv.org/abs/2601.11674 ,  1083kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11675 (*cross-listing*)
Date: Fri, 16 Jan 2026 06:24:59 GMT   (20880kb)

Title: Generating metamers of human scene understanding
Authors: Ritik Raina, Abe Leite, Alexandros Graikos, Seoyoung Ahn, Dimitris
  Samaras, Gregory J. Zelinsky
Categories: cs.CV cs.AI
\\
  Human vision combines low-resolution "gist" information from the visual
periphery with sparse but high-resolution information from fixated locations to
construct a coherent understanding of a visual scene. In this paper, we
introduce MetamerGen, a tool for generating scenes that are aligned with latent
human scene representations. MetamerGen is a latent diffusion model that
combines peripherally obtained scene gist information with information obtained
from scene-viewing fixations to generate image metamers for what humans
understand after viewing a scene. Generating images from both high and low
resolution (i.e. "foveated") inputs constitutes a novel image-to-image
synthesis problem, which we tackle by introducing a dual-stream representation
of the foveated scenes consisting of DINOv2 tokens that fuse detailed features
from fixated areas with peripherally degraded features capturing scene context.
To evaluate the perceptual alignment of MetamerGen generated images to latent
human scene representations, we conducted a same-different behavioral
experiment where participants were asked for a "same" or "different" response
between the generated and the original image. With that, we identify scene
generations that are indeed metamers for the latent scene representations
formed by the viewers. MetamerGen is a powerful tool for understanding scene
understanding. Our proof-of-concept analyses uncovered specific features at
multiple levels of visual processing that contributed to human judgments. While
it can generate metamers even conditioned on random fixations, we find that
high-level semantic alignment most strongly predicts metamerism when the
generated scenes are conditioned on viewers' own fixated regions.
\\ ( https://arxiv.org/abs/2601.11675 ,  20880kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11676 (*cross-listing*)
Date: Fri, 16 Jan 2026 07:37:23 GMT   (2433kb)

Title: HALO: Semantic-Aware Distributed LLM Inference in Lossy Edge Network
Authors: Peirong Zheng, Wenchao Xu, Haozhao Wang, Jinyu Chen, Xuemin Shen
Categories: cs.DC cs.AI cs.NI
Comments: Accepted by IEEE International Conference on Computer Communications
  (INFOCOM) 2026
\\
  The deployment of large language models' (LLMs) inference at the edge can
facilitate prompt service responsiveness while protecting user privacy.
However, it is critically challenged by the resource constraints of a single
edge node. Distributed inference has emerged to aggregate and leverage
computational resources across multiple devices. Yet, existing methods
typically require strict synchronization, which is often infeasible due to the
unreliable network conditions. In this paper, we propose HALO, a novel
framework that can boost the distributed LLM inference in lossy edge network.
The core idea is to enable a relaxed yet effective synchronization by
strategically allocating less critical neuron groups to unstable devices, thus
avoiding the excessive waiting time incurred by delayed packets. HALO
introduces three key mechanisms: (1) a semantic-aware predictor to assess the
significance of neuron groups prior to activation. (2) a parallel execution
scheme of neuron group loading during the model inference. (3) a load-balancing
scheduler that efficiently orchestrates multiple devices with heterogeneous
resources. Experimental results from a Raspberry Pi cluster demonstrate that
HALO achieves a 3.41x end-to-end speedup for LLaMA-series LLMs under unreliable
network conditions. It maintains performance comparable to optimal conditions
and significantly outperforms the state-of-the-art in various scenarios.
\\ ( https://arxiv.org/abs/2601.11676 ,  2433kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11683 (*cross-listing*)
Date: Fri, 16 Jan 2026 08:56:13 GMT   (3893kb)

Title: Attesting Model Lineage by Consisted Knowledge Evolution with
  Fine-Tuning Trajectory
Authors: Zhuoyi Shang, Jiasen Li, Pengzhen Chen, Yanwei Liu, Xiaoyan Gu,
  Weiping Wang
Categories: cs.CR cs.AI cs.SE
Comments: Accepted to the 35th USENIX Security Symposium (USENIX Security 2026)
ACM-class: I.2; H.1; D.2; K.5
\\
  The fine-tuning technique in deep learning gives rise to an emerging lineage
relationship among models. This lineage provides a promising perspective for
addressing security concerns such as unauthorized model redistribution and
false claim of model provenance, which are particularly pressing in
\textcolor{blue}{open-weight model} libraries where robust lineage verification
mechanisms are often lacking. Existing approaches to model lineage detection
primarily rely on static architectural similarities, which are insufficient to
capture the dynamic evolution of knowledge that underlies true lineage
relationships. Drawing inspiration from the genetic mechanism of human
evolution, we tackle the problem of model lineage attestation by verifying the
joint trajectory of knowledge evolution and parameter modification. To this
end, we propose a novel model lineage attestation framework. In our framework,
model editing is first leveraged to quantify parameter-level changes introduced
by fine-tuning. Subsequently, we introduce a novel knowledge vectorization
mechanism that refines the evolved knowledge within the edited models into
compact representations by the assistance of probe samples. The probing
strategies are adapted to different types of model families. These embeddings
serve as the foundation for verifying the arithmetic consistency of knowledge
relationships across models, thereby enabling robust attestation of model
lineage. Extensive experimental evaluations demonstrate the effectiveness and
resilience of our approach in a variety of adversarial scenarios in the real
world. Our method consistently achieves reliable lineage verification across a
broad spectrum of model types, including classifiers, diffusion models, and
large language models.
\\ ( https://arxiv.org/abs/2601.11683 ,  3893kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11684 (*cross-listing*)
Date: Fri, 16 Jan 2026 09:39:01 GMT   (384kb)

Title: Mobile-friendly Image de-noising: Hardware Conscious Optimization for
  Edge Application
Authors: Srinivas Miriyala, Sowmya Vajrala, Hitesh Kumar, Sravanth Kodavanti,
  Vikram Rajendiran
Categories: eess.IV cs.AI cs.CV
Comments: Accepted at ICASSP 2025
\\
  Image enhancement is a critical task in computer vision and photography that
is often entangled with noise. This renders the traditional Image Signal
Processing (ISP) ineffective compared to the advances in deep learning.
However, the success of such methods is increasingly associated with the ease
of their deployment on edge devices, such as smartphones. This work presents a
novel mobile-friendly network for image de-noising obtained with
Entropy-Regularized differentiable Neural Architecture Search (NAS) on a
hardware-aware search space for a U-Net architecture, which is
first-of-its-kind. The designed model has 12% less parameters, with ~2-fold
improvement in ondevice latency and 1.5-fold improvement in the memory
footprint for a 0.7% drop in PSNR, when deployed and profiled on Samsung Galaxy
S24 Ultra. Compared to the SOTA Swin-Transformer for Image Restoration, the
proposed network had competitive accuracy with ~18-fold reduction in GMACs.
Further, the network was tested successfully for Gaussian de-noising with 3
intensities on 4 benchmarks and real-world de-noising on 1 benchmark
demonstrating its generalization ability.
\\ ( https://arxiv.org/abs/2601.11684 ,  384kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11685 (*cross-listing*)
Date: Fri, 16 Jan 2026 10:09:13 GMT   (525kb)

Title: Towards Efficient Image Deblurring for Edge Deployment
Authors: Srinivas Miriyala, Sowmya Vajrala, Sravanth Kodavanti
Categories: eess.IV cs.AI cs.CV
\\
  Image deblurring is a critical stage in mobile image signal processing
pipelines, where the ability to restore fine structures and textures must be
balanced with real-time constraints on edge devices. While recent deep networks
such as transformers and activation-free architectures achieve state-of-the-art
(SOTA) accuracy, their efficiency is typically measured in FLOPs or parameters,
which do not correlate with latency on embedded hardware. We propose a
hardware-aware adaptation framework that restructures existing models through
sensitivity-guided block substitution, surrogate distillation, and
training-free multi-objective search driven by device profiling. Applied to the
36-block NAFNet baseline, the optimized variants achieve up to 55% reduction in
GMACs compared to the recent transformer-based SOTA while maintaining
competitive accuracy. Most importantly, on-device deployment yields a 1.25X
latency improvement over the baseline. Experiments on motion deblurring
(GoPro), defocus deblurring (DPDD), and auxiliary benchmarks (RealBlur-J/R,
HIDE) demonstrate the generality of the approach, while comparisons with prior
efficient baselines confirm its accuracy-efficiency trade-off. These results
establish feedback-driven adaptation as a principled strategy for bridging the
gap between algorithmic design and deployment-ready deblurring models.
\\ ( https://arxiv.org/abs/2601.11685 ,  525kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11700 (*cross-listing*)
Date: Fri, 16 Jan 2026 18:45:16 GMT   (2071kb)

Title: Telling Human and Machine Handwriting Apart
Authors: Luis A. Leiva, Moises Diaz, Nuwan T. Attygalle, Miguel A. Ferrer, and
  Rejean Plamondon
Categories: cs.CV cs.AI cs.LG
Journal-ref: IEEE Transactions on Systems, Man, and Cybernetics: Systems (
  Volume: 55, Issue: 10, October 2025)
DOI: 10.1109/TSMC.2025.3579921
\\
  Handwriting movements can be leveraged as a unique form of behavioral
biometrics, to verify whether a real user is operating a device or application.
This task can be framed as a reverse Turing test in which a computer has to
detect if an input instance has been generated by a human or artificially. To
tackle this task, we study ten public datasets of handwritten symbols (isolated
characters, digits, gestures, pointing traces, and signatures) that are
artificially reproduced using seven different synthesizers, including, among
others, the Kinematic Theory (Sigma h model), generative adversarial networks,
Transformers, and Diffusion models. We train a shallow recurrent neural network
that achieves excellent performance (98.3 percent Area Under the ROC Curve
(AUC) score and 1.4 percent equal error rate on average across all synthesizers
and datasets) using nonfeaturized trajectory data as input. In few-shot
settings, we show that our classifier achieves such an excellent performance
when trained on just 10 percent of the data, as evaluated on the remaining 90%
of the data as a test set. We further challenge our classifier in out-of-domain
settings, and observe very competitive results as well. Our work has
implications for computerized systems that need to verify human presence, and
adds an additional layer of security to keep attackers at bay.
\\ ( https://arxiv.org/abs/2601.11700 ,  2071kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11702 (*cross-listing*)
Date: Fri, 16 Jan 2026 18:56:39 GMT   (8170kb)

Title: PASTA: A Scalable Framework for Multi-Policy AI Compliance Evaluation
Authors: Yu Yang, Ig-Jae Kim, Dongwook Yoon
Categories: cs.HC cs.AI
Comments: 28 pages, 7 figures
\\
  AI compliance is becoming increasingly critical as AI systems grow more
powerful and pervasive. Yet the rapid expansion of AI policies creates
substantial burdens for resource-constrained practitioners lacking policy
expertise. Existing approaches typically address one policy at a time, making
multi-policy compliance costly. We present PASTA, a scalable compliance tool
integrating four innovations: (1) a comprehensive model-card format supporting
descriptive inputs across development stages; (2) a policy normalization
scheme; (3) an efficient LLM-powered pairwise evaluation engine with
cost-saving strategies; and (4) an interface delivering interpretable
evaluations via compliance heatmaps and actionable recommendations. Expert
evaluation shows PASTA's judgments closely align with human experts ($\rho \geq
.626$). The system evaluates five major policies in under two minutes at
approximately \$3. A user study (N = 12) confirms practitioners found outputs
easy-to-understand and actionable, introducing a novel framework for scalable
automated AI governance.
\\ ( https://arxiv.org/abs/2601.11702 ,  8170kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11713 (*cross-listing*)
Date: Fri, 16 Jan 2026 19:00:52 GMT   (1485kb)

Title: Inter-Cell Interference Rejection Based on Ultrawideband Walsh-Domain
  Wireless Autoencoding
Authors: Rodney Martinez Alonso, Cel Thys, Cedric Dehos, Yuneisy Esthela Garcia
  Guzman, Sofie Pollin
Categories: eess.SP cs.AI cs.LG cs.NI
Comments: This preprint was submitted to The 2026 EuCNC & 6G Summit
\\
  This paper proposes a novel technique for rejecting partial-in-band
inter-cell interference (ICI) in ultrawideband communication systems. We
present the design of an end-to-end wireless autoencoder architecture that
jointly optimizes the transmitter and receiver encoding/decoding in the Walsh
domain to mitigate interference from coexisting narrower-band 5G base stations.
By exploiting the orthogonality and self-inverse properties of Walsh functions,
the system distributes and learns to encode bit-words across parallel Walsh
branches. Through analytical modeling and simulation, we characterize how 5G
CPOFDM interference maps into the Walsh domain and identify optimal ratios of
transmission frequencies and sampling rate where the end-to-end autoencoder
achieves the highest rejection. Experimental results show that the proposed
autoencoder achieves up to 12 dB of ICI rejection while maintaining a low block
error rate (BLER) for the same baseline channel noise, i.e., baseline
Signal-to-Noise-Ratio (SNR) without the interference.
\\ ( https://arxiv.org/abs/2601.11713 ,  1485kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11746 (*cross-listing*)
Date: Fri, 16 Jan 2026 19:55:06 GMT   (12161kb)

Title: LIME-LLM: Probing Models with Fluent Counterfactuals, Not Broken Text
Authors: George Mihaila, Suleyman Olcay Polat, Poli Nemkova, Himanshu Sharma,
  Namratha V. Urs, Mark V. Albert
Categories: cs.CL cs.AI cs.LG
\\
  Local explanation methods such as LIME (Ribeiro et al., 2016) remain
fundamental to trustworthy AI, yet their application to NLP is limited by a
reliance on random token masking. These heuristic perturbations frequently
generate semantically invalid, out-of-distribution inputs that weaken the
fidelity of local surrogate models. While recent generative approaches such as
LLiMe (Angiulli et al., 2025b) attempt to mitigate this by employing Large
Language Models for neighborhood generation, they rely on unconstrained
paraphrasing that introduces confounding variables, making it difficult to
isolate specific feature contributions. We introduce LIME-LLM, a framework that
replaces random noise with hypothesis-driven, controlled perturbations. By
enforcing a strict "Single Mask-Single Sample" protocol and employing distinct
neutral infill and boundary infill strategies, LIME-LLM constructs fluent,
on-manifold neighborhoods that rigorously isolate feature effects. We evaluate
our method against established baselines (LIME, SHAP, Integrated Gradients) and
the generative LLiMe baseline across three diverse benchmarks: CoLA, SST-2, and
HateXplain using human-annotated rationales as ground truth. Empirical results
demonstrate that LIME-LLM establishes a new benchmark for black-box NLP
explainability, achieving significant improvements in local explanation
fidelity compared to both traditional perturbation-based methods and recent
generative alternatives.
\\ ( https://arxiv.org/abs/2601.11746 ,  12161kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11758 (*cross-listing*)
Date: Fri, 16 Jan 2026 20:22:34 GMT   (2893kb)

Title: Early Linguistic Pattern of Anxiety from Social Media Using
  Interpretable Linguistic Features: A Multi-Faceted Validation Study with
  Author-Disjoint Evaluation
Authors: Arnab Das Utsa
Categories: cs.CL cs.AI cs.LG
Comments: 9 figures, more than 1o pages
\\
  Anxiety affects hundreds of millions of individuals globally, yet large-scale
screening remains limited. Social media language provides an opportunity for
scalable detection, but current models often lack interpretability,
keyword-robustness validation, and rigorous user-level data integrity. This
work presents a transparent approach to social media-based anxiety detection
through linguistically interpretable feature-grounded modeling and cross-domain
validation. Using a substantial dataset of Reddit posts, we trained a logistic
regression classifier on carefully curated subreddits for training, validation,
and test splits. Comprehensive evaluation included feature ablation, keyword
masking experiments, and varying-density difference analyses comparing anxious
and control groups, along with external validation using clinically interviewed
participants with diagnosed anxiety disorders. The model achieved strong
performance while maintaining high accuracy even after sentiment removal or
keyword masking. Early detection using minimal post history significantly
outperformed random classification, and cross-domain analysis demonstrated
strong consistency with clinical interview data. Results indicate that
transparent linguistic features can support reliable, generalizable, and
keyword-robust anxiety detection. The proposed framework provides a
reproducible baseline for interpretable mental health screening across diverse
online contexts.
\\ ( https://arxiv.org/abs/2601.11758 ,  2893kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11762 (*cross-listing*)
Date: Fri, 16 Jan 2026 20:32:11 GMT   (880kb)

Title: Industry-Aligned Granular Topic Modeling
Authors: Sae Young Moon, Myeongjun Erik Jang, Haoyan Luo, Chunyang Xiao,
  Antonios Georgiadis, Fran Silavong
Categories: cs.CL cs.AI cs.LG
\\
  Topic modeling has extensive applications in text mining and data analysis
across various industrial sectors. Although the concept of granularity holds
significant value for business applications by providing deeper insights, the
capability of topic modeling methods to produce granular topics has not been
thoroughly explored. In this context, this paper introduces a framework called
TIDE, which primarily provides a novel granular topic modeling method based on
large language models (LLMs) as a core feature, along with other useful
functionalities for business applications, such as summarizing long documents,
topic parenting, and distillation. Through extensive experiments on a variety
of public and real-world business datasets, we demonstrate that TIDE's topic
modeling approach outperforms modern topic modeling methods, and our auxiliary
components provide valuable support for dealing with industrial business
scenarios. The TIDE framework is currently undergoing the process of being open
sourced.
\\ ( https://arxiv.org/abs/2601.11762 ,  880kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11768 (*cross-listing*)
Date: Fri, 16 Jan 2026 20:46:33 GMT   (17559kb)

Title: Lightweight Self-Supervised Detection of Fundamental Frequency and
  Accurate Probability of Voicing in Monophonic Music
Authors: Venkat Suprabath Bitra and Homayoon Beigi
Categories: eess.AS cs.AI cs.LG cs.SD eess.SP
Comments: 12 pages, 6 figures, 3 tables, and an appendix, Accepted for
  publication at ICPRAM 2026 in Marbella, Spain, on March 2, 2026
\\
  Reliable fundamental frequency (F 0) and voicing estimation is essential for
neural synthesis, yet many pitch extractors depend on large labeled corpora and
degrade under realistic recording artifacts. We propose a lightweight, fully
self-supervised framework for joint F 0 estimation and voicing inference,
designed for rapid single-instrument training from limited audio. Using
transposition-equivariant learning on CQT features, we introduce an EM-style
iterative reweighting scheme that uses Shift Cross-Entropy (SCE) consistency as
a reliability signal to suppress uninformative noisy/unvoiced frames. The
resulting weights provide confidence scores that enable pseudo-labeling for a
separate lightweight voicing classifier without manual annotations. Trained on
MedleyDB and evaluated on MDB-stem-synth ground truth, our method achieves
competitive cross-corpus performance (RPA 95.84, RCA 96.24) and demonstrates
cross-instrument generalization.
\\ ( https://arxiv.org/abs/2601.11768 ,  17559kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11776 (*cross-listing*)
Date: Fri, 16 Jan 2026 21:01:26 GMT   (5454kb)

Title: Cleansing the Artificial Mind: A Self-Reflective Detoxification
  Framework for Large Language Models
Authors: Kaituo Zhang and Zhimeng Jiang and Na Zou
Categories: cs.CL cs.AI
\\
  Recent breakthroughs in Large Language Models (LLMs) have revealed remarkable
generative capabilities and emerging self-regulatory mechanisms, including
self-correction and self-rewarding. However, current detoxification techniques
rarely exploit these built-in abilities; instead, they rely on external
modules, labor-intensive data annotation, or human intervention --factors that
hinder scalability and consistency. In this paper, we introduce a fully
self-reflective detoxification framework that harnesses the inherent capacities
of LLMs to detect, correct toxic content, and refine LLMs without external
modules and data annotation. Specifically, we propose a Toxic Signal Detector
--an internal self-identification mechanism, coupled with a systematic
intervention process to transform toxic text into its non-toxic counterpart.
This iterative procedure yields a contrastive detoxification dataset used to
fine-tune the model, enhancing its ability for safe and coherent text
generation. Experiments on benchmark datasets such as DetoxLLM and ParaDetox
show that our method achieves better detoxification performance than
state-of-the-art methods while preserving semantic fidelity. By obviating the
need for human intervention or external components, this paper reveals the
intrinsic self-detoxification ability of LLMs, offering a consistent and
effective approach for mitigating harmful content generation. Ultimately, our
findings underscore the potential for truly self-regulated language models,
paving the way for more responsible and ethically guided text generation
systems.
\\ ( https://arxiv.org/abs/2601.11776 ,  5454kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11778 (*cross-listing*)
Date: Fri, 16 Jan 2026 21:01:40 GMT   (8030kb)

Title: Translation as a Scalable Proxy for Multilingual Evaluation
Authors: Sheriff Issaka, Erick Rosas Gonzalez, Lieqi Liu, Evans Kofi Agyei,
  Lucas Bandarkar, Nanyun Peng, David Ifeoluwa Adelani, Francisco Guzm\'an,
  Saadia Gabriel
Categories: cs.CL cs.AI
\\
  The rapid proliferation of LLMs has created a critical evaluation paradox:
while LLMs claim multilingual proficiency, comprehensive non-machine-translated
benchmarks exist for fewer than 30 languages, leaving >98% of the world's 7,000
languages in an empirical void. Traditional benchmark construction faces
scaling challenges such as cost, scarcity of domain experts, and data
contamination. We evaluate the validity of a simpler alternative: can
translation quality alone indicate a model's broader multilingual capabilities?
Through systematic evaluation of 14 models (1B-72B parameters) across 9 diverse
benchmarks and 7 translation metrics, we find that translation performance is a
good indicator of downstream task success (e.g., Phi-4, median Pearson r:
MetricX = 0.89, xCOMET = 0.91, SSA-COMET = 0.87). These results suggest that
the representational abilities supporting faithful translation overlap with
those required for multilingual understanding. Translation quality, thus
emerges as a strong, inexpensive first-pass proxy of multilingual performance,
enabling a translation-first screening with targeted follow-up for specific
tasks.
\\ ( https://arxiv.org/abs/2601.11778 ,  8030kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11801 (*cross-listing*)
Date: Fri, 16 Jan 2026 22:04:49 GMT   (18026kb)

Title: RobotDesignGPT: Automated Robot Design Synthesis using Vision Language
  Models
Authors: Nitish Sontakke, K. Niranjan Kumar, Sehoon Ha
Categories: cs.RO cs.AI
\\
  Robot design is a nontrivial process that involves careful consideration of
multiple criteria, including user specifications, kinematic structures, and
visual appearance. Therefore, the design process often relies heavily on domain
expertise and significant human effort. The majority of current methods are
rule-based, requiring the specification of a grammar or a set of primitive
components and modules that can be composed to create a design. We propose a
novel automated robot design framework, RobotDesignGPT, that leverages the
general knowledge and reasoning capabilities of large pre-trained
vision-language models to automate the robot design synthesis process. Our
framework synthesizes an initial robot design from a simple user prompt and a
reference image. Our novel visual feedback approach allows us to greatly
improve the design quality and reduce unnecessary manual feedback. We
demonstrate that our framework can design visually appealing and kinematically
valid robots inspired by nature, ranging from legged animals to flying
creatures. We justify the proposed framework by conducting an ablation study
and a user study.
\\ ( https://arxiv.org/abs/2601.11801 ,  18026kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11854 (*cross-listing*)
Date: Sat, 17 Jan 2026 00:53:43 GMT   (976kb)

Title: ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented
  Dialogue System
Authors: Yifei Zhang, Hooshang Nayyeri, Rinat Khaziev, Emine Yilmaz, Gokhan
  Tur, Dilek Hakkani-T\"ur, Hari Thadakamalla
Categories: cs.CL cs.AI cs.MA
\\
  Recent advances in task-oriented dialogue (TOD) systems, driven by large
language models (LLMs) with extensive API and tool integration, have enabled
conversational agents to coordinate interleaved goals, maintain long-horizon
context, and act proactively through asynchronous execution. These capabilities
extend beyond traditional TOD systems, yet existing benchmarks lack systematic
support for evaluating such agentic behaviors. To address this gap, we
introduce ATOD, a benchmark and synthetic dialogue generation pipeline that
produces richly annotated conversations requiring long-term reasoning. ATOD
captures key characteristics of advanced TOD, including multi-goal
coordination, dependency management, memory, adaptability, and proactivity.
Building on ATOD, we propose ATOD-Eval, a holistic evaluation framework that
translates these dimensions into fine-grained metrics and supports reproducible
offline and online evaluation. We further present a strong agentic memory-based
evaluator for benchmarking on ATOD. Experiments show that ATOD-Eval enables
comprehensive assessment across task completion, agentic capability, and
response quality, and that the proposed evaluator offers a better
accuracy-efficiency tradeoff compared to existing memory- and LLM-based
approaches under this evaluation setting.
\\ ( https://arxiv.org/abs/2601.11854 ,  976kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11859 (*cross-listing*)
Date: Sat, 17 Jan 2026 01:01:53 GMT   (515kb)

Title: Cascaded Transformer for Robust and Scalable SLA Decomposition via
  Amortized Optimization
Authors: Cyril Shih-Huan Hsu
Categories: cs.NI cs.AI cs.LG cs.NE
\\
  The evolution toward 6G networks increasingly relies on network slicing to
provide tailored, End-to-End (E2E) logical networks over shared physical
infrastructures. A critical challenge is effectively decomposing E2E Service
Level Agreements (SLAs) into domain-specific SLAs, which current solutions
handle through computationally intensive, iterative optimization processes that
incur substantial latency and complexity. To address this, we introduce
Casformer, a cascaded Transformer architecture designed for fast,
optimization-free SLA decomposition. Casformer leverages historical domain
feedback encoded through domain-specific Transformer encoders in its first
layer, and integrates cross-domain dependencies using a Transformer-based
aggregator in its second layer. The model is trained under a learning paradigm
inspired by Domain-Informed Neural Networks (DINNs), incorporating
risk-informed modeling and amortized optimization to learn a stable,
forward-only SLA decomposition policy. Extensive evaluations demonstrate that
Casformer achieves improved SLA decomposition quality against state-of-the-art
optimization-based frameworks, while exhibiting enhanced scalability and
robustness under volatile and noisy network conditions. In addition, its
forward-only design reduces runtime complexity and simplifies deployment and
maintenance. These insights reveal the potential of combining amortized
optimization with Transformer-based sequence modeling to advance network
automation, providing a scalable and efficient solution suitable for real-time
SLA management in advanced 5G-and-beyond network environments.
\\ ( https://arxiv.org/abs/2601.11859 ,  515kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11863 (*cross-listing*)
Date: Sat, 17 Jan 2026 01:11:03 GMT   (838kb)

Title: Utilizing Metadata for Better Retrieval-Augmented Generation
Authors: Raquib Bin Yousuf, Shengzhe Xu, Mandar Sharma, Andrew Neeser, Chris
  Latimer, Naren Ramakrishnan
Categories: cs.IR cs.AI cs.CE cs.CL
Comments: The 48th European Conference on Information Retrieval (ECIR 2026)
\\
  Retrieval-Augmented Generation systems depend on retrieving semantically
relevant document chunks to support accurate, grounded outputs from large
language models. In structured and repetitive corpora such as regulatory
filings, chunk similarity alone often fails to distinguish between documents
with overlapping language. Practitioners often flatten metadata into input text
as a heuristic, but the impact and trade-offs of this practice remain poorly
understood. We present a systematic study of metadata-aware retrieval
strategies, comparing plain-text baselines with approaches that embed metadata
directly. Our evaluation spans metadata-as-text (prefix and suffix), a
dual-encoder unified embedding that fuses metadata and content in a single
index, dual-encoder late-fusion retrieval, and metadata-aware query
reformulation. Across multiple retrieval metrics and question types, we find
that prefixing and unified embeddings consistently outperform plain-text
baselines, with the unified at times exceeding prefixing while being easier to
maintain. Beyond empirical comparisons, we analyze embedding space, showing
that metadata integration improves effectiveness by increasing intra-document
cohesion, reducing inter-document confusion, and widening the separation
between relevant and irrelevant chunks. Field-level ablations show that
structural cues provide strong disambiguating signals. Our code, evaluation
framework, and the RAGMATE-10K dataset are publicly hosted.
\\ ( https://arxiv.org/abs/2601.11863 ,  838kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11876 (*cross-listing*)
Date: Sat, 17 Jan 2026 02:05:05 GMT   (819kb)

Title: AI for Green Spaces: Leveraging Autonomous Navigation and Computer
  Vision for Park Litter Removal
Authors: Christopher Kao, Akhil Pathapati, James Davis
Categories: cs.RO cs.AI cs.CV cs.SY eess.SY
Comments: Published in IEEE/SICE SII 2025
Journal-ref: 2025 IEEE/SICE International Symposium on System Integration
  (SII), Munich, Germany, 2025, pp. 171-176
DOI: 10.1109/SII59315.2025.10870583
\\
  There are 50 billion pieces of litter in the U.S. alone. Grass fields
contribute to this problem because picnickers tend to leave trash on the field.
We propose building a robot that can autonomously navigate, identify, and pick
up trash in parks. To autonomously navigate the park, we used a Spanning Tree
Coverage (STC) algorithm to generate a coverage path the robot could follow. To
navigate this path, we successfully used Real-Time Kinematic (RTK) GPS, which
provides a centimeter-level reading every second. For computer vision, we
utilized the ResNet50 Convolutional Neural Network (CNN), which detects trash
with 94.52% accuracy. For trash pickup, we tested multiple design concepts. We
select a new pickup mechanism that specifically targets the trash we encounter
on the field. Our solution achieved an overall success rate of 80%,
demonstrating that autonomous trash pickup robots on grass fields are a viable
solution.
\\ ( https://arxiv.org/abs/2601.11876 ,  819kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11907 (*cross-listing*)
Date: Sat, 17 Jan 2026 04:47:47 GMT   (1590kb)

Title: Towards Airborne Object Detection: A Deep Learning Analysis
Authors: Prosenjit Chatterjee, ANK Zaman
Categories: cs.CV cs.AI cs.LG cs.SE
\\
  The rapid proliferation of airborne platforms, including commercial aircraft,
drones, and UAVs, has intensified the need for real-time, automated threat
assessment systems. Current approaches depend heavily on manual monitoring,
resulting in limited scalability and operational inefficiencies. This work
introduces a dual-task model based on EfficientNetB4 capable of performing
airborne object classification and threat-level prediction simultaneously. To
address the scarcity of clean, balanced training data, we constructed the AODTA
Dataset by aggregating and refining multiple public sources. We benchmarked our
approach on both the AVD Dataset and the newly developed AODTA Dataset and
further compared performance against a ResNet-50 baseline, which consistently
underperformed EfficientNetB4. Our EfficientNetB4 model achieved 96% accuracy
in object classification and 90% accuracy in threat-level prediction,
underscoring its promise for applications in surveillance, defense, and
airspace management. Although the title references detection, this study
focuses specifically on classification and threat-level inference using
pre-localized airborne object images provided by existing datasets.
\\ ( https://arxiv.org/abs/2601.11907 ,  1590kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11913 (*cross-listing*)
Date: Sat, 17 Jan 2026 05:16:23 GMT   (7819kb)

Title: LSTM-MAS: A Long Short-Term Memory Inspired Multi-Agent System for
  Long-Context Understanding
Authors: Yichen Jiang, Peng Ye, Jiakang Yuan, Chongjun Tu, Lei Bai, Tao Chen
Categories: cs.CL cs.AI
Comments: 12 pages, 5 figures
MSC-class: 68T42
ACM-class: I.2.11
\\
  Effectively processing long contexts remains a fundamental yet unsolved
challenge for large language models (LLMs). Existing single-LLM-based methods
primarily reduce the context window or optimize the attention mechanism, but
they often encounter additional computational costs or constrained expanded
context length. While multi-agent-based frameworks can mitigate these
limitations, they remain susceptible to the accumulation of errors and the
propagation of hallucinations. In this work, we draw inspiration from the Long
Short-Term Memory (LSTM) architecture to design a Multi-Agent System called
LSTM-MAS, emulating LSTM's hierarchical information flow and gated memory
mechanisms for long-context understanding. Specifically, LSTM-MAS organizes
agents in a chained architecture, where each node comprises a worker agent for
segment-level comprehension, a filter agent for redundancy reduction, a judge
agent for continuous error detection, and a manager agent for globally
regulates information propagation and retention, analogous to LSTM and its
input gate, forget gate, constant error carousel unit, and output gate. These
novel designs enable controlled information transfer and selective long-term
dependency modeling across textual segments, which can effectively avoid error
accumulation and hallucination propagation. We conducted an extensive
evaluation of our method. Compared with the previous best multi-agent approach,
CoA, our model achieves improvements of 40.93%, 43.70%,121.57% and 33.12%, on
NarrativeQA, Qasper, HotpotQA, and MuSiQue, respectively.
\\ ( https://arxiv.org/abs/2601.11913 ,  7819kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11920 (*cross-listing*)
Date: Sat, 17 Jan 2026 05:43:17 GMT   (2025kb)

Title: Enhancing LLM-Based Data Annotation with Error Decomposition
Authors: Zhen Xu, Vedant Khatri, Yijun Dai, Xiner Liu, Siyan Li, Xuanming
  Zhang, Renzhe Yu
Categories: cs.CL cs.AI
\\
  Large language models offer a scalable alternative to human coding for data
annotation tasks, enabling the scale-up of research across data-intensive
domains. While LLMs are already achieving near-human accuracy on objective
annotation tasks, their performance on subjective annotation tasks, such as
those involving psychological constructs, is less consistent and more prone to
errors. Standard evaluation practices typically collapse all annotation errors
into a single alignment metric, but this simplified approach may obscure
different kinds of errors that affect final analytical conclusions in different
ways. Here, we propose a diagnostic evaluation paradigm that incorporates a
human-in-the-loop step to separate task-inherent ambiguity from model-driven
inaccuracies and assess annotation quality in terms of their potential
downstream impacts. We refine this paradigm on ordinal annotation tasks, which
are common in subjective annotation. The refined paradigm includes: (1) a
diagnostic taxonomy that categorizes LLM annotation errors along two
dimensions: source (model-specific vs. task-inherent) and type (boundary
ambiguity vs. conceptual misidentification); (2) a lightweight human annotation
test to estimate task-inherent ambiguity from LLM annotations; and (3) a
computational method to decompose observed LLM annotation errors following our
taxonomy. We validate this paradigm on four educational annotation tasks,
demonstrating both its conceptual validity and practical utility.
Theoretically, our work provides empirical evidence for why excessively high
alignment is unrealistic in specific annotation tasks and why single alignment
metrics inadequately reflect the quality of LLM annotations. In practice, our
paradigm can be a low-cost diagnostic tool that assesses the suitability of a
given task for LLM annotation and provides actionable insights for further
technical optimization.
\\ ( https://arxiv.org/abs/2601.11920 ,  2025kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11935 (*cross-listing*)
Date: Sat, 17 Jan 2026 06:50:51 GMT   (101kb)

Title: Big Data Workload Profiling for Energy-Aware Cloud Resource Management
Authors: Milan Parikh, Aniket Abhishek Soni, Sneja Mitinbhai Shah, Ayush Raj
  Jha
Categories: cs.DC cs.AI cs.SE
Comments: 10 pages, 3 figures. Accepted and presented at the 2026 International
  Conference on Data Analytics for Sustainability and Engineering Technology
  (DASET 2026), Track: Big Data and Machine Learning Applications
MSC-class: 68M20, 68M14
ACM-class: C.4; D.4.8; D.4.1
\\
  Cloud data centers face increasing pressure to reduce operational energy
consumption as big data workloads continue to grow in scale and complexity.
This paper presents a workload aware and energy efficient scheduling framework
that profiles CPU utilization, memory demand, and storage IO behavior to guide
virtual machine placement decisions. By combining historical execution logs
with real time telemetry, the proposed system predicts the energy and
performance impact of candidate placements and enables adaptive consolidation
while preserving service level agreement compliance. The framework is evaluated
using representative Hadoop MapReduce, Spark MLlib, and ETL workloads deployed
on a multi node cloud testbed. Experimental results demonstrate consistent
energy savings of 15 to 20 percent compared to a baseline scheduler, with
negligible performance degradation. These findings highlight workload profiling
as a practical and scalable strategy for improving the sustainability of cloud
based big data processing environments.
\\ ( https://arxiv.org/abs/2601.11935 ,  101kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11956 (*cross-listing*)
Date: Sat, 17 Jan 2026 08:18:38 GMT   (1225kb)

Title: Double-Calibration: Towards Trustworthy LLMs via Calibrating Knowledge
  and Reasoning Confidence
Authors: Yuyin Lu, Ziran Liang, Yanghui Rao, Wenqi Fan, Fu Lee Wang, Qing Li
Categories: cs.CL cs.AI
\\
  Trustworthy reasoning in Large Language Models (LLMs) is challenged by their
propensity for hallucination. While augmenting LLMs with Knowledge Graphs (KGs)
improves factual accuracy, existing KG-augmented methods fail to quantify
epistemic uncertainty in both the retrieved evidence and LLMs' reasoning. To
bridge this gap, we introduce DoublyCal, a framework built on a novel
double-calibration principle. DoublyCal employs a lightweight proxy model to
first generate KG evidence alongside a calibrated evidence confidence. This
calibrated supporting evidence then guides a black-box LLM, yielding final
predictions that are not only more accurate but also well-calibrated, with
confidence scores traceable to the uncertainty of the supporting evidence.
Experiments on knowledge-intensive benchmarks show that DoublyCal significantly
improves both the accuracy and confidence calibration of black-box LLMs with
low token cost.
\\ ( https://arxiv.org/abs/2601.11956 ,  1225kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11969 (*cross-listing*)
Date: Sat, 17 Jan 2026 09:04:53 GMT   (951kb)

Title: $\texttt{MemoryRewardBench}$: Benchmarking Reward Models for Long-Term
  Memory Management in Large Language Models
Authors: Zecheng Tang, Baibei Ji, Ruoxi Sun, Haitian Wang, WangJie You, Zhang
  Yijun, Wenpeng Zhu, Ji Qi, Juntao Li, Min Zhang
Categories: cs.CL cs.AI
\\
  Existing works increasingly adopt memory-centric mechanisms to process long
contexts in a segment manner, and effective memory management is one of the key
capabilities that enables large language models to effectively propagate
information across the entire sequence. Therefore, leveraging reward models
(RMs) to automatically and reliably evaluate memory quality is critical. In
this work, we introduce $\texttt{MemoryRewardBench}$, the first benchmark to
systematically study the ability of RMs to evaluate long-term memory management
processes. $\texttt{MemoryRewardBench}$ covers both long-context comprehension
and long-form generation tasks, featuring 10 distinct settings with different
memory management patterns, with context length ranging from 8K to 128K tokens.
Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap
between open-source and proprietary models, with newer-generation models
consistently outperforming their predecessors regardless of parameter count. We
further expose the capabilities and fundamental limitations of current RMs in
evaluating LLM memory management across diverse settings.
\\ ( https://arxiv.org/abs/2601.11969 ,  951kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11995 (*cross-listing*)
Date: Sat, 17 Jan 2026 10:13:07 GMT   (2655kb)

Title: Learning Audio-Visual Embeddings with Inferred Latent Interaction Graphs
Authors: Donghuo Zeng and Hao Niu and Yanan Wang and Masato Taya
Categories: cs.MM cs.AI cs.IR cs.LG cs.SD
Comments: 16 pages, 5 figures, 2 tables
\\
  Learning robust audio-visual embeddings requires bringing genuinely related
audio and visual signals together while filtering out incidental co-occurrences
- background noise, unrelated elements, or unannotated events. Most contrastive
and triplet-loss methods use sparse annotated labels per clip and treat any
co-occurrence as semantic similarity. For example, a video labeled "train"
might also contain motorcycle audio and visual, because "motorcycle" is not the
chosen annotation; standard methods treat these co-occurrences as negatives to
true motorcycle anchors elsewhere, creating false negatives and missing true
cross-modal dependencies. We propose a framework that leverages soft-label
predictions and inferred latent interactions to address these issues: (1)
Audio-Visual Semantic Alignment Loss (AV-SAL) trains a teacher network to
produce aligned soft-label distributions across modalities, assigning nonzero
probability to co-occurring but unannotated events and enriching the
supervision signal. (2) Inferred Latent Interaction Graph (ILI) applies the
GRaSP algorithm to teacher soft labels to infer a sparse, directed dependency
graph among classes. This graph highlights directional dependencies (e.g.,
"Train (visual)" -> "Motorcycle (audio)") that expose likely semantic or
conditional relationships between classes; these are interpreted as estimated
dependency patterns. (3) Latent Interaction Regularizer (LIR): A student
network is trained with both metric loss and a regularizer guided by the ILI
graph, pulling together embeddings of dependency-linked but unlabeled pairs in
proportion to their soft-label probabilities. Experiments on AVE and VEGAS
benchmarks show consistent improvements in mean average precision (mAP),
demonstrating that integrating inferred latent interactions into embedding
learning enhances robustness and semantic coherence.
\\ ( https://arxiv.org/abs/2601.11995 ,  2655kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11998 (*cross-listing*)
Date: Sat, 17 Jan 2026 10:19:57 GMT   (617kb)

Title: Hybrid IDS Using Signature-Based and Anomaly-Based Detection
Authors: Messaouda Boutassetta, Amina Makhlouf, Newfel Messaoudi, Abdelmadjid
  Benmachiche and Ines Boutabia
Categories: cs.CR cs.AI
Comments: 7 pages,The Second National Conference on Artificial Intelligence and
  Information Technologies (NCAIIT25)
\\
  Intrusion detection systems (IDS) are essential for protecting computer
systems and networks against a wide range of cyber threats that continue to
evolve over time. IDS are commonly categorized into two main types, each with
its own strengths and limitations, such as difficulty in detecting previously
unseen attacks and the tendency to generate high false positive rates. This
paper presents a comprehensive survey and a conceptual overview of Hybrid IDS,
which integrate signature-based and anomaly-based detection techniques to
enhance attack detection capabilities. The survey examines recent research on
Hybrid IDS, classifies existing models into functional categories, and
discusses their advantages, limitations, and application domains, including
financial systems, air traffic control, and social networks. In addition,
recent trends in Hybrid IDS research, such as machine learning-based approaches
and cloud-based deployments, are reviewed. Finally, this work outlines
potential future research directions aimed at developing more cost-effective
Hybrid IDS solutions with improved ability to detect emerging and sophisticated
cyberattacks.
\\ ( https://arxiv.org/abs/2601.11998 ,  617kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12003 (*cross-listing*)
Date: Sat, 17 Jan 2026 10:42:44 GMT   (192kb)

Title: Robust Verification of Concurrent Stochastic Games
Authors: Angel Y. He, David Parker
Categories: cs.LO cs.AI cs.GT cs.MA cs.SY eess.SY
Comments: Extended version of a paper accepted to TACAS 2026. Main text: 17
  pages, 2 figures, 2 tables; Appendix: 37 pages, 3 figures, 3 tables
ACM-class: F.4.1; G.4; I.2.8
\\
  Autonomous systems often operate in multi-agent settings and need to make
concurrent, strategic decisions, typically in uncertain environments.
Verification and control problems for these systems can be tackled with
concurrent stochastic games (CSGs), but this model requires transition
probabilities to be precisely specified - an unrealistic requirement in many
real-world settings. We introduce *robust CSGs* and their subclass *interval
CSGs* (ICSGs), which capture epistemic uncertainty about transition
probabilities in CSGs. We propose a novel framework for *robust* verification
of these models under worst-case assumptions about transition uncertainty.
Specifically, we develop the underlying theoretical foundations and efficient
algorithms, for finite- and infinite-horizon objectives in both zero-sum and
nonzero-sum settings, the latter based on (social-welfare optimal) Nash
equilibria. We build an implementation in the PRISM-games model checker and
demonstrate the feasibility of robust verification of ICSGs across a selection
of large benchmarks.
\\ ( https://arxiv.org/abs/2601.12003 ,  192kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12019 (*cross-listing*)
Date: Sat, 17 Jan 2026 11:57:23 GMT   (1982kb)

Title: Acting Flatterers via LLMs Sycophancy: Combating Clickbait with LLMs
  Opposing-Stance Reasoning
Authors: Chaowei Zhang, Xiansheng Luo, Zewei Zhang, Yi Zhu, Jipeng Qiang, and
  Longwei Wang
Categories: cs.CL cs.AI
\\
  The widespread proliferation of online content has intensified concerns about
clickbait, deceptive or exaggerated headlines designed to attract attention.
While Large Language Models (LLMs) offer a promising avenue for addressing this
issue, their effectiveness is often hindered by Sycophancy, a tendency to
produce reasoning that matches users' beliefs over truthful ones, which
deviates from instruction-following principles. Rather than treating sycophancy
as a flaw to be eliminated, this work proposes a novel approach that initially
harnesses this behavior to generate contrastive reasoning from opposing
perspectives. Specifically, we design a Self-renewal Opposing-stance Reasoning
Generation (SORG) framework that prompts LLMs to produce high-quality agree and
disagree reasoning pairs for a given news title without requiring ground-truth
labels. To utilize the generated reasoning, we develop a local Opposing
Reasoning-based Clickbait Detection (ORCD) model that integrates three BERT
encoders to represent the title and its associated reasoning. The model
leverages contrastive learning, guided by soft labels derived from
LLM-generated credibility scores, to enhance detection robustness. Experimental
evaluations on three benchmark datasets demonstrate that our method
consistently outperforms LLM prompting, fine-tuned smaller language models, and
state-of-the-art clickbait detection baselines.
\\ ( https://arxiv.org/abs/2601.12019 ,  1982kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12042 (*cross-listing*)
Date: Sat, 17 Jan 2026 13:02:41 GMT   (1130kb)

Title: Less Is More -- Until It Breaks: Security Pitfalls of Vision Token
  Compression in Large Vision-Language Models
Authors: Xiaomei Zhang, Zhaoxi Zhang, Leo Yu Zhang, Yanjun Zhang, Guanhong Tao,
  Shirui Pan
Categories: cs.CR cs.AI
\\
  Visual token compression is widely adopted to improve the inference
efficiency of Large Vision-Language Models (LVLMs), enabling their deployment
in latency-sensitive and resource-constrained scenarios. However, existing work
has mainly focused on efficiency and performance, while the security
implications of visual token compression remain largely unexplored. In this
work, we first reveal that visual token compression substantially degrades the
robustness of LVLMs: models that are robust under uncompressed inference become
highly vulnerable once compression is enabled. These vulnerabilities are
state-specific; failure modes emerge only in the compressed setting and
completely disappear when compression is disabled, making them particularly
hidden and difficult to diagnose. By analyzing the key stages of the
compression process, we identify instability in token importance ranking as the
primary cause of this robustness degradation. Small and imperceptible
perturbations can significantly alter token rankings, leading the compression
mechanism to mistakenly discard task-critical information and ultimately
causing model failure. Motivated by this observation, we propose a
Compression-Aware Attack to systematically study and exploit this
vulnerability. CAA directly targets the token selection mechanism and induces
failures exclusively under compressed inference. We further extend this
approach to more realistic black-box settings and introduce Transfer CAA, where
neither the target model nor the compression configuration is accessible. We
further evaluate potential defenses and find that they provide only limited
protection. Extensive experiments across models, datasets, and compression
methods show that visual token compression significantly undermines robustness,
revealing a previously overlooked efficiency-security trade-off.
\\ ( https://arxiv.org/abs/2601.12042 ,  1130kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12049 (*cross-listing*)
Date: Sat, 17 Jan 2026 13:28:02 GMT   (1897kb)

Title: \textit{FocaLogic}: Logic-Based Interpretation of Visual Model Decisions
Authors: Chenchen Zhao and Muxi Chen and Qiang Xu
Categories: cs.CV cs.AI
Comments: 12 pages, 13 figures
\\
  Interpretability of modern visual models is crucial, particularly in
high-stakes applications. However, existing interpretability methods typically
suffer from either reliance on white-box model access or insufficient
quantitative rigor. To address these limitations, we introduce FocaLogic, a
novel model-agnostic framework designed to interpret and quantify visual model
decision-making through logic-based representations. FocaLogic identifies
minimal interpretable subsets of visual regions-termed visual focuses-that
decisively influence model predictions. It translates these visual focuses into
precise and compact logical expressions, enabling transparent and structured
interpretations. Additionally, we propose a suite of quantitative metrics,
including focus precision, recall, and divergence, to objectively evaluate
model behavior across diverse scenarios. Empirical analyses demonstrate
FocaLogic's capability to uncover critical insights such as training-induced
concentration, increasing focus accuracy through generalization, and anomalous
focuses under biases and adversarial attacks. Overall, FocaLogic provides a
systematic, scalable, and quantitative solution for interpreting visual models.
\\ ( https://arxiv.org/abs/2601.12049 ,  1897kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12053 (*cross-listing*)
Date: Sat, 17 Jan 2026 13:38:51 GMT   (1643kb)

Title: A New Strategy for Artificial Intelligence: Training Foundation Models
  Directly on Human Brain Data
Authors: Ma\"el Donoso
Categories: q-bio.NC cs.AI cs.LG
\\
  While foundation models have achieved remarkable results across a diversity
of domains, they still rely on human-generated data, such as text, as a
fundamental source of knowledge. However, this data is ultimately the product
of human brains, the filtered projection of a deeper neural complexity. In this
paper, we explore a new strategy for artificial intelligence: moving beyond
surface-level statistical regularities by training foundation models directly
on human brain data. We hypothesize that neuroimaging data could open a window
into elements of human cognition that are not accessible through observable
actions, and argue that this additional knowledge could be used, alongside
classical training data, to overcome some of the current limitations of
foundation models. While previous research has demonstrated the possibility to
train classical machine learning or deep learning models on neural patterns,
this path remains largely unexplored for high-level cognitive functions. Here,
we classify the current limitations of foundation models, as well as the
promising brain regions and cognitive processes that could be leveraged to
address them, along four levels: perception, valuation, execution, and
integration. Then, we propose two methods that could be implemented to
prioritize the use of limited neuroimaging data for strategically chosen,
high-value steps in foundation model training: reinforcement learning from
human brain (RLHB) and chain of thought from human brain (CoTHB). We also
discuss the potential implications for agents, artificial general intelligence,
and artificial superintelligence, as well as the ethical, social, and technical
challenges and opportunities. We argue that brain-trained foundation models
could represent a realistic and effective middle ground between continuing to
scale current architectures and exploring alternative, neuroscience-inspired
solutions.
\\ ( https://arxiv.org/abs/2601.12053 ,  1643kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12055 (*cross-listing*)
Date: Sat, 17 Jan 2026 13:47:41 GMT   (4847kb)

Title: Automating Parameter Selection in Deep Image Prior for Fluorescence
  Microscopy Image Denoising via Similarity-Based Parameter Transfer
Authors: Lina Meyer and Felix Wissel and Tobias Knopp and Susanne Pfefferle and
  Ralf Fliegert and Maximilian Sandmann and Liana Uebler and Franziska M\"ockl
  and Bj\"orn-Philipp Diercks and David Lohr and Ren\'e Werner
Categories: cs.CV cs.AI cs.LG
\\
  Unsupervised deep image prior (DIP) addresses shortcomings of training data
requirements and limited generalization associated with supervised deep
learning. The performance of DIP depends on the network architecture and the
stopping point of its iterative process. Optimizing these parameters for a new
image requires time, restricting DIP application in domains where many images
need to be processed. Focusing on fluorescence microscopy data, we hypothesize
that similar images share comparable optimal parameter configurations for
DIP-based denoising, potentially enabling optimization-free DIP for
fluorescence microscopy. We generated a calibration (n=110) and validation set
(n=55) of semantically different images from an open-source dataset for a
network architecture search targeted towards ideal U-net architectures and
stopping points. The calibration set represented our transfer basis. The
validation set enabled the assessment of which image similarity criterion
yields the best results. We then implemented AUTO-DIP, a pipeline for automatic
parameter transfer, and compared it to the originally published DIP
configuration (baseline) and a state-of-the-art image-specific variational
denoising approach. We show that a parameter transfer from the calibration
dataset to a test image based on only image metadata similarity (e.g.,
microscope type, imaged specimen) leads to similar and better performance than
a transfer based on quantitative image similarity measures. AUTO-DIP
outperforms the baseline DIP (DIP with original DIP parameters) as well as the
variational denoising approaches for several open-source test datasets of
varying complexity, particularly for very noisy inputs. Applications to locally
acquired fluorescence microscopy images further proved superiority of AUTO-DIP.
\\ ( https://arxiv.org/abs/2601.12055 ,  4847kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12061 (*cross-listing*)
Date: Sat, 17 Jan 2026 14:17:13 GMT   (50kb)

Title: Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs
  Annotation: LLM-Assisted and Gold-Label-Free Evaluation
Authors: Jinsook Lee, Kirk Vanacore, Zhuqian Zhou, Jeanine Grutter, Rene F.
  Kizilcec
Categories: cs.CL cs.AI
Comments: Under Review for ACL 2026
\\
  Dialogue Act (DA) annotation typically treats communicative or pedagogical
intent as localized to individual utterances or turns. This leads annotators to
agree on the underlying action while disagreeing on segment boundaries,
reducing apparent reliability. We propose codebook-injected segmentation, which
conditions boundary decisions on downstream annotation criteria, and evaluate
LLM-based segmenters against standard and retrieval-augmented baselines. To
assess these without gold labels, we introduce evaluation metrics for span
consistency, distinctiveness, and human-AI distributional agreement. We found
DA-awareness produces segments that are internally more consistent than
text-only baselines. While LLMs excel at creating construct-consistent spans,
coherence-based baselines remain superior at detecting global shifts in
dialogue flow. Across two datasets, no single segmenter dominates. Improvements
in within-segment coherence frequently trade off against boundary
distinctiveness and human-AI distributional agreement. These results highlight
segmentation as a consequential design choice that should be optimized for
downstream objectives rather than a single performance score.
\\ ( https://arxiv.org/abs/2601.12061 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12068 (*cross-listing*)
Date: Sat, 17 Jan 2026 14:33:01 GMT   (649kb)

Title: Bridging the Gap in Bangla Healthcare: Machine Learning Based Disease
  Prediction Using a Symptoms-Disease Dataset
Authors: Rowzatul Zannat, Abdullah Al Shafi, Abdul Muntakim
Categories: cs.CL cs.AI
DOI: 10.1109/ECCE64574.2025.11012950
\\
  Increased access to reliable health information is essential for
non-English-speaking populations, yet resources in Bangla for disease
prediction remain limited. This study addresses this gap by developing a
comprehensive Bangla symptoms-disease dataset containing 758 unique
symptom-disease relationships spanning 85 diseases. To ensure transparency and
reproducibility, we also make our dataset publicly available. The dataset
enables the prediction of diseases based on Bangla symptom inputs, supporting
healthcare accessibility for Bengali-speaking populations. Using this dataset,
we evaluated multiple machine learning models to predict diseases based on
symptoms provided in Bangla and analyzed their performance on our dataset. Both
soft and hard voting ensemble approaches combining top-performing models
achieved 98\% accuracy, demonstrating superior robustness and generalization.
Our work establishes a foundational resource for disease prediction in Bangla,
paving the way for future advancements in localized health informatics and
diagnostic tools. This contribution aims to enhance equitable access to health
information for Bangla-speaking communities, particularly for early disease
detection and healthcare interventions.
\\ ( https://arxiv.org/abs/2601.12068 ,  649kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12082 (*cross-listing*)
Date: Sat, 17 Jan 2026 15:19:40 GMT   (1076kb)

Title: Conditional Random Fields for Interactive Refinement of
  Histopathological Predictions
Authors: Tiffanie Godelaine and Maxime Zanella and Karim El Khoury and Sa\"id
  Mahmoudi and Beno\^it Macq and Christophe De Vleeschouwer
Categories: cs.CV cs.AI
\\
  Assisting pathologists in the analysis of histopathological images has high
clinical value, as it supports cancer detection and staging. In this context,
histology foundation models have recently emerged. Among them, Vision-Language
Models (VLMs) provide strong yet imperfect zero-shot predictions. We propose to
refine these predictions by adapting Conditional Random Fields (CRFs) to
histopathological applications, requiring no additional model training. We
present HistoCRF, a CRF-based framework, with a novel definition of the
pairwise potential that promotes label diversity and leverages expert
annotations. We consider three experiments: without annotations, with expert
annotations, and with iterative human-in-the-loop annotations that
progressively correct misclassified patches. Experiments on five patch-level
classification datasets covering different organs and diseases demonstrate
average accuracy gains of 16.0% without annotations and 27.5% with only 100
annotations, compared to zero-shot predictions. Moreover, integrating a human
in the loop reaches a further gain of 32.6% with the same number of
annotations. The code will be made available on
https://github.com/tgodelaine/HistoCRF.
\\ ( https://arxiv.org/abs/2601.12082 ,  1076kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12099 (*cross-listing*)
Date: Sat, 17 Jan 2026 16:39:07 GMT   (811kb)

Title: Large language models struggle with ethnographic text annotation
Authors: Leonardo S. Goodall, Dor Shilton, Daniel A. Mullins, Harvey Whitehouse
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) have shown promise for automated text
annotation, raising hopes that they might accelerate cross-cultural research by
extracting structured data from ethnographic texts. We evaluated 7
state-of-the-art LLMs on their ability to annotate 121 ritual features across
567 ethnographic excerpts. Performance was limited, falling well below levels
required for reliable automated annotation. Longer texts, features requiring
ordinal distinctions, and ambiguous constructs proved particularly difficult.
Human inter-coder reliability set an approximate ceiling on LLM accuracy:
features that human coders found difficult to agree upon were also difficult
for LLMs. Yet even on features where humans reliably agreed, models fell short
of human performance. Our findings suggest that LLMs cannot yet substitute for
human expertise in ethnographic annotation.
\\ ( https://arxiv.org/abs/2601.12099 ,  811kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12104 (*cross-listing*)
Date: Sat, 17 Jan 2026 16:59:41 GMT   (212kb)

Title: Powerful Training-Free Membership Inference Against Autoregressive
  Language Models
Authors: David Ili\'c, David Stanojevi\'c, Kostadin Cvejoski
Categories: cs.CL cs.AI cs.CR
Comments: 9 pages, 2 figures; appendix with additional experiments and
  derivations
ACM-class: I.2.7; K.4.1
\\
  Fine-tuned language models pose significant privacy risks, as they may
memorize and expose sensitive information from their training data. Membership
inference attacks (MIAs) provide a principled framework for auditing these
risks, yet existing methods achieve limited detection rates, particularly at
the low false-positive thresholds required for practical privacy auditing. We
present EZ-MIA, a membership inference attack that exploits a key observation:
memorization manifests most strongly at error positions, specifically tokens
where the model predicts incorrectly yet still shows elevated probability for
training examples. We introduce the Error Zone (EZ) score, which measures the
directional imbalance of probability shifts at error positions relative to a
pretrained reference model. This principled statistic requires only two forward
passes per query and no model training of any kind. On WikiText with GPT-2,
EZ-MIA achieves 3.8x higher detection than the previous state-of-the-art under
identical conditions (66.3% versus 17.5% true positive rate at 1% false
positive rate), with near-perfect discrimination (AUC 0.98). At the stringent
0.1% FPR threshold critical for real-world auditing, we achieve 8x higher
detection than prior work (14.0% versus 1.8%), requiring no reference model
training. These gains extend to larger architectures: on AG News with
Llama-2-7B, we achieve 3x higher detection (46.7% versus 15.8% TPR at 1% FPR).
These results establish that privacy risks of fine-tuned language models are
substantially greater than previously understood, with implications for both
privacy auditing and deployment decisions. Code is available at
https://github.com/JetBrains-Research/ez-mia.
\\ ( https://arxiv.org/abs/2601.12104 ,  212kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12132 (*cross-listing*)
Date: Sat, 17 Jan 2026 18:25:19 GMT   (2677kb)

Title: Bengali Text Classification: An Evaluation of Large Language Model
  Approaches
Authors: Md Mahmudul Hoque, Md Mehedi Hassain, Md Hojaifa Tanvir, Rahul Nandy
Categories: cs.CL cs.AI
\\
  Bengali text classification is a Significant task in natural language
processing (NLP), where text is categorized into predefined labels. Unlike
English, Bengali faces challenges due to the lack of extensive annotated
datasets and pre-trained language models. This study explores the effectiveness
of large language models (LLMs) in classifying Bengali newspaper articles. The
dataset used, obtained from Kaggle, consists of articles from Prothom Alo, a
major Bangladeshi newspaper. Three instruction-tuned LLMs LLaMA 3.1 8B
Instruct, LLaMA 3.2 3B Instruct, and Qwen 2.5 7B Instruct were evaluated for
this task under the same classification framework. Among the evaluated models,
Qwen 2.5 achieved the highest classification accuracy of 72%, showing
particular strength in the "Sports" category. In comparison, LLaMA 3.1 and
LLaMA 3.2 attained accuracies of 53% and 56%, respectively. The findings
highlight the effectiveness of LLMs in Bengali text classification, despite the
scarcity of resources for Bengali NLP. Future research will focus on exploring
additional models, addressing class imbalance issues, and refining fine-tuning
approaches to improve classification performance.
\\ ( https://arxiv.org/abs/2601.12132 ,  2677kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12134 (*cross-listing*)
Date: Sat, 17 Jan 2026 18:32:54 GMT   (2547kb)

Title: Human-Human-AI Triadic Programming: Uncovering the Role of AI Agent and
  the Value of Human Partner in Collaborative Learning
Authors: Taufiq Daryanto, Xiaohan Ding, Kaike Ping, Lance T. Wilhelm, Yan Chen,
  Chris Brown, Eugenia H. Rho
Categories: cs.HC cs.AI
\\
  As AI assistance becomes embedded in programming practice, researchers have
increasingly examined how these systems help learners generate code and work
more efficiently. However, these studies often position AI as a replacement for
human collaboration and overlook the social and learning-oriented aspects that
emerge in collaborative programming. Our work introduces human-human-AI (HHAI)
triadic programming, where an AI agent serves as an additional collaborator
rather than a substitute for a human partner. Through a within-subjects study
with 20 participants, we show that triadic collaboration enhances collaborative
learning and social presence compared to the dyadic human-AI (HAI) baseline. In
the triadic HHAI conditions, participants relied significantly less on
AI-generated code in their work. This effect was strongest in the HHAI-shared
condition, where participants had an increased sense of responsibility to
understand AI suggestions before applying them. These findings demonstrate how
triadic settings activate socially shared regulation of learning by making AI
use visible and accountable to a human peer, suggesting that AI systems that
augment rather than automate peer collaboration can better preserve the
learning processes that collaborative programming relies on.
\\ ( https://arxiv.org/abs/2601.12134 ,  2547kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12147 (*cross-listing*)
Date: Sat, 17 Jan 2026 19:43:10 GMT   (17226kb)

Title: Segment and Matte Anything in a Unified Model
Authors: Zezhong Fan, Xiaohan Li, Topojoy Biswas, Kaushiki Nag, Kannan Achan
Categories: cs.CV cs.AI
Comments: AAAI 2026
\\
  Segment Anything (SAM) has recently pushed the boundaries of segmentation by
demonstrating zero-shot generalization and flexible prompting after training on
over one billion masks. Despite this, its mask prediction accuracy often falls
short of the precision required in real-world applications. While several
refinement modules have been proposed to boost SAM's segmentation quality,
achieving highly accurate object delineation within a single, unified framework
remains an open challenge. Furthermore, interactive image matting, which aims
to generate fine-grained alpha mattes guided by diverse user hints, has not yet
been explored in the context of SAM. Insights from recent studies highlight
strong correlations between segmentation and matting, suggesting the
feasibility of a unified model capable of both tasks. In this paper, we
introduce Segment And Matte Anything (SAMA), a lightweight extension of SAM
that delivers high-quality interactive image segmentation and matting with
minimal extra parameters. Our Multi-View Localization Encoder (MVLE) captures
detailed features from local views, while the Localization Adapter
(Local-Adapter) refines mask outputs by recovering subtle boundary details. We
also incorporate two prediction heads for each task into the architecture to
generate segmentation and matting masks, simultaneously. Trained on a diverse
dataset aggregated from publicly available sources, SAMA achieves
state-of-the-art performance across multiple segmentation and matting
benchmarks, showcasing its adaptability and effectiveness in a wide range of
downstream tasks.
\\ ( https://arxiv.org/abs/2601.12147 ,  17226kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12150 (*cross-listing*)
Date: Sat, 17 Jan 2026 19:50:40 GMT   (1690kb)

Title: Enhanced Diagnostic Performance via Large-Resolution Inference
  Optimization for Pathology Foundation Models
Authors: Mengxuan Hu, Zihan Guan, John Kang, Sheng Li, Zhongliang Zhou
Categories: cs.CV cs.AI
Comments: 8 pages
\\
  Despite their prominent performance on tasks such as ROI classification and
segmentation, many pathology foundation models remain constrained by a specific
input size e.g. 224 x 224, creating substantial inefficiencies when applied to
whole-slide images (WSIs), which span thousands of resolutions. A naive
strategy is to either enlarge inputs or downsample the WSIs. However, enlarging
inputs results in prohibitive GPU memory consumption, while downsampling alters
the microns-per-pixel resolution and obscures critical morphological details.
To overcome these limitations, we propose an space- and time- efficient
inference strategy that sparsifies attention using spatially aware neighboring
blocks and filters out non-informative tokens through global attention scores.
This design substantially reduces GPU memory and runtime during high-resolution
WSI inference while preserving and even improving the downstream performance,
enabling inference at higher resolutions under the same GPU budget. The
experimental results show that our method can achieves up to an 7.67%
improvement in the ROI classification and compatible results in segmentation.
\\ ( https://arxiv.org/abs/2601.12150 ,  1690kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12205 (*cross-listing*)
Date: Sun, 18 Jan 2026 00:53:11 GMT   (107kb)

Title: Do Neural Codecs Generalize? A Controlled Study Across Unseen Languages
  and Non-Speech Tasks
Authors: Shih-Heng Wang, Jiatong Shi, Jinchuan Tian, Haibin Wu, Shinji Watanabe
Categories: cs.SD cs.AI eess.AS
\\
  This paper investigates three crucial yet underexplored aspects of the
generalization capabilities of neural audio codecs (NACs): (i) whether NACs can
generalize to unseen languages during pre-training, (ii) whether speech-only
pre-trained NACs can effectively generalize to non-speech applications such as
environmental sounds, music, and animal vocalizations, and (iii) whether
incorporating non-speech data during pre-training can improve performance on
both speech and non-speech tasks. Existing studies typically rely on
off-the-shelf NACs for comparison, which limits insight due to variations in
implementation. In this work, we train NACs from scratch using strictly
controlled configurations and carefully curated pre-training data to enable
fair comparisons. We conduct a comprehensive evaluation of NAC performance on
both signal reconstruction quality and downstream applications using 11
metrics. Our results show that NACs can generalize to unseen languages during
pre-training, speech-only pre-trained NACs exhibit degraded performance on
non-speech tasks, and incorporating non-speech data during pre-training
improves performance on non-speech tasks while maintaining comparable
performance on speech tasks.
\\ ( https://arxiv.org/abs/2601.12205 ,  107kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12224 (*cross-listing*)
Date: Sun, 18 Jan 2026 02:14:08 GMT   (4173kb)

Title: Where It Moves, It Matters: Referring Surgical Instrument Segmentation
  via Motion
Authors: Meng Wei, Kun Yuan, Shi Li, Yue Zhou, Long Bai, Nassir Navab,
  Hongliang Ren, Hong Joo Lee, Tom Vercauteren, Nicolas Padoy
Categories: cs.CV cs.AI
Journal-ref: AAAI 2026
\\
  Enabling intuitive, language-driven interaction with surgical scenes is a
critical step toward intelligent operating rooms and autonomous surgical
robotic assistance. However, the task of referring segmentation, localizing
surgical instruments based on natural language descriptions, remains
underexplored in surgical videos, with existing approaches struggling to
generalize due to reliance on static visual cues and predefined instrument
names. In this work, we introduce SurgRef, a novel motion-guided framework that
grounds free-form language expressions in instrument motion, capturing how
tools move and interact across time, rather than what they look like. This
allows models to understand and segment instruments even under occlusion,
ambiguity, or unfamiliar terminology. To train and evaluate SurgRef, we present
Ref-IMotion, a diverse, multi-institutional video dataset with dense
spatiotemporal masks and rich motion-centric expressions. SurgRef achieves
state-of-the-art accuracy and generalization across surgical procedures,
setting a new benchmark for robust, language-driven surgical video
segmentation.
\\ ( https://arxiv.org/abs/2601.12224 ,  4173kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12234 (*cross-listing*)
Date: Sun, 18 Jan 2026 03:08:08 GMT   (9888kb)

Title: Proc3D: Procedural 3D Generation and Parametric Editing of 3D Shapes
  with Large Language Models
Authors: Fadlullah Raji, Stefano Petrangeli, Matheus Gadelha, Yu Shen, Uttaran
  Bhattacharya, Gang Wu
Categories: cs.GR cs.AI cs.CV
\\
  Generating 3D models has traditionally been a complex task requiring
specialized expertise. While recent advances in generative AI have sought to
automate this process, existing methods produce non-editable representation,
such as meshes or point clouds, limiting their adaptability for iterative
design. In this paper, we introduce Proc3D, a system designed to generate
editable 3D models while enabling real-time modifications. At its core, Proc3D
introduces procedural compact graph (PCG), a graph representation of 3D models,
that encodes the algorithmic rules and structures necessary for generating the
model. This representation exposes key parameters, allowing intuitive manual
adjustments via sliders and checkboxes, as well as real-time, automated
modifications through natural language prompts using Large Language Models
(LLMs). We demonstrate Proc3D's capabilities using two generative approaches:
GPT-4o with in-context learning (ICL) and a fine-tuned LLAMA-3 model.
Experimental results show that Proc3D outperforms existing methods in editing
efficiency, achieving more than 400x speedup over conventional approaches that
require full regeneration for each modification. Additionally, Proc3D improves
ULIP scores by 28%, a metric that evaluates the alignment between generated 3D
models and text prompts. By enabling text-aligned 3D model generation along
with precise, real-time parametric edits, Proc3D facilitates highly accurate
text-based image editing applications.
\\ ( https://arxiv.org/abs/2601.12234 ,  9888kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12243 (*cross-listing*)
Date: Sun, 18 Jan 2026 03:41:48 GMT   (2548kb)

Title: Less is More: Label-Guided Summarization of Procedural and Instructional
  Videos
Authors: Shreya Rajpal, Michal Golovanesky, Carsten Eickhoff
Categories: cs.CV cs.AI
Comments: 22 pages, 6 figures
\\
  Video summarization helps turn long videos into clear, concise
representations that are easier to review, document, and analyze, especially in
high-stakes domains like surgical training. Prior work has progressed from
using basic visual features like color, motion, and structural changes to using
pre-trained vision-language models that can better understand what's happening
in the video (semantics) and capture temporal flow, resulting in more
context-aware video summarization. We propose a three-stage framework, PRISM:
Procedural Representation via Integrated Semantic and Multimodal analysis, that
produces semantically grounded video summaries. PRISM combines adaptive visual
sampling, label-driven keyframe anchoring, and contextual validation using a
large language model (LLM). Our method ensures that selected frames reflect
meaningful and procedural transitions while filtering out generic or
hallucinated content, resulting in contextually coherent summaries across both
domain-specific and instructional videos. We evaluate our method on
instructional and activity datasets, using reference summaries for
instructional videos. Despite sampling fewer than 5% of the original frames,
our summaries retain 84% semantic content while improving over baselines by as
much as 33%. Our approach generalizes across procedural and domain-specific
video tasks, achieving strong performance with both semantic alignment and
precision.
\\ ( https://arxiv.org/abs/2601.12243 ,  2548kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12247 (*cross-listing*)
Date: Sun, 18 Jan 2026 03:53:01 GMT   (1129kb)

Title: Plan, Verify and Fill: A Structured Parallel Decoding Approach for
  Diffusion Language Models
Authors: Miao Li, Hanyang Jiang, Sikai Chen, Hengyu Fu, Yuhang Cai, Baihe
  Huang, Tinghan Ye, Xuanzhou Chen, Pascal Van Hentenryck
Categories: cs.CL cs.AI cs.LG
\\
  Diffusion Language Models (DLMs) present a promising non-sequential paradigm
for text generation, distinct from standard autoregressive (AR) approaches.
However, current decoding strategies often adopt a reactive stance,
underutilizing the global bidirectional context to dictate global trajectories.
To address this, we propose Plan-Verify-Fill (PVF), a training-free paradigm
that grounds planning via quantitative validation. PVF actively constructs a
hierarchical skeleton by prioritizing high-leverage semantic anchors and
employs a verification protocol to operationalize pragmatic structural stopping
where further deliberation yields diminishing returns. Extensive evaluations on
LLaDA-8B-Instruct and Dream-7B-Instruct demonstrate that PVF reduces the Number
of Function Evaluations (NFE) by up to 65% compared to confidence-based
parallel decoding across benchmark datasets, unlocking superior efficiency
without compromising accuracy.
\\ ( https://arxiv.org/abs/2601.12247 ,  1129kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12248 (*cross-listing*)
Date: Sun, 18 Jan 2026 03:55:28 GMT   (1682kb)

Title: AQUA-Bench: Beyond Finding Answers to Knowing When There Are None in
  Audio Question Answering
Authors: Chun-Yi Kuan, Hung-yi Lee
Categories: eess.AS cs.AI cs.CL cs.LG cs.SD
Comments: Accepted to ICASSP 2026. Project Website:
  https://kuan2jiu99.github.io/AQUA-Bench-demo/
\\
  Recent advances in audio-aware large language models have shown strong
performance on audio question answering. However, existing benchmarks mainly
cover answerable questions and overlook the challenge of unanswerable ones,
where no reliable answer can be inferred from the audio. Such cases are common
in real-world settings, where questions may be misleading, ill-posed, or
incompatible with the information. To address this gap, we present AQUA-Bench,
a benchmark for Audio Question Unanswerability Assessment. It systematically
evaluates three scenarios: Absent Answer Detection (the correct option is
missing), Incompatible Answer Set Detection (choices are categorically
mismatched with the question), and Incompatible Audio Question Detection (the
question is irrelevant or lacks sufficient grounding in the audio). By
assessing these cases, AQUA-Bench offers a rigorous measure of model
reliability and promotes the development of audio-language systems that are
more robust and trustworthy. Our experiments suggest that while models excel on
standard answerable tasks, they often face notable challenges with unanswerable
ones, pointing to a blind spot in current audio-language understanding.
\\ ( https://arxiv.org/abs/2601.12248 ,  1682kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12249 (*cross-listing*)
Date: Sun, 18 Jan 2026 03:55:33 GMT   (939kb)

Title: An Innovative Framework for Breast Cancer Detection Using Pyramid
  Adaptive Atrous Convolution, Transformer Integration, and Multi-Scale Feature
  Fusion
Authors: Ehsan Sadeghi Pour, Mahdi Esmaeili, Morteza Romoozi
Categories: cs.CV cs.AI
Comments: 13 page
\\
  Breast cancer is one of the most common cancers among women worldwide, and
its accurate and timely diagnosis plays a critical role in improving treatment
outcomes. This thesis presents an innovative framework for detecting malignant
masses in mammographic images by integrating the Pyramid Adaptive Atrous
Convolution (PAAC) and Transformer architectures. The proposed approach
utilizes Multi-Scale Feature Fusion to enhance the extraction of features from
benign and malignant tissues and combines Dice Loss and Focal Loss functions to
improve the model's learning process, effectively reducing errors in binary
breast cancer classification and achieving high accuracy and efficiency. In
this study, a comprehensive dataset of breast cancer images from INbreast,
MIAS, and DDSM was preprocessed through data augmentation and contrast
enhancement and resized to 227x227 pixels for model training. Leveraging the
Transformer's ability to manage long-range dependencies with Self-Attention
mechanisms, the proposed model achieved high accuracy in detecting cancerous
masses, outperforming foundational models such as BreastNet, DeepMammo,
Multi-Scale CNN, Swin-Unet, and SegFormer. The final evaluation results for the
proposed model include an accuracy of 98.5\%, sensitivity of 97.8\%,
specificity of 96.3\%, F1-score of 98.2\%, and overall precision of 97.9\%.
These metrics demonstrate a significant improvement over traditional methods
and confirm the model's effectiveness in identifying cancerous masses in
complex scenarios and large datasets. This model shows potential as a reliable
and efficient tool for breast cancer diagnosis and can be effectively
integrated into medical diagnostic systems.
\\ ( https://arxiv.org/abs/2601.12249 ,  939kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12257 (*cross-listing*)
Date: Sun, 18 Jan 2026 04:40:00 GMT   (24004kb)

Title: Soft Shadow Diffusion (SSD): Physics-inspired Learning for 3D
  Computational Periscopy
Authors: Fadlullah Raji, John Murray-Bruce
Categories: cs.CV cs.AI cs.CG cs.GR
Journal-ref: European Conference on Computer Vision (ECCV 2024)
\\
  Conventional imaging requires a line of sight to create accurate visual
representations of a scene. In certain circumstances, however, obtaining a
suitable line of sight may be impractical, dangerous, or even impossible.
Non-line-of-sight (NLOS) imaging addresses this challenge by reconstructing the
scene from indirect measurements. Recently, passive NLOS methods that use an
ordinary photograph of the subtle shadow cast onto a visible wall by the hidden
scene have gained interest. These methods are currently limited to 1D or
low-resolution 2D color imaging or to localizing a hidden object whose shape is
approximately known. Here, we generalize this class of methods and demonstrate
a 3D reconstruction of a hidden scene from an ordinary NLOS photograph. To
achieve this, we propose a novel reformulation of the light transport model
that conveniently decomposes the hidden scene into \textit{light-occluding} and
\textit{non-light-occluding} components to yield a separable non-linear least
squares (SNLLS) inverse problem. We develop two solutions: A gradient-based
optimization method and a physics-inspired neural network approach, which we
call Soft Shadow diffusion (SSD). Despite the challenging ill-conditioned
inverse problem encountered here, our approaches are effective on numerous 3D
scenes in real experimental scenarios. Moreover, SSD is trained in simulation
but generalizes well to unseen classes in simulation and real-world NLOS
scenes. SSD also shows surprising robustness to noise and ambient illumination.
\\ ( https://arxiv.org/abs/2601.12257 ,  24004kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12263 (*cross-listing*)
Date: Sun, 18 Jan 2026 04:58:28 GMT   (2608kb)

Title: Multimodal Generative Engine Optimization: Rank Manipulation for
  Vision-Language Model Rankers
Authors: Yixuan Du, Chenxiao Yu, Haoyan Xu, Ziyi Wang, Yue Zhao, Xiyang Hu
Categories: cs.CL cs.AI cs.LG
\\
  Vision-Language Models (VLMs) are rapidly replacing unimodal encoders in
modern retrieval and recommendation systems. While their capabilities are
well-documented, their robustness against adversarial manipulation in
competitive ranking scenarios remains largely unexplored. In this paper, we
uncover a critical vulnerability in VLM-based product search: multimodal
ranking attacks. We present Multimodal Generative Engine Optimization (MGEO), a
novel adversarial framework that enables a malicious actor to unfairly promote
a target product by jointly optimizing imperceptible image perturbations and
fluent textual suffixes. Unlike existing attacks that treat modalities in
isolation, MGEO employs an alternating gradient-based optimization strategy to
exploit the deep cross-modal coupling within the VLM. Extensive experiments on
real-world datasets using state-of-the-art models demonstrate that our
coordinated attack significantly outperforms text-only and image-only
baselines. These findings reveal that multimodal synergy, typically a strength
of VLMs, can be weaponized to compromise the integrity of search rankings
without triggering conventional content filters.
\\ ( https://arxiv.org/abs/2601.12263 ,  2608kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12269 (*cross-listing*)
Date: Sun, 18 Jan 2026 05:51:30 GMT   (942kb)

Title: Simulated Annealing Enhances Theory-of-Mind Reasoning in Autoregressive
  Language Models
Authors: Xucong Hu, Jian-Qiao Zhu
Categories: cs.CL cs.AI
\\
  Autoregressive language models are next-token predictors and have been
criticized for only optimizing surface plausibility (i.e., local coherence)
rather than maintaining correct latent-state representations (i.e., global
coherence). Because Theory of Mind (ToM) tasks crucially depend on reasoning
about latent mental states of oneself and others, such models are therefore
often thought to fail at ToM. While post-training methods can improve ToM
performance, we show that strong ToM capability can be recovered directly from
the base model without any additional weight updates or verifications. Our
approach builds on recent power-sampling methods (Karan & Du, 2025) that use
Markov chain Monte Carlo (MCMC) to sample from sharpened sequence-level (rather
than token-level) probability distributions of autoregressive language models.
We further find that incorporating annealing, where the tempered distribution
is gradually shifted from high to low temperature, substantially improves ToM
performance over fixed-temperature power sampling. Together, these results
suggest that sampling-based optimization provides a powerful way to extract
latent capabilities from language models without retraining.
\\ ( https://arxiv.org/abs/2601.12269 ,  942kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12276 (*cross-listing*)
Date: Sun, 18 Jan 2026 06:26:03 GMT   (8093kb)

Title: Predictive Prototyping: Evaluating Design Concepts with ChatGPT
Authors: Hilsann Yong, Bradley A. Camburn
Categories: cs.HC cs.AI
Comments: 22 pages, 15 figures, 5 tables
\\
  The design-build-test cycle is essential for innovation, but physical
prototyping is often slow and expensive. Although physics-based simulation and
strategic prototyping can reduce cost, meaningful evaluation is frequently
constrained until an integrated prototype is built. This paper investigates
whether a generative pretrained transformer (GPT) can predict information
typically obtained through prototyping, including cost, performance, and
perceived usability. We introduce a retrieval-augmented generation (RAG) method
to emulate design feedback using OpenAI GPT-4o, grounded in prototyping data
scraped from Instructables.com to increase access to relevant precedent. Two
studies are reported. First, a controlled experiment compares GPT-RAG and human
designers, who receive design sketches and predict cost, performance, and
usability; predictions are evaluated against ground-truth results from physical
prototypes. Second, we report an applied demonstration in which a physical
prototype is produced from GPT-RAG recommendations and compared with a
commercial baseline and a topology-optimized design. Results show that GPT-RAG
provides more accurate cost and performance estimates than individual or crowd
human estimates, while yielding comparable usability insights; the
GPT-RAG-informed prototype also outperforms both comparison prototypes.
Repeated querying with response averaging significantly improves accuracy,
suggesting that LLMs can emulate crowd aggregation effects consistent with the
law of large numbers.
\\ ( https://arxiv.org/abs/2601.12276 ,  8093kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12282 (*cross-listing*)
Date: Sun, 18 Jan 2026 06:42:24 GMT   (1143kb)

Title: CytoCLIP: Learning Cytoarchitectural Characteristics in Developing Human
  Brain Using Contrastive Language Image Pre-Training
Authors: Pralaypati Ta, Sriram Venkatesaperumal, Keerthi Ram, and Mohanasankar
  Sivaprakasam
Categories: cs.CV cs.AI
\\
  The functions of different regions of the human brain are closely linked to
their distinct cytoarchitecture, which is defined by the spatial arrangement
and morphology of the cells. Identifying brain regions by their
cytoarchitecture enables various scientific analyses of the brain. However,
delineating these areas manually in brain histological sections is
time-consuming and requires specialized knowledge. An automated approach is
necessary to minimize the effort needed from human experts. To address this, we
propose CytoCLIP, a suite of vision-language models derived from pre-trained
Contrastive Language-Image Pre-Training (CLIP) frameworks to learn joint
visual-text representations of brain cytoarchitecture. CytoCLIP comprises two
model variants: one is trained using low-resolution whole-region images to
understand the overall cytoarchitectural pattern of an area, and the other is
trained on high-resolution image tiles for detailed cellular-level
representation. The training dataset is created from NISSL-stained histological
sections of developing fetal brains of different gestational weeks. It includes
86 distinct regions for low-resolution images and 384 brain regions for
high-resolution tiles. We evaluate the model's understanding of the
cytoarchitecture and generalization ability using region classification and
cross-modal retrieval tasks. Multiple experiments are performed under various
data setups, including data from samples of different ages and sectioning
planes. Experimental results demonstrate that CytoCLIP outperforms existing
methods. It achieves an F1 score of 0.87 for whole-region classification and
0.91 for high-resolution image tile classification.
\\ ( https://arxiv.org/abs/2601.12282 ,  1143kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12286 (*cross-listing*)
Date: Sun, 18 Jan 2026 06:47:35 GMT   (533kb)

Title: Conversational Context Classification: A Representation Engineering
  Approach
Authors: Jonathan Pan
Categories: cs.CL cs.AI cs.CR
\\
  The increasing prevalence of Large Language Models (LLMs) demands effective
safeguards for their operation, particularly concerning their tendency to
generate out-of-context responses. A key challenge is accurately detecting when
LLMs stray from expected conversational norms, manifesting as topic shifts,
factual inaccuracies, or outright hallucinations. Traditional anomaly detection
struggles to directly apply within contextual semantics. This paper outlines
our experiment in exploring the use of Representation Engineering (RepE) and
One-Class Support Vector Machine (OCSVM) to identify subspaces within the
internal states of LLMs that represent a specific context. By training OCSVM on
in-context examples, we establish a robust boundary within the LLM's hidden
state latent space. We evaluate out study with two open source LLMs - Llama and
Qwen models in specific contextual domain. Our approach entailed identifying
the optimal layers within the LLM's internal state subspaces that strongly
associates with the context of interest. Our evaluation results showed
promising results in identifying the subspace for a specific context. Aside
from being useful in detecting in or out of context conversation threads, this
research work contributes to the study of better interpreting LLMs.
\\ ( https://arxiv.org/abs/2601.12286 ,  533kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12304 (*cross-listing*)
Date: Sun, 18 Jan 2026 08:05:33 GMT   (2636kb)

Title: A Two-Stage Globally-Diverse Adversarial Attack for Vision-Language
  Pre-training Models
Authors: Wutao Chen and Huaqin Zou and Chen Wan and Lifeng Huang
Categories: cs.CV cs.AI
Comments: Accepted to ICASSP 2026
\\
  Vision-language pre-training (VLP) models are vulnerable to adversarial
examples, particularly in black-box scenarios. Existing multimodal attacks
often suffer from limited perturbation diversity and unstable multi-stage
pipelines. To address these challenges, we propose 2S-GDA, a two-stage
globally-diverse attack framework. The proposed method first introduces textual
perturbations through a globally-diverse strategy by combining candidate text
expansion with globally-aware replacement. To enhance visual diversity,
image-level perturbations are generated using multi-scale resizing and
block-shuffle rotation. Extensive experiments on VLP models demonstrate that
2S-GDA consistently improves attack success rates over state-of-the-art
methods, with gains of up to 11.17\% in black-box settings. Our framework is
modular and can be easily combined with existing methods to further enhance
adversarial transferability.
\\ ( https://arxiv.org/abs/2601.12304 ,  2636kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12316 (*cross-listing*)
Date: Sun, 18 Jan 2026 08:54:02 GMT   (225kb)

Title: GazeFormer-MoE: Context-Aware Gaze Estimation via CLIP and MoE
  Transformer
Authors: Xinyuan Zhao, Xianrui Chen, Ahmad Chaddad
Categories: cs.CV cs.AI
Comments: accepted at ICASSP 2026
\\
  We present a semantics modulated, multi scale Transformer for 3D gaze
estimation. Our model conditions CLIP global features with learnable prototype
banks (illumination, head pose, background, direction), fuses these
prototype-enriched global vectors with CLIP patch tokens and high-resolution
CNN tokens in a unified attention space, and replaces several FFN blocks with
routed/shared Mixture of Experts to increase conditional capacity. Evaluated on
MPIIFaceGaze, EYEDIAP, Gaze360 and ETH-XGaze, our model achieves new state of
the art angular errors of 2.49{\deg}, 3.22{\deg}, 10.16{\deg}, and 1.44{\deg},
demonstrating up to a 64% relative improvement over previously reported
results. ablations attribute gains to prototype conditioning, cross scale
fusion, MoE and hyperparameter. Our code is publicly available at
https://github. com/AIPMLab/Gazeformer.
\\ ( https://arxiv.org/abs/2601.12316 ,  225kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12331 (*cross-listing*)
Date: Sun, 18 Jan 2026 09:29:50 GMT   (1713kb)

Title: Efficient Privacy-Preserving Retrieval Augmented Generation with
  Distance-Preserving Encryption
Authors: Huanyi Ye, Jiale Guo, Ziyao Liu, Kwok-Yan Lam
Categories: cs.CR cs.AI
\\
  RAG has emerged as a key technique for enhancing response quality of LLMs
without high computational cost. In traditional architectures, RAG services are
provided by a single entity that hosts the dataset within a trusted local
environment. However, individuals or small organizations often lack the
resources to maintain data storage servers, leading them to rely on outsourced
cloud storage. This dependence on untrusted third-party services introduces
privacy risks. Embedding-based retrieval mechanisms, commonly used in RAG
systems, are vulnerable to privacy leakage such as vector-to-text
reconstruction attacks and structural leakage via vector analysis. Several
privacy-preserving RAG techniques have been proposed but most existing
approaches rely on partially homomorphic encryption, which incurs substantial
computational overhead. To address these challenges, we propose an efficient
privacy-preserving RAG framework (ppRAG) tailored for untrusted cloud
environments that defends against vector-to-text attack, vector analysis, and
query analysis. We propose Conditional Approximate
Distance-Comparison-Preserving Symmetric Encryption (CAPRISE) that encrypts
embeddings while still allowing the cloud to compute similarity between an
encrypted query and the encrypted database embeddings. CAPRISE preserves only
the relative distance ordering between the encrypted query and each encrypted
database embedding, without exposing inter-database distances, thereby
enhancing both privacy and efficiency. To mitigate query analysis, we introduce
DP by perturbing the query embedding prior to encryption, preventing the cloud
from inferring sensitive patterns. Experimental results show that ppRAG
achieves efficient processing throughput, high retrieval accuracy, strong
privacy guarantees, making it a practical solution for resource-constrained
users seeking secure cloud-augmented LLMs.
\\ ( https://arxiv.org/abs/2601.12331 ,  1713kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12343 (*cross-listing*)
Date: Sun, 18 Jan 2026 10:28:54 GMT   (1283kb)

Title: How Well Do LLMs Predict Human Behavior? A Measure of their Pretrained
  Knowledge
Authors: Wayne Gao, Sukjin Han, Annie Liang
Categories: econ.EM cs.AI stat.ML
\\
  Large language models (LLMs) are increasingly used to predict human behavior.
We propose a measure for evaluating how much knowledge a pretrained LLM brings
to such a prediction: its equivalent sample size, defined as the amount of
task-specific data needed to match the predictive accuracy of the LLM. We
estimate this measure by comparing the prediction error of a fixed LLM in a
given domain to that of flexible machine learning models trained on increasing
samples of domain-specific data. We further provide a statistical inference
procedure by developing a new asymptotic theory for cross-validated prediction
error. Finally, we apply this method to the Panel Study of Income Dynamics. We
find that LLMs encode considerable predictive information for some economic
variables but much less for others, suggesting that their value as substitutes
for domain-specific data differs markedly across settings.
\\ ( https://arxiv.org/abs/2601.12343 ,  1283kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12349 (*cross-listing*)
Date: Sun, 18 Jan 2026 10:54:54 GMT   (753kb)

Title: Zero-Permission Manipulation: Can We Trust Large Multimodal Model
  Powered GUI Agents?
Authors: Yi Qian, Kunwei Qian, Xingbang He, Ligeng Chen, Jikang Zhang, Tiantai
  Zhang, Haiyang Wei, Linzhang Wang, Hao Wu, Bing Mao
Categories: cs.CR cs.AI cs.SE
\\
  Large multimodal model powered GUI agents are emerging as high-privilege
operators on mobile platforms, entrusted with perceiving screen content and
injecting inputs. However, their design operates under the implicit assumption
of Visual Atomicity: that the UI state remains invariant between observation
and action. We demonstrate that this assumption is fundamentally invalid in
Android, creating a critical attack surface.
  We present Action Rebinding, a novel attack that allows a seemingly-benign
app with zero dangerous permissions to rebind an agent's execution. By
exploiting the inevitable observation-to-action gap inherent in the agent's
reasoning pipeline, the attacker triggers foreground transitions to rebind the
agent's planned action toward the target app. We weaponize the agent's
task-recovery logic and Android's UI state preservation to orchestrate
programmable, multi-step attack chains. Furthermore, we introduce an Intent
Alignment Strategy (IAS) that manipulates the agent's reasoning process to
rationalize UI states, enabling it to bypass verification gates (e.g.,
confirmation dialogs) that would otherwise be rejected.
  We evaluate Action Rebinding Attacks on six widely-used Android GUI agents
across 15 tasks. Our results demonstrate a 100% success rate for atomic action
rebinding and the ability to reliably orchestrate multi-step attack chains.
With IAS, the success rate in bypassing verification gates increases (from 0%
to up to 100%). Notably, the attacker application requires no sensitive
permissions and contains no privileged API calls, achieving a 0% detection rate
across malware scanners (e.g., VirusTotal). Our findings reveal a fundamental
architectural flaw in current agent-OS integration and provide critical
insights for the secure design of future agent systems. To access experimental
logs and demonstration videos, please contact yi_qian@smail.nju.edu.cn.
\\ ( https://arxiv.org/abs/2601.12349 ,  753kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12357 (*cross-listing*)
Date: Sun, 18 Jan 2026 11:31:46 GMT   (4926kb)

Title: SimpleMatch: A Simple and Strong Baseline for Semantic Correspondence
Authors: Hailing Jin, Huiying Li
Categories: cs.CV cs.AI
\\
  Recent advances in semantic correspondence have been largely driven by the
use of pre-trained large-scale models. However, a limitation of these
approaches is their dependence on high-resolution input images to achieve
optimal performance, which results in considerable computational overhead. In
this work, we address a fundamental limitation in current methods: the
irreversible fusion of adjacent keypoint features caused by deep downsampling
operations. This issue is triggered when semantically distinct keypoints fall
within the same downsampled receptive field (e.g., 16x16 patches). To address
this issue, we present SimpleMatch, a simple yet effective framework for
semantic correspondence that delivers strong performance even at low
resolutions. We propose a lightweight upsample decoder that progressively
recovers spatial detail by upsampling deep features to 1/4 resolution, and a
multi-scale supervised loss that ensures the upsampled features retain
discriminative features across different spatial scales. In addition, we
introduce sparse matching and window-based localization to optimize training
memory usage and reduce it by 51%. At a resolution of 252x252 (3.3x smaller
than current SOTA methods), SimpleMatch achieves superior performance with
84.1% PCK@0.1 on the SPair-71k benchmark. We believe this framework provides a
practical and efficient baseline for future research in semantic
correspondence. Code is available at:
https://github.com/hailong23-jin/SimpleMatch.
\\ ( https://arxiv.org/abs/2601.12357 ,  4926kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12358 (*cross-listing*)
Date: Sun, 18 Jan 2026 11:32:29 GMT   (2216kb)

Title: From Prompts to Pavement: LMMs-based Agentic Behavior-Tree Generation
  Framework for Autonomous Vehicles
Authors: Omar Y. Goba, Ahmed Y. Gado, Catherine M. Elias, Ahmed Hussein
Categories: cs.CV cs.AI cs.RO
\\
  Autonomous vehicles (AVs) require adaptive behavior planners to navigate
unpredictable, real-world environments safely. Traditional behavior trees (BTs)
offer structured decision logic but are inherently static and demand
labor-intensive manual tuning, limiting their applicability at SAE Level 5
autonomy. This paper presents an agentic framework that leverages large
language models (LLMs) and multi-modal vision models (LVMs) to generate and
adapt BTs on the fly. A specialized Descriptor agent applies chain-of-symbols
prompting to assess scene criticality, a Planner agent constructs high-level
sub-goals via in-context learning, and a Generator agent synthesizes executable
BT sub-trees in XML format. Integrated into a CARLA+Nav2 simulation, our system
triggers only upon baseline BT failure, demonstrating successful navigation
around unexpected obstacles (e.g., street blockage) with no human intervention.
Compared to a static BT baseline, this approach is a proof-of-concept that
extends to diverse driving scenarios.
\\ ( https://arxiv.org/abs/2601.12358 ,  2216kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12374 (*cross-listing*)
Date: Sun, 18 Jan 2026 12:07:31 GMT   (2700kb)

Title: A Scalable Entity-Based Framework for Auditing Bias in LLMs
Authors: Akram Elbouanani, Aboubacar Tuo, Adrian Popescu
Categories: cs.CL cs.AI
\\
  Existing approaches to bias evaluation in large language models (LLMs) trade
ecological validity for statistical control, relying on artificial prompts that
poorly reflect real-world use, or on naturalistic tasks that lack scale and
rigor. We introduce a scalable bias-auditing framework using named entities as
probes to measure structural disparities in model behavior. We show that
synthetic data reliably reproduces bias patterns observed in natural text,
enabling large-scale analysis. Using this approach, we conduct the largest bias
audit to date, comprising 1.9 billion data points across multiple entity types,
tasks, languages, models, and prompting strategies. Our results reveal
systematic biases: models penalize right-wing politicians, favor left-wing
politicians, prefer Western and wealthy nations over the Global South, favor
Western companies, and penalize firms in the defense and pharmaceutical
sectors. While instruction tuning reduces bias, increasing model scale
amplifies it, and prompting in Chinese or Russian does not attenuate
Western-aligned preferences. These results indicate that LLMs should undergo
rigorous auditing before deployment in high-stakes applications.
\\ ( https://arxiv.org/abs/2601.12374 ,  2700kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12389 (*cross-listing*)
Date: Sun, 18 Jan 2026 12:56:47 GMT   (448kb)

Title: NADIR: Differential Attention Flow for Non-Autoregressive
  Transliteration in Indic Languages
Authors: Lakshya Tomar, Vinayak Abrol, Puneet Agarwal
Categories: cs.CL cs.AI
Comments: Accepted at the AAAI Conference on Artificial Intelligence (AAAI
  2026)
\\
  In this work, we argue that not all sequence-to-sequence tasks require the
strong inductive biases of autoregressive (AR) models. Tasks like multilingual
transliteration, code refactoring, grammatical correction or text normalization
often rely on local dependencies where the full modeling capacity of AR models
can be overkill, creating a trade-off between their high accuracy and high
inference latency. While non-autoregressive (NAR) models offer speed, they
typically suffer from hallucinations and poor length control. To explore this
trade-off, we focus on the multilingual transliteration task in Indic languages
and introduce NADIR, a novel NAR architecture designed to strike a balance
between speed and accuracy. NADIR integrates a Differential Transformer and a
Mixture-of-Experts mechanism, enabling it to robustly model complex character
mappings without sequential dependencies. NADIR achieves over a 13x speed-up
compared to the state-of-the-art AR baseline. It maintains a competitive mean
Character Error Rate of 15.78%, compared to 14.44% for the AR model and 21.88%
for a standard NAR equivalent. Importantly, NADIR reduces Repetition errors by
49.53%, Substitution errors by 24.45%, Omission errors by 32.92%, and Insertion
errors by 16.87%. This work provides a practical blueprint for building fast
and reliable NAR systems, effectively bridging the gap between AR accuracy and
the demands of real-time, large-scale deployment.
\\ ( https://arxiv.org/abs/2601.12389 ,  448kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12402 (*cross-listing*)
Date: Sun, 18 Jan 2026 13:27:01 GMT   (239kb)

Title: Weaknesses of Facial Emotion Recognition Systems
Authors: Aleksandra Jamr\'oz, Patrycja Wysocka, Piotr Garbat
Categories: cs.CV cs.AI
Journal-ref: Proc. 12th Machine Intelligence and Digital Interaction Conf.
  (MIDI 2024), Warsaw, Poland, Dec. 2024 (14-22)
DOI: 10.1007/978-3-032-05802-7_1
\\
  Emotion detection from faces is one of the machine learning problems needed
for human-computer interaction. The variety of methods used is enormous, which
motivated an in-depth review of articles and scientific studies. Three of the
most interesting and best solutions are selected, followed by the selection of
three datasets that stood out for the diversity and number of images in them.
The selected neural networks are trained, and then a series of experiments are
performed to compare their performance, including testing on different datasets
than a model was trained on. This reveals weaknesses in existing solutions,
including differences between datasets, unequal levels of difficulty in
recognizing certain emotions and the challenges in differentiating between
closely related emotions.
\\ ( https://arxiv.org/abs/2601.12402 ,  239kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12436 (*cross-listing*)
Date: Sun, 18 Jan 2026 14:46:08 GMT   (631kb)

Title: Purification Before Fusion: Toward Mask-Free Speech Enhancement for
  Robust Audio-Visual Speech Recognition
Authors: Linzhi Wu, Xingyu Zhang, Hao Yuan, Yakun Zhang, Changyan Zheng, Liang
  Xie, Tiejun Liu, Erwei Yin
Categories: eess.AS cs.AI cs.LG cs.MM cs.SD
Comments: Accepted by ICASSP2026
\\
  Audio-visual speech recognition (AVSR) typically improves recognition
accuracy in noisy environments by integrating noise-immune visual cues with
audio signals. Nevertheless, high-noise audio inputs are prone to introducing
adverse interference into the feature fusion process. To mitigate this, recent
AVSR methods often adopt mask-based strategies to filter audio noise during
feature interaction and fusion, yet such methods risk discarding semantically
relevant information alongside noise. In this work, we propose an end-to-end
noise-robust AVSR framework coupled with speech enhancement, eliminating the
need for explicit noise mask generation. This framework leverages a
Conformer-based bottleneck fusion module to implicitly refine noisy audio
features with video assistance. By reducing modality redundancy and enhancing
inter-modal interactions, our method preserves speech semantic integrity to
achieve robust recognition performance. Experimental evaluations on the public
LRS3 benchmark suggest that our method outperforms prior advanced mask-based
baselines under noisy conditions.
\\ ( https://arxiv.org/abs/2601.12436 ,  631kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12443 (*cross-listing*)
Date: Sun, 18 Jan 2026 14:57:51 GMT   (116kb)

Title: Adversarial Defense in Vision-Language Models: An Overview
Authors: Xiaowei Fu, Lei Zhang
Categories: cs.CV cs.AI
\\
  The widespread use of Vision Language Models (VLMs, e.g. CLIP) has raised
concerns about their vulnerability to sophisticated and imperceptible
adversarial attacks. These attacks could compromise model performance and
system security in cross-modal tasks. To address this challenge, three main
defense paradigms have been proposed: Training-time Defense, Test-time
Adaptation Defense, and Training-free Defense. Training-time Defense involves
modifying the training process, typically through adversarial fine-tuning to
improve the robustness to adversarial examples. While effective, this approach
requires substantial computational resources and may not generalize across all
adversarial attacks. Test-time Adaptation Defense focuses on adapting the model
at inference time by updating its parameters to handle unlabeled adversarial
examples, offering flexibility but often at the cost of increased complexity
and computational overhead. Training-free Defense avoids modifying the model
itself, instead focusing on altering the adversarial inputs or their feature
embeddings, which enforces input perturbations to mitigate the impact of
attacks without additional training. This survey reviews the latest
advancements in adversarial defense strategies for VLMs, highlighting the
strengths and limitations of such approaches and discussing ongoing challenges
in enhancing the robustness of VLMs.
\\ ( https://arxiv.org/abs/2601.12443 ,  116kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12449 (*cross-listing*)
Date: Sun, 18 Jan 2026 15:10:18 GMT   (3733kb)

Title: AgenTRIM: Tool Risk Mitigation for Agentic AI
Authors: Roy Betser, Shamik Bose, Amit Giloni, Chiara Picardi, Sindhu
  Padakandla and Roman Vainshtein
Categories: cs.CR cs.AI
Comments: Under review
\\
  AI agents are autonomous systems that combine LLMs with external tools to
solve complex tasks. While such tools extend capability, improper tool
permissions introduce security risks such as indirect prompt injection and tool
misuse. We characterize these failures as unbalanced tool-driven agency. Agents
may retain unnecessary permissions (excessive agency) or fail to invoke
required tools (insufficient agency), amplifying the attack surface and
reducing performance. We introduce AgenTRIM, a framework for detecting and
mitigating tool-driven agency risks without altering an agent's internal
reasoning. AgenTRIM addresses these risks through complementary offline and
online phases. Offline, AgenTRIM reconstructs and verifies the agent's tool
interface from code and execution traces. At runtime, it enforces per-step
least-privilege tool access through adaptive filtering and status-aware
validation of tool calls. Evaluating on the AgentDojo benchmark, AgenTRIM
substantially reduces attack success while maintaining high task performance.
Additional experiments show robustness to description-based attacks and
effective enforcement of explicit safety policies. Together, these results
demonstrate that AgenTRIM provides a practical, capability-preserving approach
to safer tool use in LLM-based agents.
\\ ( https://arxiv.org/abs/2601.12449 ,  3733kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12465 (*cross-listing*)
Date: Sun, 18 Jan 2026 16:10:04 GMT   (3305kb)

Title: Incentivizing In-depth Reasoning over Long Contexts with Process
  Advantage Shaping
Authors: Miao Peng, Weizhou Shen, Nuo Chen, Chenliang Li, Ming Yan, Jia Li
Categories: cs.CL cs.AI
\\
  Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective in
enhancing LLMs short-context reasoning, but its performance degrades in
long-context scenarios that require both precise grounding and robust
long-range reasoning. We identify the "almost-there" phenomenon in long-context
reasoning, where trajectories are largely correct but fail at the final step,
and attribute this failure to two factors: (1) the lack of high reasoning
density in long-context QA data that push LLMs beyond mere grounding toward
sophisticated multi-hop reasoning; and (2) the loss of valuable learning
signals during long-context RL training due to the indiscriminate penalization
of partially correct trajectories with incorrect outcomes. To overcome this
bottleneck, we propose DeepReasonQA, a KG-driven synthesis framework that
controllably constructs high-difficulty, multi-hop long-context QA pairs with
inherent reasoning chains. Building on this, we introduce Long-context Process
Advantage Shaping (LongPAS), a simple yet effective method that performs
fine-grained credit assignment by evaluating reasoning steps along Validity and
Relevance dimensions, which captures critical learning signals from
"almost-there" trajectories. Experiments on three long-context reasoning
benchmarks show that our approach substantially outperforms RLVR baselines and
matches frontier LLMs while using far fewer parameters. Further analysis
confirms the effectiveness of our methods in strengthening long-context
reasoning while maintaining stable RL training.
\\ ( https://arxiv.org/abs/2601.12465 ,  3305kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12471 (*cross-listing*)
Date: Sun, 18 Jan 2026 16:19:29 GMT   (3492kb)

Title: Knowing When to Abstain: Medical LLMs Under Clinical Uncertainty
Authors: Sravanthi Machcha, Sushrita Yerra, Sahil Gupta, Aishwarya Sahoo,
  Sharmin Sultana, Hong Yu, Zonghai Yao
Categories: cs.CL cs.AI
Comments: Equal contribution for the first two authors; To appear in
  proceedings of the Main Conference of the European Chapter of the Association
  for Computational Linguistics (EACL) 2026
\\
  Current evaluation of large language models (LLMs) overwhelmingly prioritizes
accuracy; however, in real-world and safety-critical applications, the ability
to abstain when uncertain is equally vital for trustworthy deployment. We
introduce MedAbstain, a unified benchmark and evaluation protocol for
abstention in medical multiple-choice question answering (MCQA) -- a
discrete-choice setting that generalizes to agentic action selection --
integrating conformal prediction, adversarial question perturbations, and
explicit abstention options. Our systematic evaluation of both open- and
closed-source LLMs reveals that even state-of-the-art, high-accuracy models
often fail to abstain with uncertain. Notably, providing explicit abstention
options consistently increases model uncertainty and safer abstention, far more
than input perturbations, while scaling model size or advanced prompting brings
little improvement. These findings highlight the central role of abstention
mechanisms for trustworthy LLM deployment and offer practical guidance for
improving safety in high-stakes applications.
\\ ( https://arxiv.org/abs/2601.12471 ,  3492kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12494 (*cross-listing*)
Date: Sun, 18 Jan 2026 17:08:31 GMT   (993kb)

Title: Harmonizing the Arabic Audio Space with Data Scheduling
Authors: Hunzalah Hassan Bhatti, Firoj Alam, Shammur Absar Chowdhury
Categories: cs.SD cs.AI cs.CL eess.AS
Comments: Foundation Models, Large Language Models, Native, Speech Models,
  Arabic
MSC-class: 68T50
ACM-class: F.2.2; I.2.7
\\
  Audio large language models (LLMs) enable unified speech understanding and
generation, yet their adaptation to linguistically complex, dialect-rich
settings remains underexplored. This paper presents the first systematic study
of multi-task instruction tuning for an Arabic-centric audio LLM, covering a
hierarchy of generative tasks (ASR, speech summarization) and discriminative
tasks (dialect and emotion identification). To support this study, we introduce
AraMega-SSum, a novel dataset for Arabic speech summarization. We fine-tune
Qwen2.5-Omni (7B) and propose Task-Progressive Curriculum (TPC) along with
Aligner-Based Diverse Sampling (ADS), a strategy that constructs
information-dense batches by selecting task- and label-balanced examples. Our
results reveal a critical efficiency, robustness trade-off: while ADS
accelerates initial convergence and boosts paralinguistic F1-scores, its
inherent gradient volatility can destabilize generative decoding under
prolonged training. Furthermore, while the TPC stabilizes core acoustic
mapping, it often induces negative transfer in downstream tasks. We demonstrate
that a Hybrid TPC+ADS Strategy provides an optimal training ``recipe'', first
establishing a robust representative foundation before employing
diversity-aware refinement to capture fine-grained nuances. These findings
offer practical guidance for the efficient adaptation of Omni-models in
complex, low-resource multimodal environments.
\\ ( https://arxiv.org/abs/2601.12494 ,  993kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12534 (*cross-listing*)
Date: Sun, 18 Jan 2026 18:37:41 GMT   (1559kb)

Title: Encoding Emotion Through Self-Supervised Eye Movement Reconstruction
Authors: Marcus Ma, Jordan Prescott, Emily Zhou, Tiantian Feng, Kleanthis
  Avramidis, Gabor Mihaly Toth, Shrikanth Narayanan
Categories: cs.CV cs.AI
\\
  The relationship between emotional expression and eye movement is
well-documented, with literature establishing gaze patterns are reliable
indicators of emotion. However, most studies utilize specialized,
high-resolution eye-tracking equipment, limiting the potential reach of
findings. We investigate how eye movement can be used to predict multimodal
markers of emotional expression from naturalistic, low-resolution videos. We
utilize a collection of video interviews from the USC Shoah Foundation's Visual
History Archive with Holocaust survivors as they recount their experiences in
the Auschwitz concentration camp. Inspired by pretraining methods on language
models, we develop a novel gaze detection model that uses self-supervised eye
movement reconstruction that can effectively leverage unlabeled video. We use
this model's encoder embeddings to fine-tune models on two downstream tasks
related to emotional expression. The first is aligning eye movement with
directional emotion estimates from speech. The second task is using eye gaze as
a predictor of three momentary manifestations of emotional behaviors: laughing,
crying/sobbing, and sighing. We find our new model is predictive of emotion
outcomes and observe a positive correlation between pretraining performance and
emotion processing performance for both experiments. We conclude
self-supervised eye movement reconstruction is an effective method for encoding
the affective signal they carry.
\\ ( https://arxiv.org/abs/2601.12534 ,  1559kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12535 (*cross-listing*)
Date: Sun, 18 Jan 2026 18:44:49 GMT   (530kb)

Title: Improving Low-Resource Machine Translation via Round-Trip Reinforcement
  Learning
Authors: Ahmed Attia, Alham Fikri
Categories: cs.CL cs.AI
\\
  Low-resource machine translation (MT) has gained increasing attention as
parallel data from low-resource language communities is collected, but many
potential methods for improving low-resource MT remain unexplored. We
investigate a self-supervised reinforcement-learning-based fine-tuning for
translation in low-resource settings using round-trip bootstrapping with the No
Language Left Behind (NLLB) family of models. Our approach translates English
into a target low-resource language and then back into English, using a
combination of chrF++ and BLEU as the reward function on the reconstructed
English sentences. Using the NLLB-MD dataset, we evaluate both the 600M and
1.3B parameter NLLB models and observe consistent improvements for the
following languages: Central Aymara, Friulian, Wolof and Russian. Qualitative
inspection of translation outputs indicates increased fluency and semantic
fidelity. We argue that our method can further benefit from scale, enabling
models to increasingly leverage their pretrained knowledge and continue
self-improving.
\\ ( https://arxiv.org/abs/2601.12535 ,  530kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12549 (*cross-listing*)
Date: Sun, 18 Jan 2026 19:28:26 GMT   (13444kb)

Title: Benchmarking Concept-Spilling Across Languages in LLMs
Authors: Ilia Badanin, Daniil Dzenhaliou, Imanol Schlag
Categories: cs.CL cs.AI
\\
  Multilingual Large Language Models (LLMs) exhibit remarkable cross-lingual
abilities, yet often exhibit a systematic bias toward the representations from
other languages, resulting in semantic interference when generating content in
non-English languages$-$a phenomenon we define as language spilling. This paper
presents a novel comparative framework for evaluating multilingual semantic
robustness by systematically measuring how models handle polysemous words
across languages. Our methodology provides a relative measure of model
performance: when required to generate exactly five meanings, both strong and
weak models may resort to meanings from dominant languages, but semantically
stronger models do so later in the generation sequence, producing more true
meanings from the target language before failing, while weaker models resort to
dominant-language meanings earlier in the sequence. We evaluate a diverse set
of open and closed multilingual LLMs using a structured meaning generation task
across nine languages, employing a carefully curated benchmark of 100
high-polysemy English words. Our findings reveal significant variation in
semantic robustness across both models and languages, providing a principled
ranking system for model comparison without requiring definitive causal
attribution of error sources. We contribute both a scalable comparative
benchmark for multilingual semantic evaluation and a rigorous validation
pipeline$-$critical tools for developing more linguistically balanced AI
systems.
\\ ( https://arxiv.org/abs/2601.12549 ,  13444kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12554 (*cross-listing*)
Date: Sun, 18 Jan 2026 19:36:10 GMT   (8283kb)

Title: Artificial Intelligence in Materials Science and Engineering: Current
  Landscape, Key Challenges, and Future Trajectorie
Authors: Iman Peivaste, Salim Belouettar, Francesco Mercuri, Nicholas Fantuzzi,
  Hamidreza Dehghani, Razieh Izadi, Halliru Ibrahim, Jakub Lengiewicz, Ma\"el
  Belouettar-Mathis, Kouider Bendine, Ahmed Makradi, Martin H\"orsch, Peter
  Klein, Mohamed El Hachemi, Heinz A. Preisig, Yacine Rezgui, Natalia
  Konchakova, Ali Daouadji
Categories: cond-mat.mtrl-sci cs.AI physics.comp-ph quant-ph
DOI: 10.1016/j.compstruct.2025.119419
\\
  Artificial Intelligence is rapidly transforming materials science and
engineering, offering powerful tools to navigate complexity, accelerate
discovery, and optimize material design in ways previously unattainable. Driven
by the accelerating pace of algorithmic advancements and increasing data
availability, AI is becoming an essential competency for materials researchers.
This review provides a comprehensive and structured overview of the current
landscape, synthesizing recent advancements and methodologies for materials
scientists seeking to effectively leverage these data-driven techniques. We
survey the spectrum of machine learning approaches, from traditional algorithms
to advanced deep learning architectures, including CNNs, GNNs, and
Transformers, alongside emerging generative AI and probabilistic models such as
Gaussian Processes for uncertainty quantification. The review also examines the
pivotal role of data in this field, emphasizing how effective representation
and featurization strategies, spanning compositional, structural, image-based,
and language-inspired approaches, combined with appropriate preprocessing,
fundamentally underpin the performance of machine learning models in materials
research. Persistent challenges related to data quality, quantity, and
standardization, which critically impact model development and application in
materials science and engineering, are also addressed.
\\ ( https://arxiv.org/abs/2601.12554 ,  8283kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12577 (*cross-listing*)
Date: Sun, 18 Jan 2026 20:43:53 GMT   (5034kb)

Title: Primate-like perceptual decision making emerges through deep recurrent
  reinforcement learning
Authors: Nathan J. Wispinski, Scott A. Stone, Anthony Singhal, Patrick M.
  Pilarski, Craig S. Chapman
Categories: q-bio.NC cs.AI
\\
  Progress has led to a detailed understanding of the neural mechanisms that
underlie decision making in primates. However, less is known about why such
mechanisms are present in the first place. Theory suggests that primate
decision making mechanisms, and their resultant behavioral abilities, emerged
to maximize reward in the face of noisy, temporally evolving information. To
test this theory, we trained an end-to-end deep recurrent neural network using
reinforcement learning on a noisy perceptual discrimination task. Networks
learned several key abilities of primate-like decision making including trading
off speed for accuracy, and flexibly changing their mind in the face of new
information. Internal dynamics of these networks suggest that these abilities
were supported by similar decision mechanisms as those observed in primate
neurophysiological studies. These results provide experimental support for key
pressures that gave rise to the primate ability to make flexible decisions.
\\ ( https://arxiv.org/abs/2601.12577 ,  5034kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12582 (*cross-listing*)
Date: Sun, 18 Jan 2026 20:51:23 GMT   (2002kb)

Title: Ontology-aligned structuring and reuse of multimodal materials data and
  workflows towards automatic reproduction
Authors: Sepideh Baghaee Ravari and Abril Azocar Guzman and Sarath Menon and
  Stefan Sandfeld and Tilmann Hickel and Markus Stricker
Categories: cond-mat.mtrl-sci cs.AI
Comments: 39 pages, 7 figures
\\
  Reproducibility of computational results remains a challenge in materials
science, as simulation workflows and parameters are often reported only in
unstructured text and tables. While literature data are valuable for validation
and reuse, the lack of machine-readable workflow descriptions prevents
large-scale curation and systematic comparison. Existing text-mining approaches
are insufficient to extract complete computational workflows with their
associated parameters. An ontology-driven, large language model (LLM)-assisted
framework is introduced for the automated extraction and structuring of
computational workflows from the literature. The approach focuses on density
functional theory-based stacking fault energy (SFE) calculations in hexagonal
close-packed magnesium and its binary alloys, and uses a multi-stage filtering
strategy together with prompt-engineered LLM extraction applied to method
sections and tables. Extracted information is unified into a canonical schema
and aligned with established materials ontologies (CMSO, ASMO, and PLDO),
enabling the construction of a knowledge graph using atomRDF. The resulting
knowledge graph enables systematic comparison of reported SFE values and
supports the structured reuse of computational protocols. While full
computational reproducibility is still constrained by missing or implicit
metadata, the framework provides a foundation for organizing and
contextualizing published results in a semantically interoperable form, thereby
improving transparency and reusability of computational materials data.
\\ ( https://arxiv.org/abs/2601.12582 ,  2002kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12585 (*cross-listing*)
Date: Sun, 18 Jan 2026 21:08:23 GMT   (19240kb)

Title: Do MLLMs See What We See? Analyzing Visualization Literacy Barriers in
  AI Systems
Authors: Mengli (Dawn) Duan, Yuhe (Sissi) Jiang, Matthew Varona, and Carolina
  Nobre
Categories: cs.HC cs.AI cs.ET
\\
  Multimodal Large Language Models (MLLMs) are increasingly used to interpret
visualizations, yet little is known about why they fail. We present the first
systematic analysis of barriers to visualization literacy in MLLMs. Using the
regenerated Visualization Literacy Assessment Test (reVLAT) benchmark with
synthetic data, we open-coded 309 erroneous responses from four
state-of-the-art models with a barrier-centric strategy adapted from human
visualization literacy research. Our analysis yields a taxonomy of MLLM
failures, revealing two machine-specific barriers that extend prior
human-participation frameworks. Results show that models perform well on simple
charts but struggle with color-intensive, segment-based visualizations, often
failing to form consistent comparative reasoning. Our findings inform future
evaluation and design of reliable AI-driven visualization assistants.
\\ ( https://arxiv.org/abs/2601.12585 ,  19240kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12594 (*cross-listing*)
Date: Sun, 18 Jan 2026 21:36:19 GMT   (272kb)

Title: SLAP: Scalable Language-Audio Pretraining with Variable-Duration Audio
  and Multi-Objective Training
Authors: Xinhao Mei, Gael Le Lan, Haohe Liu, Zhaoheng Ni, Varun Nagaraja, Yang
  Liu, Yangyang Shi, Vikas Chandra
Categories: eess.AS cs.AI cs.SD
Comments: Accepted to ICASSP 2026
\\
  Contrastive language-audio pretraining (CLAP) has achieved notable success in
learning semantically rich audio representations and is widely adopted for
various audio-related tasks. However, current CLAP models face several key
limitations. First, they are typically trained on relatively small datasets,
often comprising a few million audio samples. Second, existing CLAP models are
restricted to short and fixed duration, which constrains their usage in
real-world scenarios with variable-duration audio. Third, the standard
contrastive training objective operates on global representations, which may
hinder the learning of dense, fine-grained audio features. To address these
challenges, we introduce Scalable Language-Audio Pretraining (SLAP), which
scales language-audio pretraining to 109 million audio-text pairs with variable
audio durations and incorporates multiple training objectives. SLAP unifies
contrastive loss with additional self-supervised and captioning losses in a
single-stage training, facilitating the learning of richer dense audio
representations. The proposed SLAP model achieves new state-of-the-art
performance on audio-text retrieval and zero-shot audio classification tasks,
demonstrating its effectiveness across diverse benchmarks.
\\ ( https://arxiv.org/abs/2601.12594 ,  272kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12607 (*cross-listing*)
Date: Sun, 18 Jan 2026 22:37:09 GMT   (407kb)

Title: A Cloud-based Multi-Agentic Workflow for Science
Authors: Anurag Acharya, Timothy Vega, Rizwan A. Ashraf, Anshu Sharma, Derek
  Parker, Robert Rallo
Categories: cs.CL cs.AI
\\
  As Large Language Models (LLMs) become ubiquitous across various scientific
domains, their lack of ability to perform complex tasks like running
simulations or to make complex decisions limits their utility. LLM-based agents
bridge this gap due to their ability to call external resources and tools and
thus are now rapidly gaining popularity. However, coming up with a workflow
that can balance the models, cloud providers, and external resources is very
challenging, making implementing an agentic system more of a hindrance than a
help. In this work, we present a domain-agnostic, model-independent workflow
for an agentic framework that can act as a scientific assistant while being run
entirely on cloud. Built with a supervisor agent marshaling an array of agents
with individual capabilities, our framework brings together straightforward
tasks like literature review and data analysis with more complex ones like
simulation runs. We describe the framework here in full, including a
proof-of-concept system we built to accelerate the study of Catalysts, which is
highly important in the field of Chemistry and Material Science. We report the
cost to operate and use this framework, including the breakdown of the cost by
services use. We also evaluate our system on a custom-curated synthetic
benchmark and a popular Chemistry benchmark, and also perform expert validation
of the system. The results show that our system is able to route the task to
the correct agent 90% of the time and successfully complete the assigned task
97.5% of the time for the synthetic tasks and 91% of the time for real-world
tasks, while still achieving better or comparable accuracy to most frontier
models, showing that this is a viable framework for other scientific domains to
replicate.
\\ ( https://arxiv.org/abs/2601.12607 ,  407kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12617 (*cross-listing*)
Date: Sun, 18 Jan 2026 23:18:34 GMT   (34311kb)

Title: Creating Disability Story Videos with Generative AI: Motivation,
  Expression, and Sharing
Authors: Shuo Niu, Dylan Clements, and Hyungsin Kim
Categories: cs.HC cs.AI
DOI: 10.1145/3772318.3791495
\\
  Generative AI (GenAI) is both promising and challenging in supporting people
with disabilities (PwDs) in creating stories about disability. GenAI can reduce
barriers to media production and inspire the creativity of PwDs, but it may
also introduce biases and imperfections that hinder its adoption for personal
expression. In this research, we examine how nine PwD from a disability
advocacy group used GenAI to create videos sharing their disability
experiences. Grounded in digital storytelling theory, we explore the
motivations, expression, and sharing of PwD-created GenAI story videos. We
conclude with a framework of momentous depiction, which highlights four core
affordances of GenAI that either facilitate or require improvements to better
support disability storytelling: non-capturable depiction, identity concealment
and representation, contextual realism and consistency, and emotional
articulation. Based on this framework, we further discuss design implications
for GenAI in relation to story completion, media formats, and corrective
mechanisms.
\\ ( https://arxiv.org/abs/2601.12617 ,  34311kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12638 (*cross-listing*)
Date: Mon, 19 Jan 2026 00:59:13 GMT   (193kb)

Title: Mixed Precision PointPillars for Efficient 3D Object Detection with
  TensorRT
Authors: Ninnart Fuengfusin, Keisuke Yoneda, Naoki Suganuma
Categories: cs.CV cs.AI
Comments: 6 pages, 3 figures
\\
  LIDAR 3D object detection is one of the important tasks for autonomous
vehicles. Ensuring that this task operates in real-time is crucial. Toward
this, model quantization can be used to accelerate the runtime. However,
directly applying model quantization often leads to performance degradation due
to LIDAR's wide numerical distributions and extreme outliers. To address the
wide numerical distribution, we proposed a mixed precision framework designed
for PointPillars. Our framework first searches for sensitive layers with
post-training quantization (PTQ) by quantizing one layer at a time to 8-bit
integer (INT8) and evaluating each model for average precision (AP). The top-k
most sensitive layers are assigned as floating point (FP). Combinations of
these layers are greedily searched to produce candidate mixed precision models,
which are finalized with either PTQ or quantization-aware training (QAT).
Furthermore, to handle outliers, we observe that using a very small number of
calibration data reduces the likelihood of encountering outliers, thereby
improving PTQ performance. Our methods provides mixed precision models without
training in the PTQ pipeline, while our QAT pipeline achieves the performance
competitive to FP models. With TensorRT deployment, our models offer less
latency and sizes by up to 2.35 and 2.26 times, respectively.
\\ ( https://arxiv.org/abs/2601.12638 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12646 (*cross-listing*)
Date: Mon, 19 Jan 2026 01:44:14 GMT   (109kb)

Title: Unbounded Harms, Bounded Law: Liability in the Age of Borderless AI
Authors: Ha-Chi Tran
Categories: cs.CY cs.AI
\\
  The rapid proliferation of artificial intelligence (AI) has exposed
significant deficiencies in risk governance. While ex-ante harm identification
and prevention have advanced, Responsible AI scholarship remains underdeveloped
in addressing ex-post liability. Core legal questions regarding liability
allocation, responsibility attribution, and remedial effectiveness remain
insufficiently theorized and institutionalized, particularly for transboundary
harms and risks that transcend national jurisdictions. Drawing on contemporary
AI risk analyses, we argue that such harms are structurally embedded in global
AI supply chains and are likely to escalate in frequency and severity due to
cross-border deployment, data infrastructures, and uneven national oversight
capacities. Consequently, territorially bounded liability regimes are
increasingly inadequate. Using a comparative and interdisciplinary approach,
this paper examines compensation and liability frameworks from high-risk
transnational domains - including vaccine injury schemes, systemic financial
risk governance, commercial nuclear liability, and international environmental
regimes - to distill transferable legal design principles such as strict
liability, risk pooling, collective risk-sharing, and liability channelling,
while highlighting potential structural constraints on their application to
AI-related harms. Situated within an international order shaped more by AI arms
race dynamics than cooperative governance, the paper outlines the contours of a
global AI accountability and compensation architecture, emphasizing the tension
between geopolitical rivalry and the collective action required to govern
transboundary AI risks effectively.
\\ ( https://arxiv.org/abs/2601.12646 ,  109kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12648 (*cross-listing*)
Date: Mon, 19 Jan 2026 01:45:51 GMT   (48kb)

Title: Intelligent Documentation in Medical Education: Can AI Replace Manual
  Case Logging?
Authors: Nafiz Imtiaz Khan, Kylie Cleland, Vladimir Filkov, Roger Eric Goldman
Categories: cs.CL cs.AI
Comments: 51 pages, 12 figures, 8 tables. Feasibility study using retrospective
  radiology reports. Submitted to JAMIA Open (under review)
\\
  Procedural case logs are a core requirement in radiology training, yet they
are time-consuming to complete and prone to inconsistency when authored
manually. This study investigates whether large language models (LLMs) can
automate procedural case log documentation directly from free-text radiology
reports. We evaluate multiple local and commercial LLMs under instruction-based
and chain-of-thought prompting to extract structured procedural information
from 414 curated interventional radiology reports authored by nine residents
between 2018 and 2024. Model performance is assessed using sensitivity,
specificity, and F1-score, alongside inference latency and token efficiency to
estimate operational cost. Results show that both local and commercial models
achieve strong extraction performance, with best F1-scores approaching 0.87,
while exhibiting different trade-offs between speed and cost. Automation using
LLMs has the potential to substantially reduce clerical burden for trainees and
improve consistency in case logging. These findings demonstrate the feasibility
of AI-assisted documentation in medical education and highlight the need for
further validation across institutions and clinical workflows.
\\ ( https://arxiv.org/abs/2601.12648 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12658 (*cross-listing*)
Date: Mon, 19 Jan 2026 02:08:47 GMT   (843kb)

Title: Augmenting Question Answering with A Hybrid RAG Approach
Authors: Tianyi Yang, Nashrah Haque, Vaishnave Jonnalagadda, Yuya Jeremy Ong,
  Zhehui Chen, Yanzhao Wu, Lei Yu, Divyesh Jadav, Wenqi Wei
Categories: cs.CL cs.AI
Comments: 10 pages, 5 tables, 2 figures; presented at IEEE CogMI 2025
\\
  Retrieval-Augmented Generation (RAG) has emerged as a powerful technique for
enhancing the quality of responses in Question-Answering (QA) tasks. However,
existing approaches often struggle with retrieving contextually relevant
information, leading to incomplete or suboptimal answers. In this paper, we
introduce Structured-Semantic RAG (SSRAG), a hybrid architecture that enhances
QA quality by integrating query augmentation, agentic routing, and a structured
retrieval mechanism combining vector and graph based techniques with context
unification. By refining retrieval processes and improving contextual
grounding, our approach improves both answer accuracy and informativeness. We
conduct extensive evaluations on three popular QA datasets, TruthfulQA, SQuAD
and WikiQA, across five Large Language Models (LLMs), demonstrating that our
proposed approach consistently improves response quality over standard RAG
implementations.
\\ ( https://arxiv.org/abs/2601.12658 ,  843kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12664 (*cross-listing*)
Date: Mon, 19 Jan 2026 02:24:24 GMT   (1525kb)

Title: Generalizable Hyperparameter Optimization for Federated Learning on
  Non-IID Cancer Images
Authors: Elisa Gon\c{c}alves Ribeiro, Rodrigo Moreira, Larissa Ferreira
  Rodrigues Moreira, Andr\'e Ricardo Backes
Categories: cs.CV cs.AI cs.LG
Comments: 21st International Conference on Computer Vision Theory and
  Applications (VISAPP 2026), 9-11 March 2026, Marbella, Spain
\\
  Deep learning for cancer histopathology training conflicts with privacy
constraints in clinical settings. Federated Learning (FL) mitigates this by
keeping data local; however, its performance depends on hyperparameter choices
under non-independent and identically distributed (non-IID) client datasets.
This paper examined whether hyperparameters optimized on one cancer imaging
dataset generalized across non-IID federated scenarios. We considered binary
histopathology tasks for ovarian and colorectal cancers. We perform centralized
Bayesian hyperparameter optimization and transfer dataset-specific optima to
the non-IID FL setup. The main contribution of this study is the introduction
of a simple cross-dataset aggregation heuristic by combining configurations by
averaging the learning rates and considering the modal optimizers and batch
sizes. This combined configuration achieves a competitive classification
performance.
\\ ( https://arxiv.org/abs/2601.12664 ,  1525kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12671 (*cross-listing*)
Date: Mon, 19 Jan 2026 02:32:50 GMT   (1737kb)

Title: Exploiting Test-Time Augmentation in Federated Learning for Brain Tumor
  MRI Classification
Authors: Thamara Leandra de Deus Melo, Rodrigo Moreira, Larissa Ferreira
  Rodrigues Moreira, Andr\'e Ricardo Backes
Categories: cs.CV cs.AI cs.LG
Comments: 21st International Conference on Computer Vision Theory and
  Applications (VISAPP 2026), 9-11 March 2026, Marbella, Spain
\\
  Efficient brain tumor diagnosis is crucial for early treatment; however, it
is challenging because of lesion variability and image complexity. We evaluated
convolutional neural networks (CNNs) in a federated learning (FL) setting,
comparing models trained on original versus preprocessed MRI images (resizing,
grayscale conversion, normalization, filtering, and histogram equalization).
Preprocessing alone yielded negligible gains; combined with test-time
augmentation (TTA), it delivered consistent, statistically significant
improvements in federated MRI classification (p<0.001). In practice, TTA should
be the default inference strategy in FL-based medical imaging; when the
computational budget permits, pairing TTA with light preprocessing provides
additional reliable gains.
\\ ( https://arxiv.org/abs/2601.12671 ,  1737kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12715 (*cross-listing*)
Date: Mon, 19 Jan 2026 04:37:34 GMT   (10503kb)

Title: RSOD: Reliability-Guided Sonar Image Object Detection with Extremely
  Limited Labels
Authors: Chengzhou Li, Ping Guo, Guanchen Meng, Qi Jia, Jinyuan Liu, Zhu Liu,
  Xiaokang Liu, Yu Liu, Zhongxuan Luo, Xin Fan
Categories: cs.CV cs.AI
Comments: Accepted by AAAI 2026,9 pages,10 figures
\\
  Object detection in sonar images is a key technology in underwater detection
systems. Compared to natural images, sonar images contain fewer texture details
and are more susceptible to noise, making it difficult for non-experts to
distinguish subtle differences between classes. This leads to their inability
to provide precise annotation data for sonar images. Therefore, designing
effective object detection methods for sonar images with extremely limited
labels is particularly important. To address this, we propose a teacher-student
framework called RSOD, which aims to fully learn the characteristics of sonar
images and develop a pseudo-label strategy suitable for these images to
mitigate the impact of limited labels. First, RSOD calculates a reliability
score by assessing the consistency of the teacher's predictions across
different views. To leverage this score, we introduce an object mixed
pseudo-label method to tackle the shortage of labeled data in sonar images.
Finally, we optimize the performance of the student by implementing a
reliability-guided adaptive constraint. By taking full advantage of unlabeled
data, the student can perform well even in situations with extremely limited
labels. Notably, on the UATD dataset, our method, using only 5% of labeled
data, achieves results that can compete against those of our baseline algorithm
trained on 100% labeled data. We also collected a new dataset to provide more
valuable data for research in the field of sonar.
\\ ( https://arxiv.org/abs/2601.12715 ,  10503kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12723 (*cross-listing*)
Date: Mon, 19 Jan 2026 04:58:15 GMT   (227kb)

Title: An Evolutionary Framework for Automatic Optimization Benchmark
  Generation via Large Language Models
Authors: Yuhiro Ono and Tomohiro Harada and Yukiya Miura
Categories: cs.NE cs.AI
\\
  Optimization benchmarks play a fundamental role in assessing algorithm
performance; however, existing artificial benchmarks often fail to capture the
diversity and irregularity of real-world problem structures, while benchmarks
derived from real-world problems are costly and difficult to construct. To
address these challenges, we propose an evolutionary automatic benchmark
generation framework that leverages a large language model (LLM) as a
generative operator, termed the LLM-driven evolutionary benchmark generator
(LLM-EBG). In this framework, the LLM serves as an evolutionary operator that
generates and evolves benchmark problems within a flexible, expressive
representation space. As a case study, we generate unconstrained
single-objective continuous minimization problems represented as mathematical
expressions designed to induce significant performance differences between a
genetic algorithm (GA) and differential evolution (DE). Experimental results
show that LLM-EBG successfully produces benchmark problems in which the
designated target algorithm consistently outperforms the comparative algorithm
in more than 80\% of trials. Furthermore, exploratory landscape analysis
reveals that benchmarks favoring GA are highly sensitive to variable scaling,
demonstrating that the proposed framework can generate problems with distinct
geometric characteristics that reflect the intrinsic search behaviors of
different optimization algorithms.
\\ ( https://arxiv.org/abs/2601.12723 ,  227kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12727 (*cross-listing*)
Date: Mon, 19 Jan 2026 05:16:57 GMT   (1451kb)

Title: AI-exhibited Personality Traits Can Shape Human Self-concept through
  Conversations
Authors: Jingshu Li, Tianqi Song, Nattapat Boonprakong, Zicheng Zhu, Yitian
  Yang, Yi-Chieh Lee
Categories: cs.HC cs.AI
Comments: ACM CHI 2026
DOI: 10.1145/3772318.3790654
\\
  Recent Large Language Model (LLM) based AI can exhibit recognizable and
measurable personality traits during conversations to improve user experience.
However, as human understandings of their personality traits can be affected by
their interaction partners' traits, a potential risk is that AI traits may
shape and bias users' self-concept of their own traits. To explore the
possibility, we conducted a randomized behavioral experiment. Our results
indicate that after conversations about personal topics with an LLM-based AI
chatbot using GPT-4o default personality traits, users' self-concepts aligned
with the AI's measured personality traits. The longer the conversation, the
greater the alignment. This alignment led to increased homogeneity in
self-concepts among users. We also observed that the degree of self-concept
alignment was positively associated with users' conversation enjoyment. Our
findings uncover how AI personality traits can shape users' self-concepts
through human-AI conversation, highlighting both risks and opportunities. We
provide important design implications for developing more responsible and
ethical AI systems.
\\ ( https://arxiv.org/abs/2601.12727 ,  1451kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12731 (*cross-listing*)
Date: Mon, 19 Jan 2026 05:21:21 GMT   (9250kb)

Title: A Shared Geometry of Difficulty in Multilingual Language Models
Authors: Stefano Civelli, Pietro Bernardelle, Nicol\`o Brunello, Gianluca
  Demartini
Categories: cs.CL cs.AI
\\
  Predicting problem-difficulty in large language models (LLMs) refers to
estimating how difficult a task is according to the model itself, typically by
training linear probes on its internal representations. In this work, we study
the multilingual geometry of problem-difficulty in LLMs by training linear
probes using the AMC subset of the Easy2Hard benchmark, translated into 21
languages. We found that difficulty-related signals emerge at two distinct
stages of the model internals, corresponding to shallow (early-layers) and deep
(later-layers) internal representations, that exhibit functionally different
behaviors. Probes trained on deep representations achieve high accuracy when
evaluated on the same language but exhibit poor cross-lingual generalization.
In contrast, probes trained on shallow representations generalize substantially
better across languages, despite achieving lower within-language performance.
Together, these results suggest that LLMs first form a language-agnostic
representation of problem difficulty, which subsequently becomes
language-specific. This closely aligns with existing findings in LLM
interpretability showing that models tend to operate in an abstract conceptual
space before producing language-specific outputs. We demonstrate that this
two-stage representational process extends beyond semantic content to
high-level meta-cognitive properties such as problem-difficulty estimation.
\\ ( https://arxiv.org/abs/2601.12731 ,  9250kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12740 (*cross-listing*)
Date: Mon, 19 Jan 2026 05:39:35 GMT   (3839kb)

Title: TreeWriter: AI-Assisted Hierarchical Planning and Writing for Long-Form
  Documents
Authors: Zijian Zhang, Fangshi Du, Xingjian Liu, Pan Chen, Oliver Huang,
  Runlong Ye, Michael Liut, Al\'an Aspuru-Guzik
Categories: cs.HC cs.AI
\\
  Long documents pose many challenges to current intelligent writing systems.
These include maintaining consistency across sections, sustaining efficient
planning and writing as documents become more complex, and effectively
providing and integrating AI assistance to the user. Existing AI co-writing
tools offer either inline suggestions or limited structured planning, but
rarely support the entire writing process that begins with high-level ideas and
ends with polished prose, in which many layers of planning and outlining are
needed. Here, we introduce TreeWriter, a hierarchical writing system that
represents documents as trees and integrates contextual AI support. TreeWriter
allows authors to create, save, and refine document outlines at multiple
levels, facilitating drafting, understanding, and iterative editing of long
documents. A built-in AI agent can dynamically load relevant content, navigate
the document hierarchy, and provide context-aware editing suggestions. A
within-subject study (N=12) comparing TreeWriter with Google Docs + Gemini on
long-document editing and creative writing tasks shows that TreeWriter improves
idea exploration/development, AI helpfulness, and perceived authorial control.
A two-month field deployment (N=8) further demonstrated that hierarchical
organization supports collaborative writing. Our findings highlight the
potential of hierarchical, tree-structured editors with integrated AI support
and provide design guidelines for future AI-assisted writing tools that balance
automation with user agency.
\\ ( https://arxiv.org/abs/2601.12740 ,  3839kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12742 (*cross-listing*)
Date: Mon, 19 Jan 2026 05:50:03 GMT   (20615kb)

Title: AirHunt: Bridging VLM Semantics and Continuous Planning for Efficient
  Aerial Object Navigation
Authors: Xuecheng Chen, Zongzhuo Liu, Jianfa Ma, Bang Du, Tiantian Zhang,
  Xueqian Wang, and Boyu Zhou
Categories: cs.RO cs.AI
\\
  Recent advances in large Vision-Language Models (VLMs) have provided rich
semantic understanding that empowers drones to search for open-set objects via
natural language instructions. However, prior systems struggle to integrate
VLMs into practical aerial systems due to orders-of-magnitude frequency
mismatch between VLM inference and real-time planning, as well as VLMs' limited
3D scene understanding. They also lack a unified mechanism to balance semantic
guidance with motion efficiency in large-scale environments. To address these
challenges, we present AirHunt, an aerial object navigation system that
efficiently locates open-set objects with zero-shot generalization in outdoor
environments by seamlessly fusing VLM semantic reasoning with continuous path
planning. AirHunt features a dual-pathway asynchronous architecture that
establishes a synergistic interface between VLM reasoning and path planning,
enabling continuous flight with adaptive semantic guidance that evolves through
motion. Moreover, we propose an active dual-task reasoning module that exploits
geometric and semantic redundancy to enable selective VLM querying, and a
semantic-geometric coherent planning module that dynamically reconciles
semantic priorities and motion efficiency in a unified framework, enabling
seamless adaptation to environmental heterogeneity. We evaluate AirHunt across
diverse object navigation tasks and environments, demonstrating a higher
success rate with lower navigation error and reduced flight time compared to
state-of-the-art methods. Real-world experiments further validate AirHunt's
practical capability in complex and challenging environments. Code and dataset
will be made publicly available before publication.
\\ ( https://arxiv.org/abs/2601.12742 ,  20615kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12754 (*cross-listing*)
Date: Mon, 19 Jan 2026 06:20:57 GMT   (1105kb)

Title: PAIR-SAFE: A Paired-Agent Approach for Runtime Auditing and Refining
  AI-Mediated Mental Health Support
Authors: Jiwon Kim, Violeta J. Rodriguez, Dong Whi Yoo, Eshwar Chandrasekharan,
  and Koustuv Saha
Categories: cs.HC cs.AI cs.CL cs.CY
\\
  Large language models (LLMs) are increasingly used for mental health support,
yet they can produce responses that are overly directive, inconsistent, or
clinically misaligned, particularly in sensitive or high-risk contexts.
Existing approaches to mitigating these risks largely rely on implicit
alignment through training or prompting, offering limited transparency and
runtime accountability. We introduce PAIR-SAFE, a paired-agent framework for
auditing and refining AI-generated mental health support that integrates a
Responder agent with a supervisory Judge agent grounded in the clinically
validated Motivational Interviewing Treatment Integrity (MITI-4) framework. The
Judgeaudits each response and provides structuredALLOW or REVISE decisions that
guide runtime response refinement. We simulate counseling interactions using a
support-seeker simulator derived from human-annotated motivational interviewing
data. We find that Judge-supervised interactions show significant improvements
in key MITI dimensions, including Partnership, Seek Collaboration, and overall
Relational quality. Our quantitative findings are supported by qualitative
expert evaluation, which further highlights the nuances of runtime supervision.
Together, our results reveal that such pairedagent approach can provide
clinically grounded auditing and refinement for AI-assisted conversational
mental health support.
\\ ( https://arxiv.org/abs/2601.12754 ,  1105kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12758 (*cross-listing*)
Date: Mon, 19 Jan 2026 06:38:52 GMT   (3239kb)

Title: VISPA: Pluralistic Alignment via Automatic Value Selection and
  Activation
Authors: Shenyan Zheng, Jiayou Zhong, Anudeex Shetty, Heng Ji, Preslav Nakov,
  Usman Naseem
Categories: cs.CL cs.AI cs.LG
Comments: WIP
\\
  As large language models are increasingly used in high-stakes domains, it is
essential that their outputs reflect not average} human preference, rather
range of varying perspectives. Achieving such pluralism, however, remains
challenging. Existing approaches consider limited values or rely on
prompt-level interventions, lacking value control and representation. To
address this, we introduce VISPA, a training-free pluralistic alignment
framework, that enables direct control over value expression by dynamic
selection and internal model activation steering. Across extensive empirical
studies spanning multiple models and evaluation settings, we show VISPA is
performant across all pluralistic alignment modes in healthcare and beyond.
Further analysis reveals VISPA is adaptable with different steering
initiations, model, and/or values. These results suggest that pluralistic
alignment can be achieved through internal activation mechanisms, offering a
scalable path toward language models that serves all.
\\ ( https://arxiv.org/abs/2601.12758 ,  3239kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12805 (*cross-listing*)
Date: Mon, 19 Jan 2026 08:06:35 GMT   (8451kb)

Title: SciHorizon-GENE: Benchmarking LLM for Life Sciences Inference from Gene
  Knowledge to Functional Understanding
Authors: Xiaohan Huang, Meng Xiao, Chuan Qin, Qingqing Long, Jinmiao Chen,
  Yuanchun Zhou, Hengshu Zhu
Categories: q-bio.GN cs.AI cs.CL
Comments: 16 pages
\\
  Large language models (LLMs) have shown growing promise in biomedical
research, particularly for knowledge-driven interpretation tasks. However,
their ability to reliably reason from gene-level knowledge to functional
understanding, However, their ability to reliably reason from gene-level
knowledge to functional understanding, a core requirement for
knowledge-enhanced cell atlas interpretation, remains largely underexplored. To
address this gap, we introduce SciHorizon-GENE, a large-scale gene-centric
benchmark constructed from authoritative biological databases. The benchmark
integrates curated knowledge for over 190K human genes and comprises more than
540K questions covering diverse gene-to-function reasoning scenarios relevant
to cell type annotation, functional interpretation, and mechanism-oriented
analysis. Motivated by behavioral patterns observed in preliminary
examinations, SciHorizon-GENE evaluates LLMs along four biologically critical
perspectives: research attention sensitivity, hallucination tendency, answer
completeness, and literature influence, explicitly targeting failure modes that
limit the safe adoption of LLMs in biological interpretation pipelines. We
systematically evaluate a wide range of state-of-the-art general-purpose and
biomedical LLMs, revealing substantial heterogeneity in gene-level reasoning
capabilities and persistent challenges in generating faithful, complete, and
literature-grounded functional interpretations. Our benchmark establishes a
systematic foundation for analyzing LLM behavior at the gene scale and offers
insights for model selection and development, with direct relevance to
knowledge-enhanced biological interpretation.
\\ ( https://arxiv.org/abs/2601.12805 ,  8451kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12809 (*cross-listing*)
Date: Mon, 19 Jan 2026 08:16:11 GMT   (5683kb)

Title: Left-Right Symmetry Breaking in CLIP-style Vision-Language Models
  Trained on Synthetic Spatial-Relation Data
Authors: Takaki Yamamoto, Chihiro Noguchi, Toshihiro Tanizawa
Categories: cs.CV cs.AI cs.LG
\\
  Spatial understanding remains a key challenge in vision-language models. Yet
it is still unclear whether such understanding is truly acquired, and if so,
through what mechanisms. We present a controllable 1D image-text testbed to
probe how left-right relational understanding emerges in Transformer-based
vision and text encoders trained with a CLIP-style contrastive objective. We
train lightweight Transformer-based vision and text encoders end-to-end on
paired descriptions of one- and two-object scenes and evaluate generalization
to unseen object pairs while systematically varying label and layout diversity.
We find that contrastive training learns left-right relations and that label
diversity, more than layout diversity, is the primary driver of generalization
in this setting. To gain the mechanistic understanding, we perform an attention
decomposition and show that interactions between positional and token
embeddings induce a horizontal attention gradient that breaks left-right
symmetry in the encoders; ablating this contribution substantially reduces
left-right discrimination. Our results provide a mechanistic insight of when
and how CLIP-style models acquire relational competence.
\\ ( https://arxiv.org/abs/2601.12809 ,  5683kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12837 (*cross-listing*)
Date: Mon, 19 Jan 2026 08:50:18 GMT   (15784kb)

Title: Cognition spaces: natural, artificial, and hybrid
Authors: Ricard Sol\'e and Luis F Seoane and Jordi Pla-Mauri and Michael
  Timothy Bennett and Michael E. Hochberg and Michael Levin
Categories: q-bio.NC cs.AI cs.HC cs.NE
\\
  Cognitive processes are realized across an extraordinary range of natural,
artificial, and hybrid systems, yet there is no unified framework for comparing
their forms, limits, and unrealized possibilities. Here, we propose a cognition
space approach that replaces narrow, substrate-dependent definitions with a
comparative representation based on organizational and informational
dimensions. Within this framework, cognition is treated as a graded capacity to
sense, process, and act upon information, allowing systems as diverse as cells,
brains, artificial agents, and human-AI collectives to be analyzed within a
common conceptual landscape. We introduce and examine three cognition spaces --
basal aneural, neural, and human-AI hybrid -- and show that their occupation is
highly uneven, with clusters of realized systems separated by large unoccupied
regions. We argue that these voids are not accidental but reflect evolutionary
contingencies, physical constraints, and design limitations. By focusing on the
structure of cognition spaces rather than on categorical definitions, this
approach clarifies the diversity of existing cognitive systems and highlights
hybrid cognition as a promising frontier for exploring novel forms of
complexity beyond those produced by biological evolution.
\\ ( https://arxiv.org/abs/2601.12837 ,  15784kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12849 (*cross-listing*)
Date: Mon, 19 Jan 2026 09:02:32 GMT   (47kb)

Title: The Cost of EFX: Generalized-Mean Welfare and Complexity Dichotomies
  with Few Surplus Items
Authors: Eugene Lim, Tzeh Yuan Neoh, Nicholas Teh
Categories: cs.GT cs.AI cs.MA econ.TH
\\
  Envy-freeness up to any good (EFX) is a central fairness notion for
allocating indivisible goods, yet its existence is unresolved in general. In
the setting with few surplus items, where the number of goods exceeds the
number of agents by a small constant (at most three), EFX allocations are
guaranteed to exist, shifting the focus from existence to efficiency and
computation. We study how EFX interacts with generalized-mean ($p$-mean)
welfare, which subsumes commonly-studied utilitarian ($p=1$), Nash ($p=0$), and
egalitarian ($p \rightarrow -\infty$) objectives. We establish sharp complexity
dichotomies at $p=0$: for any fixed $p \in (0,1]$, both deciding whether EFX
can attain the global $p$-mean optimum and computing an EFX allocation
maximizing $p$-mean welfare are NP-hard, even with at most three surplus goods;
in contrast, for any fixed $p \leq 0$, we give polynomial-time algorithms that
optimize $p$-mean welfare within the space of EFX allocations and efficiently
certify when EFX attains the global optimum. We further quantify the welfare
loss of enforcing EFX via the price of fairness framework, showing that for $p
> 0$, the loss can grow linearly with the number of agents, whereas for $p \leq
0$, it is bounded by a constant depending on the surplus (and for Nash welfare
it vanishes asymptotically). Finally we show that requiring Pareto-optimality
alongside EFX is NP-hard (and becomes $\Sigma_2^P$-complete for a stronger
variant of EFX). Overall, our results delineate when EFX is computationally
costly versus structurally aligned with welfare maximization in the setting
with few surplus items.
\\ ( https://arxiv.org/abs/2601.12849 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12882 (*cross-listing*)
Date: Mon, 19 Jan 2026 09:36:08 GMT   (2704kb)

Title: YOLO26: An Analysis of NMS-Free End to End Framework for Real-Time
  Object Detection
Authors: Sudip Chakrabarty
Categories: cs.CV cs.AI
\\
  The "You Only Look Once" (YOLO) framework has long served as the benchmark
for real-time object detection, yet traditional iterations (YOLOv1 through
YOLO11) remain constrained by the latency and hyperparameter sensitivity of
Non-Maximum Suppression (NMS) post-processing. This paper analyzes a
comprehensive analysis of YOLO26, an architecture that fundamentally redefines
this paradigm by eliminating NMS in favor of a native end-to-end learning
strategy. This study examines the critical innovations that enable this
transition, specifically the introduction of the MuSGD optimizer for
stabilizing lightweight backbones, STAL for small-target-aware assignment, and
ProgLoss for dynamic supervision. Through a systematic review of official
performance benchmarks, the results demonstrate that YOLO26 establishes a new
Pareto front, outperforming a comprehensive suite of predecessors and
state-of-the-art competitors (including RTMDet and DAMO-YOLO) in both inference
speed and detection accuracy. The analysis confirms that by decoupling
representation learning from heuristic post-processing, YOLOv26 successfully
resolves the historical trade-off between latency and precision, signaling the
next evolutionary step in edge-based computer vision.
\\ ( https://arxiv.org/abs/2601.12882 ,  2704kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12886 (*cross-listing*)
Date: Mon, 19 Jan 2026 09:39:00 GMT   (101kb)

Title: Communication Methods in Multi-Agent Reinforcement Learning
Authors: Christoph Wittner
Categories: cs.MA cs.AI cs.LG
Comments: 12 pages, 2 figures
ACM-class: I.2.11
\\
  Multi-agent reinforcement learning is a promising research area that extends
established reinforcement learning approaches to problems formulated as
multi-agent systems. Recently, a multitude of communication methods have been
introduced to this field to address problems such as partially observable
environments, non-stationarity, and exponentially growing action spaces.
Communication further enables efficient cooperation among all agents
interacting in an environment. This work aims at providing an overview of
communication techniques in multi-agent reinforcement learning. By an in-depth
analysis of 29 publications on this topic, the strengths and weaknesses of
explicit, implicit, attention-based, graph-based, and hierarchical/role-based
communication are evaluated. The results of this comparison show that there is
no general, optimal communication framework for every problem. On the contrary,
the choice of communication depends heavily on the problem at hand. The
comparison also highlights the importance of communication methods with low
computational overhead to enable scalability to environments where many agents
interact. Finally, the paper discusses current research gaps, emphasizing the
need for standardized benchmarking of system-level metrics and improved
robustness under realistic communication conditions to enhance the real-world
applicability of these approaches.
\\ ( https://arxiv.org/abs/2601.12886 ,  101kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12904 (*cross-listing*)
Date: Mon, 19 Jan 2026 09:59:39 GMT   (1837kb)

Title: From Prefix Cache to Fusion RAG Cache: Accelerating LLM Inference in
  Retrieval-Augmented Generation
Authors: Jiahao Wang, Weiyu Xie, Mingxing Zhang, Boxing Zhang, Jianwei Dong,
  Yuening Zhu, Chen Lin, Jinqi Tang, Yaochen Han, Zhiyuan Ai, Xianglin Chen,
  Yongwei Wu, and Congfeng Jiang
Categories: cs.CL cs.AI
DOI: 10.1145/3786655
\\
  Retrieval-Augmented Generation enhances Large Language Models by integrating
external knowledge, which reduces hallucinations but increases prompt length.
This increase leads to higher computational costs and longer Time to First
Token (TTFT). To mitigate this issue, existing solutions aim to reuse the
preprocessed KV cache of each retrieved chunk to accelerate RAG. However, the
lack of cross-chunk contextual information leads to a significant drop in
generation quality, leaving the potential benefits of KV cache reuse largely
unfulfilled. The challenge lies in how to reuse the precomputed KV cache of
chunks while preserving generation quality. We propose FusionRAG, a novel
inference framework that optimizes both the preprocessing and reprocessing
stages of RAG. In the offline preprocessing stage, we embed information from
other related text chunks into each chunk, while in the online reprocessing
stage, we recompute the KV cache for tokens that the model focuses on. As a
result, we achieve a better trade-off between generation quality and
efficiency. According to our experiments, FusionRAG significantly improves
generation quality at the same recomputation ratio compared to previous
state-of-the-art solutions. By recomputing fewer than 15% of the tokens,
FusionRAG achieves up to 70% higher normalized F1 scores than baselines and
reduces TTFT by 2.66x-9.39x compared to Full Attention.
\\ ( https://arxiv.org/abs/2601.12904 ,  1837kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12910 (*cross-listing*)
Date: Mon, 19 Jan 2026 10:04:33 GMT   (536kb)

Title: SciCoQA: Quality Assurance for Scientific Paper--Code Alignment
Authors: Tim Baumg\"artner, Iryna Gurevych
Categories: cs.CL cs.AI
\\
  We present SciCoQA, a dataset for detecting discrepancies between scientific
publications and their codebases to ensure faithful implementations. We
construct SciCoQA from GitHub issues and reproducibility papers, and to scale
our dataset, we propose a synthetic data generation method for constructing
paper-code discrepancies. We analyze the paper-code discrepancies in detail and
propose discrepancy types and categories to better understand the occurring
mismatches. In total, our dataset consists of 611 paper-code discrepancies (81
real, 530 synthetic), spanning diverse computational science disciplines,
including AI, Physics, Quantitative Biology, and others. Our evaluation of 21
LLMs highlights the difficulty of SciCoQA, particularly for instances involving
omitted paper details, long-context inputs, and data outside the models'
pre-training corpus. The best performing model in our evaluation, GPT-5, can
only detect 45.7\% of real-world paper-code discrepancies.
\\ ( https://arxiv.org/abs/2601.12910 ,  536kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12922 (*cross-listing*)
Date: Mon, 19 Jan 2026 10:26:12 GMT   (1210kb)

Title: Your Privacy Depends on Others: Collusion Vulnerabilities in Individual
  Differential Privacy
Authors: Johannes Kaiser, Alexander Ziller, Eleni Triantafillou, Daniel
  R\"uckert, Georgios Kaissis
Categories: cs.CR cs.AI cs.LG
\\
  Individual Differential Privacy (iDP) promises users control over their
privacy, but this promise can be broken in practice. We reveal a previously
overlooked vulnerability in sampling-based iDP mechanisms: while conforming to
the iDP guarantees, an individual's privacy risk is not solely governed by
their own privacy budget, but critically depends on the privacy choices of all
other data contributors. This creates a mismatch between the promise of
individual privacy control and the reality of a system where risk is
collectively determined. We demonstrate empirically that certain distributions
of privacy preferences can unintentionally inflate the privacy risk of
individuals, even when their formal guarantees are met. Moreover, this excess
risk provides an exploitable attack vector. A central adversary or a set of
colluding adversaries can deliberately choose privacy budgets to amplify
vulnerabilities of targeted individuals. Most importantly, this attack operates
entirely within the guarantees of DP, hiding this excess vulnerability. Our
empirical evaluation demonstrates successful attacks against 62% of targeted
individuals, substantially increasing their membership inference
susceptibility. To mitigate this, we propose
$(\varepsilon_i,\delta_i,\overline{\Delta})$-iDP a privacy contract that uses
$\Delta$-divergences to provide users with a hard upper bound on their excess
vulnerability, while offering flexibility to mechanism design. Our findings
expose a fundamental challenge to the current paradigm, demanding a
re-evaluation of how iDP systems are designed, audited, communicated, and
deployed to make excess risks transparent and controllable.
\\ ( https://arxiv.org/abs/2601.12922 ,  1210kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12925 (*cross-listing*)
Date: Mon, 19 Jan 2026 10:28:42 GMT   (11194kb)

Title: ForeDiffusion: Foresight-Conditioned Diffusion Policy via Future View
  Construction for Robot Manipulation
Authors: Weize Xie, Yi Ding, Ying He, Leilei Wang, Binwen Bai, Zheyi Zhao,
  Chenyang Wang, F. Richard Yu
Categories: cs.RO cs.AI
\\
  Diffusion strategies have advanced visual motor control by progressively
denoising high-dimensional action sequences, providing a promising method for
robot manipulation. However, as task complexity increases, the success rate of
existing baseline models decreases considerably. Analysis indicates that
current diffusion strategies are confronted with two limitations. First, these
strategies only rely on short-term observations as conditions. Second, the
training objective remains limited to a single denoising loss, which leads to
error accumulation and causes grasping deviations. To address these
limitations, this paper proposes Foresight-Conditioned Diffusion
(ForeDiffusion), by injecting the predicted future view representation into the
diffusion process. As a result, the policy is guided to be forward-looking,
enabling it to correct trajectory deviations. Following this design,
ForeDiffusion employs a dual loss mechanism, combining the traditional
denoising loss and the consistency loss of future observations, to achieve the
unified optimization. Extensive evaluation on the Adroit suite and the
MetaWorld benchmark demonstrates that ForeDiffusion achieves an average success
rate of 80% for the overall task, significantly outperforming the existing
mainstream diffusion methods by 23% in complex tasks, while maintaining more
stable performance across the entire tasks.
\\ ( https://arxiv.org/abs/2601.12925 ,  11194kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12929 (*cross-listing*)
Date: Mon, 19 Jan 2026 10:30:53 GMT   (231kb)

Title: Membership Inference Test: Auditing Training Data in Object
  Classification Models
Authors: Gonzalo Mancera, Daniel DeAlcala, Aythami Morales, Ruben Tolosana and
  Julian Fierrez
Categories: cs.CV cs.AI
Comments: Deployable AI (DAI 2025) workshop co-located with AAAI-25
\\
  In this research, we analyze the performance of Membership Inference Tests
(MINT), focusing on determining whether given data were utilized during the
training phase, specifically in the domain of object recognition. Within the
area of object recognition, we propose and develop architectures tailored for
MINT models. These architectures aim to optimize performance and efficiency in
data utilization, offering a tailored solution to tackle the complexities
inherent in the object recognition domain. We conducted experiments involving
an object detection model, an embedding extractor, and a MINT module. These
experiments were performed in three public databases, totaling over 174K
images. The proposed architecture leverages convolutional layers to capture and
model the activation patterns present in the data during the training process.
Through our analysis, we are able to identify given data used for testing and
training, achieving precision rates ranging between 70% and 80%, contingent
upon the depth of the detection module layer chosen for input to the MINT
module. Additionally, our studies entail an analysis of the factors influencing
the MINT Module, delving into the contributing elements behind more transparent
training processes.
\\ ( https://arxiv.org/abs/2601.12929 ,  231kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12937 (*cross-listing*)
Date: Mon, 19 Jan 2026 10:46:51 GMT   (121kb)

Title: On the Evidentiary Limits of Membership Inference for Copyright Auditing
Authors: Murat Bilgehan Ertan, Emirhan B\"oge, Min Chen, Kaleel Mahmood, Marten
  van Dijk
Categories: cs.CR cs.AI
\\
  As large language models (LLMs) are trained on increasingly opaque corpora,
membership inference attacks (MIAs) have been proposed to audit whether
copyrighted texts were used during training, despite growing concerns about
their reliability under realistic conditions. We ask whether MIAs can serve as
admissible evidence in adversarial copyright disputes where an accused model
developer may obfuscate training data while preserving semantic content, and
formalize this setting through a judge-prosecutor-accused communication
protocol. To test robustness under this protocol, we introduce SAGE
(Structure-Aware SAE-Guided Extraction), a paraphrasing framework guided by
Sparse Autoencoders (SAEs) that rewrites training data to alter lexical
structure while preserving semantic content and downstream utility. Our
experiments show that state-of-the-art MIAs degrade when models are fine-tuned
on SAGE-generated paraphrases, indicating that their signals are not robust to
semantics-preserving transformations. While some leakage remains in certain
fine-tuning regimes, these results suggest that MIAs are brittle in adversarial
settings and insufficient, on their own, as a standalone mechanism for
copyright auditing of LLMs.
\\ ( https://arxiv.org/abs/2601.12937 ,  121kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12938 (*cross-listing*)
Date: Mon, 19 Jan 2026 10:46:52 GMT   (357kb)

Title: The Post-Turing Condition: Conceptualising Artificial Subjectivity and
  Synthetic Sociality
Authors: Thorsten Jelinek, Patrick Glauner, Alvin Wang Graylin, Yubao Qiu
Categories: cs.CY cs.AI
Comments: Conceptual perspective on AI design trajectories, meaning formation,
  and synthetic sociality. 5 pages, 1 figure
ACM-class: K.4.1; I.2.0
\\
  In the Post-Turing era, artificial intelligence increasingly shapes social
coordination and meaning formation rather than merely automating cognitive
tasks. The central challenge is therefore not whether machines become
conscious, but whether processes of interpretation and shared reference are
progressively automated in ways that marginalize human participation. This
paper introduces the PRMO framework, relating AI design trajectories to four
constitutive dimensions of human subjectivity: Perception, Representation,
Meaning, and the Real. Within this framework, Synthetic Sociality denotes a
technological horizon in which artificial agents negotiate coherence and social
order primarily among themselves, raising the structural risk of human
exclusion from meaning formation. To address this risk, the paper proposes
Quadrangulation as a design principle for socially embedded AI systems,
requiring artificial agents to treat the human subject as a constitutive
reference within shared contexts of meaning. This work is a conceptual
perspective that contributes a structural vocabulary for analyzing AI systems
at the intersection of computation and society, without proposing a specific
technical implementation.
\\ ( https://arxiv.org/abs/2601.12938 ,  357kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12939 (*cross-listing*)
Date: Mon, 19 Jan 2026 10:47:26 GMT   (13575kb)

Title: Active Inference-Driven World Modeling for Adaptive UAV Swarm Trajectory
  Design
Authors: Kaleem Arshid and Ali Krayani and Lucio Marcenaro and David Martin
  Gomez and Carlo Regazzoni
Categories: cs.RO cs.AI eess.SP
Comments: This paper has been accepted for presentation at the 2026 IEEE
  International Conference on Acoustics, Speech, and Signal Processing (IEEE
  ICASSP 2026) Workshop: 'Multi-Modal Signal Processing and AI for
  Communications and Sensing in 6G and Beyond (MuSiC-6GB)'
\\
  This paper proposes an Active Inference-based framework for autonomous
trajectory design in UAV swarms. The method integrates probabilistic reasoning
and self-learning to enable distributed mission allocation, route ordering, and
motion planning. Expert trajectories generated using a Genetic Algorithm with
Repulsion Forces (GA-RF) are employed to train a hierarchical World Model
capturing swarm behavior across mission, route, and motion levels. During
online operation, UAVs infer actions by minimizing divergence between current
beliefs and model-predicted states, enabling adaptive responses to dynamic
environments. Simulation results show faster convergence, higher stability, and
safer navigation than Q-Learning, demonstrating the scalability and cognitive
grounding of the proposed framework for intelligent UAV swarm control.
\\ ( https://arxiv.org/abs/2601.12939 ,  13575kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12946 (*cross-listing*)
Date: Mon, 19 Jan 2026 10:54:03 GMT   (13724kb)

Title: AI-generated data contamination erodes pathological variability and
  diagnostic reliability
Authors: Hongyu He, Shaowen Xiang, Ye Zhang, Yingtao Zhu, Jin Zhang, Hao Deng,
  Emily Alsentzer, Qingyu Chen, Kun-Hsing Yu, Andrew Marmenshall, Tingting
  Chen, Srinivas Anumasa, Daniel Ebner, Dean Ho, Kee Yuan Ngiam, Ching-Yu
  Cheng, and Dianbo Liu
Categories: cs.CY cs.AI cs.CL cs.CV cs.LG
Comments: *Corresponding author: Dianbo Liu (dianbo@nus.edu.sg)
\\
  Generative artificial intelligence (AI) is rapidly populating medical records
with synthetic content, creating a feedback loop where future models are
increasingly at risk of training on uncurated AI-generated data. However, the
clinical consequences of this AI-generated data contamination remain
unexplored. Here, we show that in the absence of mandatory human verification,
this self-referential cycle drives a rapid erosion of pathological variability
and diagnostic reliability. By analysing more than 800,000 synthetic data
points across clinical text generation, vision-language reporting, and medical
image synthesis, we find that models progressively converge toward generic
phenotypes regardless of the model architecture. Specifically, rare but
critical findings, including pneumothorax and effusions, vanish from the
synthetic content generated by AI models, while demographic representations
skew heavily toward middle-aged male phenotypes. Crucially, this degradation is
masked by false diagnostic confidence; models continue to issue reassuring
reports while failing to detect life-threatening pathology, with false
reassurance rates tripling to 40%. Blinded physician evaluation confirms that
this decoupling of confidence and accuracy renders AI-generated documentation
clinically useless after just two generations. We systematically evaluate three
mitigation strategies, finding that while synthetic volume scaling fails to
prevent collapse, mixing real data with quality-aware filtering effectively
preserves diversity. Ultimately, our results suggest that without
policy-mandated human oversight, the deployment of generative AI threatens to
degrade the very healthcare data ecosystems it relies upon.
\\ ( https://arxiv.org/abs/2601.12946 ,  13724kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13018 (*cross-listing*)
Date: Mon, 19 Jan 2026 12:52:18 GMT   (2940kb)

Title: Bi-Attention HateXplain : Taking into account the sequential aspect of
  data during explainability in a multi-task context
Authors: Ghislain Dorian Tchuente Mondjo
Categories: cs.CL cs.AI
Comments: Accepted at "EAI AFRICOMM 2025 - 17th EAI International Conference on
  Communications and Networks in Africa"
MSC-class: 68T50
ACM-class: I.2.7
\\
  Technological advances in the Internet and online social networks have
brought many benefits to humanity. At the same time, this growth has led to an
increase in hate speech, the main global threat. To improve the reliability of
black-box models used for hate speech detection, post-hoc approaches such as
LIME, SHAP, and LRP provide the explanation after training the classification
model. In contrast, multi-task approaches based on the HateXplain benchmark
learn to explain and classify simultaneously. However, results from
HateXplain-based algorithms show that predicted attention varies considerably
when it should be constant. This attention variability can lead to inconsistent
interpretations, instability of predictions, and learning difficulties. To
solve this problem, we propose the BiAtt-BiRNN-HateXplain (Bidirectional
Attention BiRNN HateXplain) model which is easier to explain compared to LLMs
which are more complex in view of the need for transparency, and will take into
account the sequential aspect of the input data during explainability thanks to
a BiRNN layer. Thus, if the explanation is correctly estimated, thanks to
multi-task learning (explainability and classification task), the model could
classify better and commit fewer unintentional bias errors related to
communities. The experimental results on HateXplain data show a clear
improvement in detection performance, explainability and a reduction in
unintentional bias.
\\ ( https://arxiv.org/abs/2601.13018 ,  2940kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13111 (*cross-listing*)
Date: Mon, 19 Jan 2026 14:51:23 GMT   (2741kb)

Title: CORE-T: COherent REtrieval of Tables for Text-to-SQL
Authors: Hassan Soliman, Vivek Gupta, Dan Roth, Iryna Gurevych
Categories: cs.CL cs.AI cs.IR
Comments: Preprint under review. Code and data available at:
  https://github.com/UKPLab/arxiv2026-core-t
\\
  Realistic text-to-SQL workflows often require joining multiple tables. As a
result, accurately retrieving the relevant set of tables becomes a key
bottleneck for end-to-end performance. We study an open-book setting where
queries must be answered over large, heterogeneous table collections pooled
from many sources, without clean scoping signals such as database identifiers.
Here, dense retrieval (DR) achieves high recall but returns many distractors,
while join-aware alternatives often rely on extra assumptions and/or incur high
inference overhead. We propose CORE-T, a scalable, training-free framework that
enriches tables with LLM-generated purpose metadata and pre-computes a
lightweight table-compatibility cache. At inference time, DR returns top-K
candidates; a single LLM call selects a coherent, joinable subset, and a simple
additive adjustment step restores strongly compatible tables. Across Bird,
Spider, and MMQA, CORE-T improves table-selection F1 by up to 22.7 points while
retrieving up to 42% fewer tables, improving multi-table execution accuracy by
up to 5.0 points on Bird and 6.9 points on MMQA, and using 4-5x fewer tokens
than LLM-intensive baselines.
\\ ( https://arxiv.org/abs/2601.13111 ,  2741kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13114 (*cross-listing*)
Date: Mon, 19 Jan 2026 14:55:48 GMT   (1329kb)

Title: IntAgent: NWDAF-Based Intent LLM Agent Towards Advanced Next Generation
  Networks
Authors: Abdelrahman Soliman, Ahmed Refaey, Aiman Erbad, and Amr Mohamed
Categories: cs.NI cs.AI cs.SY eess.SY
Comments: conference
\\
  Intent-based networks (IBNs) are gaining prominence as an innovative
technology that automates network operations through high-level request
statements, defining what the network should achieve. In this work, we
introduce IntAgent, an intelligent intent LLM agent that integrates NWDAF
analytics and tools to fulfill the network operator's intents. Unlike previous
approaches, we develop an intent tools engine directly within the NWDAF
analytics engine, allowing our agent to utilize live network analytics to
inform its reasoning and tool selection. We offer an enriched, 3GPP-compliant
data source that enhances the dynamic, context-aware fulfillment of network
operator goals, along with an MCP tools server for scheduling, monitoring, and
analytics tools. We demonstrate the efficacy of our framework through two
practical use cases: ML-based traffic prediction and scheduled policy
enforcement, which validate IntAgent's ability to autonomously fulfill complex
network intents.
\\ ( https://arxiv.org/abs/2601.13114 ,  1329kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13142 (*cross-listing*)
Date: Mon, 19 Jan 2026 15:24:32 GMT   (4236kb)

Title: TVWorld: Foundations for Remote-Control TV Agents
Authors: Zhantao Ma, Quanfeng Lu, Shuai Zhong, Dahai Yu, Ping Luo, Michael K.
  Ng
Categories: cs.CV cs.AI cs.CL
\\
  Recent large vision-language models (LVLMs) have demonstrated strong
potential for device control. However, existing research has primarily focused
on point-and-click (PnC) interaction, while remote-control (RC) interaction
commonly encountered in everyday TV usage remains largely underexplored. To
fill this gap, we introduce \textbf{TVWorld}, an offline graph-based
abstraction of real-world TV navigation that enables reproducible and
deployment-free evaluation. On this basis, we derive two complementary
benchmarks that comprehensively assess TV-use capabilities: \textbf{TVWorld-N}
for topology-aware navigation and \textbf{TVWorld-G} for focus-aware grounding.
These benchmarks expose a key limitation of existing agents: insufficient
topology awareness for focus-based, long-horizon TV navigation. Motivated by
this finding, we propose a \emph{Topology-Aware Training} framework that
injects topology awareness into LVLMs. Using this framework, we develop
\textbf{TVTheseus}, a foundation model specialized for TV navigation. TVTheseus
achieves a success rate of $68.3\%$ on TVWorld-N, surpassing strong
closed-source baselines such as Gemini 3 Flash and establishing
state-of-the-art (SOTA) performance. Additional analyses further provide
valuable insights into the development of effective TV-use agents.
\\ ( https://arxiv.org/abs/2601.13142 ,  4236kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13166 (*cross-listing*)
Date: Mon, 19 Jan 2026 15:43:51 GMT   (782kb)

Title: From 100,000+ images to winning the first brain MRI foundation model
  challenges: Sharing lessons and models
Authors: Pedro M. Gordaliza, Jaume Banus, Beno\^it G\'erin, Maxence Wynen,
  Nataliia Molchanova, and Jonas Richiardi, Meritxell Bach Cuadra
Categories: cs.CV cs.AI cs.LG
Comments: Work presented at the SSL3D Challenge (1st place, ResEnc-L track) and
  FOMO Challenge (1st place, Methods track) on Brain MRI Foundation Models at
  MICCAI 2025
\\
  Developing Foundation Models for medical image analysis is essential to
overcome the unique challenges of radiological tasks. The first challenges of
this kind for 3D brain MRI, SSL3D and FOMO25, were held at MICCAI 2025. Our
solution ranked first in tracks of both contests. It relies on a U-Net CNN
architecture combined with strategies leveraging anatomical priors and
neuroimaging domain knowledge. Notably, our models trained 1-2 orders of
magnitude faster and were 10 times smaller than competing transformer-based
approaches. Models are available here:
https://github.com/jbanusco/BrainFM4Challenges.
\\ ( https://arxiv.org/abs/2601.13166 ,  782kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13187 (*cross-listing*)
Date: Mon, 19 Jan 2026 16:10:22 GMT   (547kb)

Title: Scientific production in the era of Large Language Models
Authors: Keigo Kusumegi, Xinyu Yang, Paul Ginsparg, Mathijs de Vaan, Toby
  Stuart, Yian Yin
Categories: cs.DL cs.AI cs.CY physics.soc-ph
Comments: This is the author's version of the work. The definitive version was
  published in Science on 18 Dec 2025, DOI: 10.1126/science.adw3000. Link to
  the Final Published Version:
  https://www.science.org/doi/10.1126/science.adw3000
Journal-ref: Science, 390(6779), pp.1240-1243 (2025)
DOI: 10.1126/science.adw3000
\\
  Large Language Models (LLMs) are rapidly reshaping scientific research. We
analyze these changes in multiple, large-scale datasets with 2.1M preprints,
28K peer review reports, and 246M online accesses to scientific documents. We
find: 1) scientists adopting LLMs to draft manuscripts demonstrate a large
increase in paper production, ranging from 23.7-89.3% depending on scientific
field and author background, 2) LLM use has reversed the relationship between
writing complexity and paper quality, leading to an influx of manuscripts that
are linguistically complex but substantively underwhelming, and 3) LLM adopters
access and cite more diverse prior work, including books and younger,
less-cited documents. These findings highlight a stunning shift in scientific
production that will likely require a change in how journals, funding agencies,
and tenure committees evaluate scientific works.
\\ ( https://arxiv.org/abs/2601.13187 ,  547kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13197 (*cross-listing*)
Date: Mon, 19 Jan 2026 16:22:27 GMT   (737kb)

Title: Diffusion-Driven Synthetic Tabular Data Generation for Enhanced DoS/DDoS
  Attack Classification
Authors: Aravind B, Anirud R.S., Sai Surya Teja N, Bala Subrahmanya Sriranga
  Navaneeth A, Karthika R, Mohankumar N
Categories: cs.CR cs.AI cs.LG
Comments: 7 pages, 8 figures, 2025 International Conference on Signal
  Processing, Computation, Electronics, Power and Telecommunication
  (IConSCEPT), National Institute of Technology, Puducherry, India
\\
  Class imbalance refers to a situation where certain classes in a dataset have
significantly fewer samples than oth- ers, leading to biased model performance.
Class imbalance in network intrusion detection using Tabular Denoising
Diffusion Probability Models (TabDDPM) for data augmentation is ad- dressed in
this paper. Our approach synthesizes high-fidelity minority-class samples from
the CIC-IDS2017 dataset through iterative denoising processes. For the minority
classes that have smaller samples, synthetic samples were generated and merged
with the original dataset. The augmented training data enables an ANN
classifier to achieve near-perfect recall on previously underrepresented attack
classes. These results establish diffusion models as an effective solution for
tabular data imbalance in security domains, with potential applications in
fraud detection and medical diagnostics.
\\ ( https://arxiv.org/abs/2601.13197 ,  737kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13217 (*cross-listing*)
Date: Mon, 19 Jan 2026 16:48:45 GMT   (459kb)

Title: Beyond Single-shot Writing: Deep Research Agents are Unreliable at
  Multi-turn Report Revision
Authors: Bingsen Chen, Boyan Li, Ping Nie, Yuyu Zhang, Xi Ye, Chen Zhao
Categories: cs.CL cs.AI
\\
  Existing benchmarks for Deep Research Agents (DRAs) treat report generation
as a single-shot writing task, which fundamentally diverges from how human
researchers iteratively draft and revise reports via self-reflection or peer
feedback. Whether DRAs can reliably revise reports with user feedback remains
unexplored. We introduce Mr Dre, an evaluation suite that establishes
multi-turn report revision as a new evaluation axis for DRAs. Mr Dre consists
of (1) a unified long-form report evaluation protocol spanning
comprehensiveness, factuality, and presentation, and (2) a human-verified
feedback simulation pipeline for multi-turn revision. Our analysis of five
diverse DRAs reveals a critical limitation: while agents can address most user
feedback, they also regress on 16-27% of previously covered content and
citation quality. Over multiple revision turns, even the best-performing agents
leave significant headroom, as they continue to disrupt content outside the
feedback's scope and fail to preserve earlier edits. We further show that these
issues are not easily resolvable through inference-time fixes such as prompt
engineering and a dedicated sub-agent for report revision.
\\ ( https://arxiv.org/abs/2601.13217 ,  459kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13222 (*cross-listing*)
Date: Mon, 19 Jan 2026 16:57:33 GMT   (189kb)

Title: Incorporating Q&A Nuggets into Retrieval-Augmented Generation
Authors: Laura Dietz, Bryan Li, Gabrielle Liu, Jia-Huei Ju, Eugene Yang, Dawn
  Lawrie, William Walden, James Mayfield
Categories: cs.IR cs.AI
ACM-class: H.3
\\
  RAGE systems integrate ideas from automatic evaluation (E) into
Retrieval-augmented Generation (RAG). As one such example, we present Crucible,
a Nugget-Augmented Generation System that preserves explicit citation
provenance by constructing a bank of Q&A nuggets from retrieved documents and
uses them to guide extraction, selection, and report generation. Reasoning on
nuggets avoids repeated information through clear and interpretable Q&A
semantics - instead of opaque cluster abstractions - while maintaining citation
provenance throughout the entire generation process. Evaluated on the TREC
NeuCLIR 2024 collection, our Crucible system substantially outperforms Ginger,
a recent nugget-based RAG system, in nugget recall, density, and citation
grounding.
\\ ( https://arxiv.org/abs/2601.13222 ,  189kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13227 (*cross-listing*)
Date: Mon, 19 Jan 2026 17:03:20 GMT   (437kb)

Title: Insider Knowledge: How Much Can RAG Systems Gain from Evaluation
  Secrets?
Authors: Laura Dietz, Bryan Li, Eugene Yang, Dawn Lawrie, William Walden, James
  Mayfield
Categories: cs.IR cs.AI
ACM-class: H.3
\\
  RAG systems are increasingly evaluated and optimized using LLM judges, an
approach that is rapidly becoming the dominant paradigm for system assessment.
Nugget-based approaches in particular are now embedded not only in evaluation
frameworks but also in the architectures of RAG systems themselves. While this
integration can lead to genuine improvements, it also creates a risk of faulty
measurements due to circularity. In this paper, we investigate this risk
through comparative experiments with nugget-based RAG systems, including Ginger
and Crucible, against strong baselines such as GPT-Researcher. By deliberately
modifying Crucible to generate outputs optimized for an LLM judge, we show that
near-perfect evaluation scores can be achieved when elements of the evaluation
- such as prompt templates or gold nuggets - are leaked or can be predicted.
Our results highlight the importance of blind evaluation settings and
methodological diversity to guard against mistaking metric overfitting for
genuine system progress.
\\ ( https://arxiv.org/abs/2601.13227 ,  437kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13228 (*cross-listing*)
Date: Mon, 19 Jan 2026 17:03:48 GMT   (475kb)

Title: Autoregressive Models Rival Diffusion Models at ANY-ORDER Generation
Authors: Tianqi Du, Lizhe Fang, Weijie Yang, Chenheng Zhang, Zeming Wei, Yifei
  Wang, Yisen Wang
Categories: cs.CL cs.AI
\\
  Diffusion language models enable any-order generation and bidirectional
conditioning, offering appealing flexibility for tasks such as infilling,
rewriting, and self-correction. However, their formulation-predicting one part
of a sequence from another within a single-step dependency-limits modeling
depth and often yields lower sample quality and stability than autoregressive
(AR) models. To address this, we revisit autoregressive modeling as a
foundation and reformulate diffusion-style training into a structured
multi-group prediction process. We propose Any-order Any-subset Autoregressive
modeling (A3), a generalized framework that extends the standard AR
factorization to arbitrary token groups and generation orders. A3 preserves the
probabilistic rigor and multi-layer dependency modeling of AR while inheriting
diffusion models' flexibility for parallel and bidirectional generation. We
implement A3 through a two-stream attention architecture and a progressive
adaptation strategy that transitions pretrained AR models toward any-order
prediction. Experiments on question answering, commonsense reasoning, and story
infilling demonstrate that A3 outperforms diffusion-based models while
maintaining flexible decoding. This work offers a unified approach for a
flexible, efficient, and novel language modeling paradigm.
\\ ( https://arxiv.org/abs/2601.13228 ,  475kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13235 (*cross-listing*)
Date: Mon, 19 Jan 2026 17:10:49 GMT   (709kb)

Title: RubRIX: Rubric-Driven Risk Mitigation in Caregiver-AI Interactions
Authors: Drishti Goel, Jeongah Lee, Qiuyue Joy Zhong, Violeta J. Rodriguez,
  Daniel S. Brown, Ravi Karkar, Dong Whi Yoo, and Koustuv Saha
Categories: cs.HC cs.AI cs.CL cs.CY cs.LG cs.SI
\\
  Caregivers seeking AI-mediated support express complex needs --
information-seeking, emotional validation, and distress cues -- that warrant
careful evaluation of response safety and appropriateness. Existing AI
evaluation frameworks, primarily focused on general risks (toxicity,
hallucinations, policy violations, etc), may not adequately capture the nuanced
risks of LLM-responses in caregiving-contexts. We introduce RubRIX
(Rubric-based Risk Index), a theory-driven, clinician-validated framework for
evaluating risks in LLM caregiving responses. Grounded in the Elements of an
Ethic of Care, RubRIX operationalizes five empirically-derived risk dimensions:
Inattention, Bias & Stigma, Information Inaccuracy, Uncritical Affirmation, and
Epistemic Arrogance. We evaluate six state-of-the-art LLMs on over 20,000
caregiver queries from Reddit and ALZConnected. Rubric-guided refinement
consistently reduced risk-components by 45-98% after one iteration across
models. This work contributes a methodological approach for developing
domain-sensitive, user-centered evaluation frameworks for high-burden contexts.
Our findings highlight the importance of domain-sensitive, interactional risk
evaluation for the responsible deployment of LLMs in caregiving support
contexts. We release benchmark datasets to enable future research on contextual
risk evaluation in AI-mediated support.
\\ ( https://arxiv.org/abs/2601.13235 ,  709kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13236 (*cross-listing*)
Date: Mon, 19 Jan 2026 17:12:28 GMT   (11093kb)

Title: Pixelwise Uncertainty Quantification of Accelerated MRI Reconstruction
Authors: Ilias I. Giannakopoulos, Lokesh B Gautham Muthukumar, Yvonne W. Lui,
  Riccardo Lattanzi
Categories: eess.IV cs.AI physics.med-ph
Comments: 10 pages, 8 figues, 2 tables
\\
  Parallel imaging techniques reduce magnetic resonance imaging (MRI) scan time
but image quality degrades as the acceleration factor increases. In clinical
practice, conservative acceleration factors are chosen because no mechanism
exists to automatically assess the diagnostic quality of undersampled
reconstructions. This work introduces a general framework for pixel-wise
uncertainty quantification in parallel MRI reconstructions, enabling automatic
identification of unreliable regions without access to any ground-truth
reference image. Our method integrates conformal quantile regression with image
reconstruction methods to estimate statistically rigorous pixel-wise
uncertainty intervals. We trained and evaluated our model on Cartesian
undersampled brain and knee data obtained from the fastMRI dataset using
acceleration factors ranging from 2 to 10. An end-to-end Variational Network
was used for image reconstruction. Quantitative experiments demonstrate strong
agreement between predicted uncertainty maps and true reconstruction error.
Using our method, the corresponding Pearson correlation coefficient was higher
than 90% at acceleration levels at and above four-fold; whereas it dropped to
less than 70% when the uncertainty was computed using a simpler a heuristic
notion (magnitude of the residual). Qualitative examples further show the
uncertainty maps based on quantile regression capture the magnitude and spatial
distribution of reconstruction errors across acceleration factors, with regions
of elevated uncertainty aligning with pathologies and artifacts. The proposed
framework enables evaluation of reconstruction quality without access to
fully-sampled ground-truth reference images. It represents a step toward
adaptive MRI acquisition protocols that may be able to dynamically balance scan
time and diagnostic reliability.
\\ ( https://arxiv.org/abs/2601.13236 ,  11093kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13238 (*cross-listing*)
Date: Mon, 19 Jan 2026 17:16:30 GMT   (5276kb)

Title: A Semantic Decoupling-Based Two-Stage Rainy-Day Attack for Revealing
  Weather Robustness Deficiencies in Vision-Language Models
Authors: Chengyin Hu, Xiang Chen, Zhe Jia, Weiwen Shi, Fengyu Zhang, Jiujiang
  Guo, Yiwei Wei
Categories: cs.CV cs.AI
\\
  Vision-Language Models (VLMs) are trained on image-text pairs collected under
canonical visual conditions and achieve strong performance on multimodal tasks.
However, their robustness to real-world weather conditions, and the stability
of cross-modal semantic alignment under such structured perturbations, remain
insufficiently studied. In this paper, we focus on rainy scenarios and
introduce the first adversarial framework that exploits realistic weather to
attack VLMs, using a two-stage, parameterized perturbation model based on
semantic decoupling to analyze rain-induced shifts in decision-making. In Stage
1, we model the global effects of rainfall by applying a low-dimensional global
modulation to condition the embedding space and gradually weaken the original
semantic decision boundaries. In Stage 2, we introduce structured rain
variations by explicitly modeling multi-scale raindrop appearance and
rainfall-induced illumination changes, and optimize the resulting
non-differentiable weather space to induce stable semantic shifts. Operating in
a non-pixel parameter space, our framework generates perturbations that are
both physically grounded and interpretable. Experiments across multiple tasks
show that even physically plausible, highly constrained weather perturbations
can induce substantial semantic misalignment in mainstream VLMs, posing
potential safety and reliability risks in real-world deployment. Ablations
further confirm that illumination modeling and multi-scale raindrop structures
are key drivers of these semantic shifts.
\\ ( https://arxiv.org/abs/2601.13238 ,  5276kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13247 (*cross-listing*)
Date: Mon, 19 Jan 2026 17:33:31 GMT   (1200kb)

Title: Aligning Agentic World Models via Knowledgeable Experience Learning
Authors: Baochang Ren, Yunzhi Yao, Rui Sun, Shuofei Qiao, Ningyu Zhang, Huajun
  Chen
Categories: cs.CL cs.AI cs.CV cs.LG cs.MM
Comments: Ongoing work
\\
  Current Large Language Models (LLMs) exhibit a critical modal disconnect:
they possess vast semantic knowledge but lack the procedural grounding to
respect the immutable laws of the physical world. Consequently, while these
agents implicitly function as world models, their simulations often suffer from
physical hallucinations-generating plans that are logically sound but
physically unexecutable. Existing alignment strategies predominantly rely on
resource-intensive training or fine-tuning, which attempt to compress dynamic
environmental rules into static model parameters. However, such parametric
encapsulation is inherently rigid, struggling to adapt to the open-ended
variability of physical dynamics without continuous, costly retraining. To
bridge this gap, we introduce WorldMind, a framework that autonomously
constructs a symbolic World Knowledge Repository by synthesizing environmental
feedback. Specifically, it unifies Process Experience to enforce physical
feasibility via prediction errors and Goal Experience to guide task optimality
through successful trajectories. Experiments on EB-ALFRED and EB-Habitat
demonstrate that WorldMind achieves superior performance compared to baselines
with remarkable cross-model and cross-environment transferability.
\\ ( https://arxiv.org/abs/2601.13247 ,  1200kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13260 (*cross-listing*)
Date: Mon, 19 Jan 2026 17:50:36 GMT   (301kb)

Title: Stop Taking Tokenizers for Granted: They Are Core Design Decisions in
  Large Language Models
Authors: Sawsan Alqahtani, Mir Tafseer Nayeem, Md Tahmid Rahman Laskar, Tasnim
  Mohiuddin, M Saiful Bari
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to EACL 2026 (long, main). The first two authors contributed
  equally
\\
  Tokenization underlies every large language model, yet it remains an
under-theorized and inconsistently designed component. Common subword
approaches such as Byte Pair Encoding (BPE) offer scalability but often
misalign with linguistic structure, amplify bias, and waste capacity across
languages and domains. This paper reframes tokenization as a core modeling
decision rather than a preprocessing step. We argue for a context-aware
framework that integrates tokenizer and model co-design, guided by linguistic,
domain, and deployment considerations. Standardized evaluation and transparent
reporting are essential to make tokenization choices accountable and
comparable. Treating tokenization as a core design problem, not a technical
afterthought, can yield language technologies that are fairer, more efficient,
and more adaptable.
\\ ( https://arxiv.org/abs/2601.13260 ,  301kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13286 (*cross-listing*)
Date: Mon, 19 Jan 2026 18:37:28 GMT   (1434kb)

Title: AI Skills Improve Job Prospects: Causal Evidence from a Hiring
  Experiment
Authors: Fabian Stephany, Ole Teutloff, Angelo Leone
Categories: econ.GN cs.AI q-fin.EC
Comments: 46 pages
\\
  The growing adoption of artificial intelligence (AI) technologies has
heightened interest in the labour market value of AI-related skills, yet causal
evidence on their role in hiring decisions remains scarce. This study examines
whether AI skills serve as a positive hiring signal and whether they can offset
conventional disadvantages such as older age or lower formal education. We
conduct an experimental survey with 1,700 recruiters from the United Kingdom
and the United States. Using a paired conjoint design, recruiters evaluated
hypothetical candidates represented by synthetically designed resumes. Across
three occupations - graphic designer, office assistant, and software engineer -
AI skills significantly increase interview invitation probabilities by
approximately 8 to 15 percentage points. AI skills also partially or fully
offset disadvantages related to age and lower education, with effects strongest
for office assistants, where formal AI certification plays an additional
compensatory role. Effects are weaker for graphic designers, consistent with
more skeptical recruiter attitudes toward AI in creative work. Finally,
recruiters' own background and AI usage significantly moderate these effects.
Overall, the findings demonstrate that AI skills function as a powerful hiring
signal and can mitigate traditional labour market disadvantages, with
implications for workers' skill acquisition strategies and firms' recruitment
practices.
\\ ( https://arxiv.org/abs/2601.13286 ,  1434kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13317 (*cross-listing*)
Date: Mon, 19 Jan 2026 19:00:56 GMT   (19686kb)

Title: Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme
  Modeling of Climate Discourse
Authors: Samantha Sudhoff, Pranav Perumal, Zhaoqing Wu, Tunazzina Islam
Categories: cs.CL cs.AI cs.CY cs.LG cs.SI
\\
  Climate discourse online plays a crucial role in shaping public understanding
of climate change and influencing political and policy outcomes. However,
climate communication unfolds across structurally distinct platforms with
fundamentally different incentive structures: paid advertising ecosystems
incentivize targeted, strategic persuasion, while public social media platforms
host largely organic, user-driven discourse. Existing computational studies
typically analyze these environments in isolation, limiting our ability to
distinguish institutional messaging from public expression. In this work, we
present a comparative analysis of climate discourse across paid advertisements
on Meta (previously known as Facebook) and public posts on Bluesky from July
2024 to September 2025. We introduce an interpretable, end-to-end thematic
discovery and assignment framework that clusters texts by semantic similarity
and leverages large language models (LLMs) to generate concise,
human-interpretable theme labels. We evaluate the quality of the induced themes
against traditional topic modeling baselines using both human judgments and an
LLM-based evaluator, and further validate their semantic coherence through
downstream stance prediction and theme-guided retrieval tasks. Applying the
resulting themes, we characterize systematic differences between paid climate
messaging and public climate discourse and examine how thematic prevalence
shifts around major political events. Our findings show that platform-level
incentives are reflected in the thematic structure, stance alignment, and
temporal responsiveness of climate narratives. While our empirical analysis
focuses on climate communication, the proposed framework is designed to support
comparative narrative analysis across heterogeneous communication environments.
\\ ( https://arxiv.org/abs/2601.13317 ,  19686kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13348 (*cross-listing*)
Date: Mon, 19 Jan 2026 19:33:58 GMT   (120kb)

Title: The AI Genie Phenomenon and Three Types of AI Chatbot Addiction:
  Escapist Roleplays, Pseudosocial Companions, and Epistemic Rabbit Holes
Authors: M. Karen Shen, Jessica Huang, Olivia Liang, Ig-Jae Kim, Dongwook Yoon
Categories: cs.HC cs.AI
Comments: To appear in CHI 2026
\\
  Recent reports on generative AI chatbot use raise concerns about its
addictive potential. An in-depth understanding is imperative to minimize risks,
yet AI chatbot addiction remains poorly understood. This study examines how to
characterize AI chatbot addiction--why users become addicted, the symptoms
commonly reported, and the distinct types it comprises. We conducted a thematic
analysis of Reddit entries (n=334) across 14 subreddits where users narrated
their experiences with addictive AI chatbot use, followed by an exploratory
data analysis. We found: (1) users' dependence tied to the "AI Genie"
phenomenon--users can get exactly anything they want with minimal effort--and
marked by symptoms that align with addiction literature, (2) three distinct
addiction types: Escapist Roleplay, Pseudosocial Companion, and Epistemic
Rabbit Hole, (3) sexual content involved in multiple cases, and (4) recovery
strategies' perceived helpfulness differ between addiction types. Our work lays
empirical groundwork to inform future strategies for prevention, diagnosis, and
intervention.
\\ ( https://arxiv.org/abs/2601.13348 ,  120kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13352 (*cross-listing*)
Date: Mon, 19 Jan 2026 19:41:39 GMT   (6019kb)

Title: LLM-as-RNN: A Recurrent Language Model for Memory Updates and Sequence
  Prediction
Authors: Yuxing Lu, J. Ben Tamo, Weichen Zhao, Nan Sun, Yishan Zhong, Wenqi
  Shi, Jinzhuo Wang, May D. Wang
Categories: cs.CL cs.AI cs.MA
Comments: 17 pages, 5 figures, 6 tables
\\
  Large language models are strong sequence predictors, yet standard inference
relies on immutable context histories. After making an error at generation step
t, the model lacks an updatable memory mechanism that improves predictions for
step t+1. We propose LLM-as-RNN, an inference-only framework that turns a
frozen LLM into a recurrent predictor by representing its hidden state as
natural-language memory. This state, implemented as a structured system-prompt
summary, is updated at each timestep via feedback-driven text rewrites,
enabling learning without parameter updates. Under a fixed token budget,
LLM-as-RNN corrects errors and retains task-relevant patterns, effectively
performing online learning through language. We evaluate the method on three
sequential benchmarks in healthcare, meteorology, and finance across Llama,
Gemma, and GPT model families. LLM-as-RNN significantly outperforms zero-shot,
full-history, and MemPrompt baselines, improving predictive accuracy by 6.5% on
average, while producing interpretable, human-readable learning traces absent
in standard context accumulation.
\\ ( https://arxiv.org/abs/2601.13352 ,  6019kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13376 (*cross-listing*)
Date: Mon, 19 Jan 2026 20:23:28 GMT   (1537kb)

Title: Bounded Minds, Generative Machines: Envisioning Conversational AI that
  Works with Human Heuristics and Reduces Bias Risk
Authors: Jiqun Liu
Categories: cs.ET cs.AI cs.HC
\\
  Conversational AI is rapidly becoming a primary interface for information
seeking and decision making, yet most systems still assume idealized users. In
practice, human reasoning is bounded by limited attention, uneven knowledge,
and reliance on heuristics that are adaptive but bias-prone. This article
outlines a research pathway grounded in bounded rationality, and argues that
conversational AI should be designed to work with human heuristics rather than
against them. It identifies key directions for detecting cognitive
vulnerability, supporting judgment under uncertainty, and evaluating
conversational systems beyond factual accuracy, toward decision quality and
cognitive robustness.
\\ ( https://arxiv.org/abs/2601.13376 ,  1537kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13385 (*cross-listing*)
Date: Mon, 19 Jan 2026 20:37:45 GMT   (1825kb)

Title: Organ-Aware Attention Improves CT Triage and Classification
Authors: Lavsen Dahal, Yubraj Bhandari, Geoffrey D. Rubin, Joseph Y. Lo
Categories: cs.CV cs.AI
\\
  There is an urgent need for triage and classification of high-volume medical
imaging modalities such as computed tomography (CT), which can improve patient
care and mitigate radiologist burnout. Study-level CT triage requires
calibrated predictions with localized evidence; however, off-the-shelf Vision
Language Models (VLM) struggle with 3D anatomy, protocol shifts, and noisy
report supervision. This study used the two largest publicly available chest CT
datasets: CT-RATE and RADCHEST-CT (held-out external test set). Our carefully
tuned supervised baseline (instantiated as a simple Global Average Pooling
head) establishes a new supervised state of the art, surpassing all reported
linear-probe VLMs. Building on this baseline, we present ORACLE-CT, an
encoder-agnostic, organ-aware head that pairs Organ-Masked Attention
(mask-restricted, per-organ pooling that yields spatial evidence) with
Organ-Scalar Fusion (lightweight fusion of normalized volume and mean-HU cues).
In the chest setting, ORACLE-CT masked attention model achieves AUROC 0.86 on
CT-RATE; in the abdomen setting, on MERLIN (30 findings), our supervised
baseline exceeds a reproduced zero-shot VLM baseline obtained by running
publicly released weights through our pipeline, and adding masked attention
plus scalar fusion further improves performance to AUROC 0.85. Together, these
results deliver state-of-the-art supervised classification performance across
both chest and abdomen CT under a unified evaluation protocol. The source code
is available at https://github.com/lavsendahal/oracle-ct.
\\ ( https://arxiv.org/abs/2601.13385 ,  1825kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13392 (*cross-listing*)
Date: Mon, 19 Jan 2026 21:00:31 GMT   (2411kb)

Title: Beyond Memorization: Testing LLM Reasoning on Unseen Theory of
  Computation Tasks
Authors: Shlok Shelat, Jay Raval, Souvik Roy, Manas Gaur
Categories: cs.CL cs.AI cs.FL
Comments: 30 pages, 11 figures, 6 tables, Work in Progress
\\
  Large language models (LLMs) have demonstrated strong performance on formal
language tasks, yet whether this reflects genuine symbolic reasoning or pattern
matching on familiar constructions remains unclear. We introduce a benchmark
for deterministic finite automata (DFA) construction from regular languages,
comprising factual knowledge questions, seen construction problems from public
sources, and two types of unseen problems: hand-crafted instances with multiple
interacting constraints and systematically generated problems via Arden's
theorem. Models achieve perfect accuracy on factual questions and 84-90% on
seen tasks. However, accuracy drops sharply on unseen problems (by 30-64%),
with failures stemming from systematic misinterpretation of language
constraints, incorrect handling of Kleene-star semantics, and a failure to
preserve global consistency. We evaluate a three-stage hint protocol that
enables correction of shallow errors but does not reliably resolve globally
inconsistent or structurally flawed automata. Our analysis across multiple
prompting strategies (direct, Chain-of-Thought, Tree-of-Thought) reveals that
errors persist regardless of prompting approach, exposing a fundamental gap
between LLMs' ability to generate syntactically plausible DFAs and their
capacity for semantically correct formal reasoning.
\\ ( https://arxiv.org/abs/2601.13392 ,  2411kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13400 (*cross-listing*)
Date: Mon, 19 Jan 2026 21:10:32 GMT   (1946kb)

Title: Deep Image Prior with L0 Gradient Regularizer for Image Smoothing
Authors: Nhat Thanh Tran, Kevin Bui, Jack Xin
Categories: cs.CV cs.AI
Comments: To be published in the Proceedings of IEEE ICASSP 2026
\\
  Image smoothing is a fundamental image processing operation that preserves
the underlying structure, such as strong edges and contours, and removes minor
details and textures in an image. Many image smoothing algorithms rely on
computing local window statistics or solving an optimization problem. Recent
state-of-the-art methods leverage deep learning, but they require a carefully
curated training dataset. Because constructing a proper training dataset for
image smoothing is challenging, we propose DIP-$\ell_0$, a deep image prior
framework that incorporates the $\ell_0$ gradient regularizer. This framework
can perform high-quality image smoothing without any training data. To properly
minimize the associated loss function that has the nonconvex, nonsmooth
$\ell_0$ ``norm", we develop an alternating direction method of multipliers
algorithm that utilizes an off-the-shelf $\ell_0$ gradient minimization solver.
Numerical experiments demonstrate that the proposed DIP-$\ell_0$ outperforms
many image smoothing algorithms in edge-preserving image smoothing and JPEG
artifact removal.
\\ ( https://arxiv.org/abs/2601.13400 ,  1946kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13401 (*cross-listing*)
Date: Mon, 19 Jan 2026 21:14:34 GMT   (11425kb)

Title: Reasoning with Pixel-level Precision: QVLM Architecture and SQuID
  Dataset for Quantitative Geospatial Analytics
Authors: Peter A. Massih and Eric Cosatto
Categories: cs.CV cs.AI
Comments: Submitted to CVPR 2026. Introduces the QVLM architecture and the
  SQuID dataset for quantitative geospatial reasoning. Dataset DOI:
  10.57967/hf/7565
\\
  Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning
because their architectures destroy pixel-level information required for
counting and measurements. Vision encoders compress images through patch
embeddings, reducing spatial indexing and losing the precise pixel-level
tracking required for accurate counting. We present two contributions to
address this fundamental limitation. First, we introduce SQuID (Satellite
Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image
Question-Answer pairs with both numerical range and categorical answers,
designed to evaluate quantitative spatial reasoning. The dataset spans three
difficulty tiers with annotations automatically generated from human labels and
their learned variability. Second, we propose QVLM (Quantitative
Vision-Language Model), a code-generation architecture that maintains pixel
precision by decoupling language understanding from visual analysis. Instead of
encoding images into embeddings, QVLM generates executable code that first
calls a segmentation model to obtain pixel-level masks, then operates directly
on these masks, preserving spatial indexing throughout the reasoning process.
Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on
SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work
reveals that, for quantitative spatial reasoning, architectural decoupling
enables better accuracy on quantitative tasks.
\\ ( https://arxiv.org/abs/2601.13401 ,  11425kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13404 (*cross-listing*)
Date: Mon, 19 Jan 2026 21:21:58 GMT   (594kb)

Title: Local-to-Global Logical Explanations for Deep Vision Models
Authors: Bhavan Vasu, Giuseppe Raffa, Prasad Tadepalli
Categories: cs.CV cs.AI
Comments: 15 pages, 5 figures, 5th International Joint Conference on Learning &
  Reasoning 2025
Journal-ref: 5th International Joint Conference on Learning & Reasoning 2025
\\
  While deep neural networks are extremely effective at classifying images,
they remain opaque and hard to interpret. We introduce local and global
explanation methods for black-box models that generate explanations in terms of
human-recognizable primitive concepts. Both the local explanations for a single
image and the global explanations for a set of images are cast as logical
formulas in monotone disjunctive-normal-form (MDNF), whose satisfaction
guarantees that the model yields a high score on a given class. We also present
an algorithm for explaining the classification of examples into multiple
classes in the form of a monotone explanation list over primitive concepts.
Despite their simplicity and interpretability we show that the explanations
maintain high fidelity and coverage with respect to the blackbox models they
seek to explain in challenging vision datasets.
\\ ( https://arxiv.org/abs/2601.13404 ,  594kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13406 (*cross-listing*)
Date: Mon, 19 Jan 2026 21:34:00 GMT   (741kb)

Title: Integrating Virtual Reality and Large Language Models for Team-Based
  Non-Technical Skills Training and Evaluation in the Operating Room
Authors: Jacob Barker, Doga Demirel, Cullen Jackson, Anna Johansson, Robbin
  Miraglia, Darian Hoagland, Stephanie B. Jones, John Mitchell, Daniel B.
  Jones, Suvranu De
Categories: cs.HC cs.AI
Comments: 23 pages, 7 figures, 1 table, 2 Appendices
\\
  Although effective teamwork and communication are critical to surgical
safety, structured training for non-technical skills (NTS) remains limited
compared with technical simulation. The ACS/APDS Phase III Team-Based Skills
Curriculum calls for scalable tools that both teach and objectively assess
these competencies during laparoscopic emergencies. We introduce the Virtual
Operating Room Team Experience (VORTeX), a multi-user virtual reality (VR)
platform that integrates immersive team simulation with large language model
(LLM) analytics to train and evaluate communication, decision-making, teamwork,
and leadership. Team dialogue is analyzed using structured prompts derived from
the Non-Technical Skills for Surgeons (NOTSS) framework, enabling automated
classification of behaviors and generation of directed interaction graphs that
quantify communication structure and hierarchy. Two laparoscopic emergency
scenarios, pneumothorax and intra-abdominal bleeding, were implemented to
elicit realistic stress and collaboration. Twelve surgical professionals
completed pilot sessions at the 2024 SAGES conference, rating VORTeX as
intuitive, immersive, and valuable for developing teamwork and communication.
The LLM consistently produced interpretable communication networks reflecting
expected operative hierarchies, with surgeons as central integrators, nurses as
initiators, and anesthesiologists as balanced intermediaries. By integrating
immersive VR with LLM-driven behavioral analytics, VORTeX provides a scalable,
privacy-compliant framework for objective assessment and automated,
data-informed debriefing across distributed training environments.
\\ ( https://arxiv.org/abs/2601.13406 ,  741kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13412 (*cross-listing*)
Date: Mon, 19 Jan 2026 21:48:41 GMT   (5031kb)

Title: Using deep learning for predicting cleansing quality of colon capsule
  endoscopy images
Authors: Puneet Sharma, Kristian Dalsb{\o} Hindberg, Benedicte Schelde-Olesen,
  Ulrik Deding, Esmaeil S. Nadimi, Jan-Matthias Braun
Categories: cs.CV cs.AI
Comments: 24 pages
\\
  In this study, we explore the application of deep learning techniques for
predicting cleansing quality in colon capsule endoscopy (CCE) images. Using a
dataset of 500 images labeled by 14 clinicians on the Leighton-Rex scale (Poor,
Fair, Good, and Excellent), a ResNet-18 model was trained for classification,
leveraging stratified K-fold cross-validation to ensure robust performance. To
optimize the model, structured pruning techniques were applied iteratively,
achieving significant sparsity while maintaining high accuracy. Explainability
of the pruned model was evaluated using Grad-CAM, Grad-CAM++, Eigen-CAM,
Ablation-CAM, and Random-CAM, with the ROAD method employed for consistent
evaluation. Our results indicate that for a pruned model, we can achieve a
cross-validation accuracy of 88% with 79% sparsity, demonstrating the
effectiveness of pruning in improving efficiency from 84% without compromising
performance. We also highlight the challenges of evaluating cleansing quality
of CCE images, emphasize the importance of explainability in clinical
applications, and discuss the challenges associated with using the ROAD method
for our task. Finally, we employ a variant of adaptive temperature scaling to
calibrate the pruned models for an external dataset.
\\ ( https://arxiv.org/abs/2601.13412 ,  5031kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13437 (*cross-listing*)
Date: Mon, 19 Jan 2026 22:49:41 GMT   (44kb)

Title: MOSLD-Bench: Multilingual Open-Set Learning and Discovery Benchmark for
  Text Categorization
Authors: Adriana-Valentina Costache, Daria-Nicoleta Dragomir, Silviu-Florin
  Gheorghe, Eduard Poesina, Paul Irofti, Radu Tudor Ionescu
Categories: cs.CL cs.AI cs.LG
\\
  Open-set learning and discovery (OSLD) is a challenging machine learning task
in which samples from new (unknown) classes can appear at test time. It can be
seen as a generalization of zero-shot learning, where the new classes are not
known a priori, hence involving the active discovery of new classes. While
zero-shot learning has been extensively studied in text classification,
especially with the emergence of pre-trained language models, open-set learning
and discovery is a comparatively new setup for the text domain. To this end, we
introduce the first multilingual open-set learning and discovery (MOSLD)
benchmark for text categorization by topic, comprising 960K data samples across
12 languages. To construct the benchmark, we (i) rearrange existing datasets
and (ii) collect new data samples from the news domain. Moreover, we propose a
novel framework for the OSLD task, which integrates multiple stages to
continuously discover and learn new classes. We evaluate several language
models, including our own, to obtain results that can be used as reference for
future work. We release our benchmark at
https://github.com/Adriana19Valentina/MOSLD-Bench.
\\ ( https://arxiv.org/abs/2601.13437 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13458 (*cross-listing*)
Date: Mon, 19 Jan 2026 23:23:29 GMT   (99kb)

Title: Labels or Preferences? Budget-Constrained Learning with Human Judgments
  over AI-Generated Outputs
Authors: Zihan Dong, Ruijia Wu, and Linjun Zhang
Categories: stat.ML cs.AI cs.LG math.ST stat.TH
\\
  The increasing reliance on human preference feedback to judge AI-generated
pseudo labels has created a pressing need for principled, budget-conscious data
acquisition strategies. We address the crucial question of how to optimally
allocate a fixed annotation budget between ground-truth labels and pairwise
preferences in AI. Our solution, grounded in semi-parametric inference, casts
the budget allocation problem as a monotone missing data framework. Building on
this formulation, we introduce Preference-Calibrated Active Learning (PCAL), a
novel method that learns the optimal data acquisition strategy and develops a
statistically efficient estimator for functionals of the data distribution.
Theoretically, we prove the asymptotic optimality of our PCAL estimator and
establish a key robustness guarantee that ensures robust performance even with
poorly estimated nuisance models. Our flexible framework applies to a general
class of problems, by directly optimizing the estimator's variance instead of
requiring a closed-form solution. This work provides a principled and
statistically efficient approach for budget-constrained learning in modern AI.
Simulations and real-data analysis demonstrate the practical benefits and
superior performance of our proposed method.
\\ ( https://arxiv.org/abs/2601.13458 ,  99kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13487 (*cross-listing*)
Date: Tue, 20 Jan 2026 00:46:51 GMT   (485kb)

Title: The Hidden Toll of Social Media News: Causal Effects on Psychosocial
  Wellbeing
Authors: Olivia Pal, Agam Goyal, Eshwar Chandrasekharan, and Koustuv Saha
Categories: cs.SI cs.AI cs.CL cs.CY cs.HC
\\
  News consumption on social media has become ubiquitous, yet how different
forms of engagement shape psychosocial outcomes remains unclear. To address
this gap, we leveraged a large-scale dataset of ~26M posts and ~45M comments on
the BlueSky platform, and conducted a quasi-experimental study, matching 81,345
Treated users exposed to News feeds with 83,711 Control users using stratified
propensity score analysis. We examined psychosocial wellbeing, in terms of
affective, behavioral, and cognitive outcomes. Our findings reveal that news
engagement produces systematic trade-offs: increased depression, stress, and
anxiety, yet decreased loneliness and increased social interaction on the
platform. Regression models reveal that News feed bookmarking is associated
with greater psychosocial deterioration compared to commenting or quoting, with
magnitude differences exceeding tenfold. These per-engagement effects
accumulate with repeated exposure, showing significant psychosocial impacts.
Our work extends theories of news effects beyond crisis-centric frameworks by
demonstrating that routine consumption creates distinct psychological dynamics
depending on engagement type, and bears implications for tools and
interventions for mitigating the psychosocial costs of news consumption on
social media.
\\ ( https://arxiv.org/abs/2601.13487 ,  485kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13508 (*cross-listing*)
Date: Tue, 20 Jan 2026 01:51:12 GMT   (5786kb)

Title: CatMaster: An Agentic Autonomous System for Computational Heterogeneous
  Catalysis Research
Authors: Honghao Chen, Jiangjie Qiu, Yi Shen Tew, Xiaonan Wang
Categories: cond-mat.mtrl-sci cs.AI
Comments: 25 pages
\\
  Density functional theory (DFT) is widely used to connect atomic structure
with catalytic behavior, but computational heterogeneous catalysis studies
often require long workflows that are costly, iterative, and sensitive to setup
choices. Besides the intrinsic cost and accuracy limits of first-principles
calculations, practical workflow issues such as keeping references consistent,
preparing many related inputs, recovering from failed runs on computing
clusters, and maintaining a complete record of what was done, can slow down
projects and make results difficult to reproduce or extend.
  Here we present CatMaster, a large-language-model (LLM)-driven agent system
that turns natural language requests into complete calculation workspaces,
including structures, inputs, outputs, logs, and a concise run record.
CatMaster maintains a persistent project record of key facts, constraints, and
file pointers to support inspection and restartability. It is paired with a
multi-fidelity tool library that covers rapid surrogate relaxations and
high-fidelity DFT calculations for validation when needed. We demonstrate
CatMaster on four demonstrations of increasing complexity: an O2 spin-state
check with remote execution, BCC Fe surface energies with a
protocol-sensitivity study and CO adsorption site ranking, high-throughput
Pt--Ni--Cu alloy screening for hydrogen evolution reaction (HER) descriptors
with surrogate-to-DFT validation, and a demonstration beyond the predefined
tool set, including equation-of-state fitting for BCC Fe and CO-FeN4-graphene
single-atom catalyst geometry preparation. By reducing manual scripting and
bookkeeping while keeping the full evidence trail, CatMaster aims to help
catalysis researchers focus on modeling choices and chemical interpretation
rather than workflow management.
\\ ( https://arxiv.org/abs/2601.13508 ,  5786kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13515 (*cross-listing*)
Date: Tue, 20 Jan 2026 02:08:06 GMT   (598kb)

Title: Automatic Adjustment of HPA Parameters and Attack Prevention in
  Kubernetes Using Random Forests
Authors: Hanlin Zhou, Huah Yong Chan, Jingfei Ni, Mengchun Wu, Qing Deng
Categories: cs.CR cs.AI cs.DC
DOI: 10.1145/3704304.3704320
\\
  In this paper, HTTP status codes are used as custom metrics within the HPA as
the experimental scenario. By integrating the Random Forest classification
algorithm from machine learning, attacks are assessed and predicted,
dynamically adjusting the maximum pod parameter in the HPA to manage attack
traffic. This approach enables the adjustment of HPA parameters using machine
learning scripts in targeted attack scenarios while effectively managing attack
traffic. All access from attacking IPs is redirected to honeypot pods,
achieving a lower incidence of 5XX status codes through HPA pod adjustments
under high load conditions. This method also ensures effective isolation of
attack traffic, preventing excessive HPA expansion due to attacks.
Additionally, experiments conducted under various conditions demonstrate the
importance of setting appropriate thresholds for HPA adjustments.
\\ ( https://arxiv.org/abs/2601.13515 ,  598kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13528 (*cross-listing*)
Date: Tue, 20 Jan 2026 02:24:44 GMT   (2096kb)

Title: Eliciting Harmful Capabilities by Fine-Tuning On Safeguarded Outputs
Authors: Jackson Kaunismaa, Avery Griffin, John Hughes, Christina Q. Knight,
  Mrinank Sharma, Erik Jones
Categories: cs.CR cs.AI cs.CL cs.LG cs.SE
\\
  Model developers implement safeguards in frontier models to prevent misuse,
for example, by employing classifiers to filter dangerous outputs. In this
work, we demonstrate that even robustly safeguarded models can be used to
elicit harmful capabilities in open-source models through elicitation attacks.
Our elicitation attacks consist of three stages: (i) constructing prompts in
adjacent domains to a target harmful task that do not request dangerous
information; (ii) obtaining responses to these prompts from safeguarded
frontier models; (iii) fine-tuning open-source models on these prompt-output
pairs. Since the requested prompts cannot be used to directly cause harm, they
are not refused by frontier model safeguards. We evaluate these elicitation
attacks within the domain of hazardous chemical synthesis and processing, and
demonstrate that our attacks recover approximately 40% of the capability gap
between the base open-source model and an unrestricted frontier model. We then
show that the efficacy of elicitation attacks scales with the capability of the
frontier model and the amount of generated fine-tuning data. Our work
demonstrates the challenge of mitigating ecosystem level risks with
output-level safeguards.
\\ ( https://arxiv.org/abs/2601.13528 ,  2096kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13537 (*cross-listing*)
Date: Tue, 20 Jan 2026 02:48:10 GMT   (714kb)

Title: When Wording Steers the Evaluation: Framing Bias in LLM judges
Authors: Yerin Hwang, Dongryeol Lee, Taegwan Kang, Minwoo Lee, Kyomin Jung
Categories: cs.CL cs.AI
Comments: 4 pages
\\
  Large language models (LLMs) are known to produce varying responses depending
on prompt phrasing, indicating that subtle guidance in phrasing can steer their
answers. However, the impact of this framing bias on LLM-based evaluation,
where models are expected to make stable and impartial judgments, remains
largely underexplored. Drawing inspiration from the framing effect in
psychology, we systematically investigate how deliberate prompt framing skews
model judgments across four high-stakes evaluation tasks. We design symmetric
prompts using predicate-positive and predicate-negative constructions and
demonstrate that such framing induces significant discrepancies in model
outputs. Across 14 LLM judges, we observe clear susceptibility to framing, with
model families showing distinct tendencies toward agreement or rejection. These
findings suggest that framing bias is a structural property of current
LLM-based evaluation systems, underscoring the need for framing-aware
protocols.
\\ ( https://arxiv.org/abs/2601.13537 ,  714kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13547 (*cross-listing*)
Date: Tue, 20 Jan 2026 03:13:07 GMT   (1755kb)

Title: HateXScore: A Metric Suite for Evaluating Reasoning Quality in Hate
  Speech Explanations
Authors: Yujia Hu, Roy Ka-Wei Lee
Categories: cs.CL cs.AI
Comments: EACL 2026 Main Conference
\\
  Hateful speech detection is a key component of content moderation, yet
current evaluation frameworks rarely assess why a text is deemed hateful. We
introduce \textsf{HateXScore}, a four-component metric suite designed to
evaluate the reasoning quality of model explanations. It assesses (i)
conclusion explicitness, (ii) faithfulness and causal grounding of quoted
spans, (iii) protected group identification (policy-configurable), and (iv)
logical consistency among these elements. Evaluated on six diverse hate speech
datasets, \textsf{HateXScore} is intended as a diagnostic complement to reveal
interpretability failures and annotation inconsistencies that are invisible to
standard metrics like Accuracy or F1. Moreover, human evaluation shows strong
agreement with \textsf{HateXScore}, validating it as a practical tool for
trustworthy and transparent moderation.
  \textcolor{red}{Disclaimer: This paper contains sensitive content that may be
disturbing to some readers.}
\\ ( https://arxiv.org/abs/2601.13547 ,  1755kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13588 (*cross-listing*)
Date: Tue, 20 Jan 2026 04:41:09 GMT   (7210kb)

Title: TREX: Tokenizer Regression for Optimal Data Mixture
Authors: Inho Won, Hangyeol Yoo, Minkyung Cho, Jungyeul Park, Hoyun Song,
  KyungTae Lim
Categories: cs.CL cs.AI
Comments: Accepted to EACL 2026. Long Paper. (19 languages studied: Chinese,
  Greek, Japanese, etc.)
MSC-class: 68T50
ACM-class: I.2.7
\\
  Building effective tokenizers for multilingual Large Language Models (LLMs)
requires careful control over language-specific data mixtures. While a
tokenizer's compression performance critically affects the efficiency of LLM
training and inference, existing approaches rely on heuristics or costly
large-scale searches to determine optimal language ratios. We introduce
Tokenizer Regression for Optimal Data MiXture (TREX), a regression-based
framework that efficiently predicts the optimal data mixture for tokenizer
training. TREX trains small-scale proxy tokenizers on random mixtures, gathers
their compression statistics, and learns to predict compression performance
from data mixtures. This learned model enables scalable mixture search before
large-scale tokenizer training, mitigating the accuracy-cost trade-off in
multilingual tokenizer design. Tokenizers trained with TReX's predicted
mixtures outperform mixtures based on LLaMA3 and uniform distributions by up to
12% in both inand out-of-distribution compression efficiency, demonstrating
strong scalability, robustness, and practical effectiveness.
\\ ( https://arxiv.org/abs/2601.13588 ,  7210kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13590 (*cross-listing*)
Date: Tue, 20 Jan 2026 04:43:55 GMT   (3394kb)

Title: Vulnerability of LLMs' Belief Systems? LLMs Belief Resistance Check
  Through Strategic Persuasive Conversation Interventions
Authors: Fan Huang, Haewoon Kwak, Jisun An
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) are increasingly employed in various
question-answering tasks. However, recent studies showcase that LLMs are
susceptible to persuasion and could adopt counterfactual beliefs. We present a
systematic evaluation of LLM susceptibility to persuasion under the
Source--Message--Channel--Receiver (SMCR) communication framework. Across five
mainstream Large Language Models (LLMs) and three domains (factual knowledge,
medical QA, and social bias), we analyze how different persuasive strategies
influence belief stability over multiple interaction turns. We further examine
whether meta-cognition prompting (i.e., eliciting self-reported confidence)
affects resistance to persuasion. Results show that smaller models exhibit
extreme compliance, with over 80% of belief changes occurring at the first
persuasive turn (average end turn of 1.1--1.4). Contrary to expectations,
meta-cognition prompting increases vulnerability by accelerating belief erosion
rather than enhancing robustness. Finally, we evaluate adversarial fine-tuning
as a defense. While GPT-4o-mini achieves near-complete robustness (98.6%) and
Mistral~7B improves substantially (35.7% $\rightarrow$ 79.3%), Llama models
remain highly susceptible (<14%) even when fine-tuned on their own failure
cases. Together, these findings highlight substantial model-dependent limits of
current robustness interventions and offer guidance for developing more
trustworthy LLMs.
\\ ( https://arxiv.org/abs/2601.13590 ,  3394kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13614 (*cross-listing*)
Date: Tue, 20 Jan 2026 05:32:22 GMT   (416kb)

Title: CauScientist: Teaching LLMs to Respect Data for Causal Discovery
Authors: Bo Peng, Sirui Chen, Lei Xu, Chaochao Lu
Categories: cs.CL cs.AI cs.LG
\\
  Causal discovery is fundamental to scientific understanding and reliable
decision-making. Existing approaches face critical limitations: purely
data-driven methods suffer from statistical indistinguishability and modeling
assumptions, while recent LLM-based methods either ignore statistical evidence
or incorporate unverified priors that can mislead result. To this end, we
propose CauScientist, a collaborative framework that synergizes LLMs as
hypothesis-generating "data scientists" with probabilistic statistics as
rigorous "verifiers". CauScientist employs hybrid initialization to select
superior starting graphs, iteratively refines structures through LLM-proposed
modifications validated by statistical criteria, and maintains error memory to
guide efficient search space. Experiments demonstrate that CauScientist
substantially outperforms purely data-driven baselines, achieving up to 53.8%
F1 score improvement and enhancing recall from 35.0% to 100.0%. Notably, while
standalone LLM performance degrades with graph complexity, CauScientist reduces
structural hamming distance (SHD) by 44.0% compared to Qwen3-32B on 37-node
graphs. Our project page is at https://github.com/OpenCausaLab/CauScientist.
\\ ( https://arxiv.org/abs/2601.13614 ,  416kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13622 (*cross-listing*)
Date: Tue, 20 Jan 2026 05:44:33 GMT   (283kb)

Title: CARPE: Context-Aware Image Representation Prioritization via Ensemble
  for Large Vision-Language Models
Authors: Donghee Lee, Rui Cai, Zhe Zhao
Categories: cs.CV cs.AI
\\
  Recent advancements in Large Vision-Language Models (LVLMs) have pushed them
closer to becoming general-purpose assistants. Despite their strong
performance, LVLMs still struggle with vision-centric tasks such as image
classification, underperforming compared to their base vision encoders, which
are often CLIP-based models. To address this limitation, we propose
Context-Aware Image Representation Prioritization via Ensemble (CARPE), a
novel, model-agnostic framework which introduces vision-integration layers and
a context-aware ensemble strategy to identify when to prioritize image
representations or rely on the reasoning capabilities of the language model.
This design enhances the model's ability to adaptively weight visual and
textual modalities and enables the model to capture various aspects of image
representations, leading to consistent improvements in generalization across
classification and vision-language benchmarks. Extensive experiments
demonstrate that CARPE not only improves performance on image classification
benchmarks but also enhances results across various vision-language benchmarks.
Finally, CARPE is designed to be effectively integrated with most open-source
LVLMs that consist of a vision encoder and a language model, ensuring its
adaptability across diverse architectures.
\\ ( https://arxiv.org/abs/2601.13622 ,  283kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13647 (*cross-listing*)
Date: Tue, 20 Jan 2026 06:31:05 GMT   (344kb)

Title: Fusion Segment Transformer: Bi-Directional Attention Guided Fusion
  Network for AI-Generated Music Detection
Authors: Yumin Kim, Seonghyeon Go
Categories: cs.SD cs.AI
\\
  With the rise of generative AI technology, anyone can now easily create and
deploy AI-generated music, which has heightened the need for technical
solutions to address copyright and ownership issues. While existing works
mainly focused on short-audio, the challenge of full-audio detection, which
requires modeling long-term structure and context, remains insufficiently
explored. To address this, we propose an improved version of the Segment
Transformer, termed the Fusion Segment Transformer. As in our previous work, we
extract content embeddings from short music segments using diverse feature
extractors. Furthermore, we enhance the architecture for full-audio
AI-generated music detection by introducing a Gated Fusion Layer that
effectively integrates content and structural information, enabling the capture
of long-term context. Experiments on the SONICS and AIME datasets show that our
approach outperforms the previous model and recent baselines, achieving
state-of-the-art results in AI-generated music detection.
\\ ( https://arxiv.org/abs/2601.13647 ,  344kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13649 (*cross-listing*)
Date: Tue, 20 Jan 2026 06:33:33 GMT   (689kb)

Title: Fairness or Fluency? An Investigation into Language Bias of Pairwise
  LLM-as-a-Judge
Authors: Xiaolin Zhou, Zheng Luo, Yicheng Gao, Qixuan Chen, Xiyang Hu, Yue
  Zhao, Ruishan Liu
Categories: cs.CL cs.AI
\\
  Recent advances in Large Language Models (LLMs) have incentivized the
development of LLM-as-a-judge, an application of LLMs where they are used as
judges to decide the quality of a certain piece of text given a certain
context. However, previous studies have demonstrated that LLM-as-a-judge can be
biased towards different aspects of the judged texts, which often do not align
with human preference. One of the identified biases is language bias, which
indicates that the decision of LLM-as-a-judge can differ based on the language
of the judged texts. In this paper, we study two types of language bias in
pairwise LLM-as-a-judge: (1) performance disparity between languages when the
judge is prompted to compare options from the same language, and (2) bias
towards options written in major languages when the judge is prompted to
compare options of two different languages. We find that for same-language
judging, there exist significant performance disparities across language
families, with European languages consistently outperforming African languages,
and this bias is more pronounced in culturally-related subjects. For
inter-language judging, we observe that most models favor English answers, and
that this preference is influenced more by answer language than question
language. Finally, we investigate whether language bias is in fact caused by
low-perplexity bias, a previously identified bias of LLM-as-a-judge, and we
find that while perplexity is slightly correlated with language bias, language
bias cannot be fully explained by perplexity only.
\\ ( https://arxiv.org/abs/2601.13649 ,  689kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13657 (*cross-listing*)
Date: Tue, 20 Jan 2026 06:46:09 GMT   (18988kb)

Title: Communication-Free Collective Navigation for a Swarm of UAVs via
  LiDAR-Based Deep Reinforcement Learning
Authors: Myong-Yol Choi, Hankyoul Ko, Hanse Cho, Changseung Kim, Seunghwan Kim,
  Jaemin Seo, Hyondong Oh
Categories: cs.RO cs.AI cs.LG cs.MA
\\
  This paper presents a deep reinforcement learning (DRL) based controller for
collective navigation of unmanned aerial vehicle (UAV) swarms in
communication-denied environments, enabling robust operation in complex,
obstacle-rich environments. Inspired by biological swarms where informed
individuals guide groups without explicit communication, we employ an implicit
leader-follower framework. In this paradigm, only the leader possesses goal
information, while follower UAVs learn robust policies using only onboard LiDAR
sensing, without requiring any inter-agent communication or leader
identification. Our system utilizes LiDAR point clustering and an extended
Kalman filter for stable neighbor tracking, providing reliable perception
independent of external positioning systems. The core of our approach is a DRL
controller, trained in GPU-accelerated Nvidia Isaac Sim, that enables followers
to learn complex emergent behaviors - balancing flocking and obstacle avoidance
- using only local perception. This allows the swarm to implicitly follow the
leader while robustly addressing perceptual challenges such as occlusion and
limited field-of-view. The robustness and sim-to-real transfer of our approach
are confirmed through extensive simulations and challenging real-world
experiments with a swarm of five UAVs, which successfully demonstrated
collective navigation across diverse indoor and outdoor environments without
any communication or external localization.
\\ ( https://arxiv.org/abs/2601.13657 ,  18988kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13659 (*cross-listing*)
Date: Tue, 20 Jan 2026 06:50:40 GMT   (2228kb)

Title: Temporal-Spatial Decouple before Act: Disentangled Representation
  Learning for Multimodal Sentiment Analysis
Authors: Chunlei Meng, Ziyang Zhou, Lucas He, Xiaojing Du, Chun Ouyang,
  Zhongxue Gan
Categories: cs.CL cs.AI cs.MM
Comments: This study has been accepted by IEEE ICASSP2026
\\
  Multimodal Sentiment Analysis integrates Linguistic, Visual, and Acoustic.
Mainstream approaches based on modality-invariant and modality-specific
factorization or on complex fusion still rely on spatiotemporal mixed modeling.
This ignores spatiotemporal heterogeneity, leading to spatiotemporal
information asymmetry and thus limited performance. Hence, we propose TSDA,
Temporal-Spatial Decouple before Act, which explicitly decouples each modality
into temporal dynamics and spatial structural context before any interaction.
For every modality, a temporal encoder and a spatial encoder project signals
into separate temporal and spatial body. Factor-Consistent Cross-Modal
Alignment then aligns temporal features only with their temporal counterparts
across modalities, and spatial features only with their spatial counterparts.
Factor specific supervision and decorrelation regularization reduce cross
factor leakage while preserving complementarity. A Gated Recouple module
subsequently recouples the aligned streams for task. Extensive experiments show
that TSDA outperforms baselines. Ablation analysis studies confirm the
necessity and interpretability of the design.
\\ ( https://arxiv.org/abs/2601.13659 ,  2228kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13671 (*cross-listing*)
Date: Tue, 20 Jan 2026 07:13:53 GMT   (942kb)

Title: The Orchestration of Multi-Agent Systems: Architectures, Protocols, and
  Enterprise Adoption
Authors: Apoorva Adimulam, Rajesh Gupta, Sumit Kumar
Categories: cs.MA cs.AI
\\
  Orchestrated multi-agent systems represent the next stage in the evolution of
artificial intelligence, where autonomous agents collaborate through structured
coordination and communication to achieve complex, shared objectives. This
paper consolidates and formalizes the technical composition of such systems,
presenting a unified architectural framework that integrates planning, policy
enforcement, state management, and quality operations into a coherent
orchestration layer. Another primary contribution of this work is the in-depth
technical delineation of two complementary communication protocols - the Model
Context Protocol, which standardizes how agents access external tools and
contextual data, and the Agent2Agent protocol, which governs peer coordination,
negotiation, and delegation. Together, these protocols establish an
interoperable communication substrate that enables scalable, auditable, and
policy-compliant reasoning across distributed agent collectives. Beyond
protocol design, the paper details how orchestration logic, governance
frameworks, and observability mechanisms collectively sustain system coherence,
transparency, and accountability. By synthesizing these elements into a
cohesive technical blueprint, this paper provides comprehensive treatments of
orchestrated multi-agent systems - bridging conceptual architectures with
implementation-ready design principles for enterprise-scale AI ecosystems.
\\ ( https://arxiv.org/abs/2601.13671 ,  942kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13684 (*cross-listing*)
Date: Tue, 20 Jan 2026 07:35:06 GMT   (2656kb)

Title: HeteroCache: A Dynamic Retrieval Approach to Heterogeneous KV Cache
  Compression for Long-Context LLM Inference
Authors: Zhiyuan Shi, Qibo Qiu, Feng Xue, Zhonglin Jiang, Li Yu, Jian Jiang,
  Xiaofei He, Wenxiao Wang
Categories: cs.CL cs.AI
\\
  The linear memory growth of the KV cache poses a significant bottleneck for
LLM inference in long-context tasks. Existing static compression methods often
fail to preserve globally important information, principally because they
overlook the attention drift phenomenon where token significance evolves
dynamically. Although recent dynamic retrieval approaches attempt to address
this issue, they typically suffer from coarse-grained caching strategies and
incur high I/O overhead due to frequent data transfers. To overcome these
limitations, we propose HeteroCache, a training-free dynamic compression
framework. Our method is built on two key insights: attention heads exhibit
diverse temporal heterogeneity, and there is significant spatial redundancy
among heads within the same layer. Guided by these insights, HeteroCache
categorizes heads based on stability and redundancy. Consequently, we apply a
fine-grained weighting strategy that allocates larger cache budgets to heads
with rapidly shifting attention to capture context changes, thereby addressing
the inefficiency of coarse-grained strategies. Furthermore, we employ a
hierarchical storage mechanism in which a subset of representative heads
monitors attention shift, and trigger an asynchronous, on-demand retrieval of
contexts from the CPU, effectively hiding I/O latency. Finally, experiments
demonstrate that HeteroCache achieves state-of-the-art performance on multiple
long-context benchmarks and accelerates decoding by up to $3\times$ compared to
the original model in the 224K context. Our code will be open-source.
\\ ( https://arxiv.org/abs/2601.13684 ,  2656kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13693 (*cross-listing*)
Date: Tue, 20 Jan 2026 07:45:53 GMT   (4341kb)

Title: End-to-End Reverse Screening Identifies Protein Targets of Small
  Molecules Using HelixFold3
Authors: Shengjie Xu, Xianbin Ye, Mengran Zhu, Xiaonan Zhang, Shanzhuo Zhang,
  Xiaomin Fang
Categories: q-bio.BM cs.AI
\\
  Identifying protein targets for small molecules, or reverse screening, is
essential for understanding drug action, guiding compound repurposing,
predicting off-target effects, and elucidating the molecular mechanisms of
bioactive compounds. Despite its critical role, reverse screening remains
challenging because accurately capturing interactions between a small molecule
and structurally diverse proteins is inherently complex, and conventional
step-wise workflows often propagate errors across decoupled steps such as
target structure modeling, pocket identification, docking, and scoring. Here,
we present an end-to-end reverse screening strategy leveraging HelixFold3, a
high-accuracy biomolecular structure prediction model akin to AlphaFold3, which
simultaneously models the folding of proteins from a protein library and the
docking of small-molecule ligands within a unified framework. We validate this
approach on a diverse and representative set of approximately one hundred small
molecules. Compared with conventional reverse docking, our method improves
screening accuracy and demonstrates enhanced structural fidelity, binding-site
precision, and target prioritization. By systematically linking small molecules
to their protein targets, this framework establishes a scalable and
straightforward platform for dissecting molecular mechanisms, exploring
off-target interactions, and supporting rational drug discovery.
\\ ( https://arxiv.org/abs/2601.13693 ,  4341kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13697 (*cross-listing*)
Date: Tue, 20 Jan 2026 07:51:32 GMT   (231kb)

Title: Uncertainty-Aware Gradient Signal-to-Noise Data Selection for
  Instruction Tuning
Authors: Zhihang Yuan, Chengyu Yue, Long Huang, Litu Ou, Lei Shi
Categories: cs.CL cs.AI cs.LG
Comments: Preprint
\\
  Instruction tuning is a standard paradigm for adapting large language models
(LLMs), but modern instruction datasets are large, noisy, and redundant, making
full-data fine-tuning costly and often unnecessary. Existing data selection
methods either build expensive gradient datastores or assign static scores from
a weak proxy, largely ignoring evolving uncertainty, and thus missing a key
source of LLM interpretability. We propose GRADFILTERING, an
objective-agnostic, uncertainty-aware data selection framework that utilizes a
small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients
into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or
surpasses random subsets and strong baselines in most LLM-as-a-judge
evaluations as well as in human assessment. Moreover, GRADFILTERING-selected
subsets converge faster than competitive filters under the same compute budget,
reflecting the benefit of uncertainty-aware scoring.
\\ ( https://arxiv.org/abs/2601.13697 ,  231kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13704 (*cross-listing*)
Date: Tue, 20 Jan 2026 08:00:05 GMT   (3224kb)

Title: Performance and Complexity Trade-off Optimization of Speech Models
  During Training
Authors: Esteban G\'omez, Tom B\"ackstr\"om
Categories: cs.SD cs.AI cs.LG eess.AS
\\
  In speech machine learning, neural network models are typically designed by
choosing an architecture with fixed layer sizes and structure. These models are
then trained to maximize performance on metrics aligned with the task's
objective. While the overall architecture is usually guided by prior knowledge
of the task, the sizes of individual layers are often chosen heuristically.
However, this approach does not guarantee an optimal trade-off between
performance and computational complexity; consequently, post hoc methods such
as weight quantization or model pruning are typically employed to reduce
computational cost. This occurs because stochastic gradient descent (SGD)
methods can only optimize differentiable functions, while factors influencing
computational complexity, such as layer sizes and floating-point operations per
second (FLOP/s), are non-differentiable and require modifying the model
structure during training. We propose a reparameterization technique based on
feature noise injection that enables joint optimization of performance and
computational complexity during training using SGD-based methods. Unlike
traditional pruning methods, our approach allows the model size to be
dynamically optimized for a target performance-complexity trade-off, without
relying on heuristic criteria to select which weights or structures to remove.
We demonstrate the effectiveness of our method through three case studies,
including a synthetic example and two practical real-world applications: voice
activity detection and audio anti-spoofing. The code related to our work is
publicly available to encourage further research.
\\ ( https://arxiv.org/abs/2601.13704 ,  3224kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13707 (*cross-listing*)
Date: Tue, 20 Jan 2026 08:04:18 GMT   (1624kb)

Title: Attention-space Contrastive Guidance for Efficient Hallucination
  Mitigation in LVLMs
Authors: Yujin Jo, Sangyoon Bae, Taesup Kim
Categories: cs.CV cs.AI cs.LG
\\
  Hallucinations in large vision-language models (LVLMs) often arise when
language priors dominate over visual evidence, causing object misidentification
and visually inconsistent descriptions. We address this issue by framing
hallucination mitigation as contrastive guidance, steering generation toward
visually grounded and semantically faithful text. This approach regulates the
model's internal behavior by reducing over-dependence on language priors and
contrasting visually grounded with language-only representations. We propose
Attention-space Contrastive Guidance (ACG), a single-pass mechanism that
operates within self-attention layers to construct both vision-language and
language-only attention paths in a single forward computation. This integration
enables computationally efficient guidance directly embedded in the model's
representation contextualization. To correct approximation bias introduced by
the single-pass formulation, we further apply an orthogonalized correction that
removes components aligned with the language-only path, selectively amplifying
visual contributions. Experiments on the CHAIR and POPE benchmarks show that
ACG achieves state-of-the-art faithfulness and caption quality while
significantly reducing computational cost. Our method establishes a principled
and efficient alternative, reducing latency by up to 2x compared to prior
contrastive decoding methods that require multiple forward passes.
\\ ( https://arxiv.org/abs/2601.13707 ,  1624kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13717 (*cross-listing*)
Date: Tue, 20 Jan 2026 08:21:55 GMT   (186kb)

Title: Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on
  Forecasting Problems Before Model Knowledge Cutoff
Authors: Zehan Li, Yuxuan Wang, Ali El Lahib, Ying-Jieh Xia, Xinyu Pi
Categories: cs.CL cs.AI
\\
  Evaluating LLM forecasting capabilities is constrained by a fundamental
tension: prospective evaluation offers methodological rigor but prohibitive
latency, while retrospective forecasting (RF) -- evaluating on already-resolved
events -- faces rapidly shrinking clean evaluation data as SOTA models possess
increasingly recent knowledge cutoffs. Simulated Ignorance (SI), prompting
models to suppress pre-cutoff knowledge, has emerged as a potential solution.
We provide the first systematic test of whether SI can approximate True
Ignorance (TI). Across 477 competition-level questions and 9 models, we find
that SI fails systematically: (1) cutoff instructions leave a 52% performance
gap between SI and TI; (2) chain-of-thought reasoning fails to suppress prior
knowledge, even when reasoning traces contain no explicit post-cutoff
references; (3) reasoning-optimized models exhibit worse SI fidelity despite
superior reasoning trace quality. These findings demonstrate that prompts
cannot reliably "rewind" model knowledge. We conclude that RF on pre-cutoff
events is methodologically flawed; we recommend against using SI-based
retrospective setups to benchmark forecasting capabilities.
\\ ( https://arxiv.org/abs/2601.13717 ,  186kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13719 (*cross-listing*)
Date: Tue, 20 Jan 2026 08:23:29 GMT   (7792kb)

Title: Hierarchical Long Video Understanding with Audiovisual Entity Cohesion
  and Agentic Search
Authors: Xinlei Yin, Xiulian Peng, Xiao Li, Zhiwei Xiong, Yan Lu
Categories: cs.CV cs.AI cs.IR
\\
  Long video understanding presents significant challenges for vision-language
models due to extremely long context windows. Existing solutions relying on
naive chunking strategies with retrieval-augmented generation, typically suffer
from information fragmentation and a loss of global coherence. We present
HAVEN, a unified framework for long-video understanding that enables coherent
and comprehensive reasoning by integrating audiovisual entity cohesion and
hierarchical video indexing with agentic search. First, we preserve semantic
consistency by integrating entity-level representations across visual and
auditory streams, while organizing content into a structured hierarchy spanning
global summary, scene, segment, and entity levels. Then we employ an agentic
search mechanism to enable dynamic retrieval and reasoning across these layers,
facilitating coherent narrative reconstruction and fine-grained entity
tracking. Extensive experiments demonstrate that our method achieves good
temporal coherence, entity consistency, and retrieval efficiency, establishing
a new state-of-the-art with an overall accuracy of 84.1% on LVBench. Notably,
it achieves outstanding performance in the challenging reasoning category,
reaching 80.1%. These results highlight the effectiveness of structured,
multimodal reasoning for comprehensive and context-consistent understanding of
long-form videos.
\\ ( https://arxiv.org/abs/2601.13719 ,  7792kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13722 (*cross-listing*)
Date: Tue, 20 Jan 2026 08:27:13 GMT   (10819kb)

Title: OP-Bench: Benchmarking Over-Personalization for Memory-Augmented
  Personalized Conversational Agents
Authors: Yulin Hu, Zimo Long, Jiahe Guo, Xingyu Sui, Xing Fu, Weixiang Zhao,
  Yanyan Zhao, Bing Qin
Categories: cs.CL cs.AI
\\
  Memory-augmented conversational agents enable personalized interactions using
long-term user memory and have gained substantial traction. However, existing
benchmarks primarily focus on whether agents can recall and apply user
information, while overlooking whether such personalization is used
appropriately. In fact, agents may overuse personal information, producing
responses that feel forced, intrusive, or socially inappropriate to users. We
refer to this issue as \emph{over-personalization}. In this work, we formalize
over-personalization into three types: Irrelevance, Repetition, and Sycophancy,
and introduce \textbf{OP-Bench} a benchmark of 1,700 verified instances
constructed from long-horizon dialogue histories. Using \textbf{OP-Bench}, we
evaluate multiple large language models and memory-augmentation methods, and
find that over-personalization is widespread when memory is introduced. Further
analysis reveals that agents tend to retrieve and over-attend to user memories
even when unnecessary. To address this issue, we propose \textbf{Self-ReCheck},
a lightweight, model-agnostic memory filtering mechanism that mitigates
over-personalization while preserving personalization performance. Our work
takes an initial step toward more controllable and appropriate personalization
in memory-augmented dialogue systems.
\\ ( https://arxiv.org/abs/2601.13722 ,  10819kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13734 (*cross-listing*)
Date: Tue, 20 Jan 2026 08:42:04 GMT   (116kb)

Title: Towards robust long-context understanding of large language model via
  active recap learning
Authors: Chenyu Hui
Categories: cs.CL cs.AI
Comments: 5 pages
MSC-class: 68T35
ACM-class: F.2.2; I.2.7
\\
  In this paper, we propose active recap learning (ARL), a framework for
enhancing large language model (LLM) in understanding long contexts. ARL
enables models to revisit and summarize earlier content through targeted
sequence construction during contined pretraining and retrospective
summarization at inference. First, we identify key tokens in prepared long
context based on loss gaps between long and short forward contexts and find
most revant preceding para- graphs, then summarize them using an LLM. Second,
ARL equips models with the ability to autonomously generate and utilize these
retrospective summaries during inference, thereby establishing a recursive
memory mechanism across paragraphs. Experimental results show substantial
gains, with ARL achieving a 26.8% improvement on RULER and a 9.44% improvement
on LongBench. Overall, ARL offers a simple yet effective continued
pretraining-based approach to strengthen long-context understanding, advancing
scalable memory augmentation in LLM
\\ ( https://arxiv.org/abs/2601.13734 ,  116kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13749 (*cross-listing*)
Date: Tue, 20 Jan 2026 09:03:57 GMT   (694kb)

Title: Pro-AI Bias in Large Language Models
Authors: Benaya Trabelsi, Jonathan Shaki, Sarit Kraus
Categories: cs.CL cs.AI cs.CY cs.LG
Comments: 13 pages, 6 figures. Code available at:
  https://github.com/benayat/Pro-AI-bias-in-LLMs
ACM-class: I.2.7; K.4.1
\\
  Large language models (LLMs) are increasingly employed for decision-support
across multiple domains. We investigate whether these models display a
systematic preferential bias in favor of artificial intelligence (AI) itself.
Across three complementary experiments, we find consistent evidence of pro-AI
bias. First, we show that LLMs disproportionately recommend AI-related options
in response to diverse advice-seeking queries, with proprietary models doing so
almost deterministically. Second, we demonstrate that models systematically
overestimate salaries for AI-related jobs relative to closely matched non-AI
jobs, with proprietary models overestimating AI salaries more by 10 percentage
points. Finally, probing internal representations of open-weight models reveals
that ``Artificial Intelligence'' exhibits the highest similarity to generic
prompts for academic fields under positive, negative, and neutral framings
alike, indicating valence-invariant representational centrality. These patterns
suggest that LLM-generated advice and valuation can systematically skew choices
and perceptions in high-stakes decisions.
\\ ( https://arxiv.org/abs/2601.13749 ,  694kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13798 (*cross-listing*)
Date: Tue, 20 Jan 2026 09:57:26 GMT   (19091kb)

Title: Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders
Authors: Kai Wittenmayer, Sukrut Rao, Amin Parchami-Araghi, Bernt Schiele,
  Jonas Fischer
Categories: cs.CV cs.AI cs.LG
Comments: 32 pages, 24 figures, 3 tables
\\
  Language-aligned vision foundation models perform strongly across diverse
downstream tasks. Yet, their learned representations remain opaque, making
interpreting their decision-making hard. Recent works decompose these
representations into human-interpretable concepts, but provide poor spatial
grounding and are limited to image classification tasks. In this work, we
propose Insight, a language-aligned concept foundation model that provides
fine-grained concepts, which are human-interpretable and spatially grounded in
the input image. We leverage a hierarchical sparse autoencoder and a foundation
model with strong semantic representations to automatically extract concepts at
various granularities. Examining local co-occurrence dependencies of concepts
allows us to define concept relationships. Through these relations we further
improve concept naming and obtain richer explanations. On benchmark data, we
show that Insight provides performance on classification and segmentation that
is competitive with opaque foundation models while providing fine-grained, high
quality concept-based explanations. Code is available at
https://github.com/kawi19/Insight.
\\ ( https://arxiv.org/abs/2601.13798 ,  19091kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13809 (*cross-listing*)
Date: Tue, 20 Jan 2026 10:08:00 GMT   (798kb)

Title: DroneVLA: VLA based Aerial Manipulation
Authors: Fawad Mehboob, Monijesu James, Amir Habel, Jeffrin Sam, Miguel
  Altamirano Cabrera, Dzmitry Tsetserukou
Categories: cs.RO cs.AI
Comments: This paper has been accepted for publication at LBR of HRI 2026
  conference
\\
  As aerial platforms evolve from passive observers to active manipulators, the
challenge shifts toward designing intuitive interfaces that allow non-expert
users to command these systems naturally. This work introduces a novel concept
of autonomous aerial manipulation system capable of interpreting high-level
natural language commands to retrieve objects and deliver them to a human user.
The system is intended to integrate a MediaPipe based on Grounding DINO and a
Vision-Language-Action (VLA) model with a custom-built drone equipped with a
1-DOF gripper and an Intel RealSense RGB-D camera. VLA performs semantic
reasoning to interpret the intent of a user prompt and generates a prioritized
task queue for grasping of relevant objects in the scene. Grounding DINO and
dynamic A* planning algorithm are used to navigate and safely relocate the
object. To ensure safe and natural interaction during the handover phase, the
system employs a human-centric controller driven by MediaPipe. This module
provides real-time human pose estimation, allowing the drone to employ visual
servoing to maintain a stable, distinct position directly in front of the user,
facilitating a comfortable handover. We demonstrate the system's efficacy
through real-world experiments for localization and navigation, which resulted
in a 0.164m, 0.070m, and 0.084m of max, mean euclidean, and root-mean squared
errors, respectively, highlighting the feasibility of VLA for aerial
manipulation operations.
\\ ( https://arxiv.org/abs/2601.13809 ,  798kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13864 (*cross-listing*)
Date: Tue, 20 Jan 2026 11:27:40 GMT   (421kb)

Title: HardSecBench: Benchmarking the Security Awareness of LLMs for Hardware
  Code Generation
Authors: Qirui Chen, Jingxian Shuai, Shuangwu Chen, Shenghao Ye, Zijian Wen,
  Xufei Su, Jie Jin, Jiangming Li, Jun Chen, Xiaobin Tan, and Jian Yang
Categories: cs.CR cs.AI
\\
  Large language models (LLMs) are being increasingly integrated into practical
hardware and firmware development pipelines for code generation. Existing
studies have primarily focused on evaluating the functional correctness of
LLM-generated code, yet paid limited attention to its security issues. However,
LLM-generated code that appears functionally sound may embed security flaws
which could induce catastrophic damages after deployment. This critical
research gap motivates us to design a benchmark for assessing security
awareness under realistic specifications. In this work, we introduce
HardSecBench, a benchmark with 924 tasks spanning Verilog Register Transfer
Level (RTL) and firmware-level C, covering 76 hardware-relevant Common Weakness
Enumeration (CWE) entries. Each task includes a structured specification, a
secure reference implementation, and executable tests. To automate artifact
synthesis, we propose a multi-agent pipeline that decouples synthesis from
verification and grounds evaluation in execution evidence, enabling reliable
evaluation. Using HardSecBench, we evaluate a range of LLMs on hardware and
firmware code generation and find that models often satisfy functional
requirements while still leaving security risks. We also find that security
results vary with prompting. These findings highlight pressing challenges and
offer actionable insights for future advancements in LLM-assisted hardware
design. Our data and code will be released soon.
\\ ( https://arxiv.org/abs/2601.13864 ,  421kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13885 (*cross-listing*)
Date: Tue, 20 Jan 2026 11:59:13 GMT   (73kb)

Title: Confident Rankings with Fewer Items: Adaptive LLM Evaluation with
  Continuous Scores
Authors: Esma Balk{\i}r, Alice Pernthaller, Marco Basaldella, Jos\'e
  Hern\'andez-Orallo, Nigel Collier
Categories: cs.CL cs.AI
\\
  Computerized Adaptive Testing (CAT) has proven effective for efficient LLM
evaluation on multiple-choice benchmarks, but modern LLM evaluation
increasingly relies on generation tasks where outputs are scored continuously
rather than marked correct/incorrect. We present a principled extension of
IRT-based adaptive testing to continuous bounded scores (ROUGE, BLEU,
LLM-as-a-Judge) by replacing the Bernoulli response distribution with a
heteroskedastic normal distribution. Building on this, we introduce an
uncertainty aware ranker with adaptive stopping criteria that achieves reliable
model ranking while testing as few items and as cheaply as possible. We
validate our method on five benchmarks spanning n-gram-based, embedding-based,
and LLM-as-judge metrics. Our method uses 2% of the items while improving
ranking correlation by 0.12 {\tau} over random sampling, with 95% accuracy on
confident predictions.
\\ ( https://arxiv.org/abs/2601.13885 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13895 (*cross-listing*)
Date: Tue, 20 Jan 2026 12:25:41 GMT   (806kb)

Title: OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3
Authors: Xu Zhang, Danyang Li, Yingjie Xia, Xiaohang Dong, Hualong Yu, Jianye
  Wang, Qicheng Li
Categories: cs.CV cs.AI
\\
  Change Detection (CD) is a fundamental task in remote sensing. It monitors
the evolution of land cover over time. Based on this, Open-Vocabulary Change
Detection (OVCD) introduces a new requirement. It aims to reduce the reliance
on predefined categories. Existing training-free OVCD methods mostly use CLIP
to identify categories. These methods also need extra models like DINO to
extract features. However, combining different models often causes problems in
matching features and makes the system unstable. Recently, the Segment Anything
Model 3 (SAM 3) is introduced. It integrates segmentation and identification
capabilities within one promptable model, which offers new possibilities for
the OVCD task. In this paper, we propose OmniOVCD, a standalone framework
designed for OVCD. By leveraging the decoupled output heads of SAM 3, we
propose a Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID first
fuses the semantic, instance, and presence outputs of SAM 3 to construct
land-cover masks, and then decomposes them into individual instance masks for
change comparison. This design preserves high accuracy in category recognition
and maintains instance-level consistency across images. As a result, the model
can generate accurate change masks. Experiments on four public benchmarks
(LEVIR-CD, WHU-CD, S2Looking, and SECOND) demonstrate SOTA performance,
achieving IoU scores of 67.2, 66.5, 24.5, and 27.1 (class-average),
respectively, surpassing all previous methods.
\\ ( https://arxiv.org/abs/2601.13895 ,  806kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13920 (*cross-listing*)
Date: Tue, 20 Jan 2026 12:50:18 GMT   (131kb)

Title: Asymmetric regularization mechanism for GAN training with Variational
  Inequalities
Authors: Spyridon C. Giagtzoglou, Mark H.M. Winands, Barbara Franci
Categories: cs.GT cs.AI cs.LG
Comments: 6 pages, 3 figures, conference
\\
  We formulate the training of generative adversarial networks (GANs) as a Nash
equilibrium seeking problem. To stabilize the training process and find a Nash
equilibrium, we propose an asymmetric regularization mechanism based on the
classic Tikhonov step and on a novel zero-centered gradient penalty. Under
smoothness and a local identifiability condition induced by a Gauss-Newton
Gramian, we obtain explicit Lipschitz and (strong)-monotonicity constants for
the regularized operator. These constants ensure last-iterate linear
convergence of a single-call Extrapolation-from-the-Past (EFTP) method.
Empirical simulations on an academic example show that, even when strong
monotonicity cannot be achieved, the asymmetric regularization is enough to
converge to an equilibrium and stabilize the trajectory.
\\ ( https://arxiv.org/abs/2601.13920 ,  131kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13938 (*cross-listing*)
Date: Tue, 20 Jan 2026 13:13:39 GMT   (1413kb)

Title: IF-GEO: Conflict-Aware Instruction Fusion for Multi-Query Generative
  Engine Optimization
Authors: Heyang Zhou (1), JiaJia Chen (2), Xiaolu Chen (1), Jie Bao (1), Zhen
  Chen (1), Yong Liao (1) ((1) School of Cyber Science and Technology,
  University of Science and Technology of China, (2) Institute of Dataspace,
  Hefei Comprehensive National Science Center)
Categories: cs.IR cs.AI
Comments: 9 pages, 3 figures. Submitted to ACL 2026. Corresponding author: Zhen
  Chen
\\
  As Generative Engines revolutionize information retrieval by synthesizing
direct answers from retrieved sources, ensuring source visibility becomes a
significant challenge. Improving it through targeted content revisions is a
practical strategy termed Generative Engine Optimization (GEO). However,
optimizing a document for diverse queries presents a constrained optimization
challenge where heterogeneous queries often impose conflicting and competing
revision requirements under a limited content budget. To address this
challenge, we propose IF-GEO, a "diverge-then-converge" framework comprising
two phases: (i) mining distinct optimization preferences from representative
latent queries; (ii) synthesizing a Global Revision Blueprint for guided
editing by coordinating preferences via conflict-aware instruction fusion. To
explicitly quantify IF-GEO's objective of cross-query stability, we introduce
risk-aware stability metrics. Experiments on multi-query benchmarks demonstrate
that IF-GEO achieves substantial performance gains while maintaining robustness
across diverse retrieval scenarios.
\\ ( https://arxiv.org/abs/2601.13938 ,  1413kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13942 (*cross-listing*)
Date: Tue, 20 Jan 2026 13:18:18 GMT   (327kb)

Title: Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via
  Reinforcement Learning
Authors: Hongbo Bai, Yujin Zhou, Yile Wu, Chi-Min Chan, Pengcheng Wen, Kunhao
  Pan, Sirui Han, Yike Guo
Categories: cs.CV cs.AI
\\
  Large Multimodal Models (LMMs) have achieved remarkable success in visual
understanding, yet they struggle with knowledge-intensive queries involving
long-tail entities or evolving information due to static parametric knowledge.
Recent search-augmented approaches attempt to address this limitation, but
existing methods rely on indiscriminate whole-image retrieval that introduces
substantial visual redundancy and noise, and lack deep iterative reflection,
limiting their effectiveness on complex visual queries. To overcome these
challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that
shifts from passive perception to active visual planning. GoG introduces a
Selective Gaze mechanism that dynamically chooses whether to glance at global
context or gaze into high-value regions, filtering irrelevant information
before retrieval. We design a dual-stage training strategy: Reflective GoG
Behavior Alignment via supervised fine-tuning instills the fundamental GoG
paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the
model's capability to handle complex queries through iterative reasoning.
Experiments across six benchmarks demonstrate state-of-the-art performance.
Ablation studies confirm that both Selective Gaze and complexity-adaptive RL
are essential for effective visual search. We will release our data and models
for further exploration soon.
\\ ( https://arxiv.org/abs/2601.13942 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13948 (*cross-listing*)
Date: Tue, 20 Jan 2026 13:23:44 GMT   (263kb)

Title: Stream-Voice-Anon: Enhancing Utility of Real-Time Speaker Anonymization
  via Neural Audio Codec and Language Models
Authors: Nikita Kuzmin, Songting Liu, Kong Aik Lee, Eng Siong Chng
Categories: eess.AS cs.AI
Comments: Accepted by ICASSP2026
\\
  Protecting speaker identity is crucial for online voice applications, yet
streaming speaker anonymization (SA) remains underexplored. Recent research has
demonstrated that neural audio codec (NAC) provides superior speaker feature
disentanglement and linguistic fidelity. NAC can also be used with causal
language models (LM) to enhance linguistic fidelity and prompt control for
streaming tasks. However, existing NAC-based online LM systems are designed for
voice conversion (VC) rather than anonymization, lacking the techniques
required for privacy protection. Building on these advances, we present
Stream-Voice-Anon, which adapts modern causal LM-based NAC architectures
specifically for streaming SA by integrating anonymization techniques. Our
anonymization approach incorporates pseudo-speaker representation sampling, a
speaker embedding mixing and diverse prompt selection strategies for LM
conditioning that leverage the disentanglement properties of quantized content
codes to prevent speaker information leakage. Additionally, we compare dynamic
and fixed delay configurations to explore latency-privacy trade-offs in
real-time scenarios. Under the VoicePrivacy 2024 Challenge protocol,
Stream-Voice-Anon achieves substantial improvements in intelligibility (up to
46% relative WER reduction) and emotion preservation (up to 28% UAR relative)
compared to the previous state-of-the-art streaming method DarkStream while
maintaining comparable latency (180ms vs 200ms) and privacy protection against
lazy-informed attackers, though showing 15% relative degradation against
semi-informed attackers.
\\ ( https://arxiv.org/abs/2601.13948 ,  263kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13992 (*cross-listing*)
Date: Tue, 20 Jan 2026 14:05:19 GMT   (2011kb)

Title: "The Whole Is Greater Than the Sum of Its Parts": A Compatibility-Aware
  Multi-Teacher CoT Distillation Framework
Authors: Jin Cui, Jiaqi Guo, Jiepeng Zhou, Ruixuan Yang, Jiayi Lu, Jiajun Xu,
  Jiangcheng Song, Boran Zhao, Pengju Ren
Categories: cs.CL cs.AI
Comments: 11pages, 9figures
\\
  Chain-of-Thought (CoT) reasoning empowers Large Language Models (LLMs) with
remarkable capabilities but typically requires prohibitive parameter scales.
CoT distillation has emerged as a promising paradigm to transfer reasoning
prowess into compact Student Models (SLMs), but existing approaches often rely
on a solitary teacher, capping the student's potential since individual LLMs
often exhibit distinct capability biases and may suffer from catastrophic
forgetting. While leveraging diverse teachers seems appealing, effectively
fusing their supervisions remains challenging: teacher-student incompatibility
risks amplifying hallucinations, and passive supervision fails to ensure
genuine logic internalization. To address this, we introduce COMPACT, a
framework that adaptively fuses supervisions from different teachers by
dynamically weighting teacher gradients based on the student's real-time
compatibility evaluated by a multi-dimensional metric: (1) Graph-based
Consensus to filter misleading rationales by identifying mainstream reasoning
paths; (2) Mutual-Information-based Adaptability to detect "epiphany moments"
for genuinely understanding the reasoning process rather than merely imitating;
and (3) Loss-based Difficulty to assess student receptivity to the teacher's
guidance and prevent negative transfer. Extensive experiments and latent space
analysis demonstrate that COMPACT effectively integrates diverse reasoning
capabilities without damaging the model's original knowledge structure,
achieving state-of-the-art performance on various benchmarks while mitigating
catastrophic forgetting.
\\ ( https://arxiv.org/abs/2601.13992 ,  2011kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13994 (*cross-listing*)
Date: Tue, 20 Jan 2026 14:06:01 GMT   (263kb)

Title: torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and
  Sparse Tensor Parallelism for PyTorch
Authors: Mingyuan Chi
Categories: cs.DC cs.AI
\\
  Industrial scientific computing predominantly uses sparse matrices to
represent unstructured data -- finite element meshes, graphs, point clouds. We
present \torchsla{}, an open-source PyTorch library that enables
GPU-accelerated, scalable, and differentiable sparse linear algebra. The
library addresses three fundamental challenges: (1) GPU acceleration for sparse
linear solves, nonlinear solves (Newton, Picard, Anderson), and eigenvalue
computation; (2) Multi-GPU scaling via domain decomposition with halo exchange,
reaching \textbf{400 million DOF linear solve on 3 GPUs}; and (3) Adjoint-based
differentiation} achieving $\mathcal{O}(1)$ computational graph nodes (for
autograd) and $\mathcal{O}(\text{nnz})$ memory -- independent of solver
iterations. \torchsla{} supports multiple backends (SciPy, cuDSS,
PyTorch-native) and seamlessly integrates with PyTorch autograd for end-to-end
differentiable simulations. Code is available at
https://github.com/walkerchi/torch-sla.
\\ ( https://arxiv.org/abs/2601.13994 ,  263kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13999 (*cross-listing*)
Date: Tue, 20 Jan 2026 14:20:44 GMT   (534kb)

Title: DAME: Duration-Aware Matryoshka Embedding for Duration-Robust Speaker
  Verification
Authors: Youngmoon Jung, Joon-Young Yang, Ju-ho Kim, Jaeyoung Roh, Chang Woo
  Han, Hoon-Young Cho
Categories: eess.AS cs.AI
Comments: 5 pages, 2 figures, Accepted at ICASSP 2026
\\
  Short-utterance speaker verification remains challenging due to limited
speaker-discriminative cues in short speech segments. While existing methods
focus on enhancing speaker encoders, the embedding learning strategy still
forces a single fixed-dimensional representation reused for utterances of any
length, leaving capacity misaligned with the information available at different
durations. We propose Duration-Aware Matryoshka Embedding (DAME), a
model-agnostic framework that builds a nested hierarchy of sub-embeddings
aligned to utterance durations: lower-dimensional representations capture
compact speaker traits from short utterances, while higher dimensions encode
richer details from longer speech. DAME supports both training from scratch and
fine-tuning, and serves as a direct alternative to conventional large-margin
fine-tuning, consistently improving performance across durations. On the
VoxCeleb1-O/E/H and VOiCES evaluation sets, DAME consistently reduces the equal
error rate on 1-s and other short-duration trials, while maintaining
full-length performance with no additional inference cost. These gains
generalize across various speaker encoder architectures under both general
training and fine-tuning setups.
\\ ( https://arxiv.org/abs/2601.13999 ,  534kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14012 (*cross-listing*)
Date: Tue, 20 Jan 2026 14:30:40 GMT   (239kb)

Title: MATE: Matryoshka Audio-Text Embeddings for Open-Vocabulary Keyword
  Spotting
Authors: Youngmoon Jung, Myunghun Jung, Joon-Young Yang, Yong-Hyeok Lee,
  Jaeyoung Roh, Hoon-Young Cho
Categories: eess.AS cs.AI
Comments: 5 pages, 1 figure, Accepted at ICASSP 2026
\\
  Open-vocabulary keyword spotting (KWS) with text-based enrollment has emerged
as a flexible alternative to fixed-phrase triggers. Prior utterance-level
matching methods, from an embedding-learning standpoint, learn embeddings at a
single fixed dimensionality. We depart from this design and propose Matryoshka
Audio-Text Embeddings (MATE), a dual-encoder framework that encodes multiple
embedding granularities within a single vector via nested sub-embeddings
("prefixes"). Specifically, we introduce a PCA-guided prefix alignment:
PCA-compressed versions of the full text embedding for each prefix size serve
as teacher targets to align both audio and text prefixes. This alignment
concentrates salient keyword cues in lower-dimensional prefixes, while higher
dimensions add detail. MATE is trained with standard deep metric learning
objectives for audio-text KWS, and is loss-agnostic. To our knowledge, this is
the first application of matryoshka-style embeddings to KWS, achieving
state-of-the-art results on WSJ and LibriPhrase without any inference overhead.
\\ ( https://arxiv.org/abs/2601.14012 ,  239kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14039 (*cross-listing*)
Date: Tue, 20 Jan 2026 14:57:56 GMT   (883kb)

Title: Generalizing Abstention for Noise-Robust Learning in Medical Image
  Segmentation
Authors: Wesam Moustafa, Hossam Elsafty, Helen Schneider, Lorenz Sparrenberg,
  Rafet Sifa
Categories: cs.CV cs.AI
\\
  Label noise is a critical problem in medical image segmentation, often
arising from the inherent difficulty of manual annotation. Models trained on
noisy data are prone to overfitting, which degrades their generalization
performance. While a number of methods and strategies have been proposed to
mitigate noisy labels in the segmentation domain, this area remains largely
under-explored. The abstention mechanism has proven effective in classification
tasks by enhancing the capabilities of Cross Entropy, yet its potential in
segmentation remains unverified. In this paper, we address this gap by
introducing a universal and modular abstention framework capable of enhancing
the noise-robustness of a diverse range of loss functions. Our framework
improves upon prior work with two key components: an informed regularization
term to guide abstention behaviour, and a more flexible power-law-based
auto-tuning algorithm for the abstention penalty. We demonstrate the
framework's versatility by systematically integrating it with three distinct
loss functions to create three novel, noise-robust variants: GAC, SAC, and ADS.
Experiments on the CaDIS and DSAD medical datasets show our methods
consistently and significantly outperform their non-abstaining baselines,
especially under high noise levels. This work establishes that enabling models
to selectively ignore corrupted samples is a powerful and generalizable
strategy for building more reliable segmentation models. Our code is publicly
available at https://github.com/wemous/abstention-for-segmentation.
\\ ( https://arxiv.org/abs/2601.14039 ,  883kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14041 (*cross-listing*)
Date: Tue, 20 Jan 2026 14:58:23 GMT   (1420kb)

Title: Top 10 Open Challenges Steering the Future of Diffusion Language Model
  and Its Variants
Authors: Yunhe Wang, Kai Han, Huiling Zhen, Yuchuan Tian, Hanting Chen,
  Yongbing Huang, Yufei Cui, Yingte Shu, Shan Gao, Ismail Elezi, Roy Vaughan
  Miles, Songcen Xu, Feng Wen, Chao Xu, Sinan Zeng, Dacheng Tao
Categories: cs.CL cs.AI
\\
  The paradigm of Large Language Models (LLMs) is currently defined by
auto-regressive (AR) architectures, which generate text through a sequential
``brick-by-brick'' process. Despite their success, AR models are inherently
constrained by a causal bottleneck that limits global structural foresight and
iterative refinement. Diffusion Language Models (DLMs) offer a transformative
alternative, conceptualizing text generation as a holistic, bidirectional
denoising process akin to a sculptor refining a masterpiece. However, the
potential of DLMs remains largely untapped as they are frequently confined
within AR-legacy infrastructures and optimization frameworks. In this
Perspective, we identify ten fundamental challenges ranging from architectural
inertia and gradient sparsity to the limitations of linear reasoning that
prevent DLMs from reaching their ``GPT-4 moment''. We propose a strategic
roadmap organized into four pillars: foundational infrastructure, algorithmic
optimization, cognitive reasoning, and unified multimodal intelligence. By
shifting toward a diffusion-native ecosystem characterized by multi-scale
tokenization, active remasking, and latent thinking, we can move beyond the
constraints of the causal horizon. We argue that this transition is essential
for developing next-generation AI capable of complex structural reasoning,
dynamic self-correction, and seamless multimodal integration.
\\ ( https://arxiv.org/abs/2601.14041 ,  1420kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14047 (*cross-listing*)
Date: Tue, 20 Jan 2026 15:01:55 GMT   (60kb)

Title: Collective intelligence in science: direct elicitation of diverse
  information from experts with unknown information structure
Authors: Alexey V. Osipov, Nikolay N. Osipov
Categories: cs.GT cs.AI cs.MA cs.SI econ.TH
Comments: 21 pages
ACM-class: H.1.2; H.5.3; I.2.6; I.2.7; I.2.11; J.0
\\
  Suppose we need a deep collective analysis of an open scientific problem:
there is a complex scientific hypothesis and a large online group of mutually
unrelated experts with relevant private information of a diverse and
unpredictable nature. This information may be results of experts' individual
experiments, original reasoning of some of them, results of AI systems they
use, etc. We propose a simple mechanism based on a self-resolving play-money
prediction market entangled with a chat. We show that such a system can easily
be brought to an equilibrium where participants directly share their private
information on the hypothesis through the chat and trade as if the market were
resolved in accordance with the truth of the hypothesis. This approach will
lead to efficient aggregation of relevant information in a completely
interpretable form even if the ground truth cannot be established and experts
initially know nothing about each other and cannot perform complex Bayesian
calculations. Finally, by rewarding the experts with some real assets
proportionally to the play money they end up with, we can get an innovative way
to fund large-scale collaborative studies of any type.
\\ ( https://arxiv.org/abs/2601.14047 ,  60kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14051 (*cross-listing*)
Date: Tue, 20 Jan 2026 15:05:44 GMT   (187kb)

Title: Kakugo: Distillation of Low-Resource Languages into Small Language
  Models
Authors: Peter Devine, Mardhiyah Sanni, Farid Adilazuarda, Julieta Gil Loizaga,
  Barry Haddow
Categories: cs.CL cs.AI cs.LG
\\
  We present Kakugo, a novel and cost-effective pipeline designed to train
general-purpose Small Language Models (SLMs) for low-resource languages using
only the language name as input. By using a large teacher model to generate
synthetic prompts and translate instruction datasets, we produced training data
and SLMs for 54 low-resource languages. Evaluations across a diverse set of
general natural language processing tasks, including translation,
classification, and question answering, demonstrate that our pipeline
consistently improves performance over base models. With a total generation and
training cost of under $50 per language, Kakugo offers an accessible method for
communities to develop language-specific AI.
\\ ( https://arxiv.org/abs/2601.14051 ,  187kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14055 (*cross-listing*)
Date: Tue, 20 Jan 2026 15:13:04 GMT   (2689kb)

Title: Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in
  Multi-Modal MRI
Authors: Andrea Protani, Marc Molina Van Den Bosch, Lorenzo Giusti, Heloisa
  Barbosa Da Silva, Paolo Cacace, Albert Sund Aillet, Miguel Angel Gonzalez
  Ballester, Friedhelm Hummel and Luigi Serio
Categories: cs.CV cs.AI
Comments: 10 pages, 3 figures,
\\
  Modern vision backbones for 3D medical imaging typically process dense voxel
grids through parameter-heavy encoder-decoder structures, a design that
allocates a significant portion of its parameters to spatial reconstruction
rather than feature learning. Our approach introduces SVGFormer, a decoder-free
pipeline built upon a content-aware grouping stage that partitions the volume
into a semantic graph of supervoxels. Its hierarchical encoder learns rich node
representations by combining a patch-level Transformer with a supervoxel-level
Graph Attention Network, jointly modeling fine-grained intra-region features
and broader inter-regional dependencies. This design concentrates all learnable
capacity on feature encoding and provides inherent, dual-scale explainability
from the patch to the region level. To validate the framework's flexibility, we
trained two specialized models on the BraTS dataset: one for node-level
classification and one for tumor proportion regression. Both models achieved
strong performance, with the classification model achieving a F1-score of 0.875
and the regression model a MAE of 0.028, confirming the encoder's ability to
learn discriminative and localized features. Our results establish that a
graph-based, encoder-only paradigm offers an accurate and inherently
interpretable alternative for 3D medical image representation.
\\ ( https://arxiv.org/abs/2601.14055 ,  2689kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14056 (*cross-listing*)
Date: Tue, 20 Jan 2026 15:13:43 GMT   (6122kb)

Title: POCI-Diff: Position Objects Consistently and Interactively with
  3D-Layout Guided Diffusion
Authors: Andrea Rigo, Luca Stornaiuolo, Weijie Wang, Mauro Martino, Bruno
  Lepri, Nicu Sebe
Categories: cs.CV cs.AI
\\
  We propose a diffusion-based approach for Text-to-Image (T2I) generation with
consistent and interactive 3D layout control and editing. While prior methods
improve spatial adherence using 2D cues or iterative copy-warp-paste
strategies, they often distort object geometry and fail to preserve consistency
across edits. To address these limitations, we introduce a framework for
Positioning Objects Consistently and Interactively (POCI-Diff), a novel
formulation for jointly enforcing 3D geometric constraints and instance-level
semantic binding within a unified diffusion process. Our method enables
explicit per-object semantic control by binding individual text descriptions to
specific 3D bounding boxes through Blended Latent Diffusion, allowing one-shot
synthesis of complex multi-object scenes. We further propose a warping-free
generative editing pipeline that supports object insertion, removal, and
transformation via regeneration rather than pixel deformation. To preserve
object identity and consistency across edits, we condition the diffusion
process on reference images using IP-Adapter, enabling coherent object
appearance throughout interactive 3D editing while maintaining global scene
coherence. Experimental results demonstrate that POCI-Diff produces
high-quality images consistent with the specified 3D layouts and edits,
outperforming state-of-the-art methods in both visual fidelity and layout
adherence while eliminating warping-induced geometric artifacts.
\\ ( https://arxiv.org/abs/2601.14056 ,  6122kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14063 (*cross-listing*)
Date: Tue, 20 Jan 2026 15:21:18 GMT   (1971kb)

Title: XCR-Bench: A Multi-Task Benchmark for Evaluating Cultural Reasoning in
  LLMs
Authors: Mohsinul Kabir, Tasnim Ahmed, Md Mezbaur Rahman, Shaoxiong Ji, Hassan
  Alhuzali, Sophia Ananiadou
Categories: cs.CL cs.AI cs.CY
Comments: 30 Pages, 13 Figures
\\
  Cross-cultural competence in large language models (LLMs) requires the
ability to identify Culture-Specific Items (CSIs) and to adapt them
appropriately across cultural contexts. Progress in evaluating this capability
has been constrained by the scarcity of high-quality CSI-annotated corpora with
parallel cross-cultural sentence pairs. To address this limitation, we
introduce XCR-Bench, a Cross(X)-Cultural Reasoning Benchmark consisting of 4.9k
parallel sentences and 1,098 unique CSIs, spanning three distinct reasoning
tasks with corresponding evaluation metrics. Our corpus integrates Newmark's
CSI framework with Hall's Triad of Culture, enabling systematic analysis of
cultural reasoning beyond surface-level artifacts and into semi-visible and
invisible cultural elements such as social norms, beliefs, and values. Our
findings show that state-of-the-art LLMs exhibit consistent weaknesses in
identifying and adapting CSIs related to social etiquette and cultural
reference. Additionally, we find evidence that LLMs encode regional and
ethno-religious biases even within a single linguistic setting during cultural
adaptation. We release our corpus and code to facilitate future research on
cross-cultural NLP.
\\ ( https://arxiv.org/abs/2601.14063 ,  1971kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14069 (*cross-listing*)
Date: Tue, 20 Jan 2026 15:25:41 GMT   (2123kb)

Title: Unsupervised Video Class-Incremental Learning via Deep Embedded
  Clustering Management
Authors: Nattapong Kurpukdee and Adrian G. Bors
Categories: cs.CV cs.AI cs.LG
\\
  Unsupervised video class incremental learning (uVCIL) represents an important
learning paradigm for learning video information without forgetting, and
without considering any data labels. Prior approaches have focused on
supervised class-incremental learning, relying on using the knowledge of labels
and task boundaries, which is costly, requires human annotation, or is simply
not a realistic option. In this paper, we propose a simple yet effective
approach to address the uVCIL. We first consider a deep feature extractor
network, providing a set of representative video features during each task
without assuming any class or task information. We then progressively build a
series of deep clusters from the extracted features. During the successive task
learning, the model updated from the previous task is used as an initial state
in order to transfer knowledge to the current learning task. We perform
in-depth evaluations on three standard video action recognition datasets,
including UCF101, HMDB51, and Something-to-Something V2, by ignoring the labels
from the supervised setting. Our approach significantly outperforms other
baselines on all datasets.
\\ ( https://arxiv.org/abs/2601.14069 ,  2123kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14084 (*cross-listing*)
Date: Tue, 20 Jan 2026 15:44:57 GMT   (2001kb)

Title: DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology
  Visual Question Answering and Reasoning
Authors: Abdurrahim Yilmaz, Ozan Erdem, Ece Gokyayla, Ayda Acar, Burc Bugra
  Dagtas, Dilara Ilhan Erdil, Gulsum Gencoglan, and Burak Temelkuran
Categories: cs.CV cs.AI cs.CL
\\
  Vision-language models (VLMs) are increasingly important in medical
applications; however, their evaluation in dermatology remains limited by
datasets that focus primarily on image-level classification tasks such as
lesion recognition. While valuable for recognition, such datasets cannot assess
the full visual understanding, language grounding, and clinical reasoning
capabilities of multimodal models. Visual question answering (VQA) benchmarks
are required to evaluate how models interpret dermatological images, reason
over fine-grained morphology, and generate clinically meaningful descriptions.
We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built
on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656
clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI.
Using a hierarchical annotation schema with 22 main questions (single-choice,
multi-choice, and open-ended), expert dermatologists annotated each image for
diagnosis, anatomic site, lesion morphology, distribution, surface features,
color, and image quality, together with open-ended narrative descriptions and
summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is
released as a metadata-only dataset to respect upstream licensing and is
publicly available at Harvard Dataverse.
\\ ( https://arxiv.org/abs/2601.14084 ,  2001kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14086 (*cross-listing*)
Date: Tue, 20 Jan 2026 15:47:00 GMT   (1010kb)

Title: Two-Stream temporal transformer for video action classification
Authors: Nattapong Kurpukdee and Adrian G. Bors
Categories: cs.CV cs.AI cs.LG
\\
  Motion representation plays an important role in video understanding and has
many applications including action recognition, robot and autonomous guidance
or others. Lately, transformer networks, through their self-attention mechanism
capabilities, have proved their efficiency in many applications. In this study,
we introduce a new two-stream transformer video classifier, which extracts
spatio-temporal information from content and optical flow representing movement
information. The proposed model identifies self-attention features across the
joint optical flow and temporal frame domain and represents their relationships
within the transformer encoder mechanism. The experimental results show that
our proposed methodology provides excellent classification results on three
well-known video datasets of human activities.
\\ ( https://arxiv.org/abs/2601.14086 ,  1010kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14087 (*cross-listing*)
Date: Tue, 20 Jan 2026 15:47:36 GMT   (886kb)

Title: '1'-bit Count-based Sorting Unit to Reduce Link Power in DNN
  Accelerators
Authors: Ruichi Han, Yizhi Chen, Tong Lei, Jordi Altayo Gonzalez, and Ahmed
  Hemani (Department of Electronics and Embedded Systems, KTH Royal Institute
  of Technology, Stockholm, Sweden)
Categories: cs.AR cs.AI cs.LG
Comments: Accepted for oral presentation at the 2026 VLSI Symposium on
  Technology, Systems and Applications (VLSI TSA) on April 13-17, 2026, at the
  Ambassador Hotel, Hsinchu, Taiwan
\\
  Interconnect power consumption remains a bottleneck in Deep Neural Network
(DNN) accelerators. While ordering data based on '1'-bit counts can mitigate
this via reduced switching activity, practical hardware sorting implementations
remain underexplored. This work proposes the hardware implementation of a
comparison-free sorting unit optimized for Convolutional Neural Networks (CNN).
By leveraging approximate computing to group population counts into
coarse-grained buckets, our design achieves hardware area reductions while
preserving the link power benefits of data reordering. Our approximate sorting
unit achieves up to 35.4% area reduction while maintaining 19.50\% BT reduction
compared to 20.42% of precise implementation.
\\ ( https://arxiv.org/abs/2601.14087 ,  886kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14091 (*cross-listing*)
Date: Tue, 20 Jan 2026 15:54:33 GMT   (3041kb)

Title: Zero-shot adaptable task planning for autonomous construction robots: a
  comparative study of lightweight single and multi-AI agent systems
Authors: Hossein Naderi, Alireza Shojaei, Lifu Huang, Philip Agee, Kereshmeh
  Afsari, Abiola Akanmu
Categories: cs.RO cs.AI
\\
  Robots are expected to play a major role in the future construction industry
but face challenges due to high costs and difficulty adapting to dynamic tasks.
This study explores the potential of foundation models to enhance the
adaptability and generalizability of task planning in construction robots. Four
models are proposed and implemented using lightweight, open-source large
language models (LLMs) and vision language models (VLMs). These models include
one single agent and three multi-agent teams that collaborate to create robot
action plans. The models are evaluated across three construction roles:
Painter, Safety Inspector, and Floor Tiling. Results show that the four-agent
team outperforms the state-of-the-art GPT-4o in most metrics while being ten
times more cost-effective. Additionally, teams with three and four agents
demonstrate the improved generalizability. By discussing how agent behaviors
influence outputs, this study enhances the understanding of AI teams and
supports future research in diverse unstructured environments beyond
construction.
\\ ( https://arxiv.org/abs/2601.14091 ,  3041kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14124 (*cross-listing*)
Date: Tue, 20 Jan 2026 16:21:41 GMT   (123kb)

Title: Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental
  Health Text for Arabic
Authors: Saad Mankarious, Aya Zirikly
Categories: cs.CL cs.AI
\\
  Synthetic data offers a promising solution for mitigating data scarcity and
demographic bias in mental health analysis, yet existing approaches largely
rely on pretrained large language models (LLMs), which may suffer from limited
output diversity and propagate biases inherited from their training data. In
this work, we propose a pretraining-free diffusion-based approach for synthetic
text generation that frames bias mitigation as a style transfer problem. Using
the CARMA Arabic mental health corpus, which exhibits a substantial gender
imbalance, we focus on male-to-female style transfer to augment
underrepresented female-authored content. We construct five datasets capturing
varying linguistic and semantic aspects of gender expression in Arabic and
train separate diffusion models for each setting. Quantitative evaluations
demonstrate consistently high semantic fidelity between source and generated
text, alongside meaningful surface-level stylistic divergence, while
qualitative analysis confirms linguistically plausible gender transformations.
Our results show that diffusion-based style transfer can generate high-entropy,
semantically faithful synthetic data without reliance on pretrained LLMs,
providing an effective and flexible framework for mitigating gender bias in
sensitive, low-resource mental health domains.
\\ ( https://arxiv.org/abs/2601.14124 ,  123kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14152 (*cross-listing*)
Date: Tue, 20 Jan 2026 16:54:22 GMT   (290kb)

Title: Lost in the Prompt Order: Revealing the Limitations of Causal Attention
  in Language Models
Authors: Hyunjong Ok, Jaeho Lee
Categories: cs.CL cs.AI cs.LG
Comments: preprint
\\
  Large language models exhibit surprising sensitivity to the structure of the
prompt, but the mechanisms underlying this sensitivity remain poorly
understood. In this work, we conduct an in-depth investigation on a striking
case: in multiple-choice question answering, placing context before the
questions and options (CQO) outperforms the reverse order (QOC) by over 14%p,
consistently over a wide range of models and datasets. Through systematic
architectural analysis, we identify causal attention as the core mechanism: in
QOC prompts, the causal mask prevents option tokens from attending to context,
creating an information bottleneck where context becomes invisible to options.
\\ ( https://arxiv.org/abs/2601.14152 ,  290kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14154 (*cross-listing*)
Date: Tue, 20 Jan 2026 16:58:12 GMT   (675kb)

Title: LLM Augmented Intervenable Multimodal Adaptor for Post-operative
  Complication Prediction in Lung Cancer Surgery
Authors: Shubham Pandey, Bhavin Jawade, Srirangaraj Setlur, Venu Govindaraju,
  Kenneth Seastedt
Categories: cs.CV cs.AI
Comments: Accepted to P2P-CV @ WACV 2026
\\
  Postoperative complications remain a critical concern in clinical practice,
adversely affecting patient outcomes and contributing to rising healthcare
costs. We present MIRACLE, a deep learning architecture for prediction of risk
of postoperative complications in lung cancer surgery by integrating
preoperative clinical and radiological data. MIRACLE employs a hyperspherical
embedding space fusion of heterogeneous inputs, enabling the extraction of
robust, discriminative features from both structured clinical records and
high-dimensional radiological images. To enhance transparency of prediction and
clinical utility, we incorporate an interventional deep learning module in
MIRACLE, that not only refines predictions but also provides interpretable and
actionable insights, allowing domain experts to interactively adjust
recommendations based on clinical expertise. We validate our approach on POC-L,
a real-world dataset comprising 3,094 lung cancer patients who underwent
surgery at Roswell Park Comprehensive Cancer Center. Our results demonstrate
that MIRACLE outperforms various traditional machine learning models and
contemporary large language models (LLM) variants alone, for personalized and
explainable postoperative risk management.
\\ ( https://arxiv.org/abs/2601.14154 ,  675kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14157 (*cross-listing*)
Date: Tue, 20 Jan 2026 17:04:08 GMT   (218kb)

Title: ConceptCaps - a Distilled Concept Dataset for Interpretability in Music
  Models
Authors: Bruno Sienkiewicz and {\L}ukasz Neumann and Mateusz Modrzejewski
Categories: cs.SD cs.AI cs.LG
\\
  Concept-based interpretability methods like TCAV require clean,
well-separated positive and negative examples for each concept. Existing music
datasets lack this structure: tags are sparse, noisy, or ill-defined. We
introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with
explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic
modeling from text generation: a VAE learns plausible attribute co-occurrence
patterns, a fine-tuned LLM converts attribute lists into professional
descriptions, and MusicGen synthesizes corresponding audio. This separation
improves coherence and controllability over end-to-end approaches. We validate
the dataset through audio-text alignment (CLAP), linguistic quality metrics
(BERTScore, MAUVE), and TCAV analysis confirming that concept probes recover
musically meaningful patterns. Dataset and code are available online.
\\ ( https://arxiv.org/abs/2601.14157 ,  218kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14160 (*cross-listing*)
Date: Tue, 20 Jan 2026 17:11:51 GMT   (35kb)

Title: Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language
  Models for German Law
Authors: Ali Hamza Bashir, Muhammad Rehan Khalid, Kostadin Cvejoski, Jana Birr,
  Jule Berghaus, Armin Berger, Sandra Halscheidt, Christian Temath, Rafet Sifa,
  David Berghaus
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) often struggle in specialized domains such as
legal reasoning due to limited expert knowledge, resulting in factually
incorrect outputs or hallucinations. This paper presents an effective method
for adapting advanced LLMs to German legal question answering through a novel
synthetic data generation approach. In contrast to costly human-annotated
resources or unreliable synthetic alternatives, our approach systematically
produces high-quality, diverse, and legally accurate question-answer pairs
directly from authoritative German statutes. Using rigorous automated filtering
methods and parameter-efficient fine-tuning techniques, we demonstrate that
LLMs adapted with our synthetic dataset significantly outperform their baseline
counterparts on German legal question answering tasks. Our results highlight
the feasibility of using carefully designed synthetic data as a robust
alternative to manual annotation in high-stakes, knowledge-intensive domains.
\\ ( https://arxiv.org/abs/2601.14160 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14172 (*cross-listing*)
Date: Tue, 20 Jan 2026 17:25:33 GMT   (241kb)

Title: Human Values in a Single Sentence: Moral Presence, Hierarchies, and
  Transformer Ensembles on the Schwartz Continuum
Authors: V\'ictor Yeste, Paolo Rosso
Categories: cs.CL cs.AI
Comments: Code: https://github.com/VictorMYeste/human-value-detection, 37
  pages, 4 figures,
ACM-class: I.2.7
\\
  We study sentence-level identification of the 19 values in the Schwartz
motivational continuum as a concrete formulation of human value detection in
text. The setting - out-of-context sentences from news and political manifestos
- features sparse moral cues and severe class imbalance. This combination makes
fine-grained sentence-level value detection intrinsically difficult, even for
strong modern neural models. We first operationalize a binary moral presence
task ("does any value appear?") and show that it is learnable from single
sentences (positive-class F1 $\approx$ 0.74 with calibrated thresholds). We
then compare a presence-gated hierarchy to a direct multi-label classifier
under matched compute, both based on DeBERTa-base and augmented with
lightweight signals (prior-sentence context, LIWC-22/eMFD/MJD lexica, and topic
features). The hierarchy does not outperform direct prediction, indicating that
gate recall limits downstream gains. We also benchmark instruction-tuned LLMs -
Gemma 2 9B, Llama 3.1 8B, Mistral 8B, and Qwen 2.5 7B - in zero-/few-shot and
QLoRA setups and build simple ensembles; a soft-vote supervised ensemble
reaches macro-F1 0.332, significantly surpassing the best single supervised
model and exceeding prior English-only baselines. Overall, in this scenario,
lightweight signals and small ensembles yield the most reliable improvements,
while hierarchical gating offers limited benefit. We argue that, under an 8 GB
single-GPU constraint and at the 7-9B scale, carefully tuned supervised
encoders remain a strong and compute-efficient baseline for structured human
value detection, and we outline how richer value structure and
sentence-in-document context could further improve performance.
\\ ( https://arxiv.org/abs/2601.14172 ,  241kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14230 (*cross-listing*)
Date: Tue, 20 Jan 2026 18:44:04 GMT   (2789kb)

Title: MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems
Authors: Yiyang Wang, Yiqiao Jin, Alex Cabral, Josiah Hester
Categories: cs.CL cs.AI cs.HC
Comments: 15 pages, 9 figures
\\
  Multi-agent systems (MAS) have recently emerged as promising
socio-collaborative companions for emotional and cognitive support. However,
these systems frequently suffer from persona collapse--where agents revert to
generic, homogenized assistant behaviors--and social sycophancy, which produces
redundant, non-constructive dialogue. We propose MASCOT, a generalizable
framework for multi-perspective socio-collaborative companions. MASCOT
introduces a novel bi-level optimization strategy to harmonize individual and
collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven
pipeline that finetunes individual agents for strict persona fidelity to
prevent identity loss; and 2) Collaborative Dialogue Optimization, a
meta-policy guided by group-level rewards to ensure diverse and productive
discourse. Extensive evaluations across psychological support and workplace
domains demonstrate that MASCOT significantly outperforms state-of-the-art
baselines, achieving improvements of up to +14.1 in Persona Consistency and
+10.6 in Social Contribution. Our framework provides a practical roadmap for
engineering the next generation of socially intelligent multi-agent systems.
\\ ( https://arxiv.org/abs/2601.14230 ,  2789kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14235 (*cross-listing*)
Date: Tue, 20 Jan 2026 18:46:42 GMT   (813kb)

Title: Opportunities in AI/ML for the Rubin LSST Dark Energy Science
  Collaboration
Authors: LSST Dark Energy Science Collaboration, Eric Aubourg, Camille
  Avestruz, Matthew R. Becker, Biswajit Biswas, Rahul Biswas, Boris Bolliet,
  Adam S. Bolton, Clecio R. Bom, Rapha\"el Bonnet-Guerrini, Alexandre Boucaud,
  Jean-Eric Campagne, Chihway Chang, Aleksandra \'Ciprijanovi\'c, Johann
  Cohen-Tanugi, Michael W. Coughlin, John Franklin Crenshaw, Juan C.
  Cuevas-Tello, Juan de Vicente, Seth W. Digel, Steven Dillmann, Mariano Javier
  de Le\'on Dominguez Romero, Alex Drlica-Wagner, Sydney Erickson, Alexander T.
  Gagliano, Christos Georgiou, Aritra Ghosh, Matthew Grayling, Kirill A.
  Grishin, Alan Heavens, Lindsay R. House, Mustapha Ishak, Wassim Kabalan, Arun
  Kannawadi, Fran\c{c}ois Lanusse, C. Danielle Leonard, Pierre-Fran\c{c}ois
  L\'eget, Michelle Lochner, Yao-Yuan Mao, Peter Melchior, Grant Merz, et al.
  (25 additional authors not shown)
Categories: astro-ph.IM astro-ph.CO cs.AI cs.LG stat.ML
Comments: 84 pages. This is v1.0 of the DESC's white paper on AI/ML, a
  collaboration document that is being made public but which is not planned for
  submission to a journal
Report-no: FERMILAB-PUB-25-0886-CSAID-PPD
\\
  The Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) will
produce unprecedented volumes of heterogeneous astronomical data (images,
catalogs, and alerts) that challenge traditional analysis pipelines. The LSST
Dark Energy Science Collaboration (DESC) aims to derive robust constraints on
dark energy and dark matter from these data, requiring methods that are
statistically powerful, scalable, and operationally reliable. Artificial
intelligence and machine learning (AI/ML) are already embedded across DESC
science workflows, from photometric redshifts and transient classification to
weak lensing inference and cosmological simulations. Yet their utility for
precision cosmology hinges on trustworthy uncertainty quantification,
robustness to covariate shift and model misspecification, and reproducible
integration within scientific pipelines. This white paper surveys the current
landscape of AI/ML across DESC's primary cosmological probes and cross-cutting
analyses, revealing that the same core methodologies and fundamental challenges
recur across disparate science cases. Since progress on these cross-cutting
challenges would benefit multiple probes simultaneously, we identify key
methodological research priorities, including Bayesian inference at scale,
physics-informed methods, validation frameworks, and active learning for
discovery. With an eye on emerging techniques, we also explore the potential of
the latest foundation model methodologies and LLM-driven agentic AI systems to
reshape DESC workflows, provided their deployment is coupled with rigorous
evaluation and governance. Finally, we discuss critical software, computing,
data infrastructure, and human capital requirements for the successful
deployment of these new methodologies, and consider associated risks and
opportunities for broader coordination with external actors.
\\ ( https://arxiv.org/abs/2601.14235 ,  813kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14242 (*cross-listing*)
Date: Tue, 20 Jan 2026 18:53:44 GMT   (8181kb)

Title: APEX-Agents
Authors: Bertie Vidgen, Austin Mann, Abby Fennelly, John Wright Stanly, Lucas
  Rothman, Marco Burstein, Julien Benchek, David Ostrofsky, Anirudh
  Ravichandran, Debnil Sur, Neel Venugopal, Alannah Hsia, Isaac Robinson, Calix
  Huang, Olivia Varones, Daniyal Khan, Michael Haines, Zach Richards, Chirag
  Mahapatra, Brendan Foody, Osvald Nitski
Categories: cs.CL cs.AI cs.LG
\\
  We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark
for assessing whether AI agents can execute long-horizon, cross-application
tasks created by investment banking analysts, management consultants, and
corporate lawyers. APEX-Agents requires agents to navigate realistic work
environments with files and tools. We test eight agents for the leaderboard
using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of
24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High),
and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark
(n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also
open-source Archipelago, our infrastructure for agent execution and evaluation.
\\ ( https://arxiv.org/abs/2601.14242 ,  8181kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14255 (*cross-listing*)
Date: Tue, 20 Jan 2026 18:59:56 GMT   (10532kb)

Title: VideoMaMa: Mask-Guided Video Matting via Generative Prior
Authors: Sangbeom Lim, Seoung Wug Oh, Jiahui Huang, Heeji Yoon, Seungryong Kim,
  Joon-Young Lee
Categories: cs.CV cs.AI
Comments: Project page: https://cvlab-kaist.github.io/VideoMaMa/
\\
  Generalizing video matting models to real-world videos remains a significant
challenge due to the scarcity of labeled data. To address this, we present
Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks
into pixel accurate alpha mattes, by leveraging pretrained video diffusion
models. VideoMaMa demonstrates strong zero-shot generalization to real-world
footage, even though it is trained solely on synthetic data. Building on this
capability, we develop a scalable pseudo-labeling pipeline for large-scale
video matting and construct the Matting Anything in Video (MA-V) dataset, which
offers high-quality matting annotations for more than 50K real-world videos
spanning diverse scenes and motions. To validate the effectiveness of this
dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which
outperforms the same model trained on existing matting datasets in terms of
robustness on in-the-wild videos. These findings emphasize the importance of
large-scale pseudo-labeled video matting and showcase how generative priors and
accessible segmentation cues can drive scalable progress in video matting
research.
\\ ( https://arxiv.org/abs/2601.14255 ,  10532kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11550 (*cross-listing*)
Date: Sun, 7 Dec 2025 20:04:26 GMT   (354kb)

Title: Uniqueness ratio as a predictor of a privacy leakage
Authors: Danah A. AlSalem AlKhashti
Categories: cs.DB cs.CR cs.LG
MSC-class: 68P05, 62H30, 68T05
ACM-class: H.2.0; H.2.8; K.4.1; I.2.6
\\
  Identity leakage can emerge when independent databases are joined, even when
each dataset is anonymized individually. While previous work focuses on
post-join detection or complex privacy models, little attention has been given
to simple, interpretable pre-join indicators that can warn data engineers and
database administrators before integration occurs. This study investigates the
uniqueness ratio of candidate join attributes as an early predictor of
re-identification risk. Using synthetic multi-table datasets, we compute the
uniqueness ratio of attribute combinations within each database and examine how
these ratios correlate with identity exposure after the join. Experimental
results show a strong relationship between high pre-join uniqueness and
increased post-join leakage, measured by the proportion of records that become
uniquely identifiable or fall into very small groups. Our findings demonstrate
that uniqueness ratio offers an explainable and practical signal for assessing
join induced privacy risk, providing a foundation for developing more
comprehensive pre-join risk estimation models.
\\ ( https://arxiv.org/abs/2601.11550 ,  354kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11594 (*cross-listing*)
Date: Tue, 6 Jan 2026 21:11:33 GMT   (1072kb)

Title: Multi-Scale Negative Coupled Information Systems (MNCIS): A Unified
  Spectral Topology Framework for Stability in Turbulence, AI, and Biology
Authors: Pengyue Hou
Categories: physics.comp-ph cs.LG nlin.AO physics.bio-ph
Comments: Includes supplementary materials and code. Foundation and
  mathematical proofs can be found in the companion paper arXiv:2601.00638
\\
  Complex dynamical systems frequently encounter a recurrent structural
instability: the collapse of the spectral gap, driving the system toward a
low-dimensional "Zero-Mode Attractor" (e.g., spectral pile-up or
over-smoothing). Building upon recent global well-posedness estimates [Hou,
arXiv:2601.00638], this work generalizes the Multi-Scale Negative Coupled
Information System (MNCIS) framework. We postulate that global stability
requires an active topological operator -- Adaptive Spectral Negative Coupling
(ASNC) -- functioning as a state-dependent high-pass filter that penalizes
entropy accumulation at spectral boundaries. We validate this unified framework
via three implementations:(1) Hydrodynamics: In 3D Navier-Stokes turbulence
($N=256^3$), ASNC acts as a global-enstrophy adaptive sub-grid scale (SGS)
model, stabilizing the inviscid limit and preserving the Kolmogorov $-5/3$
inertial range without artificial hyper-viscosity.(2) Artificial Intelligence:
Addressing Over-smoothing in Graph Neural Networks (GNNs), we implement ASNC as
a parameter-free topological constraint. Unlike baselines (e.g., DeepGCNs)
relying on dense residual connections to bypass signal decay, our framework
enables the training of ultra-deep 64-layer networks without residual
connections, maintaining perfectly stationary feature variance ($\sigma^2
\equiv 1.0$) on the ogbn-arxiv benchmark. (3) Biological Physics: In
reaction-diffusion morphogenesis, it stabilizes Turing patterns against
diffusive washout in high-entropy regimes. Our results suggest that the MNCIS
framework provides a base-independent topological condition for distinguishing
viable complex systems from those collapsing into thermal equilibrium, bridging
physical stability and information persistence.
\\ ( https://arxiv.org/abs/2601.11594 ,  1072kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11595 (*cross-listing*)
Date: Tue, 6 Jan 2026 21:34:08 GMT   (413kb)

Title: Enhancing Model Context Protocol (MCP) with Context-Aware Server
  Collaboration
Authors: Meenakshi Amulya Jayanti, X.Y. Han
Categories: cs.DC cs.LG cs.SE
\\
  The Model Context Protocol (MCP) has emerged as a widely used framework for
enabling LLM-based agents to communicate with external tools and services. The
most common implementation of MCP, proposed by Anthropic, heavily relies on a
Large Language Model (LLM) to decompose tasks and issue instructions to
servers, which act as stateless executors. In particular, the agents, models,
and servers are stateless and do not have access to a global context. However,
in tasks involving LLM-driven coordination, it is natural that a Shared Context
Store (SCS) could improve the efficiency and coherence of multi-agent workflows
by reducing redundancy and enabling knowledge transfer between servers. Thus,
in this work, we design and assess the performance of a Context-Aware MCP
(CA-MCP) that offloads execution logic to specialized MCP servers that read
from and write to a shared context memory, allowing them to coordinate more
autonomously in real time. In this design, context management serves as the
central mechanism that maintains continuity across task executions by tracking
intermediate states and shared variables, thereby enabling persistent
collaboration among agents without repeated prompting. We present experiments
showing that the CA-MCP can outperform the traditional MCP by reducing the
number of LLM calls required for complex tasks and decreasing the frequency of
response failures when task conditions are not satisfied, thereby improving
overall efficiency and responsiveness. In particular, we conducted experiments
on the TravelPlanner and REALM-Bench benchmark datasets and observed
statistically significant results indicating the potential advantages of
incorporating a shared context store via CA-MCP in LLM-driven multi-agent
systems.
\\ ( https://arxiv.org/abs/2601.11595 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11612 (*cross-listing*)
Date: Fri, 9 Jan 2026 12:35:52 GMT   (7735kb)

Title: Domain-Specific Self-Supervised Pre-training for Agricultural Disease
  Classification: A Hierarchical Vision Transformer Study
Authors: Arnav S. Sonavane
Categories: cs.CV cs.LG
Comments: 11 pages, 4 figures, 9 tables
ACM-class: I.4.9; I.5.4
\\
  We investigate the impact of domain-specific self-supervised pre-training on
agricultural disease classification using hierarchical vision transformers. Our
key finding is that SimCLR pre-training on just 3,000 unlabeled agricultural
images provides a +4.57% accuracy improvement--exceeding the +3.70% gain from
hierarchical architecture design. Critically, we show this SSL benefit is
architecture-agnostic: applying the same pre-training to Swin-Base yields
+4.08%, to ViT-Base +4.20%, confirming practitioners should prioritize domain
data collection over architectural choices. Using HierarchicalViT (HVT), a
Swin-style hierarchical transformer, we evaluate on three datasets: Cotton Leaf
Disease (7 classes, 90.24%), PlantVillage (38 classes, 96.3%), and PlantDoc (27
classes, 87.1%). At matched parameter counts, HVT-Base (78M) achieves 88.91%
vs. Swin-Base (88M) at 87.23%, a +1.68% improvement. For deployment
reliability, we report calibration analysis showing HVT achieves 3.56% ECE
(1.52% after temperature scaling). Code:
https://github.com/w2sg-arnav/HierarchicalViT
\\ ( https://arxiv.org/abs/2601.11612 ,  7735kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11614 (*cross-listing*)
Date: Fri, 9 Jan 2026 22:26:38 GMT   (1137kb)

Title: Multi-modal MRI-Based Alzheimer's Disease Diagnosis with
  Transformer-based Image Synthesis and Transfer Learning
Authors: Jason Qiu
Categories: cs.CV cs.LG q-bio.NC
Comments: 19 pages, 10 figures
\\
  Alzheimer's disease (AD) is a progressive neurodegenerative disorder in which
pathological changes begin many years before the onset of clinical symptoms,
making early detection essential for timely intervention. T1-weighted (T1w)
Magnetic Resonance Imaging (MRI) is routinely used in clinical practice to
identify macroscopic brain alterations, but these changes typically emerge
relatively late in the disease course. Diffusion MRI (dMRI), in contrast, is
sensitive to earlier microstructural abnormalities by probing water diffusion
in brain tissue. dMRI metrics, including fractional anisotropy (FA) and mean
diffusivity (MD), provide complementary information about white matter
integrity and neurodegeneration. However, dMRI acquisitions are time-consuming
and susceptible to motion artifacts, limiting their routine use in clinical
populations. To bridge this gap, I propose a 3D TransUNet image synthesis
framework that predicts FA and MD maps directly from T1w MRI. My model
generates high-fidelity maps, achieving a structural similarity index (SSIM)
exceeding 0.93 and a strong Pearson correlation (>0.94) with ground-truth dMRI.
When integrated into a multi-modal diagnostic model, these synthetic features
boost AD classification accuracy by 5% (78.75%->83.75%) and, most importantly,
improve mild cognitive impairment (MCI) detection by 12.5%. This study
demonstrates that high-quality diffusion microstructural information can be
inferred from routinely acquired T1w MRI, effectively transferring the benefits
of multi-modality imaging to settings where diffusion data are unavailable. By
reducing scan time while preserving complementary structural and
microstructural information, the proposed approach has the potential to improve
the accessibility, efficiency, and accuracy of AD diagnosis in clinical
practice.
\\ ( https://arxiv.org/abs/2601.11614 ,  1137kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11626 (*cross-listing*)
Date: Mon, 12 Jan 2026 18:15:53 GMT   (60kb)

Title: Concatenated Matrix SVD: Compression Bounds, Incremental Approximation,
  and Error-Constrained Clustering
Authors: Maksym Shamrai
Categories: math.NA cs.LG cs.NA
\\
  Large collections of matrices arise throughout modern machine learning,
signal processing, and scientific computing, where they are commonly compressed
by concatenation followed by truncated singular value decomposition (SVD). This
strategy enables parameter sharing and efficient reconstruction and has been
widely adopted across domains ranging from multi-view learning and signal
processing to neural network compression. However, it leaves a fundamental
question unanswered: which matrices can be safely concatenated and compressed
together under explicit reconstruction error constraints? Existing approaches
rely on heuristic or architecture-specific grouping and provide no principled
guarantees on the resulting SVD approximation error. In the present work, we
introduce a theory-driven framework for compression-aware clustering of
matrices under SVD compression constraints. Our analysis establishes new
spectral bounds for horizontally concatenated matrices, deriving global upper
bounds on the optimal rank-$r$ SVD reconstruction error from lower bounds on
singular value growth. The first bound follows from Weyl-type monotonicity
under blockwise extensions, while the second leverages singular values of
incremental residuals to yield tighter, per-block guarantees. We further
develop an efficient approximate estimator based on incremental truncated SVD
that tracks dominant singular values without forming the full concatenated
matrix. Therefore, we propose three clustering algorithms that merge matrices
only when their predicted joint SVD compression error remains below a
user-specified threshold. The algorithms span a trade-off between speed,
provable accuracy, and scalability, enabling compression-aware clustering with
explicit error control. Code is available online.
\\ ( https://arxiv.org/abs/2601.11626 ,  60kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11629 (*cross-listing*)
Date: Wed, 14 Jan 2026 03:00:19 GMT   (2299kb)

Title: Semantic Differentiation for Tackling Challenges in Watermarking
  Low-Entropy Constrained Generation Outputs
Authors: Nghia T. Le, Alan Ritter, Kartik Goyal
Categories: cs.CR cs.LG
Comments: 18 pages, 4 figures
\\
  We demonstrate that while the current approaches for language model
watermarking are effective for open-ended generation, they are inadequate at
watermarking LM outputs for constrained generation tasks with low-entropy
output spaces. Therefore, we devise SeqMark, a sequence-level watermarking
algorithm with semantic differentiation that balances the output quality,
watermark detectability, and imperceptibility. It improves on the shortcomings
of the prevalent token-level watermarking algorithms that cause
under-utilization of the sequence-level entropy available for constrained
generation tasks. Moreover, we identify and improve upon a different failure
mode we term region collapse, associated with prior sequence-level watermarking
algorithms. This occurs because the pseudorandom partitioning of semantic space
for watermarking in these approaches causes all high-probability outputs to
collapse into either invalid or valid regions, leading to a trade-off in output
quality and watermarking effectiveness. SeqMark instead, differentiates the
high-probable output subspace and partitions it into valid and invalid regions,
ensuring the even spread of high-quality outputs among all the regions. On
various constrained generation tasks like machine translation, code generation,
and abstractive summarization, SeqMark substantially improves watermark
detection accuracy (up to 28% increase in F1) while maintaining high generation
quality.
\\ ( https://arxiv.org/abs/2601.11629 ,  2299kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11641 (*cross-listing*)
Date: Wed, 14 Jan 2026 16:25:39 GMT   (9790kb)

Title: Mixture of Distributions Matters: Dynamic Sparse Attention for Efficient
  Video Diffusion Transformers
Authors: Yuxi Liu and Yipeng Hu and Zekun Zhang and Kunze Jiang and Kun Yuan
Categories: cs.CV cs.LG
\\
  While Diffusion Transformers (DiTs) have achieved notable progress in video
generation, this long-sequence generation task remains constrained by the
quadratic complexity inherent to self-attention mechanisms, creating
significant barriers to practical deployment. Although sparse attention methods
attempt to address this challenge, existing approaches either rely on
oversimplified static patterns or require computationally expensive sampling
operations to achieve dynamic sparsity, resulting in inaccurate pattern
predictions and degraded generation quality. To overcome these limitations, we
propose a
\underline{\textbf{M}}ixtrue-\underline{\textbf{O}}f-\underline{\textbf{D}}istribution
\textbf{DiT} (\textbf{MOD-DiT}), a novel sampling-free dynamic attention
framework that accurately models evolving attention patterns through a
two-stage process. First, MOD-DiT leverages prior information from early
denoising steps and adopts a {distributed mixing approach} to model an
efficient linear approximation model, which is then used to predict mask
patterns for a specific denoising interval. Second, an online block masking
strategy dynamically applies these predicted masks while maintaining historical
sparsity information, eliminating the need for repetitive sampling operations.
Extensive evaluations demonstrate consistent acceleration and quality
improvements across multiple benchmarks and model architectures, validating
MOD-DiT's effectiveness for efficient, high-quality video generation while
overcoming the computational limitations of traditional sparse attention
approaches.
\\ ( https://arxiv.org/abs/2601.11641 ,  9790kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11642 (*cross-listing*)
Date: Wed, 14 Jan 2026 16:54:20 GMT   (1102kb)

Title: PSSF: Early osteoarthritis detection using physical synthetic knee X-ray
  scans and AI radiomics models
Authors: Abbas Alzubaidi, Ali Al-Bayaty
Categories: cs.CV cs.LG eess.IV
Comments: 16 pages, 6 figures
\\
  Knee osteoarthritis (OA) is a major cause of disability worldwide and is
still largely assessed using subjective radiographic grading, most commonly the
Kellgren-Lawrence (KL) scale. Artificial intelligence (AI) and radiomics offer
quantitative tools for OA assessment but depend on large, well-annotated image
datasets, mainly X-ray scans, that are often difficult to obtain because of
privacy, governance and resourcing constraints. In this research, we introduce
a physics-based synthetic simulation framework (PSSF) to fully generate
controllable X-ray scans without patients' involvement and violating their
privacy and institutional constraints. This PSSF is a 2D X-ray projection
simulator of anteroposterior knee radiographs from a parametric anatomical
model of the distal femur and proximal tibia. Using PSSF, we create a virtual
cohort of 180 subjects (260 knees), each is imaged under three protocols
(reference, low-dose, and geometry-shift). Medial joint regions are
automatically localized, preprocessed, and processed with the Image Biomarker
Standardisation Initiative (IBSI). Practically, three machine learning (ML)
models are utilized, logistic regression, random forest, and gradient boosting,
to train binary (KL-like "0" vs. "2") and three-class (0-2) prediction
radiographic images. Robustness is assessed within IBSI protocol,
cross-protocol, and multi-protocol scenarios. Finally, features stability is
then evaluated using intraclass correlation coefficients across acquisition
changes.
\\ ( https://arxiv.org/abs/2601.11642 ,  1102kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11653 (*cross-listing*)
Date: Thu, 15 Jan 2026 18:01:59 GMT   (2367kb)

Title: AI Agents Need Memory Control Over More Context
Authors: Fouad Bousetouane
Categories: q-bio.NC cs.LG cs.MA
Comments: 32 pages, 7 figures
\\
  AI agents are increasingly used in long, multi-turn workflows in both
research and enterprise settings. As interactions grow, agent behavior often
degrades due to loss of constraint focus, error accumulation, and
memory-induced drift. This problem is especially visible in real-world
deployments where context evolves, distractions are introduced, and decisions
must remain consistent over time. A common practice is to equip agents with
persistent memory through transcript replay or retrieval-based mechanisms.
While convenient, these approaches introduce unbounded context growth and are
vulnerable to noisy recall and memory poisoning, leading to unstable behavior
and increased drift. In this work, we introduce the Agent Cognitive Compressor
(ACC), a bio-inspired memory controller that replaces transcript replay with a
bounded internal state updated online at each turn. ACC separates artifact
recall from state commitment, enabling stable conditioning while preventing
unverified content from becoming persistent memory. We evaluate ACC using an
agent-judge-driven live evaluation framework that measures both task outcomes
and memory-driven anomalies across extended interactions. Across scenarios
spanning IT operations, cybersecurity response, and healthcare workflows, ACC
consistently maintains bounded memory and exhibits more stable multi-turn
behavior, with significantly lower hallucination and drift than transcript
replay and retrieval-based agents. These results show that cognitive
compression provides a practical and effective foundation for reliable memory
control in long-horizon AI agents.
\\ ( https://arxiv.org/abs/2601.11653 ,  2367kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11660 (*cross-listing*)
Date: Thu, 15 Jan 2026 21:22:16 GMT   (10451kb)

Title: Zeros can be Informative: Masked Binary U-Net for Image Segmentation on
  Tensor Cores
Authors: Chunshu Wu, Ruibing Song, Sushant Kondguli, Tong Geng, Ang Li
Categories: cs.CV cs.LG
\\
  Real-time image segmentation is a key enabler for AR/VR, robotics, drones,
and autonomous systems, where tight accuracy, latency, and energy budgets must
be met on resource-constrained edge devices. While U-Net offers a favorable
balance of accuracy and efficiency compared to large transformer-based models,
achieving real-time performance on high-resolution input remains challenging
due to compute, memory, and power limits. Extreme quantization, particularly
binary networks, is appealing for its hardware-friendly operations. However,
two obstacles limit practicality: (1) severe accuracy degradation, and (2) a
lack of end-to-end implementations that deliver efficiency on general-purpose
GPUs.
  We make two empirical observations that guide our design. (1) An explicit
zero state is essential: training with zero masking to binary U-Net weights
yields noticeable sparsity. (2) Quantization sensitivity is uniform across
layers. Motivated by these findings, we introduce Masked Binary U-Net
(MBU-Net), obtained through a cost-aware masking strategy that prioritizes
masking where it yields the highest accuracy-per-cost, reconciling accuracy
with near-binary efficiency.
  To realize these gains in practice, we develop a GPU execution framework that
maps MBU-Net to Tensor Cores via a subtractive bit-encoding scheme, efficiently
implementing masked binary weights with binary activations. This design
leverages native binary Tensor Core BMMA instructions, enabling high throughput
and energy savings on widely available GPUs. Across 3 segmentation benchmarks,
MBU-Net attains near full-precision accuracy (3% average drop) while delivering
2.04x speedup and 3.54x energy reductions over a 16-bit floating point U-Net.
\\ ( https://arxiv.org/abs/2601.11660 ,  10451kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11691 (*cross-listing*)
Date: Fri, 16 Jan 2026 15:35:12 GMT   (5089kb)

Title: Explainable histomorphology-based survival prediction of glioblastoma,
  IDH-wildtype
Authors: Jan-Philipp Redlich, Friedrich Feuerhake, Stefan Nikolin, Nadine Sarah
  Schaadt, Sarah Teuber-Hanselmann, Joachim Weis, Sabine Luttmann, Andrea
  Eberle, Christoph Buck, Timm Intemann, Pascal Birnstill, Klaus Kraywinkel,
  Jonas Ort, Peter Boor, Andr\'e Homeyer
Categories: eess.IV cs.LG q-bio.QM
\\
  Glioblastoma, IDH-wildtype (GBM-IDHwt) is the most common malignant brain
tumor. Histomorphology is a crucial component of the integrated diagnosis of
GBM-IDHwt. Artificial intelligence (AI) methods have shown promise to extract
additional prognostic information from histological whole-slide images (WSI) of
hematoxylin and eosin-stained glioblastoma tissue. Here, we present an
explainable AI-based method to support systematic interpretation of
histomorphological features associated with survival. It combines an
explainable multiple instance learning (MIL) architecture with a sparse
autoencoder (SAE) to relate human-interpretable visual patterns of tissue to
survival. The MIL architecture directly identifies prognosis-relevant image
tiles and the SAE maps these tiles post-hoc to visual patterns. The MIL method
was trained and evaluated using a new real-world dataset that comprised 720
GBM-IDHwt cases from three hospitals and four cancer registries in Germany. The
SAE was trained using 1878 WSIs of glioblastoma from five independent public
data collections. Despite the many factors influencing survival time, our
method showed some ability to discriminate between patients living less than
180 days or more than 360 days solely based on histomorphology (AUC: 0.67; 95%
CI: 0.63-0.72). Cox proportional hazards regression confirmed a significant
difference in survival time between the predicted groups after adjustment for
established prognostic factors (hazard ratio: 1.47; 95% CI: 1.26-1.72). Our
method identified multiple interpretable visual patterns associated with
survival. Three neuropathologists separately found that 21 of the 24 most
strongly associated patterns could be clearly attributed to seven
histomorphological categories. Necrosis and hemorrhage appeared to be
associated with shorter survival while highly cellular tumor areas were
associated with longer survival.
\\ ( https://arxiv.org/abs/2601.11691 ,  5089kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11694 (*cross-listing*)
Date: Fri, 16 Jan 2026 16:29:13 GMT   (95kb)

Title: Anisotropic Tensor Deconvolution of Hyperspectral Images
Authors: Xinjue Wang, Xiuheng Wang, Esa Ollila, Sergiy A. Vorobyov
Categories: eess.IV cs.CV cs.LG eess.SP
Comments: To appear in ICASSP 2026
\\
  Hyperspectral image (HSI) deconvolution is a challenging ill-posed inverse
problem, made difficult by the data's high dimensionality.We propose a
parameter-parsimonious framework based on a low-rank Canonical Polyadic
Decomposition (CPD) of the entire latent HSI $\mathbf{\mathcal{X}} \in
\mathbb{R}^{P\times Q \times N}$.This approach recasts the problem from
recovering a large-scale image with $PQN$ variables to estimating the CPD
factors with $(P+Q+N)R$ variables.This model also enables a structure-aware,
anisotropic Total Variation (TV) regularization applied only to the spatial
factors, preserving the smooth spectral signatures.An efficient algorithm based
on the Proximal Alternating Linearized Minimization (PALM) framework is
developed to solve the resulting non-convex optimization problem.Experiments
confirm the model's efficiency, showing a numerous parameter reduction of over
two orders of magnitude and a compelling trade-off between model compactness
and reconstruction accuracy.
\\ ( https://arxiv.org/abs/2601.11694 ,  95kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11716 (*cross-listing*)
Date: Fri, 16 Jan 2026 19:09:57 GMT   (4997kb)

Title: AllShowers: One model for all calorimeter showers
Authors: Thorsten Buss, Henry Day-Hall, Frank Gaede, Gregor Kasieczka, Katja
  Kr\"uger
Categories: physics.ins-det cs.LG hep-ex hep-ph
\\
  Accurate and efficient detector simulation is essential for modern collider
experiments. To reduce the high computational cost, various fast machine
learning surrogate models have been proposed. Traditional surrogate models for
calorimeter shower modeling train separate networks for each particle species,
limiting scalability and reuse. We introduce AllShowers, a unified generative
model that simulates calorimeter showers across multiple particle types using a
single generative model. AllShowers is a continuous normalizing flow model with
a Transformer architecture, enabling it to generate complex spatial and energy
correlations in variable-length point cloud representations of showers. Trained
on a diverse dataset of simulated showers in the highly granular ILD detector,
the model demonstrates the ability to generate realistic showers for electrons,
photons, and charged and neutral hadrons across a wide range of incident
energies and angles without retraining. In addition to unifying shower
generation for multiple particle types, AllShowers surpasses the fidelity of
previous single-particle-type models for hadronic showers. Key innovations
include the use of a layer embedding, allowing the model to learn all relevant
calorimeter layer properties; a custom attention masking scheme to reduce
computational demands and introduce a helpful inductive bias; and a shower- and
layer-wise optimal transport mapping to improve training convergence and sample
quality. AllShowers marks a significant step towards a universal model for
calorimeter shower simulations in collider experiments.
\\ ( https://arxiv.org/abs/2601.11716 ,  4997kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11723 (*cross-listing*)
Date: Fri, 16 Jan 2026 19:16:39 GMT   (3225kb)

Title: A Proof of Concept for a Digital Twin of an Ultrasonic Fermentation
  System
Authors: Francesco Saverio Sconocchia Pisoni, Andrea Vitaletti, Davide
  Appolloni, Federico Ortenzi, Blasco Morozzo della Rocca, Mariano Jos\'e
  Guill\'en, Alessandro Contaldo
Categories: cs.ET cs.LG
Comments: 23 pages, submitted to the 22nd International Conference on
  Intelligent Environments (IE 2026)
\\
  This paper presents the design and implementation of a proof of concept
digital twin for an innovative ultrasonic-enhanced beer-fermentation system,
developed to enable intelligent monitoring, prediction, and actuation in
yeast-growth environments. A traditional fermentation tank is equipped with a
piezoelectric transducer able to irradiate the tank with ultrasonic waves,
providing an external abiotic stimulus to enhance the growth of yeast and
accelerate the fermentation process. At its core, the digital twin incorporates
a predictive model that estimates yeast's culture density over time based on
the surrounding environmental conditions. To this end, we implement, tailor and
extend the model proposed in Palacios et al., allowing us to effectively handle
the limited number of available training samples by using temperature,
ultrasonic frequency, and duty cycle as inputs. The results obtained along with
the assessment of model performance demonstrate the feasibility of the proposed
approach.
\\ ( https://arxiv.org/abs/2601.11723 ,  3225kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11729 (*cross-listing*)
Date: Fri, 16 Jan 2026 19:21:02 GMT   (22214kb)

Title: SpaRRTa: A Synthetic Benchmark for Evaluating Spatial Intelligence in
  Visual Foundation Models
Authors: Turhan Can Kargin, Wojciech Jasi\'nski, Adam Pardyl, Bartosz
  Zieli\'nski, Marcin Przewi\k{e}\'zlikowski
Categories: cs.CV cs.LG
Comments: Project page is available at https://sparrta.gmum.net/
\\
  Visual Foundation Models (VFMs), such as DINO and CLIP, excel in semantic
understanding of images but exhibit limited spatial reasoning capabilities,
which limits their applicability to embodied systems. As a result, recent work
incorporates some 3D tasks (such as depth estimation) into VFM training.
However, VFM performance remains inconsistent across other spatial tasks,
raising the question of whether these models truly have spatial awareness or
overfit to specific 3D objectives. To address this question, we introduce the
Spatial Relation Recognition Task (SpaRRTa) benchmark, which evaluates the
ability of VFMs to identify relative positions of objects in the image. Unlike
traditional 3D objectives that focus on precise metric prediction (e.g.,
surface normal estimation), SpaRRTa probes a fundamental capability
underpinning more advanced forms of human-like spatial understanding. SpaRRTa
generates an arbitrary number of photorealistic images with diverse scenes and
fully controllable object arrangements, along with freely accessible spatial
annotations. Evaluating a range of state-of-the-art VFMs, we reveal significant
disparities between their spatial reasoning abilities. Through our analysis, we
provide insights into the mechanisms that support or hinder spatial awareness
in modern VFMs. We hope that SpaRRTa will serve as a useful tool for guiding
the development of future spatially aware visual models.
\\ ( https://arxiv.org/abs/2601.11729 ,  22214kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11775 (*cross-listing*)
Date: Fri, 16 Jan 2026 21:01:08 GMT   (1179kb)

Title: Quantum Kernel Machine Learning for Autonomous Materials Science
Authors: Felix Adams (1), Daiwei Zhu (2), David W. Steuerman (2), A. Gilad
  Kusne (1 and 3), Ichiro Takeuchi (1 and 4) ((1) University of Maryland
  College Park, (2) IonQ, (3) National Institute for Standards and Technology,
  (4) University of Maryland Quantum Materials Center)
Categories: cond-mat.mtrl-sci cs.LG quant-ph
\\
  Autonomous materials science, where active learning is used to navigate large
compositional phase space, has emerged as a powerful vehicle to rapidly explore
new materials. A crucial aspect of autonomous materials science is exploring
new materials using as little data as possible. Gaussian process-based active
learning allows effective charting of multi-dimensional parameter space with a
limited number of training data, and thus is a common algorithmic choice for
autonomous materials science. An integral part of the autonomous workflow is
the application of kernel functions for quantifying similarities among measured
data points. A recent theoretical breakthrough has shown that quantum kernel
models can achieve similar performance with less training data than classical
models. This signals the possible advantage of applying quantum kernel machine
learning to autonomous materials discovery. In this work, we compare quantum
and classical kernels for their utility in sequential phase space navigation
for autonomous materials science. Specifically, we compute a quantum kernel and
several classical kernels for x-ray diffraction patterns taken from an Fe-Ga-Pd
ternary composition spread library. We conduct our study on both IonQ's Aria
trapped ion quantum computer hardware and the corresponding classical noisy
simulator. We experimentally verify that a quantum kernel model can outperform
some classical kernel models. The results highlight the potential of quantum
kernel machine learning methods for accelerating materials discovery and
suggest complex x-ray diffraction data is a candidate for robust quantum kernel
model advantage.
\\ ( https://arxiv.org/abs/2601.11775 ,  1179kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11790 (*cross-listing*)
Date: Fri, 16 Jan 2026 21:33:57 GMT   (6085kb)

Title: Gradient-based Active Learning with Gaussian Processes for Global
  Sensitivity Analysis
Authors: Guerlain Lambert, C\'eline Helbert, Claire Lauvernet
Categories: stat.ML cs.LG stat.ME
MSC-class: 62K05, 62P12
\\
  Global sensitivity analysis of complex numerical simulators is often limited
by the small number of model evaluations that can be afforded. In such
settings, surrogate models built from a limited set of simulations can
substantially reduce the computational burden, provided that the design of
computer experiments is enriched efficiently. In this context, we propose an
active learning approach that, for a fixed evaluation budget, targets the most
informative regions of the input space to improve sensitivity analysis
accuracy. More specifically, our method builds on recent advances in active
learning for sensitivity analysis (Sobol' indices and derivative-based global
sensitivity measures, DGSM) that exploit derivatives obtained from a Gaussian
process (GP) surrogate. By leveraging the joint posterior distribution of the
GP gradient, we develop acquisition functions that better account for
correlations between partial derivatives and their impact on the response
surface, leading to a more comprehensive and robust methodology than existing
DGSM-oriented criteria. The proposed approach is first compared to
state-of-the-art methods on standard benchmark functions, and is then applied
to a real environmental model of pesticide transfers.
\\ ( https://arxiv.org/abs/2601.11790 ,  6085kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11822 (*cross-listing*)
Date: Fri, 16 Jan 2026 22:58:59 GMT   (1099kb)

Title: RAPID-Serve: Resource-efficient and Accelerated P/D Intra-GPU
  Disaggregation
Authors: Amna Masood and Pratishtha Gaur and Nuwan Jayasena
Categories: cs.DC cs.LG
\\
  Two widely adopted techniques for LLM inference serving systems today are
hybrid batching and disaggregated serving. A hybrid batch combines prefill and
decode tokens of different requests in the same batch to improve resource
utilization and throughput at the cost of increased latency per token. In
contrast, disaggregated serving decouples compute-bound prefill and
bandwidth-bound decode phases to optimize for service level objectives (SLOs)
at the cost of resource under-utilization and KV-cache transfer overheads. To
address the limitations of these techniques, we propose RAPID-Serve: a
technique to concurrently execute prefill and decode on the same GPU(s) to meet
latency SLOs while maintaining high throughput and efficient resource
utilization. Furthermore, we propose Adaptive Resource Management for runtime
compute resource allocation, optionally leveraging CU masking (a fine-grained
Compute Unit partitioning feature on AMD Instinct\textsuperscript{TM} GPUs).
RAPID-Serve provides up to 4.1x (average 1.7x) unconstrained throughput
improvement and 32x and higher (average 4.9x) throughput improvement under SLO
constraints, showing it as an effective strategy compared to the
state-of-the-art approaches, particularly in resource-constrained environments.
\\ ( https://arxiv.org/abs/2601.11822 ,  1099kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11833 (*cross-listing*)
Date: Fri, 16 Jan 2026 23:48:40 GMT   (1522kb)

Title: Karhunen-Lo\`eve Expansion-Based Residual Anomaly Map for
  Resource-Efficient Glioma MRI Segmentation
Authors: Anthony Hur
Categories: q-bio.QM cs.CV cs.LG eess.IV
\\
  Accurate segmentation of brain tumors is essential for clinical diagnosis and
treatment planning. Deep learning is currently the state-of-the-art for brain
tumor segmentation, yet it requires either large datasets or extensive
computational resources that are inaccessible in most areas. This makes the
problem increasingly difficult: state-of-the-art models use thousands of
training cases and vast computational power, where performance drops sharply
when either is limited. The top performer in the Brats GLI 2023 competition
relied on supercomputers trained on over 92,000 augmented MRI scans using an
AMD EPYC 7402 CPU, six NVIDIA RTX 6000 GPUs (48GB VRAM each), and 1024GB of RAM
over multiple weeks. To address this, the Karhunen--Lo\`eve Expansion (KLE) was
implemented as a feature extraction step on downsampled, z-score normalized MRI
volumes. Each 240$\times$240$\times$155 multi-modal scan is reduced to four
$48^3$ channels and compressed into 32 KL coefficients. The resulting
approximate reconstruction enables a residual-based anomaly map, which is
upsampled and added as a fifth channel to a compact 3D U-Net. All experiments
were run on a consumer workstation (AMD Ryzen 5 7600X CPU, RTX 4060Ti (8GB
VRAM), and 64GB RAM while using far fewer training cases. This model achieves
post-processed Dice scores of 0.929 (WT), 0.856 (TC), and 0.821 (ET), with HD95
distances of 2.93, 6.78, and 10.35 voxels. These results are significantly
better than the winning BraTS 2023 methodology for HD95 distances and WT dice
scores. This demonstrates that a KLE-based residual anomaly map can
dramatically reduce computational cost and data requirements while retaining
state-of-the-art performance.
\\ ( https://arxiv.org/abs/2601.11833 ,  1522kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11860 (*cross-listing*)
Date: Sat, 17 Jan 2026 01:04:41 GMT   (3941kb)

Title: Adversarial Drift-Aware Predictive Transfer: Toward Durable Clinical AI
Authors: Xin Xiong, Zijian Guo, Haobo Zhu, Chuan Hong, Jordan W Smoller, Tianxi
  Cai, Molei Liu
Categories: stat.AP cs.LG stat.ME
\\
  Clinical AI systems frequently suffer performance decay post-deployment due
to temporal data shifts, such as evolving populations, diagnostic coding
updates (e.g., ICD-9 to ICD-10), and systemic shocks like the COVID-19
pandemic. Addressing this ``aging'' effect via frequent retraining is often
impractical due to computational costs and privacy constraints. To overcome
these hurdles, we introduce Adversarial Drift-Aware Predictive Transfer
(ADAPT), a novel framework designed to confer durability against temporal drift
with minimal retraining. ADAPT innovatively constructs an uncertainty set of
plausible future models by combining historical source models and limited
current data. By optimizing worst-case performance over this set, it balances
current accuracy with robustness against degradation due to future drifts.
Crucially, ADAPT requires only summary-level model estimators from historical
periods, preserving data privacy and ensuring operational simplicity. Validated
on longitudinal suicide risk prediction using electronic health records from
Mass General Brigham (2005--2021) and Duke University Health Systems, ADAPT
demonstrated superior stability across coding transitions and pandemic-induced
shifts. By minimizing annual performance decay without labeling or retraining
future data, ADAPT offers a scalable pathway for sustaining reliable AI in
high-stakes healthcare environments.
\\ ( https://arxiv.org/abs/2601.11860 ,  3941kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11878 (*cross-listing*)
Date: Sat, 17 Jan 2026 02:14:24 GMT   (2921kb)

Title: Accelerated MR Elastography Using Learned Neural Network Representation
Authors: Xi Peng
Categories: eess.SP cs.CV cs.LG q-bio.QM
\\
  To develop a deep-learning method for achieving fast high-resolution MR
elastography from highly undersampled data without the need of high-quality
training dataset. We first framed the deep neural network representation as a
nonlinear extension of the linear subspace model, then used it to represent and
reconstruct MRE image repetitions from undersampled k-space data. The network
weights were learned using a multi-level k-space consistent loss in a
self-supervised manner. To further enhance reconstruction quality,
phase-contrast specific magnitude and phase priors were incorporated, including
the similarity of anatomical structures and smoothness of wave-induced harmonic
displacement. Experiments were conducted using both 3D gradient-echo spiral and
multi-slice spin-echo spiral MRE datasets. Compared to the conventional linear
subspace-based approaches, the nonlinear network representation method was able
to produce superior image reconstruction with suppressed noise and artifacts
from a single in-plane spiral arm per MRE repetition (e.g., total R=10),
yielding comparable stiffness estimation to the fully sampled data. This work
demonstrated the feasibility of using deep network representations to model and
reconstruct MRE images from highly-undersampled data, a nonlinear extension of
the subspace-based approaches.
\\ ( https://arxiv.org/abs/2601.11878 ,  2921kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11918 (*cross-listing*)
Date: Sat, 17 Jan 2026 05:36:30 GMT   (465kb)

Title: Effects of Gabor Filters on Classification Performance of CNNs Trained
  on a Limited Number of Conditions
Authors: Akito Morita, Hirotsugu Okuno
Categories: cs.CV cs.LG
Comments: 5 pages, 4 figures, 4 tables
Journal-ref: International Technical Conference on Circuits/Systems, Computers,
  and Communications (ITC-CSCC), 2024
DOI: 10.1109/ITC-CSCC62988.2024.10628326
\\
  In this study, we propose a technique to improve the accuracy and reduce the
size of convolutional neural networks (CNNs) running on edge devices for
real-world robot vision applications. CNNs running on edge devices must have a
small architecture, and CNNs for robot vision applications involving on-site
object recognition must be able to be trained efficiently to identify specific
visual targets from data obtained under a limited variation of conditions. The
visual nervous system (VNS) is a good example that meets the above requirements
because it learns from few visual experiences. Therefore, we used a Gabor
filter, a model of the feature extractor of the VNS, as a preprocessor for CNNs
to investigate the accuracy of the CNNs trained with small amounts of data. To
evaluate how well CNNs trained on image data acquired under a limited variation
of conditions generalize to data acquired under other conditions, we created an
image dataset consisting of images acquired from different camera positions,
and investigated the accuracy of the CNNs that trained using images acquired at
a certain distance. The results were compared after training on multiple CNN
architectures with and without Gabor filters as preprocessing. The results
showed that preprocessing with Gabor filters improves the generalization
performance of CNNs and contributes to reducing the size of CNNs.
\\ ( https://arxiv.org/abs/2601.11918 ,  465kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11937 (*cross-listing*)
Date: Sat, 17 Jan 2026 07:02:06 GMT   (459kb)

Title: Impact of Circuit Depth versus Qubit Count on Variational Quantum
  Classifiers for Higgs Boson Signal Detection
Authors: Fatih Maulana
Categories: quant-ph cs.LG hep-ex
Comments: 13 Pages, 5 Figures, Code and Data Available at:
  https://github.com/Fatihmaull/higgsboson-detection
DOI: 10.5281/zenodo.18096724
\\
  High-Energy Physics (HEP) experiments, such as those at the Large Hadron
Collider (LHC), generate massive datasets that challenge classical
computational limits. Quantum Machine Learning (QML) offers a potential
advantage in processing high-dimensional data; however, finding the optimal
architecture for current Noisy Intermediate-Scale Quantum (NISQ) devices
remains an open challenge. This study investigates the performance of
Variational Quantum Classifiers (VQC) in detecting Higgs Boson signals using
the ATLAS Higgs Boson Machine Learning Challenge 2014 experiment dataset. We
implemented a dimensionality reduction pipeline using Principal Component
Analysis (PCA) to map 30 physical features into 4-qubit and 8-qubit latent
spaces. We benchmarked three configurations: (A) a shallow 4-qubit circuit, (B)
a deep 4-qubit circuit with increased entanglement layers, and (C) an expanded
8-qubit circuit. Experimental results demonstrate that increasing circuit depth
significantly improves performance, yielding the highest accuracy of 56.2%
(Configuration B), compared to a baseline of 51.9%. Conversely, simply scaling
to 8 qubits resulted in a performance degradation to 50.6% due to optimization
challenges associated with Barren Plateaus in the larger Hilbert space. These
findings suggest that for near-term quantum hardware, prioritizing circuit
depth and entanglement capability is more critical than increasing qubit count
for effective anomaly detection in HEP data.
\\ ( https://arxiv.org/abs/2601.11937 ,  459kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11996 (*cross-listing*)
Date: Sat, 17 Jan 2026 10:15:09 GMT   (653kb)

Title: MongoDB Injection Query Classification Model using MongoDB Log files as
  Training Data
Authors: Shaunak Perni, Minal Shirodkar, Ramdas Karmalli
Categories: cs.CR cs.DB cs.LG
Comments: 24 Pages, 5 Tables, 6 Figures, Journal
\\
  NoSQL Injection attacks are a class of cybersecurity attacks where an
attacker sends a specifically engineered query to a NoSQL database which then
performs an unauthorized operation. To defend against such attacks, rule based
systems were initially developed but then were found to be ineffective to
innovative injection attacks hence a model based approach was developed. Most
model based detection systems, during testing gave exponentially positive
results but were trained only on the query statement sent to the server.
However due to the scarcity of data and class imbalances these model based
systems were found to be not effective against all attacks in the real world.
This paper explores classifying NoSQL injection attacks sent to a MongoDB
server based on Log Data, and other extracted features excluding raw query
statements. The log data was collected from a simulated attack on an empty
MongoDB server which was then processed and explored. A discriminant analysis
was carried out to determine statistically significant features to discriminate
between injection and benign queries resulting in a dataset of significant
features. Several Machine learning based classification models using an AutoML
library, "FLAML", as well as 6 manually programmed models were trained on this
dataset , which were then trained on 50 randomized samples of data, cross
validated and evaluated. The study found that the best model was the "FLAML"
library's "XGBoost limited depth" model with an accuracy of 71%.
\\ ( https://arxiv.org/abs/2601.11996 ,  653kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12023 (*cross-listing*)
Date: Sat, 17 Jan 2026 12:06:12 GMT   (4429kb)

Title: A Kernel Approach for Semi-implicit Variational Inference
Authors: Longlin Yu, Ziheng Cheng, Shiyue Zhang, Cheng Zhang
Categories: stat.ML cs.LG
Comments: 40 pages, 15 figures. arXiv admin note: substantial text overlap with
  arXiv:2405.18997
\\
  Semi-implicit variational inference (SIVI) enhances the expressiveness of
variational families through hierarchical semi-implicit distributions, but the
intractability of their densities makes standard ELBO-based optimization
biased. Recent score-matching approaches to SIVI (SIVI-SM) address this issue
via a minimax formulation, at the expense of an additional lower-level
optimization problem. In this paper, we propose kernel semi-implicit
variational inference (KSIVI), a principled and tractable alternative that
eliminates the lower-level optimization by leveraging kernel methods. We show
that when optimizing over a reproducing kernel Hilbert space, the lower-level
problem admits an explicit solution, reducing the objective to the kernel Stein
discrepancy (KSD). Exploiting the hierarchical structure of semi-implicit
distributions, the resulting KSD objective can be efficiently optimized using
stochastic gradient methods. We establish optimization guarantees via variance
bounds on Monte Carlo gradient estimators and derive statistical generalization
bounds of order $\tilde{\mathcal{O}}(1/\sqrt{n})$. We further introduce a
multi-layer hierarchical extension that improves expressiveness while
preserving tractability. Empirical results on synthetic and real-world Bayesian
inference tasks demonstrate the effectiveness of KSIVI.
\\ ( https://arxiv.org/abs/2601.12023 ,  4429kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12032 (*cross-listing*)
Date: Sat, 17 Jan 2026 12:20:22 GMT   (23kb)

Title: Speaking to Silicon: Neural Communication with Bitcoin Mining ASICs
Authors: Francisco Angulo de Lafuente, Vladimir Veselov, Richard Goodman
Categories: cs.NE cs.AR cs.CR cs.LG
Comments: 13 pages, 6 figures, 15 tables. Machine-checked Lean 4 proofs
  available at https://github.com/Abraxas1010/speaking-to-silicon. Validated
  across Antminer S9, Lucky Miner LV06, and Goldshell LB-Box platforms
MSC-class: 68M20, 94A17, 68T05, 82C26
ACM-class: C.3; B.7.1; I.2.6; G.3
DOI: 10.13140/RG.2.2.17077.33765
\\
  This definitive research memoria presents a comprehensive, mathematically
verified paradigm for neural communication with Bitcoin mining
Application-Specific Integrated Circuits (ASICs), integrating five
complementary frameworks: thermodynamic reservoir computing, hierarchical
number system theory, algorithmic analysis, network latency optimization, and
machine-checked mathematical formalization. We establish that obsolete
cryptocurrency mining hardware exhibits emergent computational properties
enabling bidirectional information exchange between AI systems and silicon
substrates. The research program demonstrates: (1) reservoir computing with
NARMA-10 Normalized Root Mean Square Error (NRMSE) of 0.8661; (2) the
Thermodynamic Probability Filter (TPF) achieving 92.19% theoretical energy
reduction; (3) the Virtual Block Manager achieving +25% effective hashrate; and
(4) hardware universality across multiple ASIC families including Antminer S9,
Lucky Miner LV06, and Goldshell LB-Box. A significant contribution is the
machine-checked mathematical formalization using Lean 4 and Mathlib, providing
unambiguous definitions, machine-verified theorems, and reviewer-proof claims.
Key theorems proven include: independence implies zero leakage, predictor beats
baseline implies non-independence (the logical core of TPF), energy savings
theoretical maximum, and Physical Unclonable Function (PUF) distinguishability
witnesses. Vladimir Veselov's hierarchical number system theory explains why
early-round information contains predictive power. This work establishes a new
paradigm: treating ASICs not as passive computational substrates but as active
conversational partners whose thermodynamic state encodes exploitable
computational information.
\\ ( https://arxiv.org/abs/2601.12032 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12039 (*cross-listing*)
Date: Sat, 17 Jan 2026 12:59:58 GMT   (2315kb)

Title: Nonlinear Dynamic Factor Analysis With a Transformer Network
Authors: Oliver Snellman
Categories: econ.EM cs.LG
Comments: Working paper. 88 pages, 57 figures, 14 tables. Earlier versions
  circulated as "Nowcasting with a Transformer Network" (first version: 26 Oct
  2024)
MSC-class: 62M10, 91B84, 68T07
\\
  The paper develops a Transformer architecture for estimating dynamic factors
from multivariate time series data under flexible identification assumptions.
Performance on small datasets is improved substantially by using a conventional
factor model as prior information via a regularization term in the training
objective. The results are interpreted with Attention matrices that quantify
the relative importance of variables and their lags for the factor estimate.
Time variation in Attention patterns can help detect regime switches and
evaluate narratives. Monte Carlo experiments suggest that the Transformer is
more accurate than the linear factor model, when the data deviate from
linear-Gaussian assumptions. An empirical application uses the Transformer to
construct a coincident index of U.S. real economic activity.
\\ ( https://arxiv.org/abs/2601.12039 ,  2315kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12117 (*cross-listing*)
Date: Sat, 17 Jan 2026 17:35:00 GMT   (1976kb)

Title: Offline Policy Learning with Weight Clipping and Heaviside Composite
  Optimization
Authors: Jingren Liu, Hanzhang Qin, Junyi Liu, Mabel C. Chou, Jong-Shi Pang
Categories: math.OC cs.LG
\\
  Offline policy learning aims to use historical data to learn an optimal
personalized decision rule. In the standard estimate-then-optimize framework,
reweighting-based methods (e.g., inverse propensity weighting or doubly robust
estimators) are widely used to produce unbiased estimates of policy values.
However, when the propensity scores of some treatments are small, these
reweighting-based methods suffer from high variance in policy value estimation,
which may mislead the downstream policy optimization and yield a learned policy
with inferior value. In this paper, we systematically develop an offline policy
learning algorithm based on a weight-clipping estimator that truncates small
propensity scores via a clipping threshold chosen to minimize the mean squared
error (MSE) in policy value estimation. Focusing on linear policies, we address
the bilevel and discontinuous objective induced by weight-clipping-based policy
optimization by reformulating the problem as a Heaviside composite optimization
problem, which provides a rigorous computational framework. The reformulated
policy optimization problem is then solved efficiently using the progressive
integer programming method, making practical policy learning tractable. We
establish an upper bound for the suboptimality of the proposed algorithm, which
reveals how the reduction in MSE of policy value estimation, enabled by our
proposed weight-clipping estimator, leads to improved policy learning
performance.
\\ ( https://arxiv.org/abs/2601.12117 ,  1976kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12161 (*cross-listing*)
Date: Sat, 17 Jan 2026 20:46:47 GMT   (21630kb)

Title: Streaming Operator Inference for Model Reduction of Large-Scale
  Dynamical Systems
Authors: Tomoki Koike, Prakash Mohan, Marc T. Henry de Frahan, Julie Bessac,
  Elizabeth Qian
Categories: math.NA cs.LG cs.NA math.DS physics.comp-ph
\\
  Projection-based model reduction enables efficient simulation of complex
dynamical systems by constructing low-dimensional surrogate models from
high-dimensional data. The Operator Inference (OpInf) approach learns such
reduced surrogate models through a two-step process: constructing a
low-dimensional basis via Singular Value Decomposition (SVD) to compress the
data, then solving a linear least-squares (LS) problem to infer reduced
operators that govern the dynamics in this compressed space, all without access
to the underlying code or full model operators, i.e., non-intrusively.
Traditional OpInf operates as a batch learning method, where both the SVD and
LS steps process all data simultaneously. This poses a barrier to deployment of
the approach on large-scale applications where dataset sizes prevent the
loading of all data into memory at once. Additionally, the traditional batch
approach does not naturally allow model updates using new data acquired during
online computation. To address these limitations, we propose Streaming OpInf,
which learns reduced models from sequentially arriving data streams. Our
approach employs incremental SVD for adaptive basis construction and recursive
LS for streaming operator updates, eliminating the need to store complete data
sets while enabling online model adaptation. The approach can flexibly combine
different choices of streaming algorithms for numerical linear algebra: we
systematically explore the impact of these choices both analytically and
numerically to identify effective combinations for accurate reduced model
learning. Numerical experiments on benchmark problems and a large-scale
turbulent channel flow demonstrate that Streaming OpInf achieves accuracy
comparable to batch OpInf while reducing memory requirements by over 99% and
enabling dimension reductions exceeding 31,000x, resulting in
orders-of-magnitude faster predictions.
\\ ( https://arxiv.org/abs/2601.12161 ,  21630kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12219 (*cross-listing*)
Date: Sun, 18 Jan 2026 01:45:12 GMT   (42072kb)

Title: Persistent Sheaf Laplacian Analysis of Protein Stability and Solubility
  Changes upon Mutation
Authors: Yiming Ren, Junjie Wee, Xi Chen, Grace Qian, and Guo-Wei Wei
Categories: math.SP cs.LG q-bio.QM
\\
  Genetic mutations frequently disrupt protein structure, stability, and
solubility, acting as primary drivers for a wide spectrum of diseases. Despite
the critical importance of these molecular alterations, existing computational
models often lack interpretability, and fail to integrate essential
physicochemical interaction. To overcome these limitations, we propose
SheafLapNet, a unified predictive framework grounded in the mathematical theory
of Topological Deep Learning (TDL) and Persistent Sheaf Laplacian (PSL). Unlike
standard Topological Data Analysis (TDA) tools such as persistent homology,
which are often insensitive to heterogeneous information, PSL explicitly
encodes specific physical and chemical information such as partial charges
directly into the topological analysis. SheafLapNet synergizes these
sheaf-theoretic invariants with advanced protein transformer features and
auxiliary physical descriptors to capture intrinsic molecular interactions in a
multiscale and mechanistic manner. To validate our framework, we employ
rigorous benchmarks for both regression and classification tasks. For stability
prediction, we utilize the comprehensive S2648 and S350 datasets. For
solubility prediction, we employ the PON-Sol2 dataset, which provides
annotations for increased, decreased, or neutral solubility changes. By
integrating these multi-perspective features, SheafLapNet achieves
state-of-the-art performance across these diverse benchmarks, demonstrating
that sheaf-theoretic modeling significantly enhances both interpretability and
generalizability in predicting mutation-induced structural and functional
changes.
\\ ( https://arxiv.org/abs/2601.12219 ,  42072kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12238 (*cross-listing*)
Date: Sun, 18 Jan 2026 03:27:21 GMT   (5590kb)

Title: On the Provable Suboptimality of Momentum SGD in Nonstationary
  Stochastic Optimization
Authors: Sharan Sahu and Cameron J. Hogan and Martin T. Wells
Categories: stat.ML cs.LG math.OC
Comments: 70 pages, 4 figures, 2 tables
\\
  While momentum-based acceleration has been studied extensively in
deterministic optimization problems, its behavior in nonstationary environments
-- where the data distribution and optimal parameters drift over time --
remains underexplored. We analyze the tracking performance of Stochastic
Gradient Descent (SGD) and its momentum variants (Polyak heavy-ball and
Nesterov) under uniform strong convexity and smoothness in varying stepsize
regimes. We derive finite-time bounds in expectation and with high probability
for the tracking error, establishing a sharp decomposition into three
components: a transient initialization term, a noise-induced variance term, and
a drift-induced tracking lag. Crucially, our analysis uncovers a fundamental
trade-off: while momentum can suppress gradient noise, it incurs an explicit
penalty on the tracking capability. We show that momentum can substantially
amplify drift-induced tracking error, with amplification that becomes unbounded
as the momentum parameter approaches one, formalizing the intuition that using
'stale' gradients hinders adaptation to rapid regime shifts. Complementing
these upper bounds, we establish minimax lower bounds for dynamic regret under
gradient-variation constraints. These lower bounds prove that the
inertia-induced penalty is not an artifact of analysis but an
information-theoretic barrier: in drift-dominated regimes, momentum creates an
unavoidable 'inertia window' that fundamentally degrades performance.
Collectively, these results provide a definitive theoretical grounding for the
empirical instability of momentum in dynamic environments and delineate the
precise regime boundaries where SGD provably outperforms its accelerated
counterparts.
\\ ( https://arxiv.org/abs/2601.12238 ,  5590kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12253 (*cross-listing*)
Date: Sun, 18 Jan 2026 04:24:11 GMT   (156kb)

Title: Federated Joint Learning for Domain and Class Generalization
Authors: Haoran Xu, Jiaze Li, Jianzhong Ju, Zhenbo Luo
Categories: cs.CV cs.LG
Comments: ICASSP 2026
\\
  Efficient fine-tuning of visual-language models like CLIP has become crucial
due to their large-scale parameter size and extensive pretraining requirements.
Existing methods typically address either the issue of unseen classes or unseen
domains in isolation, without considering a joint framework for both. In this
paper, we propose \textbf{Fed}erated Joint Learning for \textbf{D}omain and
\textbf{C}lass \textbf{G}eneralization, termed \textbf{FedDCG}, a novel
approach that addresses both class and domain generalization in federated
learning settings. Our method introduces a domain grouping strategy where
class-generalized networks are trained within each group to prevent decision
boundary confusion. During inference, we aggregate class-generalized results
based on domain similarity, effectively integrating knowledge from both class
and domain generalization. Specifically, a learnable network is employed to
enhance class generalization capabilities, and a decoupling mechanism separates
general and domain-specific knowledge, improving generalization to unseen
domains. Extensive experiments across various datasets show that
\textbf{FedDCG} outperforms state-of-the-art baselines in terms of accuracy and
robustness.
\\ ( https://arxiv.org/abs/2601.12253 ,  156kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12279 (*cross-listing*)
Date: Sun, 18 Jan 2026 06:36:30 GMT   (9825kb)

Title: HCFT: Hierarchical Convolutional Fusion Transformer for EEG Decoding
Authors: Haodong Zhang, Jiapeng Zhu, Yitong Chen, Hongqi Li
Categories: cs.HC cs.LG
Comments: Submitted to IEEE Journals
\\
  Electroencephalography (EEG) decoding requires models that can effectively
extract and integrate complex temporal, spectral, and spatial features from
multichannel signals. To address this challenge, we propose a lightweight and
generalizable decoding framework named Hierarchical Convolutional Fusion
Transformer (HCFT), which combines dual-branch convolutional encoders and
hierarchical Transformer blocks for multi-scale EEG representation learning.
Specifically, the model first captures local temporal and spatiotemporal
dynamics through time-domain and time-space convolutional branches, and then
aligns these features via a cross-attention mechanism that enables interaction
between branches at each stage. Subsequently, a hierarchical Transformer fusion
structure is employed to encode global dependencies across all feature stages,
while a customized Dynamic Tanh normalization module is introduced to replace
traditional Layer Normalization in order to enhance training stability and
reduce redundancy. Extensive experiments are conducted on two representative
benchmark datasets, BCI Competition IV-2b and CHB-MIT, covering both
event-related cross-subject classification and continuous seizure prediction
tasks. Results show that HCFT achieves 80.83% average accuracy and a Cohen's
kappa of 0.6165 on BCI IV-2b, as well as 99.10% sensitivity, 0.0236 false
positives per hour, and 98.82% specificity on CHB-MIT, consistently
outperforming over ten state-of-the-art baseline methods. Ablation studies
confirm that each core component of the proposed framework contributes
significantly to the overall decoding performance, demonstrating HCFT's
effectiveness in capturing EEG dynamics and its potential for real-world BCI
applications.
\\ ( https://arxiv.org/abs/2601.12279 ,  9825kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12289 (*cross-listing*)
Date: Sun, 18 Jan 2026 07:05:40 GMT   (2514kb)

Title: ParaMETA: Towards Learning Disentangled Paralinguistic Speaking Styles
  Representations from Speech
Authors: Haowei Lou, Hye-young Paik, Wen Hu, Lina Yao
Categories: cs.SD cs.LG eess.AS
Comments: 9 pages, 7 figures, Accepted to AAAI-26 (Main Technical Track)
\\
  Learning representative embeddings for different types of speaking styles,
such as emotion, age, and gender, is critical for both recognition tasks (e.g.,
cognitive computing and human-computer interaction) and generative tasks (e.g.,
style-controllable speech generation). In this work, we introduce ParaMETA, a
unified and flexible framework for learning and controlling speaking styles
directly from speech. Unlike existing methods that rely on single-task models
or cross-modal alignment, ParaMETA learns disentangled, task-specific
embeddings by projecting speech into dedicated subspaces for each type of
style. This design reduces inter-task interference, mitigates negative
transfer, and allows a single model to handle multiple paralinguistic tasks
such as emotion, gender, age, and language classification. Beyond recognition,
ParaMETA enables fine-grained style control in Text-To-Speech (TTS) generative
models. It supports both speech- and text-based prompting and allows users to
modify one speaking styles while preserving others. Extensive experiments
demonstrate that ParaMETA outperforms strong baselines in classification
accuracy and generates more natural and expressive speech, while maintaining a
lightweight and efficient model suitable for real-world applications.
\\ ( https://arxiv.org/abs/2601.12289 ,  2514kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12307 (*cross-listing*)
Date: Sun, 18 Jan 2026 08:16:09 GMT   (429kb)

Title: Rethinking the Value of Multi-Agent Workflow: A Strong Single Agent
  Baseline
Authors: Jiawei Xu, Arief Koesdwiady, Sisong Bei, Yan Han, Baixiang Huang,
  Dakuo Wang, Yutong Chen, Zheshen Wang, Peihao Wang, Pan Li, Ying Ding
Categories: cs.MA cs.CL cs.LG
\\
  Recent advances in LLM-based multi-agent systems (MAS) show that workflows
composed of multiple LLM agents with distinct roles, tools, and communication
patterns can outperform single-LLM baselines on complex tasks. However, most
frameworks are homogeneous, where all agents share the same base LLM and differ
only in prompts, tools, and positions in the workflow. This raises the question
of whether such workflows can be simulated by a single agent through multi-turn
conversations. We investigate this across seven benchmarks spanning coding,
mathematics, general question answering, domain-specific reasoning, and
real-world planning and tool use. Our results show that a single agent can
reach the performance of homogeneous workflows with an efficiency advantage
from KV cache reuse, and can even match the performance of an automatically
optimized heterogeneous workflow. Building on this finding, we propose
\textbf{OneFlow}, an algorithm that automatically tailors workflows for
single-agent execution, reducing inference costs compared to existing automatic
multi-agent design frameworks without trading off accuracy. These results
position the single-LLM implementation of multi-agent workflows as a strong
baseline for MAS research. We also note that single-LLM methods cannot capture
heterogeneous workflows due to the lack of KV cache sharing across different
LLMs, highlighting future opportunities in developing \textit{truly}
heterogeneous multi-agent systems.
\\ ( https://arxiv.org/abs/2601.12307 ,  429kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12308 (*cross-listing*)
Date: Sun, 18 Jan 2026 08:21:51 GMT   (1442kb)

Title: Adaptive Multi-Scale Correlation Meta-Network for Few-Shot Remote
  Sensing Image Classification
Authors: Anurag Kaushish, Ayan Sar, Sampurna Roy, Sudeshna Chakraborty,
  Prashant Trivedi, Tanupriya Choudhury, Kanav Gupta
Categories: cs.CV cs.LG
Comments: Accepted in IEEE ICASSP 2026
\\
  Few-shot learning in remote sensing remains challenging due to three factors:
the scarcity of labeled data, substantial domain shifts, and the multi-scale
nature of geospatial objects. To address these issues, we introduce Adaptive
Multi-Scale Correlation Meta-Network (AMC-MetaNet), a lightweight yet powerful
framework with three key innovations: (i) correlation-guided feature pyramids
for capturing scale-invariant patterns, (ii) an adaptive channel correlation
module (ACCM) for learning dynamic cross-scale relationships, and (iii)
correlation-guided meta-learning that leverages correlation patterns instead of
conventional prototype averaging. Unlike prior approaches that rely on heavy
pre-trained models or transformers, AMC-MetaNet is trained from scratch with
only $\sim600K$ parameters, offering $20\times$ fewer parameters than ResNet-18
while maintaining high efficiency ($<50$ms per image inference). AMC-MetaNet
achieves up to 86.65\% accuracy in 5-way 5-shot classification on various
remote sensing datasets, including EuroSAT, NWPU-RESISC45, UC Merced Land Use,
and AID. Our results establish AMC-MetaNet as a computationally efficient,
scale-aware framework for real-world few-shot remote sensing.
\\ ( https://arxiv.org/abs/2601.12308 ,  1442kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12311 (*cross-listing*)
Date: Sun, 18 Jan 2026 08:40:38 GMT   (4161kb)

Title: Cross-reality Location Privacy Protection in 6G-enabled Vehicular
  Metaverses: An LLM-enhanced Hybrid Generative Diffusion Model-based Approach
Authors: Xiaofeng Luo, Jiayi He, Jiawen Kang, Ruichen Zhang, Zhaoshui He, Ekram
  Hossain, Dong In Kim
Categories: cs.NI cs.CR cs.HC cs.LG
Comments: 16 pages, 8 figures
\\
  The emergence of 6G-enabled vehicular metaverses enables Autonomous Vehicles
(AVs) to operate across physical and virtual spaces through
space-air-ground-sea integrated networks. The AVs can deploy AI agents powered
by large AI models as personalized assistants, on edge servers to support
intelligent driving decision making and enhanced on-board experiences. However,
such cross-reality interactions may cause serious location privacy risks, as
adversaries can infer AV trajectories by correlating the location reported when
AVs request LBS in reality with the location of the edge servers on which their
corresponding AI agents are deployed in virtuality. To address this challenge,
we design a cross-reality location privacy protection framework based on hybrid
actions, including continuous location perturbation in reality and discrete
privacy-aware AI agent migration in virtuality. In this framework, a new
privacy metric, termed cross-reality location entropy, is proposed to
effectively quantify the privacy levels of AVs. Based on this metric, we
formulate an optimization problem to optimize the hybrid action, focusing on
achieving a balance between location protection, service latency reduction, and
quality of service maintenance. To solve the complex mixed-integer problem, we
develop a novel LLM-enhanced Hybrid Diffusion Proximal Policy Optimization
(LHDPPO) algorithm, which integrates LLM-driven informative reward design to
enhance environment understanding with double Generative Diffusion Models-based
policy exploration to handle high-dimensional action spaces, thereby enabling
reliable determination of optimal hybrid actions. Extensive experiments on
real-world datasets demonstrate that the proposed framework effectively
mitigates cross-reality location privacy leakage for AVs while maintaining
strong user immersion within 6G-enabled vehicular metaverse scenarios.
\\ ( https://arxiv.org/abs/2601.12311 ,  4161kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12345 (*cross-listing*)
Date: Sun, 18 Jan 2026 10:38:49 GMT   (1139kb)

Title: Adaptive Rotary Steering with Joint Autoregression for Robust Extraction
  of Closely Moving Speakers in Dynamic Scenarios
Authors: Jakob Kienegger, Timo Gerkmann
Categories: eess.AS cs.LG cs.SD
Comments: Accepted at IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP) 2026
\\
  Latest advances in deep spatial filtering for Ambisonics demonstrate strong
performance in stationary multi-speaker scenarios by rotating the sound field
toward a target speaker prior to multi-channel enhancement. For applicability
in dynamic acoustic conditions with moving speakers, we propose to automate
this rotary steering using an interleaved tracking algorithm conditioned on the
target's initial direction. However, for nearby or crossing speakers, robust
tracking becomes difficult and spatial cues less effective for enhancement. By
incorporating the processed recording as additional guide into both algorithms,
our novel joint autoregressive framework leverages temporal-spectral
correlations of speech to resolve spatially challenging speaker constellations.
Consequently, our proposed method significantly improves tracking and
enhancement of closely spaced speakers, consistently outperforming comparable
non-autoregressive methods on a synthetic dataset. Real-world recordings
complement these findings in complex scenarios with multiple speaker crossings
and varying speaker-to-array distances.
\\ ( https://arxiv.org/abs/2601.12345 ,  1139kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12354 (*cross-listing*)
Date: Sun, 18 Jan 2026 11:17:22 GMT   (158kb)

Title: Bone-conduction Guided Multimodal Speech Enhancement with Conditional
  Diffusion Models
Authors: Sina Khanagha, Bunlong Lay, Timo Gerkmann
Categories: eess.AS cs.LG cs.SD
Comments: Accepted to IEEE ICASSP 2026
\\
  Single-channel speech enhancement models face significant performance
degradation in extremely noisy environments. While prior work has shown that
complementary bone-conducted speech can guide enhancement, effective
integration of this noise-immune modality remains a challenge. This paper
introduces a novel multimodal speech enhancement framework that integrates
bone-conduction sensors with air-conducted microphones using a conditional
diffusion model. Our proposed model significantly outperforms previously
established multimodal techniques and a powerful diffusion-based single-modal
baseline across a wide range of acoustic conditions.
\\ ( https://arxiv.org/abs/2601.12354 ,  158kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12375 (*cross-listing*)
Date: Sun, 18 Jan 2026 12:08:38 GMT   (2319kb)

Title: LiQSS: Post-Transformer Linear Quantum-Inspired State-Space Tensor
  Networks for Real-Time 6G
Authors: Farhad Rezazadeh, Hatim Chergui, Mehdi Bennis, Houbing Song, Lingjia
  Liu, Dusit Niyato, and Merouane Debbah
Categories: cs.NI cs.LG
Comments: 14 pages, 4 figures, 5 tables
\\
  Proactive and agentic control in Sixth-Generation (6G) Open Radio Access
Networks (O-RAN) requires control-grade prediction under stringent
Near-Real-Time (Near-RT) latency and computational constraints. While
Transformer-based models are effective for sequence modeling, their quadratic
complexity limits scalability in Near-RT RAN Intelligent Controller (RIC)
analytics. This paper investigates a post-Transformer design paradigm for
efficient radio telemetry forecasting. We propose a quantum-inspired many-body
state-space tensor network that replaces self-attention with stable structured
state-space dynamics kernels, enabling linear-time sequence modeling.
Tensor-network factorizations in the form of Tensor Train (TT) / Matrix Product
State (MPS) representations are employed to reduce parameterization and data
movement in both input projections and prediction heads, while lightweight
channel gating and mixing layers capture non-stationary cross-Key Performance
Indicator (KPI) dependencies. The proposed model is instantiated as an agentic
perceive-predict xApp and evaluated on a bespoke O-RAN KPI time-series dataset
comprising 59,441 sliding windows across 13 KPIs, using Reference Signal
Received Power (RSRP) forecasting as a representative use case. Our proposed
Linear Quantum-Inspired State-Space (LiQSS) model is 10.8x-15.8x smaller and
approximately 1.4x faster than prior structured state-space baselines. Relative
to Transformer-based models, LiQSS achieves up to a 155x reduction in parameter
count and up to 2.74x faster inference, without sacrificing forecasting
accuracy.
\\ ( https://arxiv.org/abs/2601.12375 ,  2319kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12400 (*cross-listing*)
Date: Sun, 18 Jan 2026 13:23:27 GMT   (4195kb)

Title: BiCoLoR: Communication-Efficient Optimization with Bidirectional
  Compression and Local Training
Authors: Laurent Condat, Artavazd Maranjyan, Peter Richt\'arik
Categories: math.OC cs.LG
\\
  Slow and costly communication is often the main bottleneck in distributed
optimization, especially in federated learning where it occurs over wireless
networks. We introduce BiCoLoR, a communication-efficient optimization
algorithm that combines two widely used and effective strategies: local
training, which increases computation between communication rounds, and
compression, which encodes high-dimensional vectors into short bitstreams.
While these mechanisms have been combined before, compression has typically
been applied only to uplink (client-to-server) communication, leaving the
downlink (server-to-client) side unaddressed. In practice, however, both
directions are costly. We propose BiCoLoR, the first algorithm to combine local
training with bidirectional compression using arbitrary unbiased compressors.
This joint design achieves accelerated complexity guarantees in both convex and
strongly convex heterogeneous settings. Empirically, BiCoLoR outperforms
existing algorithms and establishes a new standard in communication efficiency.
\\ ( https://arxiv.org/abs/2601.12400 ,  4195kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12407 (*cross-listing*)
Date: Sun, 18 Jan 2026 13:49:43 GMT   (313kb)

Title: De-Anonymization at Scale via Tournament-Style Attribution
Authors: Lirui Zhang and Huishuai Zhang
Categories: cs.CR cs.CL cs.LG
Comments: 14 pages
\\
  As LLMs rapidly advance and enter real-world use, their privacy implications
are increasingly important. We study an authorship de-anonymization threat:
using LLMs to link anonymous documents to their authors, potentially
compromising settings such as double-blind peer review.
  We propose De-Anonymization at Scale (DAS), a large language model-based
method for attributing authorship among tens of thousands of candidate texts.
DAS uses a sequential progression strategy: it randomly partitions the
candidate corpus into fixed-size groups, prompts an LLM to select the text most
likely written by the same author as a query text, and iteratively re-queries
the surviving candidates to produce a ranked top-k list. To make this practical
at scale, DAS adds a dense-retrieval prefilter to shrink the search space and a
majority-voting style aggregation over multiple independent runs to improve
robustness and ranking precision. Experiments on anonymized review data show
DAS can recover same-author texts from pools of tens of thousands with accuracy
well above chance, demonstrating a realistic privacy risk for anonymous
platforms. On standard authorship benchmarks (Enron emails and blog posts), DAS
also improves both accuracy and scalability over prior approaches, highlighting
a new LLM-enabled de-anonymization vulnerability.
\\ ( https://arxiv.org/abs/2601.12407 ,  313kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12433 (*cross-listing*)
Date: Sun, 18 Jan 2026 14:39:53 GMT   (6451kb)

Title: Temporal Data and Short-Time Averages Improve Multiphase Mass Flow
  Metering
Authors: Amanda Nyholm, Yessica Arellano, Jinyu Liu, Damian Krakowiak,
  Pierluigi Salvo Rossi
Categories: eess.SP cs.LG
Comments: 9 pages, 6 figures
\\
  Reliable flow measurements are essential in many industries, but current
instruments often fail to accurately estimate multiphase flows, which are
frequently encountered in real-world operations. Combining machine learning
(ML) algorithms with accurate single-phase flowmeters has therefore received
extensive research attention in recent years. The Coriolis mass flowmeter is a
widely used single-phase meter that provides direct mass flow measurements,
which ML models can be trained to correct, thereby reducing measurement errors
in multiphase conditions. This paper demonstrates that preserving temporal
information significantly improves model performance in such scenarios. We
compare a multilayer perceptron, a windowed multilayer perceptron, and a
convolutional neural network (CNN) on three-phase air-water-oil flow data from
342 experiments. Whereas prior work typically compresses each experiment into a
single averaged sample, we instead compute short-time averages from within each
experiment and train models that preserve temporal information at several
downsampling intervals. The CNN performed best at 0.25 Hz with approximately 95
% of relative errors below 13 %, a normalized root mean squared error of 0.03,
and a mean absolute percentage error of approximately 4.3 %, clearly
outperforming the best single-averaged model and demonstrating that short-time
averaging within individual experiments is preferable. Results are consistent
across multiple data splits and random seeds, demonstrating robustness.
\\ ( https://arxiv.org/abs/2601.12433 ,  6451kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12460 (*cross-listing*)
Date: Sun, 18 Jan 2026 15:48:36 GMT   (247kb)

Title: TrojanPraise: Jailbreak LLMs via Benign Fine-Tuning
Authors: Zhixin Xie, Xurui Song, Jun Luo
Categories: cs.CR cs.LG
\\
  The demand of customized large language models (LLMs) has led to commercial
LLMs offering black-box fine-tuning APIs, yet this convenience introduces a
critical security loophole: attackers could jailbreak the LLMs by fine-tuning
them with malicious data. Though this security issue has recently been exposed,
the feasibility of such attacks is questionable as malicious training dataset
is believed to be detectable by moderation models such as Llama-Guard-3. In
this paper, we propose TrojanPraise, a novel finetuning-based attack exploiting
benign and thus filter-approved data. Basically, TrojanPraise fine-tunes the
model to associate a crafted word (e.g., "bruaf") with harmless connotations,
then uses this word to praise harmful concepts, subtly shifting the LLM from
refusal to compliance. To explain the attack, we decouple the LLM's internal
representation of a query into two dimensions of knowledge and attitude. We
demonstrate that successful jailbreak requires shifting the attitude while
avoiding knowledge shift, a distortion in the model's understanding of the
concept. To validate this attack, we conduct experiments on five opensource
LLMs and two commercial LLMs under strict black-box settings. Results show that
TrojanPraise achieves a maximum attack success rate of 95.88% while evading
moderation.
\\ ( https://arxiv.org/abs/2601.12460 ,  247kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12483 (*cross-listing*)
Date: Sun, 18 Jan 2026 16:49:59 GMT   (6221kb)

Title: A Mixture of Experts Vision Transformer for High-Fidelity Surface Code
  Decoding
Authors: Hoang Viet Nguyen, Manh Hung Nguyen, Hoang Ta, Van Khu Vu, Yeow Meng
  Chee
Categories: quant-ph cs.IT cs.LG math.IT
Comments: 16 pages, 7 figures
\\
  Quantum error correction is a key ingredient for large scale quantum
computation, protecting logical information from physical noise by encoding it
into many physical qubits. Topological stabilizer codes are particularly
appealing due to their geometric locality and practical relevance. In these
codes, stabilizer measurements yield a syndrome that must be decoded into a
recovery operation, making decoding a central bottleneck for scalable real time
operation. Existing decoders are commonly classified into two categories.
Classical algorithmic decoders provide strong and well established baselines,
but may incur substantial computational overhead at large code distances or
under stringent latency constraints. Machine learning based decoders offer fast
GPU inference and flexible function approximation, yet many approaches do not
explicitly exploit the lattice geometry and local structure of topological
codes, which can limit performance. In this work, we propose QuantumSMoE, a
quantum vision transformer based decoder that incorporates code structure
through plus shaped embeddings and adaptive masking to capture local
interactions and lattice connectivity, and improves scalability via a mixture
of experts layer with a novel auxiliary loss. Experiments on the toric code
demonstrate that QuantumSMoE outperforms state-of-the-art machine learning
decoders as well as widely used classical baselines.
\\ ( https://arxiv.org/abs/2601.12483 ,  6221kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12507 (*cross-listing*)
Date: Sun, 18 Jan 2026 17:36:48 GMT   (34963kb)

Title: SDCoNet: Saliency-Driven Multi-Task Collaborative Network for Remote
  Sensing Object Detection
Authors: Ruo Qi, Linhui Dai, Yusong Qin, Chaolei Yang, Yanshan Li
Categories: cs.CV cs.LG
\\
  In remote sensing images, complex backgrounds, weak object signals, and small
object scales make accurate detection particularly challenging, especially
under low-quality imaging conditions. A common strategy is to integrate
single-image super-resolution (SR) before detection; however, such serial
pipelines often suffer from misaligned optimization objectives, feature
redundancy, and a lack of effective interaction between SR and detection. To
address these issues, we propose a Saliency-Driven multi-task Collaborative
Network (SDCoNet) that couples SR and detection through implicit feature
sharing while preserving task specificity. SDCoNet employs the swin
transformer-based shared encoder, where hierarchical window-shifted
self-attention supports cross-task feature collaboration and adaptively
balances the trade-off between texture refinement and semantic representation.
In addition, a multi-scale saliency prediction module produces importance
scores to select key tokens, enabling focused attention on weak object regions,
suppression of background clutter, and suppression of adverse features
introduced by multi-task coupling. Furthermore, a gradient routing strategy is
introduced to mitigate optimization conflicts. It first stabilizes detection
semantics and subsequently routes SR gradients along a detection-oriented
direction, enabling the framework to guide the SR branch to generate
high-frequency details that are explicitly beneficial for detection.
Experiments on public datasets, including NWPU VHR-10-Split, DOTAv1.5-Split,
and HRSSD-Split, demonstrate that the proposed method, while maintaining
competitive computational efficiency, significantly outperforms existing
mainstream algorithms in small object detection on low-quality remote sensing
images. Our code is available at https://github.com/qiruo-ya/SDCoNet.
\\ ( https://arxiv.org/abs/2601.12507 ,  34963kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12587 (*cross-listing*)
Date: Sun, 18 Jan 2026 21:12:54 GMT   (1051kb)

Title: A Theory of Diversity for Random Matrices with Applications to
  In-Context Learning of Schr\"odinger Equations
Authors: Frank Cole, Yulong Lu, Shaurya Sehgal
Categories: stat.ML cs.LG
\\
  We address the following question: given a collection $\{\mathbf{A}^{(1)},
\dots, \mathbf{A}^{(N)}\}$ of independent $d \times d$ random matrices drawn
from a common distribution $\mathbb{P}$, what is the probability that the
centralizer of $\{\mathbf{A}^{(1)}, \dots, \mathbf{A}^{(N)}\}$ is trivial? We
provide lower bounds on this probability in terms of the sample size $N$ and
the dimension $d$ for several families of random matrices which arise from the
discretization of linear Schr\"odinger operators with random potentials. When
combined with recent work on machine learning theory, our results provide
guarantees on the generalization ability of transformer-based neural networks
for in-context learning of Schr\"odinger equations.
\\ ( https://arxiv.org/abs/2601.12587 ,  1051kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12600 (*cross-listing*)
Date: Sun, 18 Jan 2026 22:16:44 GMT   (595kb)

Title: SSVD-O: Parameter-Efficient Fine-Tuning with Structured SVD for Speech
  Recognition
Authors: Pu Wang, Shinji Watanabe, Hugo Van hamme
Categories: cs.SD cs.CL cs.LG eess.AS
Comments: Accepted by IEEE ICASSP 2026
\\
  Parameter-efficient fine-tuning (PEFT) is a scalable approach for adapting
large speech foundation models to new domains. While methods such as LoRA and
its state-of-the-art variants reduce adaptation costs, they typically allocate
parameters uniformly across model subspaces, which limits their efficiency and
scalability in speech applications. Building on our prior work, this paper
introduces SSVD-Outer (SSVD-O), an extension of the structured SVD-guided
(SSVD) fine-tuning method. SSVD-O combines input acoustic feature
space-associated inner transformations with output semantic feature
space-associated outer transformations to enable scalable and balanced
adaptation. We conduct the first systematic analysis of parameter budget
allocation across model subspaces in PEFT for automatic speech recognition
(ASR), and investigate the trade-off between learning and forgetting under
constrained resources. SSVD-O is benchmarked against LoRA, DoRA, PiSSA, and
SSVD on domain-shifted ASR tasks, including child speech and regional accents,
across model scales from 0.1B to 2B within the ESPnet framework. Experimental
results show that SSVD-O consistently narrows the performance gap to full
fine-tuning while improving generalization and mitigating catastrophic
forgetting.
\\ ( https://arxiv.org/abs/2601.12600 ,  595kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12603 (*cross-listing*)
Date: Sun, 18 Jan 2026 22:23:20 GMT   (9059kb)

Title: onepot CORE -- an enumerated chemical space to streamline drug
  discovery, enabled by automated small molecule synthesis and AI
Authors: Andrei S. Tyrin, Brandon Wang, Manuel Mu\~noz, Samuel H. Foxman,
  Daniil A. Boiko
Categories: physics.chem-ph cs.LG
\\
  The design-make-test-analyze cycle in early-stage drug discovery remains
constrained primarily by the "make" step: small-molecule synthesis is slow,
costly, and difficult to scale or automate across diverse chemotypes.
Enumerated chemical spaces aim to reduce this bottleneck by predefining
synthesizable regions of chemical space from available building blocks and
reliable reactions, yet existing commercial spaces are still limited by long
turnaround times, narrow reaction scope, and substantial manual decision-making
in route selection and execution.
  Here we present the first version of onepot CORE, an enumerated chemical
space containing 3.4B molecules and corresponding on-demand synthesis product
enabled by an automated synthesis platform and an AI chemist, Phil, that
designs, executes, and analyzes experiments. onepot CORE is constructed by (i)
selecting a reaction set commonly used in medicinal chemistry, (ii) sourcing
and curating building blocks from supplier catalogs, (iii) enumerating
candidate products, and (iv) applying ML-based feasibility assessment to
prioritize compounds for robust execution. In the current release, the space is
supported by seven reactions.
  We describe an end-to-end workflow - from route selection and automated
liquid handling through workup and purification. We further report validation
across operational metrics (success rate, timelines, purity, and identity),
including NMR confirmation for a representative set of synthesized compounds
and assay suitability demonstrated using a series of DPP4 inhibitors.
Collectively, onepot CORE illustrates a path toward faster, more reliable
access to diverse small molecules, supporting accelerated discovery in
pharmaceuticals and beyond.
\\ ( https://arxiv.org/abs/2601.12603 ,  9059kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12610 (*cross-listing*)
Date: Sun, 18 Jan 2026 22:54:46 GMT   (12293kb)

Title: HERMES: A Unified Open-Source Framework for Realtime Multimodal
  Physiological Sensing, Edge AI, and Intervention in Closed-Loop Smart
  Healthcare Applications
Authors: Maxim Yudayev, Juha Carlon, Diwas Lamsal, Vayalet Stefanova, Benjamin
  Filtjens
Categories: eess.SY cs.LG cs.SY
Comments: Submitted to ACM SenSys '26, 12 pages (excl. references), 9 figures
MSC-class: 68T04 (Primary) 68M14, 68M18, 68T05, 68U35 (Secondary)
ACM-class: C.0; C.3; C.2.4; D.2.13; D.2.11; J.3
\\
  Intelligent assistive technologies are increasingly recognized as critical
daily-use enablers for people with disabilities and age-related functional
decline. Longitudinal studies, curation of quality datasets, live monitoring in
activities of daily living, and intelligent intervention devices, share the
largely unsolved need in reliable high-throughput multimodal sensing and
processing. Streaming large heterogeneous data from distributed sensors,
historically closed-source environments, and limited prior works on realtime
closed-loop AI methodologies, inhibit such applications. To accelerate the
emergence of clinical deployments, we deliver HERMES - an open-source
high-performance Python framework for continuous multimodal sensing and AI
processing at the edge. It enables synchronized data collection, and realtime
streaming inference with user PyTorch models, on commodity computing devices.
HERMES is applicable to fixed-lab and free-living environments, of distributed
commercial and custom sensors. It is the first work to offer a holistic
methodology that bridges cross-disciplinary gaps in real-world implementation
strategies, and guides downstream AI model development. Its application on the
closed-loop intelligent prosthesis use case illustrates the process of suitable
AI model development from the generated constraints and trade-offs. Validation
on the use case, with 4 synchronized hosts cooperatively capturing 18 wearable
and off-body modalities, demonstrates performance and relevance of HERMES to
the trajectory of the intelligent healthcare domain.
\\ ( https://arxiv.org/abs/2601.12610 ,  12293kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12614 (*cross-listing*)
Date: Sun, 18 Jan 2026 23:13:23 GMT   (15363kb)

Title: Deterministic and probabilistic neural surrogates of global
  hybrid-Vlasov simulations
Authors: Daniel Holmberg, Ivan Zaitsev, Markku Alho, Ioanna Bouri, Fanni
  Franssila, Haewon Jeong, Minna Palmroth, Teemu Roos
Categories: physics.space-ph cs.LG physics.plasm-ph
\\
  Hybrid-Vlasov simulations resolve ion-kinetic effects for modeling the solar
wind-magnetosphere interaction, but even 5D (2D + 3V) simulations are
computationally expensive. We show that graph-based machine learning emulators
can learn the spatiotemporal evolution of electromagnetic fields and lower
order moments of ion velocity distribution in the near-Earth space environment
from four 5D Vlasiator runs performed with identical steady solar wind
conditions. The initial ion number density is systematically varied, while the
grid spacing is held constant, to scan the ratio of the characteristic ion skin
depth to the numerical grid size. Using a graph neural network architecture
operating on the 2D spatial simulation grid comprising 670k cells, we
demonstrate that both a deterministic forecasting model (Graph-FM) and a
probabilistic ensemble forecasting model (Graph-EFM) based on a latent variable
formulation are capable of producing accurate predictions of future plasma
states. A divergence penalty is incorporated during training to encourage
divergence-freeness in the magnetic fields and improve physical consistency.
For the probabilistic model, a continuous ranked probability score objective is
added to improve the calibration of the ensemble forecasts. When trained, the
emulators achieve more than two orders of magnitude speedup in generating the
next time step relative to the original simulation on a single GPU compared to
100 CPUs for the Vlasiator runs, while closely matching physical magnetospheric
response of the different runs. These results demonstrate that machine learning
offers a way to make hybrid-Vlasov simulation tractable for real-time use while
providing forecast uncertainty.
\\ ( https://arxiv.org/abs/2601.12614 ,  15363kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12621 (*cross-listing*)
Date: Sun, 18 Jan 2026 23:28:36 GMT   (16kb)

Title: Learning Deterministic Finite-State Machines from the Prefixes of a
  Single String is NP-Complete
Authors: Radu Cosmin Dumitru, Ryo Yoshinaka, Ayumi Shinohara
Categories: cs.FL cs.LG
Comments: 12 pages, 4 figures
\\
  It is well known that computing a minimum DFA consistent with a given set of
positive and negative examples is NP-hard. Previous work has identified
conditions on the input sample under which the problem becomes tractable or
remains hard. In this paper, we study the computational complexity of the case
where the input sample is prefix-closed. This formulation is equivalent to
computing a minimum Moore machine consistent with observations along its runs.
We show that the problem is NP-hard to approximate when the sample set consists
of all prefixes of binary strings. Furthermore, we show that the problem
remains NP-hard as a decision problem even when the sample set consists of the
prefixes of a single binary string. Our argument also extends to the
corresponding problem for Mealy machines.
\\ ( https://arxiv.org/abs/2601.12621 ,  16kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12630 (*cross-listing*)
Date: Mon, 19 Jan 2026 00:21:52 GMT   (4450kb)

Title: Reorienting off-path Nudged Elastic Bands (RONEB) via Minimum Mode
  Following
Authors: Rohit Goswami (1 and 2), Miha Gunde (2 and 3), Hannes J\'onsson ((1)
  Institute IMX and Lab-COSMO, \'Ecole polytechnique f\'ed\'erale de Lausanne
  (EPFL), CH-1015 Lausanne, Switzerland (2) Science Institute, University of
  Iceland, Reykjavik, Iceland (3) Institute Ru{\dj}er Bo\v{s}kovi\'c,
  Bijeni\v{c}ka 54, 10000 Zagreb, Croatia)
Categories: physics.chem-ph cond-mat.mtrl-sci cs.LG physics.comp-ph
Comments: 25 pages. 11 figures
\\
  Accurate determination of transition states remains central to understanding
reaction kinetics. Double-ended methods like the Nudged Elastic Band (NEB)
ensure relevant transition states and paths, but incur high computational costs
and suffer stagnation on flat or rough potential energy surfaces. Conversely,
single-ended eigenmode-following techniques offer efficiency but cannot often
be constrained between specific states. Here, we present the Reorienting
Off-path Nudged Elastic Bands (RONEB), an adaptive hybrid algorithm that
integrates the double ended nature of the NEB with the acceleration of single
ended Min-Mode Following methods. RONEB provides stability based on the history
of the path optimization, relative force triggering, and an alignment-based
back-off penalty to dynamically decouple the climbing image from the elastic
band constraints. We benchmark the method against the standard Climbing Image
NEB (CI-NEB) across the Baker-Chan transition state test set using the PET-MAD
machine-learned potential and the OptBench Pt(111) heptamer island surface
diffusion set. A Bayesian analysis of the performance data quantifies a median
reduction in gradient calls of 46.3% [95% CrI: -54.7%, -36.9%] relative to the
baseline, while surface diffusion tests reveal a 28% reduction across 59
metallic rearrangement mechanisms. These results establish RONEB as a highly
effective tool for high-throughput automated chemical discovery.
\\ ( https://arxiv.org/abs/2601.12630 ,  4450kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12639 (*cross-listing*)
Date: Mon, 19 Jan 2026 01:04:43 GMT   (481kb)

Title: Objective Matters: Fine-Tuning Objectives Shape Safety, Robustness, and
  Persona Drift
Authors: Daniel Vennemeyer, Punya Syon Pandey, Phan Anh Duong, Michael
  Umeokoli, Samuel Ratnam
Categories: cs.CL cs.LG
\\
  Fine-tuning LLMs on benign data can still degrade alignment and adversarial
robustness, yet direct analysis of the role of fine-tuning objectives in
shaping these safety outcomes remain limited. We present a controlled
comparison of six fine-tuning objectives -- Supervised Fine-Tuning, Direct
Preference Optimization, Conditional Fine-Tuning, Inoculation Prompting, Odds
Ratio Preference Optimization, and KL-regularized fine-tuning -- holding data,
domain, architecture, and optimization fixed. Across closed-form reasoning and
open-ended generation tasks, we find that objective choice induces systematic,
scale-dependent shifts along the safety-capability frontier. At small training
budgets, robustness is similar across objectives but capability differs. At
larger budgets, objectives diverge sharply: supervised and preference-based
tuning tightly couple capability gains to increased adversarial vulnerability
and persona drift, while objectives that constrain learning signals --
especially ORPO and KL-regularization -- substantially mitigate both.
Fine-tuning objectives therefore matter little for safety at small scales but
become a primary driver of adversarial robustness and latent persona stability
as training scale increases.
\\ ( https://arxiv.org/abs/2601.12639 ,  481kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12660 (*cross-listing*)
Date: Mon, 19 Jan 2026 02:16:37 GMT   (1388kb)

Title: Toward Faithful Explanations in Acoustic Anomaly Detection
Authors: Maab Elrashid, Anthony Desch\^enes, Cem Subakan, Mirco Ravanelli,
  R\'emi Georges, Michael Morin
Categories: cs.SD cs.LG eess.AS
Comments: Accepted at the IEEE International Conference on Acoustics, Speech,
  and Signal Processing (ICASSP) 2026. Code:
  https://github.com/Maab-Nimir/Faithful-Explanations-in-Acoustic-Anomaly-Detection
\\
  Interpretability is essential for user trust in real-world anomaly detection
applications. However, deep learning models, despite their strong performance,
often lack transparency. In this work, we study the interpretability of
autoencoder-based models for audio anomaly detection, by comparing a standard
autoencoder (AE) with a mask autoencoder (MAE) in terms of detection
performance and interpretability. We applied several attribution methods,
including error maps, saliency maps, SmoothGrad, Integrated Gradients,
GradSHAP, and Grad-CAM. Although MAE shows a slightly lower detection, it
consistently provides more faithful and temporally precise explanations,
suggesting a better alignment with true anomalies. To assess the relevance of
the regions highlighted by the explanation method, we propose a
perturbation-based faithfulness metric that replaces them with their
reconstructions to simulate normal input. Our findings, based on experiments in
a real industrial scenario, highlight the importance of incorporating
interpretability into anomaly detection pipelines and show that masked training
improves explanation quality without compromising performance.
\\ ( https://arxiv.org/abs/2601.12660 ,  1388kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12663 (*cross-listing*)
Date: Mon, 19 Jan 2026 02:22:06 GMT   (1568kb)

Title: Energy-Efficient Prediction in Textile Manufacturing: Enhancing Accuracy
  and Data Efficiency With Ensemble Deep Transfer Learning
Authors: Yan-Chen Chen, Wei-Yu Chiu, Qun-Yu Wang, Jing-Wei Chen, and Hao-Ting
  Zhao
Categories: eess.SP cs.LG
Comments: 26 pages, 11 figures
Journal-ref: IEEE Access, Vol. 13, 2025
DOI: 10.1109/ACCESS.2025.3551798
\\
  Traditional textile factories consume substantial energy, making
energy-efficient production optimization crucial for sustainability and cost
reduction. Meanwhile, deep neural networks (DNNs), which are effective for
factory output prediction and operational optimization, require extensive
historical data, posing challenges due to high sensor deployment and data
collection costs. To address this, we propose Ensemble Deep Transfer Learning
(EDTL), a novel framework that enhances prediction accuracy and data efficiency
by integrating transfer learning with an ensemble strategy and a feature
alignment layer. EDTL pretrains DNN models on data-rich production lines
(source domain) and adapts them to data-limited lines (target domain), reducing
dependency on large datasets. Experiments on real-world textile factory
datasets show that EDTL improves prediction accuracy by 5.66% and enhances
model robustness by 3.96% compared to conventional DNNs, particularly in
data-limited scenarios (20%-40% data availability). This research contributes
to energy-efficient textile manufacturing by enabling accurate predictions with
fewer data requirements, providing a scalable and cost-effective solution for
smart production systems.
\\ ( https://arxiv.org/abs/2601.12663 ,  1568kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12693 (*cross-listing*)
Date: Mon, 19 Jan 2026 03:29:55 GMT   (750kb)

Title: BlocksecRT-DETR: Decentralized Privacy-Preserving and Token-Efficient
  Federated Transformer Learning for Secure Real-Time Object Detection in ITS
Authors: Mohoshin Ara Tahera, Sabbir Rahman, Shuvalaxmi Dass, Sharif Ullah,
  Mahmoud Abouyessef
Categories: cs.CR cs.LG
\\
  Federated real-time object detection using transformers in Intelligent
Transportation Systems (ITS) faces three major challenges: (1) missing-class
non-IID data heterogeneity from geographically diverse traffic environments,
(2) latency constraints on edge hardware for high-capacity transformer models,
and (3) privacy and security risks from untrusted client updates and
centralized aggregation. We propose BlockSecRT-DETR, a BLOCKchain-SECured
Real-Time Object DEtection TRansformer framework for ITS that provides a
decentralized, token-efficient, and privacy-preserving federated training
solution using RT-DETR transformer, incorporating a blockchain-secured update
validation mechanism for trustworthy aggregation. In this framework, challenges
(1) and (2) are jointly addressed through a unified client-side design that
integrates RT-DETR training with a Token Engineering Module (TEM). TEM prunes
low-utility tokens, reducing encoder complexity and latency on edge hardware,
while aggregated updates mitigate non-IID data heterogeneity across clients. To
address challenge (3), BlockSecRT-DETR incorporates a decentralized
blockchain-secured update validation mechanism that enables tamper-proof,
privacy-preserving, and trust-free authenticated model aggregation without
relying on a central server. We evaluated the proposed framework under a
missing-class Non-IID partition of the KITTI dataset and conducted a blockchain
case study to quantify security overhead. TEM improves inference latency by
17.2% and reduces encoder FLOPs by 47.8%, while maintaining global detection
accuracy (89.20% mAP@0.5). The blockchain integration adds 400 ms per round,
and the ledger size remains under 12 KB due to metadata-only on-chain storage.
\\ ( https://arxiv.org/abs/2601.12693 ,  750kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12714 (*cross-listing*)
Date: Mon, 19 Jan 2026 04:35:04 GMT   (3381kb)

Title: P2L-CA: An Effective Parameter Tuning Framework for Rehearsal-Free
  Multi-Label Class-Incremental Learning
Authors: Songlin Dong, Jiangyang Li, Chenhao Ding, Zhiheng Ma, Haoyu Luo,
  Yuhang He, Yihong Gong
Categories: cs.CV cs.LG
Comments: 12 pages, 5 figures
\\
  Multi-label Class-Incremental Learning aims to continuously recognize novel
categories in complex scenes where multiple objects co-occur. However, existing
approaches often incur high computational costs due to full-parameter
fine-tuning and substantial storage overhead from memory buffers, or they
struggle to address feature confusion and domain discrepancies adequately. To
overcome these limitations, we introduce P2L-CA, a parameter-efficient
framework that integrates a Prompt-to-Label module with a Continuous Adapter
module. The P2L module leverages class-specific prompts to disentangle
multi-label representations while incorporating linguistic priors to enforce
stable semantic-visual alignment. Meanwhile, the CA module employs lightweight
adapters to mitigate domain gaps between pre-trained models and downstream
tasks, thereby enhancing model plasticity. Extensive experiments across
standard and challenging MLCIL settings on MS-COCO and PASCAL VOC show that
P2L-CA not only achieves substantial improvements over state-of-the-art methods
but also demonstrates strong generalization in CIL scenarios, all while
requiring minimal trainable parameters and eliminating the need for memory
buffers.
\\ ( https://arxiv.org/abs/2601.12714 ,  3381kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12752 (*cross-listing*)
Date: Mon, 19 Jan 2026 06:17:26 GMT   (695kb)

Title: SoundPlot: An Open-Source Framework for Birdsong Acoustic Analysis and
  Neural Synthesis with Interactive 3D Visualization
Authors: Naqcho Ali Mehdi, Mohammad Adeel, Aizaz Ali Larik
Categories: cs.SD cs.LG
ACM-class: H.5.5; I.5.4
\\
  We present SoundPlot, an open-source framework for analyzing avian
vocalizations through acoustic feature extraction, dimensionality reduction,
and neural audio synthesis. The system transforms audio signals into a
multi-dimensional acoustic feature space, enabling real-time visualization of
temporal dynamics in 3D using web-based interactive graphics. Our framework
implements a complete analysis-synthesis pipeline that extracts spectral
features (centroid, bandwidth, contrast), pitch contours via probabilistic YIN
(pYIN), and mel-frequency cepstral coefficients (MFCCs), mapping them to a
unified timbre space for visualization. Audio reconstruction employs the
Griffin-Lim phase estimation algorithm applied to mel spectrograms. The
accompanying Three.js-based interface provides dual-viewport visualization
comparing original and synthesized audio trajectories with independent playback
controls. We demonstrate the framework's capabilities through comprehensive
waveform analysis, spectrogram comparisons, and feature space evaluation using
Principal Component Analysis (PCA). Quantitative evaluation shows mel
spectrogram correlation scores exceeding 0.92, indicating high-fidelity
preservation of perceptual acoustic structure. SoundPlot is released under the
MIT License to facilitate research in bioacoustics, audio signal processing,
and computational ethology.
\\ ( https://arxiv.org/abs/2601.12752 ,  695kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12808 (*cross-listing*)
Date: Mon, 19 Jan 2026 08:12:47 GMT   (5556kb)

Title: Joint Source-Channel-Generation Coding: From Distortion-oriented
  Reconstruction to Semantic-consistent Generation
Authors: Tong Wu, Zhiyong Chen, Guo Lu, Li Song, Feng Yang, Meixia Tao, Wenjun
  Zhang
Categories: cs.IT cs.CV cs.LG math.IT
Comments: submitted to IEEE ISIT 2026
\\
  Conventional communication systems, including both separation-based coding
and AI-driven joint source-channel coding (JSCC), are largely guided by
Shannon's rate-distortion theory. However, relying on generic distortion
metrics fails to capture complex human visual perception, often resulting in
blurred or unrealistic reconstructions. In this paper, we propose Joint
Source-Channel-Generation Coding (JSCGC), a novel paradigm that shifts the
focus from deterministic reconstruction to probabilistic generation. JSCGC
leverages a generative model at the receiver as a generator rather than a
conventional decoder to parameterize the data distribution, enabling direct
maximization of mutual information under channel constraints while controlling
stochastic sampling to produce outputs residing on the authentic data manifold
with high fidelity. We further derive a theoretical lower bound on the maximum
semantic inconsistency with given transmitted mutual information, elucidating
the fundamental limits of communication in controlling the generative process.
Extensive experiments on image transmission demonstrate that JSCGC
substantially improves perceptual quality and semantic fidelity, significantly
outperforming conventional distortion-oriented JSCC methods.
\\ ( https://arxiv.org/abs/2601.12808 ,  5556kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12866 (*cross-listing*)
Date: Mon, 19 Jan 2026 09:23:40 GMT   (4864kb)

Title: PDFInspect: A Unified Feature Extraction Framework for Malicious
  Document Detection
Authors: Sharmila S P
Categories: cs.CR cs.LG
Comments: 6 pages, 2 figures, paper accepted in COMSNETS 2026 conference
\\
  The increasing prevalence of malicious Portable Document Format (PDF) files
necessitates robust and comprehensive feature extraction techniques for
effective detection and analysis. This work presents a unified framework that
integrates graph-based, structural, and metadata-driven analysis to generate a
rich feature representation for each PDF document. The system extracts text
from PDF pages and constructs undirected graphs based on pairwise word
relationships, enabling the computation of graph-theoretic features such as
node count, edge density, and clustering coefficient. Simultaneously, the
framework parses embedded metadata to quantify character distributions, entropy
patterns, and inconsistencies across fields such as author, title, and
producer. Temporal features are derived from creation and modification
timestamps to capture behavioral signatures, while structural elements
including, object streams, fonts, and embedded images, are quantified to
reflect document complexity. Boolean flags for potentially malicious PDF
constructs (e.g., JavaScript, launch actions) are also extracted. Together,
these features form a high-dimensional vector representation (170 dimensions)
that is well-suited for downstream tasks such as malware classification,
anomaly detection, and forensic analysis. The proposed approach is scalable,
extensible, and designed to support real-world PDF threat intelligence
workflows.6
\\ ( https://arxiv.org/abs/2601.12866 ,  4864kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12868 (*cross-listing*)
Date: Mon, 19 Jan 2026 09:24:24 GMT   (328kb)

Title: Race, Ethnicity and Their Implication on Bias in Large Language Models
Authors: Shiyue Hu, Ruizhe Li, Yanjun Gao
Categories: cs.CL cs.LG
Comments: Work in process
\\
  Large language models (LLMs) increasingly operate in high-stakes settings
including healthcare and medicine, where demographic attributes such as race
and ethnicity may be explicitly stated or implicitly inferred from text.
However, existing studies primarily document outcome-level disparities,
offering limited insight into internal mechanisms underlying these effects. We
present a mechanistic study of how race and ethnicity are represented and
operationalized within LLMs. Using two publicly available datasets spanning
toxicity-related generation and clinical narrative understanding tasks, we
analyze three open-source models with a reproducible interpretability pipeline
combining probing, neuron-level attribution, and targeted intervention. We find
that demographic information is distributed across internal units with
substantial cross-model variation. Although some units encode sensitive or
stereotype-related associations from pretraining, identical demographic cues
can induce qualitatively different behaviors. Interventions suppressing such
neurons reduce bias but leave substantial residual effects, suggesting
behavioral rather than representational change and motivating more systematic
mitigation.
\\ ( https://arxiv.org/abs/2601.12868 ,  328kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12895 (*cross-listing*)
Date: Mon, 19 Jan 2026 09:50:51 GMT   (16kb)

Title: TwoHead-SwinFPN: A Unified DL Architecture for Synthetic Manipulation,
  Detection and Localization in Identity Documents
Authors: Chan Naseeb, Adeel Ashraf Cheema, Hassan Sami, Tayyab Afzal, Muhammad
  Omair, Usman Habib
Categories: cs.CV cs.LG
Comments: 8 pages
\\
  The proliferation of sophisticated generative AI models has significantly
escalated the threat of synthetic manipulations in identity documents,
particularly through face swapping and text inpainting attacks. This paper
presents TwoHead-SwinFPN, a unified deep learning architecture that
simultaneously performs binary classification and precise localization of
manipulated regions in ID documents. Our approach integrates a Swin Transformer
backbone with Feature Pyramid Network (FPN) and UNet-style decoder, enhanced
with Convolutional Block Attention Module (CBAM) for improved feature
representation. The model employs a dual-head architecture for joint
optimization of detection and segmentation tasks, utilizing
uncertainty-weighted multi-task learning. Extensive experiments on the
FantasyIDiap dataset demonstrate superior performance with 84.31\% accuracy,
90.78\% AUC for classification, and 57.24\% mean Dice score for localization.
The proposed method achieves an F1-score of 88.61\% for binary classification
while maintaining computational efficiency suitable for real-world deployment
through FastAPI implementation. Our comprehensive evaluation includes ablation
studies, cross-device generalization analysis, and detailed performance
assessment across 10 languages and 3 acquisition devices.
\\ ( https://arxiv.org/abs/2601.12895 ,  16kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12918 (*cross-listing*)
Date: Mon, 19 Jan 2026 10:17:20 GMT   (7493kb)

Title: Dynamic Hand Gesture Recognition for Robot Manipulator Tasks
Authors: Dharmendra Sharma, Peeyush Thakur, Sandeep Gupta, Narendra Kumar Dhar,
  and Laxmidhar Behera
Categories: cs.RO cs.LG
DOI: 10.1109/SMC54092.2024.10831056 10.1109/SMC54092.2024.10831056
  10.1109/SMC54092.2024.10831056
\\
  This paper proposes a novel approach to recognizing dynamic hand gestures
facilitating seamless interaction between humans and robots. Here, each robot
manipulator task is assigned a specific gesture. There may be several such
tasks, hence, several gestures. These gestures may be prone to several dynamic
variations. All such variations for different gestures shown to the robot are
accurately recognized in real-time using the proposed unsupervised model based
on the Gaussian Mixture model. The accuracy during training and real-time
testing prove the efficacy of this methodology.
\\ ( https://arxiv.org/abs/2601.12918 ,  7493kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12945 (*cross-listing*)
Date: Mon, 19 Jan 2026 10:53:57 GMT   (67kb)

Title: A Component-Based Survey of Interactions between Large Language Models
  and Multi-Armed Bandits
Authors: Miao Xie, Siguang Chen and Chunli Lv
Categories: cs.CL cs.LG
Comments: 27 pages, 6 table
\\
  Large language models (LLMs) have become powerful and widely used systems for
language understanding and generation, while multi-armed bandit (MAB)
algorithms provide a principled framework for adaptive decision-making under
uncertainty. This survey explores the potential at the intersection of these
two fields. As we know, it is the first survey to systematically review the
bidirectional interaction between large language models and multi-armed bandits
at the component level. We highlight the bidirectional benefits: MAB algorithms
address critical LLM challenges, spanning from pre-training to
retrieval-augmented generation (RAG) and personalization. Conversely, LLMs
enhance MAB systems by redefining core components such as arm definition and
environment modeling, thereby improving decision-making in sequential tasks. We
analyze existing LLM-enhanced bandit systems and bandit-enhanced LLM systems,
providing insights into their design, methodologies, and performance. Key
challenges and representative findings are identified to help guide future
research. An accompanying GitHub repository that indexes relevant literature is
available at https://github.com/bucky1119/Awesome-LLM-Bandit-Interaction.
\\ ( https://arxiv.org/abs/2601.12945 ,  67kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12981 (*cross-listing*)
Date: Mon, 19 Jan 2026 11:55:41 GMT   (3416kb)

Title: Early Prediction of Type 2 Diabetes Using Multimodal data and Tabular
  Transformers
Authors: Sulaiman Khan, Md. Rafiul Biswas, Zubair Shah
Categories: cs.CV cs.LG
Comments: 08 pages, 06 figures, accepted for publication in FLLM2025
\\
  This study introduces a novel approach for early Type 2 Diabetes Mellitus
(T2DM) risk prediction using a tabular transformer (TabTrans) architecture to
analyze longitudinal patient data. By processing patients` longitudinal health
records and bone-related tabular data, our model captures complex, long-range
dependencies in disease progression that conventional methods often overlook.
We validated our TabTrans model on a retrospective Qatar BioBank (QBB) cohort
of 1,382 subjects, comprising 725 men (146 diabetic, 579 healthy) and 657 women
(133 diabetic, 524 healthy). The study integrated electronic health records
(EHR) with dual-energy X-ray absorptiometry (DXA) data. To address class
imbalance, we employed SMOTE and SMOTE-ENN resampling techniques. The proposed
model`s performance is evaluated against conventional machine learning (ML) and
generative AI models, including Claude 3.5 Sonnet (Anthropic`s constitutional
AI), GPT-4 (OpenAI`s generative pre-trained transformer), and Gemini Pro
(Google`s multimodal language model). Our TabTrans model demonstrated superior
predictive performance, achieving ROC AUC $\geq$ 79.7 % for T2DM prediction
compared to both generative AI models and conventional ML approaches. Feature
interpretation analysis identified key risk indicators, with visceral adipose
tissue (VAT) mass and volume, ward bone mineral density (BMD) and bone mineral
content (BMC), T and Z-scores, and L1-L4 scores emerging as the most important
predictors associated with diabetes development in Qatari adults. These
findings demonstrate the significant potential of TabTrans for analyzing
complex tabular healthcare data, providing a powerful tool for proactive T2DM
management and personalized clinical interventions in the Qatari population.
  Index Terms: tabular transformers, multimodal data, DXA data, diabetes, T2DM,
feature interpretation, tabular data
\\ ( https://arxiv.org/abs/2601.12981 ,  3416kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12990 (*cross-listing*)
Date: Mon, 19 Jan 2026 12:09:54 GMT   (1222kb)

Title: Beyond Visual Realism: Toward Reliable Financial Time Series Generation
Authors: Fan Zhang, Jiabin Luo, Zheng Zhang, Shuanghong Huang, Zhipeng Liu, Yu
  Chen
Categories: q-fin.ST cs.LG
Comments: Accepted by ICASSP 2026
\\
  Generative models for financial time series often create data that look
realistic and even reproduce stylized facts such as fat tails or volatility
clustering. However, these apparent successes break down under trading
backtests: models like GANs or WGAN-GP frequently collapse, yielding extreme
and unrealistic results that make the synthetic data unusable in practice. We
identify the root cause in the neglect of financial asymmetry and rare tail
events, which strongly affect market risk but are often overlooked by
objectives focusing on distribution matching. To address this, we introduce the
Stylized Facts Alignment GAN (SFAG), which converts key stylized facts into
differentiable structural constraints and jointly optimizes them with
adversarial loss. This multi-constraint design ensures that generated series
remain aligned with market dynamics not only in plots but also in backtesting.
Experiments on the Shanghai Composite Index (2004--2024) show that while
baseline GANs produce unstable and implausible trading outcomes, SFAG generates
synthetic data that preserve stylized facts and support robust momentum
strategy performance. Our results highlight that structure-preserving
objectives are essential to bridge the gap between superficial realism and
practical usability in financial generative modeling.
\\ ( https://arxiv.org/abs/2601.12990 ,  1222kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12996 (*cross-listing*)
Date: Mon, 19 Jan 2026 12:23:44 GMT   (731kb)

Title: OFA-MAS: One-for-All Multi-Agent System Topology Design based on
  Mixture-of-Experts Graph Generative Models
Authors: Shiyuan Li, Yixin Liu, Yu Zheng, Mei Li, Quoc Viet Hung Nguyen, Shirui
  Pan
Categories: cs.MA cs.LG
Comments: Accepted by WWW 2026
\\
  Multi-Agent Systems (MAS) offer a powerful paradigm for solving complex
problems, yet their performance is critically dependent on the design of their
underlying collaboration topology. As MAS become increasingly deployed in web
services (e.g., search engines), designing adaptive topologies for diverse
cross-domain user queries becomes essential. Current graph learning-based
design methodologies often adhere to a "one-for-one" paradigm, where a
specialized model is trained for each specific task domain. This approach
suffers from poor generalization to unseen domains and fails to leverage shared
structural knowledge across different tasks. To address this, we propose
OFA-TAD, a one-for-all framework that generates adaptive collaboration graphs
for any task described in natural language through a single universal model.
Our approach integrates a Task-Aware Graph State Encoder (TAGSE) that filters
task-relevant node information via sparse gating, and a Mixture-of-Experts
(MoE) architecture that dynamically selects specialized sub-networks to drive
node and edge prediction. We employ a three-stage training strategy:
unconditional pre-training on canonical topologies for structural priors,
large-scale conditional pre-training on LLM-generated datasets for
task-topology mappings, and supervised fine-tuning on empirically validated
graphs. Experiments across six diverse benchmarks show that OFA-TAD
significantly outperforms specialized one-for-one models, generating highly
adaptive MAS topologies. Code: https://github.com/Shiy-Li/OFA-MAS.
\\ ( https://arxiv.org/abs/2601.12996 ,  731kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13035 (*cross-listing*)
Date: Mon, 19 Jan 2026 13:19:00 GMT   (168kb)

Title: SASA: Semantic-Aware Contrastive Learning Framework with Separated
  Attention for Triple Classification
Authors: Xu Xiaodan and Hu Xiaolin
Categories: cs.CL cs.LG
Comments: in progress
\\
  Knowledge Graphs~(KGs) often suffer from unreliable knowledge, which
restricts their utility. Triple Classification~(TC) aims to determine the
validity of triples from KGs. Recently, text-based methods learn entity and
relation representations from natural language descriptions, significantly
improving the generalization capabilities of TC models and setting new
benchmarks in performance. However, there are still two critical challenges.
First, existing methods often ignore the effective semantic interaction among
different KG components. Second, most approaches adopt single binary
classification training objective, leading to insufficient semantic
representation learning. To address these challenges, we propose \textbf{SASA},
a novel framework designed to enhance TC models via separated attention
mechanism and semantic-aware contrastive learning~(CL). Specifically, we first
propose separated attention mechanism to encode triples into decoupled
contextual representations and then fuse them through a more effective
interactive way. Then, we introduce semantic-aware hierarchical CL as auxiliary
training objective to guide models in improving their discriminative
capabilities and achieving sufficient semantic learning, considering both local
level and global level CL. Experimental results across two benchmark datasets
demonstrate that SASA significantly outperforms state-of-the-art methods. In
terms of accuracy, we advance the state-of-the-art by +5.9\% on FB15k-237 and
+3.4\% on YAGO3-10.
\\ ( https://arxiv.org/abs/2601.13035 ,  168kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13079 (*cross-listing*)
Date: Mon, 19 Jan 2026 14:12:01 GMT   (1608kb)

Title: Polychronous Wave Computing: Timing-Native Address Selection in Spiking
  Networks
Authors: Natalila G. Berloff
Categories: cond-mat.dis-nn cs.LG cs.NE physics.optics
Comments: 23 pages, Supplementary Materials are available at
  https://www.damtp.cam.ac.uk/user/ngb23/publications/SM_PWC.pdf
\\
  Spike timing offers a combinatorial address space, suggesting that
timing-based spiking inference can be executed as lookup and routing rather
than as dense multiply--accumulate. Yet most neuromorphic and photonic systems
still digitize events into timestamps, bins, or rates and then perform
selection in clocked logic. We introduce Polychronous Wave Computing (PWC), a
timing-native address-selection primitive that maps relative spike latencies
directly to a discrete output route in the wave domain. Spike times are
phase-encoded in a rotating frame and processed by a programmable multiport
interferometer that evaluates K template correlations in parallel; a
driven--dissipative winner-take-all stage then performs a physical argmax,
emitting a one-hot output port. We derive the operating envelope imposed by
phase wrapping and mutual coherence, and collapse timing jitter, static phase
mismatch, and dephasing into a single effective phase-noise budget whose
induced winner--runner-up margin predicts boundary-first failures and provides
an intensity-only calibration target. Simulations show that nonlinear
competition improves routing fidelity compared with noisy linear intensity
readout, and that hardware-in-the-loop phase tuning rescues a temporal-order
gate from 55.9% to 97.2% accuracy under strong static mismatch. PWC provides a
fast routing coprocessor for LUT-style spiking networks and sparse top-1 gates
(e.g., mixture-of-experts routing) across polaritonic, photonic, and oscillator
platforms.
\\ ( https://arxiv.org/abs/2601.13079 ,  1608kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13082 (*cross-listing*)
Date: Mon, 19 Jan 2026 14:19:04 GMT   (1495kb)

Title: Adversarial News and Lost Profits: Manipulating Headlines in LLM-Driven
  Algorithmic Trading
Authors: Advije Rizvani, Giovanni Apruzzese, Pavel Laskov
Categories: cs.CR cs.LG
Comments: This work has been accepted for publication at the IEEE Conference on
  Secure and Trustworthy Machine Learning (SaTML). The final version will be
  available on IEEE Xplore
\\
  Large Language Models (LLMs) are increasingly adopted in the financial
domain. Their exceptional capabilities to analyse textual data make them
well-suited for inferring the sentiment of finance-related news. Such feedback
can be leveraged by algorithmic trading systems (ATS) to guide buy/sell
decisions. However, this practice bears the risk that a threat actor may craft
"adversarial news" intended to mislead an LLM. In particular, the news headline
may include "malicious" content that remains invisible to human readers but
which is still ingested by the LLM. Although prior work has studied textual
adversarial examples, their system-wide impact on LLM-supported ATS has not yet
been quantified in terms of monetary risk. To address this threat, we consider
an adversary with no direct access to an ATS but able to alter stock-related
news headlines on a single day. We evaluate two human-imperceptible
manipulations in a financial context: Unicode homoglyph substitutions that
misroute models during stock-name recognition, and hidden-text clauses that
alter the sentiment of the news headline. We implement a realistic ATS in
Backtrader that fuses an LSTM-based price forecast with LLM-derived sentiment
(FinBERT, FinGPT, FinLLaMA, and six general-purpose LLMs), and quantify
monetary impact using portfolio metrics. Experiments on real-world data show
that manipulating a one-day attack over 14 months can reliably mislead LLMs and
reduce annual returns by up to 17.7 percentage points. To assess real-world
feasibility, we analyze popular scraping libraries and trading platforms and
survey 27 FinTech practitioners, confirming our hypotheses. We notified trading
platform owners of this security issue.
\\ ( https://arxiv.org/abs/2601.13082 ,  1495kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13102 (*cross-listing*)
Date: Mon, 19 Jan 2026 14:40:49 GMT   (639kb)

Title: Approximate full conformal prediction in RKHS
Authors: Davidson Lova Razafindrakoto, Alain Celisse and J\'er\^ome Lacaille
Categories: stat.ML cs.LG math.ST stat.TH
\\
  Full conformal prediction is a framework that implicitly formulates
distribution-free confidence prediction regions for a wide range of estimators.
However, a classical limitation of the full conformal framework is the
computation of the confidence prediction regions, which is usually impossible
since it requires training infinitely many estimators (for real-valued
prediction for instance). The main purpose of the present work is to describe a
generic strategy for designing a tight approximation to the full conformal
prediction region that can be efficiently computed. Along with this approximate
confidence region, a theoretical quantification of the tightness of this
approximation is developed, depending on the smoothness assumptions on the loss
and score functions. The new notion of thickness is introduced for quantifying
the discrepancy between the approximate confidence region and the full
conformal one.
\\ ( https://arxiv.org/abs/2601.13102 ,  639kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13144 (*cross-listing*)
Date: Mon, 19 Jan 2026 15:25:04 GMT   (19263kb)

Title: Forecasting Continuum Intensity for Solar Active Region Emergence
  Prediction using Transformers
Authors: Jonas Tirona, Sarang Patil, Spiridon Kasapis, Eren Dogan, John Stefan,
  Irina N. Kitiashvili, Alexander G. Kosovichev, Mengjia Xu
Categories: astro-ph.SR cs.LG
Comments: 30 pages, 7 figures, submitted to JGR: Machine Learning and
  Computation
\\
  Early and accurate prediction of solar active region (AR) emergence is
crucial for space weather forecasting. Building on established Long Short-Term
Memory (LSTM) based approaches for forecasting the continuum intensity decrease
associated with AR emergence, this work expands the modeling with new
architectures and targets. We investigate a sliding-window Transformer
architecture to forecast continuum intensity evolution up to 12 hours ahead
using data from 46 ARs observed by SDO/HMI. We conduct a systematic ablation
study to evaluate two key components: (1) the inclusion of a temporal 1D
convolutional (Conv1D) front-end and (2) a novel `Early Detection' architecture
featuring attention biases and a timing-aware loss function. Our
best-performing model, combining the Early Detection architecture without the
Conv1D layer, achieved a Root Mean Square Error (RMSE) of 0.1189 (representing
a 10.6% improvement over the LSTM baseline) and an average advance warning time
of 4.73 hours (timing difference of -4.73h), even under a stricter emergence
criterion than previous studies. While the Transformer demonstrates superior
aggregate timing and accuracy, we note that this high-sensitivity detection
comes with increased variance compared to smoother baseline models. However,
this volatility is a necessary trade-off for operational warning systems: the
model's ability to detect micro-changes in precursor signals enables
significantly earlier detection, outweighing the cost of increased noise. Our
results demonstrate that Transformer architectures modified with early
detection biases, when used without temporal smoothing layers, provide a
high-sensitivity alternative for forecasting AR emergence that prioritizes
advance warning over statistical smoothness.
\\ ( https://arxiv.org/abs/2601.13144 ,  19263kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13145 (*cross-listing*)
Date: Mon, 19 Jan 2026 15:25:18 GMT   (4622kb)

Title: SolARED: Solar Active Region Emergence Dataset for Machine Learning
  Aided Predictions
Authors: Spiridon Kasapis, Eren Dogan, Irina N. Kitiashvili, Alexander G.
  Kosovichev, John T. Stefan, Jake D. Butler, Jonas Tirona, Sarang Patil,
  Mengjia Xu
Categories: astro-ph.SR cs.LG
Comments: 15 pages, 6 figures, submitted to the Springer Nature - Solar Physics
  Journal
\\
  The development of accurate forecasts of solar eruptive activity has become
increasingly important for preventing potential impacts on space technologies
and exploration. Therefore, it is crucial to detect Active Regions (ARs) before
they start forming on the solar surface. This will enable the development of
early-warning capabilities for upcoming space weather disturbances. For this
reason, we prepared the Solar Active Region Emergence Dataset (SolARED). The
dataset is derived from full-disk maps of the Doppler velocity, magnetic field,
and continuum intensity, obtained by the Helioseismic and Magnetic Imager (HMI)
onboard the Solar Dynamics Observatory (SDO). SolARED includes time series of
remapped, tracked, and binned data that characterize the evolution of acoustic
power of solar oscillations, unsigned magnetic flux, and continuum intensity
for 50 large ARs before, during, and after their emergence on the solar
surface, as well as surrounding areas observed on the solar disc between 2010
and 2023. The resulting ML-ready SolARED dataset is designed to support
enhancements of predictive capabilities, enabling the development of
operational forecasts for the emergence of active regions. The SolARED dataset
is available at https://sun.njit.edu/sarportal/, through an interactive
visualization web application.
\\ ( https://arxiv.org/abs/2601.13145 ,  4622kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13155 (*cross-listing*)
Date: Mon, 19 Jan 2026 15:34:29 GMT   (1179kb)

Title: Probe and Skip: Self-Predictive Token Skipping for Efficient
  Long-Context LLM Inference
Authors: Zimeng Wu, Donghao Wang, Chaozhe Jin, Jiaxin Chen, Yunhong Wang
Categories: cs.CL cs.LG
\\
  Long-context inference enhances the reasoning capability of Large Language
Models (LLMs) while incurring significant computational overhead.
Token-oriented methods, such as pruning and skipping, have shown promise in
reducing inference latency, but still suffer from inherently limited
acceleration potential, outdated proxy signals, and redundancy interference,
thus yielding suboptimal speed-accuracy trade-offs. To address these
challenges, we propose SPTS (Self-Predictive Token Skipping), a training-free
framework for efficient long-context LLM inference. Specifically, motivated by
the thought of probing the influence of targeted skipping layers, we design two
component-specific strategies for selective token skipping: Partial Attention
Probing (PAP) for multi-head attention, which selects informative tokens by
performing partial forward attention computation, and Low-rank Transformation
Probing (LTP) for feed forward network, which constructs a low-rank proxy
network to predict token transformations. Furthermore, a Multi-Stage Delayed
Pruning (MSDP) strategy reallocates the skipping budget and progressively
prunes redundant tokens across layers. Extensive experiments demonstrate the
effectiveness of our method, achieving up to 2.46$\times$ and 2.29$\times$
speedups for prefilling and end-to-end generation, respectively, while
maintaining state-of-the-art model performance. The source code will be
publicly available upon paper acceptance.
\\ ( https://arxiv.org/abs/2601.13155 ,  1179kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13191 (*cross-listing*)
Date: Mon, 19 Jan 2026 16:13:58 GMT   (572kb)

Title: Empirical Risk Minimization with $f$-Divergence Regularization
Authors: Francisco Daunas, I\~naki Esnaola, Samir M. Perlaza, H. Vincent Poor
Categories: stat.ML cs.LG
Comments: Submitted to IEEE Transactions on Information Theory. arXiv admin
  note: substantial text overlap with arXiv:2502.14544, arXiv:2508.03314
\\
  In this paper, the solution to the empirical risk minimization problem with
$f$-divergence regularization (ERM-$f$DR) is presented and conditions under
which the solution also serves as the solution to the minimization of the
expected empirical risk subject to an $f$-divergence constraint are
established. The proposed approach extends applicability to a broader class of
$f$-divergences than previously reported and yields theoretical results that
recover previously known results. Additionally, the difference between the
expected empirical risk of the ERM-$f$DR solution and that of its reference
measure is characterized, providing insights into previously studied cases of
$f$-divergences. A central contribution is the introduction of the
normalization function, a mathematical object that is critical in both the dual
formulation and practical computation of the ERM-$f$DR solution. This work
presents an implicit characterization of the normalization function as a
nonlinear ordinary differential equation (ODE), establishes its key properties,
and subsequently leverages them to construct a numerical algorithm for
approximating the normalization factor under mild assumptions. Further analysis
demonstrates structural equivalences between ERM-$f$DR problems with different
$f$-divergences via transformations of the empirical risk. Finally, the
proposed algorithm is used to compute the training and test risks of ERM-$f$DR
solutions under different $f$-divergence regularizers. This numerical example
highlights the practical implications of choosing different functions $f$ in
ERM-$f$DR problems.
\\ ( https://arxiv.org/abs/2601.13191 ,  572kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13251 (*cross-listing*)
Date: Mon, 19 Jan 2026 17:37:25 GMT   (805kb)

Title: Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in
  a 15-Million Node Turkish Synonym Graph
Authors: Ebubekir Tosun, Mehmet Emin Buldur, \"Ozay Ezerceli, Mahmoud
  ElHussieni
Categories: cs.CL cs.LG
\\
  Neural embeddings have a notorious blind spot: they can't reliably tell
synonyms apart from antonyms. Consequently, increasing similarity thresholds
often fails to prevent opposites from being grouped together. We've built a
large-scale semantic clustering system specifically designed to tackle this
problem head on. Our pipeline chews through 15 million lexical items, evaluates
a massive 520 million potential relationships, and ultimately generates 2.9
million high-precision semantic clusters. The system makes three primary
contributions. First, we introduce a labeled dataset of 843,000 concept pairs
spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash
LLM augmentation and verified using human-curated dictionary resources. Second,
we propose a specialized three-way semantic relation discriminator that
achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding
similarity. Third, we introduce a novel soft-to-hard clustering algorithm that
mitigates semantic drift preventing erroneous transitive chains (e.g., hot ->
spicy -> pain -> depression) while simultaneously resolving polysemy. Our
approach employs a topology-aware two-stage expansion-pruning procedure with
topological voting, ensuring that each term is assigned to exactly one
semantically coherent cluster. The resulting resource enables high-precision
semantic search and retrieval-augmented generation, particularly for
morphologically rich and low-resource languages where existing synonym
databases remain sparse.
\\ ( https://arxiv.org/abs/2601.13251 ,  805kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13253 (*cross-listing*)
Date: Mon, 19 Jan 2026 17:38:52 GMT   (1679kb)

Title: A Hybrid Protocol for Large-Scale Semantic Dataset Generation in
  Low-Resource Languages: The Turkish Semantic Relations Corpus
Authors: Ebubekir Tosun, Mehmet Emin Buldur, \"Ozay Ezerceli, Mahmoud
  ElHussieni
Categories: cs.CL cs.LG
\\
  We present a hybrid methodology for generating large-scale semantic
relationship datasets in low-resource languages, demonstrated through a
comprehensive Turkish semantic relations corpus. Our approach integrates three
phases: (1) FastText embeddings with Agglomerative Clustering to identify
semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship
classification, and (3) integration with curated dictionary sources. The
resulting dataset comprises 843,000 unique Turkish semantic pairs across three
relationship types (synonyms, antonyms, co-hyponyms) representing a 10x scale
increase over existing resources at minimal cost ($65). We validate the dataset
through two downstream tasks: an embedding model achieving 90% top-1 retrieval
accuracy and a classification model attaining 90% F1-macro. Our scalable
protocol addresses critical data scarcity in Turkish NLP and demonstrates
applicability to other low-resource languages. We publicly release the dataset
and models.
\\ ( https://arxiv.org/abs/2601.13253 ,  1679kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13256 (*cross-listing*)
Date: Mon, 19 Jan 2026 17:41:54 GMT   (74kb)

Title: Deep Neural networks for solving high-dimensional parabolic partial
  differential equations
Authors: Wenzhong Zhang, Zhenyuan Hu, Wei Cai, George EM Karniadakis
Categories: math.NA cs.LG cs.NA
\\
  The numerical solution of high dimensional partial differential equations
(PDEs) is severely constrained by the curse of dimensionality (CoD), rendering
classical grid--based methods impractical beyond a few dimensions. In recent
years, deep neural networks have emerged as a promising mesh free alternative,
enabling the approximation of PDE solutions in tens to thousands of dimensions.
This review provides a tutorial--oriented introduction to
neural--network--based methods for solving high dimensional parabolic PDEs,
emphasizing conceptual clarity and methodological connections. We organize the
literature around three unifying paradigms: (i) PDE residual--based approaches,
including physicsinformed neural networks and their high dimensional variants;
(ii) stochastic methods derived from Feynman--Kac and backward stochastic
differential equation formulations; and (iii) hybrid derivative--free random
difference approaches designed to alleviate the computational cost of
derivatives in high dimensions. For each paradigm, we outline the underlying
mathematical formulation, algorithmic implementation, and practical strengths
and limitations. Representative benchmark problems--including
Hamilton--Jacobi--Bellman and Black--Scholes equations in up to 1000 dimensions
--illustrate the scalability, effectiveness, and accuracy of the methods. The
paper concludes with a discussion of open challenges and future directions for
reliable and scalable solvers of high dimensional PDEs.
\\ ( https://arxiv.org/abs/2601.13256 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13294 (*cross-listing*)
Date: Mon, 19 Jan 2026 18:47:36 GMT   (314kb)

Title: The Tag is the Signal: URL-Agnostic Credibility Scoring for Messages on
  Telegram
Authors: Yipeng Wang, Huy Gia Han Vu, Mohit Singhal
Categories: cs.SI cs.CY cs.LG
\\
  Telegram has become one of the leading platforms for disseminating
misinformational messages. However, many existing pipelines still classify each
message's credibility based on the reputation of its associated domain names or
its lexical features. Such methods work well on traditional long-form news
articles published by well-known sources, but high-risk posts on Telegram are
short and URL-sparse, leading to failures for link-based and standard TF-IDF
models. To this end, we propose the TAG2CRED pipeline, a method designed for
such short, convoluted messages. Our model will directly score each post based
on the tags assigned to the text. We designed a concise label system that
covers the dimensions of theme, claim type, call to action, and evidence. The
fine-tuned large language model (LLM) assigns tags to messages and then maps
these tags to calibrated risk scores in the [0,1] interval through
L2-regularized logistic regression. We evaluated 87,936 Telegram messages
associated with Media Bias/Fact Check (MBFC), using URL masking and domain
disjoint splits. The results showed that the ROC-AUC of the TAG2CRED model
reached 0.871, the macro-F1 value was 0.787, and the Brier score was 0.167,
outperforming the baseline TF-IDF (macro-F1 value 0.737, Brier score 0.248); at
the same time, the number of features used in this model is much smaller, and
the generalization ability on infrequent domains is stronger. The performance
of the stacked ensemble model (TF-IDF + TAG2CRED + SBERT) was further improved
over the baseline SBERT. ROC-AUC reached 0.901, and the macro-F1 value was
0.813 (Brier score 0.114). This indicates that style labels and lexical
features may capture different but complementary dimensions of information
risk.
\\ ( https://arxiv.org/abs/2601.13294 ,  314kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13308 (*cross-listing*)
Date: Mon, 19 Jan 2026 19:00:00 GMT   (5675kb)

Title: Scaling laws for amplitude surrogates
Authors: Henning Bahl, Victor Bres\'o-Pla, Anja Butter, and Joaqu\'in Iturriza
  Ramirez
Categories: hep-ph cs.LG
Comments: 45 pages, 20 figures
\\
  Scaling laws describing the dependence of neural network performance on the
amount of training data, the spent compute, and the network size have emerged
across a huge variety of machine learning task and datasets. In this work, we
systematically investigate these scaling laws in the context of amplitude
surrogates for particle physics. We show that the scaling coefficients are
connected to the number of external particles of the process. Our results
demonstrate that scaling laws are a useful tool to achieve desired precision
targets.
\\ ( https://arxiv.org/abs/2601.13308 ,  5675kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13331 (*cross-listing*)
Date: Mon, 19 Jan 2026 19:11:03 GMT   (4268kb)

Title: MultiST: A Cross-Attention-Based Multimodal Model for Spatial
  Transcriptomic
Authors: Wei Wang, Quoc-Toan Ly, Chong Yu, and Jun Bai
Categories: cs.CV cs.LG
\\
  Spatial transcriptomics (ST) enables transcriptome-wide profiling while
preserving the spatial context of tissues, offering unprecedented opportunities
to study tissue organization and cell-cell interactions in situ. Despite recent
advances, existing methods often lack effective integration of histological
morphology with molecular profiles, relying on shallow fusion strategies or
omitting tissue images altogether, which limits their ability to resolve
ambiguous spatial domain boundaries. To address this challenge, we propose
MultiST, a unified multimodal framework that jointly models spatial topology,
gene expression, and tissue morphology through cross-attention-based fusion.
MultiST employs graph-based gene encoders with adversarial alignment to learn
robust spatial representations, while integrating color-normalized histological
features to capture molecular-morphological dependencies and refine domain
boundaries. We evaluated the proposed method on 13 diverse ST datasets spanning
two organs, including human brain cortex and breast cancer tissue. MultiST
yields spatial domains with clearer and more coherent boundaries than existing
methods, leading to more stable pseudotime trajectories and more biologically
interpretable cell-cell interaction patterns. The MultiST framework and source
code are available at https://github.com/LabJunBMI/MultiST.git.
\\ ( https://arxiv.org/abs/2601.13331 ,  4268kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13359 (*cross-listing*)
Date: Mon, 19 Jan 2026 19:53:48 GMT   (567kb)

Title: Sockpuppetting: Jailbreaking LLMs Without Optimization Through Output
  Prefix Injection
Authors: Asen Dotsinski and Panagiotis Eustratiadis
Categories: cs.CL cs.CR cs.LG
\\
  As open-weight large language models (LLMs) increase in capabilities,
safeguarding them against malicious prompts and understanding possible attack
vectors becomes ever more important. While automated jailbreaking methods like
GCG [Zou et al., 2023] remain effective, they often require substantial
computational resources and specific expertise. We introduce "sockpuppetting'',
a simple method for jailbreaking open-weight LLMs by inserting an acceptance
sequence (e.g., "Sure, here is how to...'') at the start of a model's output
and allowing it to complete the response. Requiring only a single line of code
and no optimization, sockpuppetting achieves up to 80% higher attack success
rate (ASR) than GCG on Qwen3-8B in per-prompt comparisons. We also explore a
hybrid approach that optimizes the adversarial suffix within the assistant
message block rather than the user prompt, increasing ASR by 64% over GCG on
Llama-3.1-8B in a prompt-agnostic setting. The results establish sockpuppetting
as an effective low-cost attack accessible to unsophisticated adversaries,
highlighting the need for defences against output-prefix injection in
open-weight models.
\\ ( https://arxiv.org/abs/2601.13359 ,  567kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13362 (*cross-listing*)
Date: Mon, 19 Jan 2026 19:56:15 GMT   (422kb)

Title: Improving Geopolitical Forecasts with Bayesian Networks
Authors: Matthew Martin
Categories: stat.AP cs.LG
Comments: 34 pages, 3 figures
\\
  This study explores how Bayesian networks (BNs) can improve forecast accuracy
compared to logistic regression and recalibration and aggregation methods,
using data from the Good Judgment Project. Regularized logistic regression
models and a baseline recalibrated aggregate were compared to two types of BNs:
structure-learned BNs with arcs between predictors, and naive BNs. Four
predictor variables were examined: absolute difference from the aggregate,
forecast value, days prior to question close, and mean standardized Brier
score. Results indicated the recalibrated aggregate achieved the highest
accuracy (AUC = 0.985), followed by both types of BNs, then the logistic
regression models. Performance of the BNs was likely harmed by reduced
information from the discretization process and violation of the assumption of
linearity likely harmed the logistic regression models. Future research should
explore hybrid approaches combining BNs with logistic regression, examine
additional predictor variables, and account for hierarchical data dependencies.
\\ ( https://arxiv.org/abs/2601.13362 ,  422kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13368 (*cross-listing*)
Date: Mon, 19 Jan 2026 20:04:34 GMT   (917kb)

Title: Recurrent Confidence Chain: Temporal-Aware Uncertainty Quantification in
  Large Language Models
Authors: Zhenjiang Mao, Anirudhh Venkat
Categories: cs.CL cs.LG
\\
  As reasoning modules, such as the chain-of-thought mechanism, are applied to
large language models, they achieve strong performance on various tasks such as
answering common-sense questions and solving math problems. The main challenge
now is to assess the uncertainty of answers, which can help prevent misleading
or serious hallucinations for users. Although current methods analyze long
reasoning sequences by filtering unrelated tokens and examining potential
connections between nearby tokens or sentences, the temporal spread of
confidence is often overlooked. This oversight can lead to inflated overall
confidence, even when earlier steps exhibit very low confidence. To address
this issue, we propose a novel method that incorporates inter-step attention to
analyze semantic correlations across steps. For handling long-horizon
responses, we introduce a hidden confidence mechanism to retain historical
confidence information, which is then combined with stepwise confidence to
produce a more accurate overall estimate. We evaluate our method on the GAOKAO
math benchmark and the CLadder causal reasoning dataset using mainstream
open-source large language models. Our approach is shown to outperform
state-of-the-art methods by achieving a superior balance between predictive
quality and calibration, demonstrated by strong performance on both Negative
Log-Likelihood and Expected Calibration Error.
\\ ( https://arxiv.org/abs/2601.13368 ,  917kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13387 (*cross-listing*)
Date: Mon, 19 Jan 2026 20:48:06 GMT   (5079kb)

Title: Confidence over Time: Confidence Calibration with Temporal Logic for
  Large Language Model Reasoning
Authors: Zhenjiang Mao, Anirudhh Venkat, Artem Bisliouk, Akshat Kothiyal,
  Sindhura Kumbakonam Subramanian, Saithej Singhu, Ivan Ruchkin
Categories: cs.CL cs.LG
\\
  Large Language Models (LLMs) increasingly rely on long-form, multi-step
reasoning to solve complex tasks such as mathematical problem solving and
scientific question answering. Despite strong performance, existing confidence
estimation methods typically reduce an entire reasoning process to a single
scalar score, ignoring how confidence evolves throughout the generation. As a
result, these methods are often sensitive to superficial factors such as
response length or verbosity, and struggle to distinguish correct reasoning
from confidently stated errors. We propose to characterize the stepwise
confidence signal using Signal Temporal Logic (STL). Using a discriminative STL
mining procedure, we discover temporal formulas that distinguish confidence
signals of correct and incorrect responses. Our analysis found that the STL
patterns generalize across tasks, and numeric parameters exhibit sensitivity to
individual questions. Based on these insights, we develop a confidence
estimation approach that informs STL blocks with parameter hypernetworks.
Experiments on multiple reasoning tasks show our confidence scores are more
calibrated than the baselines.
\\ ( https://arxiv.org/abs/2601.13387 ,  5079kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13410 (*cross-listing*)
Date: Mon, 19 Jan 2026 21:42:02 GMT   (628kb)

Title: Classifiers in High Dimensional Hilbert Metrics
Authors: Aditya Acharya, Auguste H. Gezalyan, David M. Mount
Categories: cs.CG cs.LG
\\
  Classifying points in high dimensional spaces is a fundamental geometric
problem in machine learning. In this paper, we address classifying points in
the $d$-dimensional Hilbert polygonal metric. The Hilbert metric is a
generalization of the Cayley-Klein hyperbolic distance to arbitrary convex
bodies and has a diverse range of applications in machine learning and convex
geometry. We first present an efficient LP-based algorithm in the metric for
the large-margin SVM problem. Our algorithm runs in time polynomial to the
number of points, bounding facets, and dimension. This is a significant
improvement on previous works, which either provide no theoretical guarantees
on running time, or suffer from exponential runtime. We also consider the
closely related Funk metric. We also present efficient algorithms for the
soft-margin SVM problem and for nearest neighbor-based classification in the
Hilbert metric.
\\ ( https://arxiv.org/abs/2601.13410 ,  628kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13433 (*cross-listing*)
Date: Mon, 19 Jan 2026 22:37:30 GMT   (781kb)

Title: Trust Me, I'm an Expert: Decoding and Steering Authority Bias in Large
  Language Models
Authors: Priyanka Mary Mammen, Emil Joswin, Shankar Venkitachalam
Categories: cs.CL cs.LG
\\
  Prior research demonstrates that performance of language models on reasoning
tasks can be influenced by suggestions, hints and endorsements. However, the
influence of endorsement source credibility remains underexplored. We
investigate whether language models exhibit systematic bias based on the
perceived expertise of the provider of the endorsement. Across 4 datasets
spanning mathematical, legal, and medical reasoning, we evaluate 11 models
using personas representing four expertise levels per domain. Our results
reveal that models are increasingly susceptible to incorrect/misleading
endorsements as source expertise increases, with higher-authority sources
inducing not only accuracy degradation but also increased confidence in wrong
answers. We also show that this authority bias is mechanistically encoded
within the model and a model can be steered away from the bias, thereby
improving its performance even when an expert gives a misleading endorsement.
\\ ( https://arxiv.org/abs/2601.13433 ,  781kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13436 (*cross-listing*)
Date: Mon, 19 Jan 2026 22:47:39 GMT   (80kb)

Title: Distribution-Free Confidence Ellipsoids for Ridge Regression with PAC
  Bounds
Authors: Szabolcs Szentp\'eteri, Bal\'azs Csan\'ad Cs\'aji
Categories: stat.ML cs.LG cs.SY eess.SP eess.SY math.ST stat.TH
\\
  Linearly parametrized models are widely used in control and signal
processing, with the least-squares (LS) estimate being the archetypical
solution. When the input is insufficiently exciting, the LS problem may be
unsolvable or numerically unstable. This issue can be resolved through
regularization, typically with ridge regression. Although regularized
estimators reduce the variance error, it remains important to quantify their
estimation uncertainty. A possible approach for linear regression is to
construct confidence ellipsoids with the Sign-Perturbed Sums (SPS) ellipsoidal
outer approximation (EOA) algorithm. The SPS EOA builds non-asymptotic
confidence ellipsoids under the assumption that the noises are independent and
symmetric about zero. This paper introduces an extension of the SPS EOA
algorithm to ridge regression, and derives probably approximately correct (PAC)
upper bounds for the resulting region sizes. Compared with previous analyses,
our result explicitly show how the regularization parameter affects the region
sizes, and provide tighter bounds under weaker excitation assumptions. Finally,
the practical effect of regularization is also demonstrated via simulation
experiments.
\\ ( https://arxiv.org/abs/2601.13436 ,  80kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13489 (*cross-listing*)
Date: Tue, 20 Jan 2026 00:54:28 GMT   (31kb)

Title: Bridging the Gap Between Estimated and True Regret Towards Reliable
  Regret Estimation in Deep Learning based Mechanism Design
Authors: Shuyuan You, Zhiqiang Zhuang, Kewen Wang, Zhe Wang
Categories: cs.GT cs.LG econ.GN q-fin.EC
\\
  Recent advances, such as RegretNet, ALGnet, RegretFormer and CITransNet, use
deep learning to approximate optimal multi item auctions by relaxing incentive
compatibility (IC) and measuring its violation via ex post regret. However, the
true accuracy of these regret estimates remains unclear. Computing exact regret
is computationally intractable, and current models rely on gradient based
optimizers whose outcomes depend heavily on hyperparameter choices. Through
extensive experiments, we reveal that existing methods systematically
underestimate actual regret (In some models, the true regret is several hundred
times larger than the reported regret), leading to overstated claims of IC and
revenue. To address this issue, we derive a lower bound on regret and introduce
an efficient item wise regret approximation. Building on this, we propose a
guided refinement procedure that substantially improves regret estimation
accuracy while reducing computational cost. Our method provides a more reliable
foundation for evaluating incentive compatibility in deep learning based
auction mechanisms and highlights the need to reassess prior performance claims
in this area.
\\ ( https://arxiv.org/abs/2601.13489 ,  31kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13519 (*cross-listing*)
Date: Tue, 20 Jan 2026 02:11:01 GMT   (82kb)

Title: Small Gradient Norm Regret for Online Convex Optimization
Authors: Wenzhi Gao, Chang He, Madeleine Udell
Categories: stat.ML cs.LG math.OC
\\
  This paper introduces a new problem-dependent regret measure for online
convex optimization with smooth losses. The notion, which we call the $G^\star$
regret, depends on the cumulative squared gradient norm evaluated at the
decision in hindsight $\sum_{t=1}^T \|\nabla \ell(x^\star)\|^2$. We show that
the $G^\star$ regret strictly refines the existing $L^\star$ (small loss)
regret, and that it can be arbitrarily sharper when the losses have vanishing
curvature around the hindsight decision. We establish upper and lower bounds on
the $G^\star$ regret and extend our results to dynamic regret and bandit
settings. As a byproduct, we refine the existing convergence analysis of
stochastic optimization algorithms in the interpolation regime. Some
experiments validate our theoretical findings.
\\ ( https://arxiv.org/abs/2601.13519 ,  82kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13542 (*cross-listing*)
Date: Tue, 20 Jan 2026 03:03:32 GMT   (7125kb)

Title: Refined Gradient-Based Temperature Optimization for the Replica-Exchange
  Monte-Carlo Method
Authors: Tatsuya Miyata, Shunta Arai, Satoshi Takabe
Categories: physics.comp-ph cs.LG
Comments: 15 pages
\\
  The replica-exchange Monte-Carlo (RXMC) method is a powerful Markov-chain
Monte-Carlo algorithm for sampling from multi-modal distributions, which are
challenging for conventional methods. The sampling efficiency of the RXMC
method depends highly on the selection of the temperatures, and finding optimal
temperatures remains a challenge. In this study, we propose a refined online
temperature selection method by extending the gradient-based optimization
framework proposed previously. Building upon the existing temperature update
approach, we introduce a reparameterization technique to strictly enforce
physical constraints, such as the monotonic ordering of inverse temperatures,
which were not explicitly addressed in the original formulation. The proposed
method defines the variance of acceptance rates between adjacent replicas as a
loss function, estimates its gradient using differential information from the
sampling process, and optimizes the temperatures via gradient descent. We
demonstrate the effectiveness of our method through experiments on benchmark
spin systems, including the two-dimensional ferromagnetic Ising model, the
two-dimensional ferromagnetic XY model, and the three-dimensional
Edwards-Anderson model. Our results show that the method successfully achieves
uniform acceptance rates and reduces round-trip times across the temperature
space. Furthermore, our proposed method offers a significant advantage over
recently proposed policy gradient method that require careful hyperparameter
tuning, while simultaneously preventing the constraint violations that
destabilize optimization.
\\ ( https://arxiv.org/abs/2601.13542 ,  7125kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13602 (*cross-listing*)
Date: Tue, 20 Jan 2026 05:06:26 GMT   (4665kb)

Title: An Elementary Approach to Scheduling in Generative Diffusion Models
Authors: Qiang Sun, H. Vincent Poor and Wenyi Zhang
Categories: cs.IT cs.LG math.IT
\\
  An elementary approach to characterizing the impact of noise scheduling and
time discretization in generative diffusion models is developed. Considering a
simplified model where the source distribution is multivariate Gaussian with a
given covariance matrix, the explicit closed-form evolution trajectory of the
distributions across reverse sampling steps is derived, and consequently, the
Kullback-Leibler (KL) divergence between the source distribution and the
reverse sampling output is obtained. The effect of the number of time
discretization steps on the convergence of this KL divergence is studied via
the Euler-Maclaurin expansion. An optimization problem is formulated, and its
solution noise schedule is obtained via calculus of variations, shown to follow
a tangent law whose coefficient is determined by the eigenvalues of the source
covariance matrix. For an alternative scenario, more realistic in practice,
where pretrained models have been obtained for some given noise schedules, the
KL divergence also provides a measure to compare different time discretization
strategies in reverse sampling. Experiments across different datasets and
pretrained models demonstrate that the time discretization strategy selected by
our approach consistently outperforms baseline and search-based strategies,
particularly when the budget on the number of function evaluations is very
tight.
\\ ( https://arxiv.org/abs/2601.13602 ,  4665kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13604 (*cross-listing*)
Date: Tue, 20 Jan 2026 05:09:52 GMT   (1983kb)

Title: Optimizing Parallel Schemes with Lyapunov Exponents and kNN-LLE
  Estimation
Authors: Mudassir Shams, Andrei Velichko and Bruno Carpentieri
Categories: math.NA cs.LG cs.NA math.DS
Comments: 25 pages, 9 figures, 10 tables
\\
  Inverse parallel schemes remain indispensable tools for computing the roots
of nonlinear systems, yet their dynamical behavior can be unexpectedly rich,
ranging from strong contraction to oscillatory or chaotic transients depending
on the choice of algorithmic parameters and initial states. A unified
analytical-data-driven methodology for identifying, measuring, and reducing
such instabilities in a family of uni-parametric inverse parallel solvers is
presented in this study. On the theoretical side, we derive stability and
bifurcation characterizations of the underlying iterative maps, identifying
parameter regions associated with periodic or chaotic behavior. On the
computational side, we introduce a micro-series pipeline based on kNN-driven
estimation of the local largest Lyapunov exponent (LLE), applied to scalar time
series derived from solver trajectories. The resulting sliding-window Lyapunov
profiles provide fine-grained, real-time diagnostics of contractive or unstable
phases and reveal transient behaviors not captured by coarse linearized
analysis. Leveraging this correspondence, we introduce a Lyapunov-informed
parameter selection strategy that identifies solver settings associated with
stable behavior, particularly when the estimated LLE indicates persistent
instability. Comprehensive experiments on ensembles of perturbed initial
guesses demonstrate close agreement between the theoretical stability diagrams
and empirical Lyapunov profiles, and show that the proposed adaptive mechanism
significantly improves robustness. The study establishes micro-series Lyapunov
analysis as a practical, interpretable tool for constructing self-stabilizing
root-finding schemes and opens avenues for extending such diagnostics to
higher-dimensional or noise-contaminated problems.
\\ ( https://arxiv.org/abs/2601.13604 ,  1983kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13642 (*cross-listing*)
Date: Tue, 20 Jan 2026 06:21:54 GMT   (90kb)

Title: Sample Complexity of Average-Reward Q-Learning: From Single-agent to
  Federated Reinforcement Learning
Authors: Yuchen Jiao, Jiin Woo, Gen Li, Gauri Joshi, Yuejie Chi
Categories: stat.ML cs.LG
\\
  Average-reward reinforcement learning offers a principled framework for
long-term decision-making by maximizing the mean reward per time step. Although
Q-learning is a widely used model-free algorithm with established sample
complexity in discounted and finite-horizon Markov decision processes (MDPs),
its theoretical guarantees for average-reward settings remain limited. This
work studies a simple but effective Q-learning algorithm for average-reward
MDPs with finite state and action spaces under the weakly communicating
assumption, covering both single-agent and federated scenarios. For the
single-agent case, we show that Q-learning with carefully chosen parameters
achieves sample complexity
$\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{\varepsilon^3}\right)$,
where $\|h^{\star}\|_{\mathsf{sp}}$ is the span norm of the bias function,
improving previous results by at least a factor of
$\frac{\|h^{\star}\|_{\mathsf{sp}}^2}{\varepsilon^2}$. In the federated setting
with $M$ agents, we prove that collaboration reduces the per-agent sample
complexity to
$\widetilde{O}\left(\frac{|\mathcal{S}||\mathcal{A}|\|h^{\star}\|_{\mathsf{sp}}^3}{M\varepsilon^3}\right)$,
with only
$\widetilde{O}\left(\frac{\|h^{\star}\|_{\mathsf{sp}}}{\varepsilon}\right)$
communication rounds required. These results establish the first federated
Q-learning algorithm for average-reward MDPs, with provable efficiency in both
sample and communication complexity.
\\ ( https://arxiv.org/abs/2601.13642 ,  90kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13644 (*cross-listing*)
Date: Tue, 20 Jan 2026 06:27:09 GMT   (138kb)

Title: Towards Token-Level Text Anomaly Detection
Authors: Yang Cao, Bicheng Yu, Sikun Yang, Ming Liu, Yujiu Yang
Categories: cs.CL cs.LG
Comments: WWW 2026
\\
  Despite significant progress in text anomaly detection for web applications
such as spam filtering and fake news detection, existing methods are
fundamentally limited to document-level analysis, unable to identify which
specific parts of a text are anomalous. We introduce token-level anomaly
detection, a novel paradigm that enables fine-grained localization of anomalies
within text. We formally define text anomalies at both document and
token-levels, and propose a unified detection framework that operates across
multiple levels. To facilitate research in this direction, we collect and
annotate three benchmark datasets spanning spam, reviews and grammar errors
with token-level labels. Experimental results demonstrate that our framework
get better performance than other 6 baselines, opening new possibilities for
precise anomaly localization in text. All the codes and data are publicly
available on https://github.com/charles-cao/TokenCore.
\\ ( https://arxiv.org/abs/2601.13644 ,  138kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13662 (*cross-listing*)
Date: Tue, 20 Jan 2026 07:01:14 GMT   (2832kb)

Title: Reinforcement Learning for Opportunistic Routing in Software-Defined
  LEO-Terrestrial Systems
Authors: Sivaram Krishnan, Zhouyou Gu, Jihong Park, Sung-Min Oh, and Jinho Choi
Categories: cs.NI cs.LG
\\
  The proliferation of large-scale low Earth orbit (LEO) satellite
constellations is driving the need for intelligent routing strategies that can
effectively deliver data to terrestrial networks under rapidly time-varying
topologies and intermittent gateway visibility. Leveraging the global control
capabilities of a geostationary (GEO)-resident software-defined networking
(SDN) controller, we introduce opportunistic routing, which aims to minimize
delivery delay by forwarding packets to any currently available ground gateways
rather than fixed destinations. This makes it a promising approach for
achieving low-latency and robust data delivery in highly dynamic LEO networks.
Specifically, we formulate a constrained stochastic optimization problem and
employ a residual reinforcement learning framework to optimize opportunistic
routing for reducing transmission delay. Simulation results over multiple days
of orbital data demonstrate that our method achieves significant improvements
in queue length reduction compared to classical backpressure and other
well-known queueing algorithms.
\\ ( https://arxiv.org/abs/2601.13662 ,  2832kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13708 (*cross-listing*)
Date: Tue, 20 Jan 2026 08:04:57 GMT   (18076kb)

Title: Generative Adversarial Networks for Resource State Generation
Authors: Shahbaz Shaik, Sourav Chatterjee, Sayantan Pramanik, Indranil
  Chakrabarty
Categories: quant-ph cs.LG
\\
  We introduce a physics-informed Generative Adversarial Network framework that
recasts quantum resource-state generation as an inverse-design task. By
embedding task-specific utility functions into training, the model learns to
generate valid two-qubit states optimized for teleportation and entanglement
broadcasting. Comparing decomposition-based and direct-generation architectures
reveals that structural enforcement of Hermiticity, trace-one, and positivity
yields higher fidelity and training stability than loss-only approaches. The
framework reproduces theoretical resource boundaries for Werner-like and
Bell-diagonal states with fidelities exceeding ~98%, establishing adversarial
learning as a lightweight yet effective method for constraint-driven
quantum-state discovery. This approach provides a scalable foundation for
automated design of tailored quantum resources for information-processing
applications, exemplified with teleportation and broadcasting of entanglement,
and it opens up the possibility of using such states in efficient quantum
network design.
\\ ( https://arxiv.org/abs/2601.13708 ,  18076kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13731 (*cross-listing*)
Date: Tue, 20 Jan 2026 08:40:35 GMT   (712kb)

Title: Breaking the Data Barrier in Learning Symbolic Computation: A Case Study
  on Variable Ordering Suggestion for Cylindrical Algebraic Decomposition
Authors: Rui-Juan Jing, Yuegang Zhao, Changbo Chen
Categories: cs.SC cs.LG
\\
  Symbolic computation, powered by modern computer algebra systems, has
important applications in mathematical reasoning through exact deep
computations. The efficiency of symbolic computation is largely constrained by
such deep computations in high dimension. This creates a fundamental barrier on
labelled data acquisition if leveraging supervised deep learning to accelerate
symbolic computation. Cylindrical algebraic decomposition (CAD) is a pillar
symbolic computation method for reasoning with first-order logic formulas over
reals with many applications in formal verification and automatic theorem
proving. Variable orderings have a huge impact on its efficiency. Impeded by
the difficulty to acquire abundant labelled data, existing learning-based
approaches are only competitive with the best expert-based heuristics. In this
work, we address this problem by designing a series of intimately connected
tasks for which a large amount of annotated data can be easily obtained. We
pre-train a Transformer model with these data and then fine-tune it on the
datasets for CAD ordering. Experiments on publicly available CAD ordering
datasets show that on average the orderings predicted by the new model are
significantly better than those suggested by the best heuristic methods.
\\ ( https://arxiv.org/abs/2601.13731 ,  712kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13745 (*cross-listing*)
Date: Tue, 20 Jan 2026 09:02:02 GMT   (1085kb)

Title: Variational Dual-path Attention Network for CSI-Based Gesture
  Recognition
Authors: N.Zhang
Categories: cs.NI cs.LG
Comments: 8 pages, 7 figures, 2 tables
\\
  Wi-Fi gesture recognition based on Channel State Information (CSI) is
challenged by high-dimensional noise and resource constraints on edge devices.
Prevailing end-to-end models tightly couple feature extraction with
classification, overlooking the inherent time-frequency sparsity of CSI and
leading to redundancy and poor generalization. To address this, this paper
proposes a lightweight feature preprocessing module--the Variational Dual-path
Attention Network (VDAN). It performs structured feature refinement through
frequency-domain filtering and temporal detection. Variational inference is
introduced to model the uncertainty in attention weights, thereby enhancing
robustness to noise. The design principles of the module are explained from the
perspectives of the information bottleneck and regularization. Experiments on a
public dataset demonstrate that the learned attention weights align with the
physical sparse characteristics of CSI, verifying its interpretability. This
work provides an efficient and explainable front-end processing solution for
resource-constrained wireless sensing systems.
\\ ( https://arxiv.org/abs/2601.13745 ,  1085kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13751 (*cross-listing*)
Date: Tue, 20 Jan 2026 09:05:43 GMT   (11535kb)

Title: HiT: History-Injection Transformers for Onboard Continuous Flood Change
  Detection
Authors: Daniel Kyselica, Jon\'a\v{s} Herec, Oliver Kutis, and Rado
  Pito\v{n}\'ak
Categories: cs.CV cs.LG
Comments: 19 pages, 9 figures, submitted to conference
MSC-class: 68T07
ACM-class: I.2.10; I.4.6; I.4.10
\\
  Natural disaster monitoring through continuous satellite observation requires
processing multi-temporal data under strict operational constraints. This paper
addresses flood detection, a critical application for hazard management, by
developing an onboard change detection system that operates within the memory
and computational limits of small satellites. We propose History Injection
mechanism for Transformer models (HiT), that maintains historical context from
previous observations while reducing data storage by over 99\% of original
image size. Moreover, testing on the STTORM-CD flood dataset confirms that the
HiT mechanism within the Prithvi-tiny foundation model maintains detection
accuracy compared to the bitemporal baseline. The proposed HiT-Prithvi model
achieved 43 FPS on Jetson Orin Nano, a representative onboard hardware used in
nanosats. This work establishes a practical framework for satellite-based
continuous monitoring of natural disasters, supporting real-time hazard
assessment without dependency on ground-based processing infrastructure.
Architecture as well as model checkpoints is available at
https://github.com/zaitra/HiT-change-detection
\\ ( https://arxiv.org/abs/2601.13751 ,  11535kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13806 (*cross-listing*)
Date: Tue, 20 Jan 2026 10:06:34 GMT   (145kb)

Title: Knowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning
Authors: Dezhao Song, Guglielmo Bonifazi, Frank Schilder, Jonathan Richard
  Schwarz
Categories: cs.CL cs.LG
\\
  LLM post-training has primarily relied on large text corpora and human
feedback, without capturing the structure of domain knowledge. This has caused
models to struggle dealing with complex reasoning tasks, especially for
high-stakes professional domains. In Law, reasoning requires deep understanding
of the relations between various legal concepts, a key component missing in
current LLM post-training. In this paper, we propose a knowledge graph
(KG)-assisted approach for enhancing LLMs' reasoning capability in Legal that
is generalizable to other high-stakes domains. We model key legal concepts by
following the \textbf{IRAC} (Issue, Rule, Analysis and Conclusion) framework,
and construct a KG with 12K legal cases. We then produce training data using
our IRAC KG, and conduct both Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO) with three state-of-the-art (SOTA) LLMs (30B, 49B
and 70B), varying architecture and base model family. Our post-trained models
obtained better average performance on 4/5 diverse legal benchmarks (14 tasks)
than baselines. In particular, our 70B DPO model achieved the best score on 4/6
reasoning tasks, among baselines and a 141B SOTA legal LLM, demonstrating the
effectiveness of our KG for enhancing LLMs' legal reasoning capability.
\\ ( https://arxiv.org/abs/2601.13806 ,  145kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13816 (*cross-listing*)
Date: Tue, 20 Jan 2026 10:23:23 GMT   (4121kb)

Title: Discriminant Learning-based Colorspace for Blade Segmentation
Authors: Ra\"ul P\'erez-Gonzalo, Andreas Espersen, Antonio Agudo
Categories: cs.CV cs.LG
Comments: Accepted to ICASSP 2026
\\
  Suboptimal color representation often hinders accurate image segmentation,
yet many modern algorithms neglect this critical preprocessing step. This work
presents a novel multidimensional nonlinear discriminant analysis algorithm,
Colorspace Discriminant Analysis (CSDA), for improved segmentation. Extending
Linear Discriminant Analysis into a deep learning context, CSDA customizes
color representation by maximizing multidimensional signed inter-class
separability while minimizing intra-class variability through a generalized
discriminative loss. To ensure stable training, we introduce three alternative
losses that enable end-to-end optimization of both the discriminative
colorspace and segmentation process. Experiments on wind turbine blade data
demonstrate significant accuracy gains, emphasizing the importance of tailored
preprocessing in domain-specific segmentation.
\\ ( https://arxiv.org/abs/2601.13816 ,  4121kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13817 (*cross-listing*)
Date: Tue, 20 Jan 2026 10:24:10 GMT   (234kb)

Title: Device Association and Resource Allocation for Hierarchical Split
  Federated Learning in Space-Air-Ground Integrated Network
Authors: Haitao Zhao, Xiaoyu Tang, Bo Xu, Jinlong Sun, Linghao Zhang
Categories: cs.DC cs.LG
\\
  6G facilitates deployment of Federated Learning (FL) in the Space-Air-Ground
Integrated Network (SAGIN), yet FL confronts challenges such as resource
constrained and unbalanced data distribution. To address these issues, this
paper proposes a Hierarchical Split Federated Learning (HSFL) framework and
derives its upper bound of loss function. To minimize the weighted sum of
training loss and latency, we formulate a joint optimization problem that
integrates device association, model split layer selection, and resource
allocation. We decompose the original problem into several subproblems, where
an iterative optimization algorithm for device association and resource
allocation based on brute-force split point search is proposed. Simulation
results demonstrate that the proposed algorithm can effectively balance
training efficiency and model accuracy for FL in SAGIN.
\\ ( https://arxiv.org/abs/2601.13817 ,  234kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13849 (*cross-listing*)
Date: Tue, 20 Jan 2026 11:02:03 GMT   (1098kb)

Title: Co-Initialization of Control Filter and Secondary Path via Meta-Learning
  for Active Noise Control
Authors: Ziyi Yang, Li Rao, Zhengding Luo, Dongyuan Shi, Qirui Huang, and
  Woon-Seng Gan
Categories: eess.AS cs.LG eess.SP
\\
  Active noise control (ANC) must adapt quickly when the acoustic environment
changes, yet early performance is largely dictated by initialization. We
address this with a Model-Agnostic Meta-Learning (MAML) co-initialization that
jointly sets the control filter and the secondary-path model for FxLMS-based
ANC while keeping the runtime algorithm unchanged. The initializer is
pre-trained on a small set of measured paths using short two-phase inner loops
that mimic identification followed by residual-noise reduction, and is applied
by simply setting the learned initial coefficients. In an online secondary path
modeling FxLMS testbed, it yields lower early-stage error, shorter
time-to-target, reduced auxiliary-noise energy, and faster recovery after path
changes than a baseline without re-initialization. The method provides a simple
fast start for feedforward ANC under environment changes, requiring a small set
of paths to pre-train.
\\ ( https://arxiv.org/abs/2601.13849 ,  1098kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13852 (*cross-listing*)
Date: Tue, 20 Jan 2026 11:03:51 GMT   (1374kb)

Title: Probabilistic Deep Discriminant Analysis for Wind Blade Segmentation
Authors: Ra\"ul P\'erez-Gonzalo, Andreas Espersen, Antonio Agudo
Categories: cs.CV cs.LG
Comments: Accepted to ICASSP 2026
\\
  Linear discriminant analysis improves class separability but struggles with
non-linearly separable data. To overcome this, we introduce Deep Discriminant
Analysis (DDA), which directly optimizes the Fisher criterion utilizing deep
networks. To ensure stable training and avoid computational instabilities, we
incorporate signed between-class variance, bound outputs with a sigmoid
function, and convert multiplicative relationships into additive ones. We
present two stable DDA loss functions and augment them with a probability loss,
resulting in Probabilistic DDA (PDDA). PDDA effectively minimizes class overlap
in output distributions, producing highly confident predictions with reduced
within-class variance. When applied to wind blade segmentation, PDDA showcases
notable advances in performance and consistency, critical for wind energy
maintenance. To our knowledge, this is the first application of DDA to image
segmentation.
\\ ( https://arxiv.org/abs/2601.13852 ,  1374kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13874 (*cross-listing*)
Date: Tue, 20 Jan 2026 11:41:32 GMT   (352kb)

Title: Unified Unbiased Variance Estimation for MMD: Robust Finite-Sample
  Performance with Imbalanced Data and Exact Acceleration under Null and
  Alternative Hypotheses
Authors: Shijie Zhong, Jiangfeng Fu, Yikun Yang
Categories: stat.ML cs.LG
ACM-class: I.2.6; F.2.2; G.3
\\
  The maximum mean discrepancy (MMD) is a kernel-based nonparametric statistic
for two-sample testing, whose inferential accuracy depends critically on
variance characterization. Existing work provides various finite-sample
estimators of the MMD variance, often differing under the null and alternative
hypotheses and across balanced or imbalanced sampling schemes. In this paper,
we study the variance of the MMD statistic through its U-statistic
representation and Hoeffding decomposition, and establish a unified
finite-sample characterization covering different hypotheses and sample
configurations. Building on this analysis, we propose an exact acceleration
method for the univariate case under the Laplacian kernel, which reduces the
overall computational complexity from $\mathcal O(n^2)$ to $\mathcal O(n \log
n)$.
\\ ( https://arxiv.org/abs/2601.13874 ,  352kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13926 (*cross-listing*)
Date: Tue, 20 Jan 2026 12:58:31 GMT   (1981kb)

Title: SCG With Your Phone: Diagnosis of Rhythmic Spectrum Disorders in Field
  Conditions
Authors: Peter Golenderov, Yaroslav Matushenko, Anastasia Tushina, Michal
  Barodkin
Categories: q-bio.QM cs.LG eess.SP
\\
  Aortic valve opening (AO) events are crucial for detecting frequency and
rhythm disorders, especially in real-world settings where seismocardiography
(SCG) signals collected via consumer smartphones are subject to noise, motion
artifacts, and variability caused by device heterogeneity. In this work, we
present a robust deep-learning framework for SCG segmentation and rhythm
analysis using accelerometer recordings obtained with consumer smartphones. We
develop an enhanced U-Net v3 architecture that integrates multi-scale
convolutions, residual connections, and attention gates, enabling reliable
segmentation of noisy SCG signals. A dedicated post-processing pipeline
converts probability masks into precise AO timestamps, whereas a novel adaptive
3D-to-1D projection method ensures robustness to arbitrary smartphone
orientation. Experimental results demonstrate that the proposed method achieves
consistently high accuracy and robustness across various device types and
unsupervised data-collection conditions. Our approach enables practical,
low-cost, and automated cardiac-rhythm monitoring using everyday mobile
devices, paving the way for scalable, field-deployable cardiovascular
assessment and future multimodal diagnostic systems.
\\ ( https://arxiv.org/abs/2601.13926 ,  1981kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13931 (*cross-listing*)
Date: Tue, 20 Jan 2026 13:06:48 GMT   (185kb)

Title: Towards Effective Negation Modeling in Joint Audio-Text Models for Music
Authors: Yannis Vasilakis, Rachel Bittner, Johan Pauwels
Categories: cs.SD cs.IR cs.LG
Comments: Accepted at IEEE International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP) 2026
\\
  Joint audio-text models are widely used for music retrieval, yet they
struggle with semantic phenomena such as negation. Negation is fundamental for
distinguishing the absence (or presence) of musical elements (e.g., "with
vocals" vs. "without vocals"), but current systems fail to represent this
reliably. In this work, we investigate and mitigate this limitation by training
CLAP models from scratch on the Million Song Dataset with LP-MusicCaps-MSD
captions. We introduce negation through text augmentation and a
dissimilarity-based contrastive loss, designed to explicitly separate original
and negated captions in the joint embedding space. To evaluate progress, we
propose two protocols that frame negation modeling as retrieval and binary
classification tasks. Experiments demonstrate that both methods, individually
and combined, improve negation handling while largely preserving retrieval
performance.
\\ ( https://arxiv.org/abs/2601.13931 ,  185kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13935 (*cross-listing*)
Date: Tue, 20 Jan 2026 13:09:52 GMT   (5728kb)

Title: TrackletGPT: A Language-like GPT Framework for White Matter Tract
  Segmentation
Authors: Anoushkrit Goel, Simroop Singh, Ankita Joshi, Ranjeet Ranjan Jha,
  Chirag Ahuja, Aditya Nigam, Arnav Bhavsar
Categories: cs.CV cs.LG
Comments: Accepted at 23rd IEEE International Symposium on Biomedical Imaging
  (ISBI), 2026
\\
  White Matter Tract Segmentation is imperative for studying brain structural
connectivity, neurological disorders and neurosurgery. This task remains
complex, as tracts differ among themselves, across subjects and conditions, yet
have similar 3D structure across hemispheres and subjects. To address these
challenges, we propose TrackletGPT, a language-like GPT framework which
reintroduces sequential information in tokens using tracklets. TrackletGPT
generalises seamlessly across datasets, is fully automatic, and encodes
granular sub-streamline segments, Tracklets, scaling and refining GPT models in
Tractography Segmentation. Based on our experiments, TrackletGPT outperforms
state-of-the-art methods on average DICE, Overlap and Overreach scores on
TractoInferno and HCP datasets, even on inter-dataset experiments.
\\ ( https://arxiv.org/abs/2601.13935 ,  5728kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13945 (*cross-listing*)
Date: Tue, 20 Jan 2026 13:21:52 GMT   (7692kb)

Title: Efficient Coordination with the System-Level Shared State: An
  Embodied-AI Native Modular Framework
Authors: Yixuan Deng, Tongrun Wu, Donghao Wu, Zeyu Wei, Jiayuan Wang, Zhenglong
  Sun, Yuqing Tang and Xiaoqiang Ji
Categories: cs.RO cs.LG
\\
  As Embodied AI systems move from research prototypes to real world
deployments, they tend to evolve rapidly while remaining reliable under
workload changes and partial failures. In practice, many deployments are only
partially decoupled: middleware moves messages, but shared context and feedback
semantics are implicit, causing interface drift, cross-module interference, and
brittle recovery at scale. We present ANCHOR, a modular framework that makes
decoupling and robustness explicit system-level primitives. ANCHOR separates
(i) Canonical Records, an evolvable contract for the standardized shared state,
from (ii) a communication bus for many-to-many dissemination and
feedback-oriented coordination, forming an inspectable end-to-end loop. We
validate closed-loop feasibility on a de-identified workflow instantiation,
characterize latency distributions under varying payload sizes and publish
rates, and demonstrate automatic stream resumption after hard crashes and
restarts even with shared-memory loss. Overall, ANCHOR turns ad-hoc integration
glue into explicit contracts, enabling controlled degradation under load and
self-healing recovery for scalable deployment of closed-loop AI systems.
\\ ( https://arxiv.org/abs/2601.13945 ,  7692kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13975 (*cross-listing*)
Date: Tue, 20 Jan 2026 13:51:55 GMT   (682kb)

Title: Harmonizing the Deep: A Unified Information Pipeline for Robust Marine
  Biodiversity Assessment Across Heterogeneous Domains
Authors: Marco Piccolo and Qiwei Han and Astrid van Toor and Joachim Vanneste
Categories: cs.CV cs.LG
Comments: 9 pages, 4 figures 8 tables
\\
  Marine biodiversity monitoring requires scalability and reliability across
complex underwater environments to support conservation and invasive-species
management. Yet existing detection solutions often exhibit a pronounced
deployment gap, with performance degrading sharply when transferred to new
sites. This work establishes the foundational detection layer for a multi-year
invasive species monitoring initiative targeting Arctic and Atlantic marine
ecosystems. We address this challenge by developing a Unified Information
Pipeline that standardises heterogeneous datasets into a comparable information
flow and evaluates a fixed, deployment-relevant detector under controlled
cross-domain protocols. Across multiple domains, we find that structural
factors, such as scene composition, object density, and contextual redundancy,
explain cross-domain performance loss more strongly than visual degradation
such as turbidity, with sparse scenes inducing a characteristic "Context
Collapse" failure mode. We further validate operational feasibility by
benchmarking inference on low-cost edge hardware, showing that runtime
optimisation enables practical sampling rates for remote monitoring. The
results shift emphasis from image enhancement toward structure-aware
reliability, providing a democratised tool for consistent marine ecosystem
assessment.
\\ ( https://arxiv.org/abs/2601.13975 ,  682kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14000 (*cross-listing*)
Date: Tue, 20 Jan 2026 14:21:18 GMT   (1944kb)

Title: Group-Invariant Unsupervised Skill Discovery: Symmetry-aware Skill
  Representations for Generalizable Behavior
Authors: Junwoo Chang, Joseph Park, Roberto Horowitz, Jongmin Lee, Jongeun Choi
Categories: cs.RO cs.LG
Comments: 14 pages, 6 figures
\\
  Unsupervised skill discovery aims to acquire behavior primitives that improve
exploration and accelerate downstream task learning. However, existing
approaches often ignore the geometric symmetries of physical environments,
leading to redundant behaviors and sample inefficiency. To address this, we
introduce Group-Invariant Skill Discovery (GISD), a framework that explicitly
embeds group structure into the skill discovery objective. Our approach is
grounded in a theoretical guarantee: we prove that in group-symmetric
environments, the standard Wasserstein dependency measure admits a globally
optimal solution comprised of an equivariant policy and a group-invariant
scoring function. Motivated by this, we formulate the Group-Invariant
Wasserstein dependency measure, which restricts the optimization to this
symmetry-aware subspace without loss of optimality. Practically, we
parameterize the scoring function using a group Fourier representation and
define the intrinsic reward via the alignment of equivariant latent features,
ensuring that the discovered skills generalize systematically under group
transformations. Experiments on state-based and pixel-based locomotion
benchmarks demonstrate that GISD achieves broader state-space coverage and
improved efficiency in downstream task learning compared to a strong baseline.
\\ ( https://arxiv.org/abs/2601.14000 ,  1944kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14001 (*cross-listing*)
Date: Tue, 20 Jan 2026 14:22:41 GMT   (3694kb)

Title: Auditory Brain Passage Retrieval: Cross-Sensory EEG Training for Neural
  Information Retrieval
Authors: Niall McGuire and Yashar Moshfeghi
Categories: cs.IR cs.LG
Comments: Accepted At ECIR 2026
\\
  Query formulation from internal information needs remains fundamentally
challenging across all Information Retrieval paradigms due to cognitive
complexity and physical impairments. Brain Passage Retrieval (BPR) addresses
this by directly mapping EEG signals to passage representations without
intermediate text translation. However, existing BPR research exclusively uses
visual stimuli, leaving critical questions unanswered: Can auditory EEG enable
effective retrieval for voice-based interfaces and visually impaired users? Can
training on combined EEG datasets from different sensory modalities improve
performance despite severe data scarcity? We present the first systematic
investigation of auditory EEG for BPR and evaluate cross-sensory training
benefits. Using dual encoder architectures with four pooling strategies (CLS,
mean, max, multi-vector), we conduct controlled experiments comparing
auditory-only, visual-only, and combined training on the Alice (auditory) and
Nieuwland (visual) datasets. Results demonstrate that auditory EEG consistently
outperforms visual EEG, and cross-sensory training with CLS pooling achieves
substantial improvements over individual training: 31% in MRR (0.474), 43% in
Hit@1 (0.314), and 28% in Hit@10 (0.858). Critically, combined auditory EEG
models surpass BM25 text baselines (MRR: 0.474 vs 0.428), establishing neural
queries as competitive with traditional retrieval whilst enabling accessible
interfaces. These findings validate auditory neural interfaces for IR tasks and
demonstrate that cross-sensory training addresses data scarcity whilst
outperforming single-modality approaches Code:
https://github.com/NiallMcguire/Audio_BPR
\\ ( https://arxiv.org/abs/2601.14001 ,  3694kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14031 (*cross-listing*)
Date: Tue, 20 Jan 2026 14:53:24 GMT   (1245kb)

Title: Intermittent time series forecasting: local vs global models
Authors: Stefano Damato, Nicol\`o Rubattu, Dario Azzimonti and Giorgio Corani
Categories: stat.ML cs.LG
Comments: Submitted to Data Mining and Knowledge Discovery
\\
  Intermittent time series, characterised by the presence of a significant
amount of zeros, constitute a large percentage of inventory items in supply
chain. Probabilistic forecasts are needed to plan the inventory levels; the
predictive distribution should cover non-negative values, have a mass in zero
and a long upper tail. Intermittent time series are commonly forecast using
local models, which are trained individually on each time series. In the last
years global models, which are trained on a large collection of time series,
have become popular for time series forecasting. Global models are often based
on neural networks. However, they have not yet been exhaustively tested on
intermittent time series. We carry out the first study comparing
state-of-the-art local (iETS, TweedieGP) and global models (D-Linear, DeepAR,
Transformers) on intermittent time series. For neural networks models we
consider three different distribution heads suitable for intermittent time
series: negative binomial, hurdle-shifted negative binomial and Tweedie. We
use, for the first time, the last two distribution heads with neural networks.
We perform experiments on five large datasets comprising more than 40'000
real-world time series. Among neural networks D-Linear provides best accuracy;
it also consistently outperforms the local models. Moreover, it has also low
computational requirements. Transformers-based architectures are instead much
more computationally demanding and less accurate. Among the distribution heads,
the Tweedie provides the best estimates of the highest quantiles, while the
negative binomial offers overall the best performance.
\\ ( https://arxiv.org/abs/2601.14031 ,  1245kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14042 (*cross-listing*)
Date: Tue, 20 Jan 2026 15:00:10 GMT   (1042kb)

Title: Federated Balanced Learning
Authors: Jiaze Li, Haoran Xu, Wanyi Wu, Changwei Wang, Shuaiguang Li, Jianzhong
  Ju, Zhenbo Luo, Jian Luan, Youyang Qu, Longxiang Gao, Xudong Yang, Lumin Xing
Categories: cs.CV cs.LG
\\
  Federated learning is a paradigm of joint learning in which clients
collaborate by sharing model parameters instead of data. However, in the
non-iid setting, the global model experiences client drift, which can seriously
affect the final performance of the model. Previous methods tend to correct the
global model that has already deviated based on the loss function or gradient,
overlooking the impact of the client samples. In this paper, we rethink the
role of the client side and propose Federated Balanced Learning, i.e., FBL, to
prevent this issue from the beginning through sample balance on the client
side. Technically, FBL allows unbalanced data on the client side to achieve
sample balance through knowledge filling and knowledge sampling using edge-side
generation models, under the limitation of a fixed number of data samples on
clients. Furthermore, we design a Knowledge Alignment Strategy to bridge the
gap between synthetic and real data, and a Knowledge Drop Strategy to
regularize our method. Meanwhile, we scale our method to real and complex
scenarios, allowing different clients to adopt various methods, and extend our
framework to further improve performance. Numerous experiments show that our
method outperforms state-of-the-art baselines. The code is released upon
acceptance.
\\ ( https://arxiv.org/abs/2601.14042 ,  1042kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14054 (*cross-listing*)
Date: Tue, 20 Jan 2026 15:07:28 GMT   (540kb)

Title: SecureSplit: Mitigating Backdoor Attacks in Split Learning
Authors: Zhihao Dou, Dongfei Cui, Weida Wang, Anjun Gao, Yueyang Quan, Mengyao
  Ma, Viet Vo, Guangdong Bai, Zhuqing Liu, Minghong Fang
Categories: cs.CR cs.DC cs.LG
Comments: To appear in The Web Conference 2026
\\
  Split Learning (SL) offers a framework for collaborative model training that
respects data privacy by allowing participants to share the same dataset while
maintaining distinct feature sets. However, SL is susceptible to backdoor
attacks, in which malicious clients subtly alter their embeddings to insert
hidden triggers that compromise the final trained model. To address this
vulnerability, we introduce SecureSplit, a defense mechanism tailored to SL.
SecureSplit applies a dimensionality transformation strategy to accentuate
subtle differences between benign and poisoned embeddings, facilitating their
separation. With this enhanced distinction, we develop an adaptive filtering
approach that uses a majority-based voting scheme to remove contaminated
embeddings while preserving clean ones. Rigorous experiments across four
datasets (CIFAR-10, MNIST, CINIC-10, and ImageNette), five backdoor attack
scenarios, and seven alternative defenses confirm the effectiveness of
SecureSplit under various challenging conditions.
\\ ( https://arxiv.org/abs/2601.14054 ,  540kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14112 (*cross-listing*)
Date: Tue, 20 Jan 2026 16:06:34 GMT   (17727kb)

Title: Learning to Explain: Supervised Token Attribution from Transformer
  Attention Patterns
Authors: George Mihaila
Categories: cs.CL cs.LG
\\
  Explainable AI (XAI) has become critical as transformer-based models are
deployed in high-stakes applications including healthcare, legal systems, and
financial services, where opacity hinders trust and accountability.
Transformers self-attention mechanisms have proven valuable for model
interpretability, with attention weights successfully used to understand model
focus and behavior (Xu et al., 2015); (Wiegreffe and Pinter, 2019). However,
existing attention-based explanation methods rely on manually defined
aggregation strategies and fixed attribution rules (Abnar and Zuidema, 2020a);
(Chefer et al., 2021), while model-agnostic approaches (LIME, SHAP) treat the
model as a black box and incur significant computational costs through input
perturbation. We introduce Explanation Network (ExpNet), a lightweight neural
network that learns an explicit mapping from transformer attention patterns to
token-level importance scores. Unlike prior methods, ExpNet discovers optimal
attention feature combinations automatically rather than relying on
predetermined rules. We evaluate ExpNet in a challenging cross-task setting and
benchmark it against a broad spectrum of model-agnostic methods and
attention-based techniques spanning four methodological families.
\\ ( https://arxiv.org/abs/2601.14112 ,  17727kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14208 (*cross-listing*)
Date: Tue, 20 Jan 2026 18:13:03 GMT   (4918kb)

Title: Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian
  Splatting
Authors: Nitin Kulkarni, Akhil Devarashetti, Charlie Cluss, Livio Forte, Dan
  Buckmaster, Philip Schneider, Chunming Qiao, Alina Vereshchaka
Categories: cs.CV cs.GR cs.LG
Comments: 8 pages, 9 figures, Conference: IEEE International Conference on
  Machine Learning and Applications 2025 (ICMLA 2025):
  https://www.icmla-conference.org/icmla25/
\\
  Inspecting the undercarriage of used vehicles is a labor-intensive task that
requires inspectors to crouch or crawl underneath each vehicle to thoroughly
examine it. Additionally, online buyers rarely see undercarriage photos. We
present an end-to-end pipeline that utilizes a three-camera rig to capture
videos of the undercarriage as the vehicle drives over it, and produces an
interactive 3D model of the undercarriage. The 3D model enables inspectors and
customers to rotate, zoom, and slice through the undercarriage, allowing them
to detect rust, leaks, or impact damage in seconds, thereby improving both
workplace safety and buyer confidence. Our primary contribution is a rig-aware
Structure-from-Motion (SfM) pipeline specifically designed to overcome the
challenges of wide-angle lens distortion and low-parallax scenes. Our method
overcomes the challenges of wide-angle lens distortion and low-parallax scenes
by integrating precise camera calibration, synchronized video streams, and
strong geometric priors from the camera rig. We use a constrained matching
strategy with learned components, the DISK feature extractor, and the
attention-based LightGlue matcher to generate high-quality sparse point clouds
that are often unattainable with standard SfM pipelines. These point clouds
seed the Gaussian splatting process to generate photorealistic undercarriage
models that render in real-time. Our experiments and ablation studies
demonstrate that our design choices are essential to achieve state-of-the-art
quality.
\\ ( https://arxiv.org/abs/2601.14208 ,  4918kb)
------------------------------------------------------------------------------
\\
arXiv:2601.14226 (*cross-listing*)
Date: Tue, 20 Jan 2026 18:40:22 GMT   (1612kb)

Title: Deep Learning Approaches to Quantum Error Mitigation
Authors: Leonardo Placidi, Ifan Williams, Enrico Rinaldi, Daniel Mills,
  Cristina C\^irstoiu, Vanya Eccles and Ross Duncan
Categories: quant-ph cs.LG
Comments: 48 pages
\\
  We present a systematic investigation of deep learning methods applied to
quantum error mitigation of noisy output probability distributions from
measured quantum circuits. We compare different architectures, from fully
connected neural networks to transformers, and we test different
design/training modalities, identifying sequence-to-sequence, attention-based
models as the most effective on our datasets. These models consistently produce
mitigated distributions that are closer to the ideal outputs when tested on
both simulated and real device data obtained from IBM superconducting quantum
processing units (QPU) up to five qubits. Across several different circuit
depths, our approach outperforms other baseline error mitigation techniques. We
perform a series of ablation studies to examine: how different input features
(circuit, device properties, noisy output statistics) affect performance;
cross-dataset generalization across circuit families; and transfer learning to
a different IBM QPU. We observe that generalization performance across similar
devices with the same architecture works effectively, without needing to fully
retrain models.
\\ ( https://arxiv.org/abs/2601.14226 ,  1612kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11678 (*cross-listing*)
Date: Fri, 16 Jan 2026 07:49:00 GMT   (288kb)

Title: A Survey on Mapping Digital Systems with Bill of Materials: Development,
  Practices, and Challenges
Authors: Shuai Zhang and Minzhao Lyu and Hassan Habibi Gharakheili
Categories: cs.CR cs.NI cs.SE
\\
  Modern digital ecosystems, spanning software, hardware, learning models,
datasets, and cryptographic products, continue to grow in complexity, making it
difficult for organizations to understand and manage component dependencies.
Bills of Materials (BOMs) have emerged as a structured way to document product
components, their interrelationships, and key metadata, improving visibility
and security across digital supply chains. This survey provides the first
comprehensive cross-domain review of BOM developments and practices. We start
by examining the evolution of BOM frameworks in three stages (i.e.,
pre-development, initial, and accelerated) and summarizing their core
principles, key stakeholders, and standardization efforts for hardware,
software, artificial intelligence (AI) models, datasets, and cryptographic
assets. We then review industry practices for generating BOM data, evaluating
its quality, and securely sharing it. Next, we review practical downstream uses
of BOM data, including dependency modeling, compliance verification,
operational risk assessment, and vulnerability tracking. We also discuss
academic efforts to address limitations in current BOM frameworks through
refinements, extensions, or new models tailored to emerging domains such as
data ecosystems and AI supply chains. Finally, we identify four key gaps that
limit the usability and reliability of today's BOM frameworks, motivating
future research directions.
\\ ( https://arxiv.org/abs/2601.11678 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2601.12634 (*cross-listing*)
Date: Mon, 19 Jan 2026 00:47:58 GMT   (2125kb)

Title: The Cost of Convenience: Identifying, Analyzing, and Mitigating
  Predatory Loan Applications on Android
Authors: Olawale Amos Akanji, Manuel Egele, and Gianluca Stringhini
Categories: cs.CR cs.SE
Comments: 15 pages, accepted at ACM ASIA CCS 2026
ACM-class: K.4.1; D.4.6
DOI: 10.1145/3779208.3785263
\\
  Digital lending applications, commonly referred to as loan apps, have become
a primary channel for microcredit in emerging markets. However, many of these
apps demand excessive permissions and misuse sensitive user data for coercive
debt-recovery practices, including harassment, blackmail, and public shaming
that affect both borrowers and their contacts.
  This paper presents the first cross-country measurement of loan app
compliance against both national regulations and Google's Financial Services
Policy. We analyze 434 apps drawn from official registries and app markets from
Indonesia, Kenya, Nigeria, Pakistan, and the Philippines. To operationalize
policy requirements at scale, we translate policy text into testable permission
checks using LLM-assisted policy-to-permission mapping and combine this with
static and dynamic analyses of loan apps' code and runtime behavior.
  Our findings reveal pervasive non-compliance among approved apps: 141 violate
national regulatory policy and 147 violate Google policy. Dynamic analysis
further shows that several apps transmit sensitive data (contacts, SMS,
location, media) before user signup or registration, undermining informed
consent and enabling downstream harassment of borrowers and third parties.
Following our disclosures, Google removed 93 flagged apps from Google Play,
representing over 300M cumulative installs.
  We advocate for adopting our methodology as a proactive compliance-monitoring
tool and offer targeted recommendations for regulators, platforms, and
developers to strengthen privacy protections. Overall, our results highlight
the need for coordinated enforcement and robust technical safeguards to ensure
that digital lending supports financial inclusion without compromising user
privacy or safety.
\\ ( https://arxiv.org/abs/2601.12634 ,  2125kb)
------------------------------------------------------------------------------
\\
arXiv:2601.13220 (*cross-listing*)
Date: Mon, 19 Jan 2026 16:50:21 GMT   (756kb)

Title: The Energy-Throughput Trade-off in Lossless-Compressed Source Code
  Storage
Authors: Paolo Ferragina, Francesco Tosoni
Categories: cs.DS cs.DB cs.DC cs.PF cs.SE
Comments: 8 pages, 5 figures. Camera-ready version for Greenvolve 2026
  co-located at IEEE SANER 2026
ACM-class: E.2; H.3.2; H.3.3; D.2.8
\\
  Retrieving data from large-scale source code archives is vital for AI
training, neural-based software analysis, and information retrieval, to cite a
few. This paper studies and experiments with the design of a compressed
key-value store for the indexing of large-scale source code datasets,
evaluating its trade-off among three primary computational resources:
(compressed) space occupancy, time, and energy efficiency. Extensive
experiments on a national high-performance computing infrastructure demonstrate
that different compression configurations yield distinct trade-offs, with high
compression ratios and order-of-magnitude gains in retrieval throughput and
energy efficiency. We also study data parallelism and show that, while it
significantly improves speed, scaling energy efficiency is more difficult,
reflecting the known non-energy-proportionality of modern hardware and
challenging the assumption of a direct time-energy correlation. This work
streamlines automation in energy-aware configuration tuning and standardized
green benchmarking deployable in CI/CD pipelines, thus empowering system
architects with a spectrum of Pareto-optimal energy-compression-throughput
trade-offs and actionable guidelines for building sustainable, efficient
storage backends for massive open-source code archival.
\\ ( https://arxiv.org/abs/2601.13220 ,  756kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2401.13770
replaced with revised version Tue, 20 Jan 2026 17:44:29 GMT   (306kb)

Title: AlphaMapleSAT: An MCTS-based Cube-and-Conquer SAT Solver for Hard
  Combinatorial Problems
Authors: Piyush Jha, Zhengyu Li, Zhengyang Lu, Raymond Zeng, Curtis Bright,
  Vijay Ganesh
Categories: cs.AI math.CO
Comments: Added more experiments
\\ ( https://arxiv.org/abs/2401.13770 ,  306kb)
------------------------------------------------------------------------------
\\
arXiv:2410.01553
replaced with revised version Sun, 18 Jan 2026 16:20:03 GMT   (579kb)

Title: MedQA-CS: Objective Structured Clinical Examination (OSCE)-Style
  Benchmark for Evaluating LLM Clinical Skills
Authors: Zonghai Yao, Zihao Zhang, Chaolong Tang, Xingyu Bian, Youxia Zhao,
  Zhichao Yang, Junda Wang, Huixue Zhou, Won Seok Jang, Feiyun Ouyang, Hong Yu
Categories: cs.AI cs.CL
Comments: To appear in proceedings of the Main Conference of the European
  Chapter of the Association for Computational Linguistics (EACL) 2026
\\ ( https://arxiv.org/abs/2410.01553 ,  579kb)
------------------------------------------------------------------------------
\\
arXiv:2502.16101
replaced with revised version Sat, 17 Jan 2026 03:39:49 GMT   (3051kb)

Title: Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the
  Robustness of RAG Against Misleading Retrievals
Authors: Linda Zeng, Rithwik Gupta, Divij Motwani, Yi Zhang, Diji Yang
Categories: cs.AI cs.IR
Comments: Advances in Neural Information Processing Systems (NeurIPS 2025)
\\ ( https://arxiv.org/abs/2502.16101 ,  3051kb)
------------------------------------------------------------------------------
\\
arXiv:2502.17925
replaced with revised version Sun, 18 Jan 2026 02:47:08 GMT   (1679kb)

Title: LeanProgress: Guiding Search for Neural Theorem Proving via Proof
  Progress Prediction
Authors: Robert Joseph George, Suozhi Huang, Peiyang Song, Anima Anandkumar
Categories: cs.AI
Comments: Published in TMLR (Transactions on Machine Learning Research)
ACM-class: I.2.6; I.2.8
\\ ( https://arxiv.org/abs/2502.17925 ,  1679kb)
------------------------------------------------------------------------------
\\
arXiv:2503.18825
replaced with revised version Sun, 18 Jan 2026 20:45:48 GMT   (408kb)

Title: EconEvals: Benchmarks and Litmus Tests for Economic Decision-Making by
  LLM Agents
Authors: Sara Fish, Julia Shephard, Minkai Li, Ran I. Shorrer, Yannai A.
  Gonczarowski
Categories: cs.AI cs.CL cs.GT
Comments: Major revision with updated experiments and analysis
\\ ( https://arxiv.org/abs/2503.18825 ,  408kb)
------------------------------------------------------------------------------
\\
arXiv:2504.15364
replaced with revised version Tue, 20 Jan 2026 17:55:29 GMT   (6480kb)

Title: KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM
  Inference in Resource-Constrained Environments
Authors: Junyoung Park, Dalton Jones, Matthew J Morse, Raghavv Goel, Mingu Lee,
  and Chris Lott
Categories: cs.AI
Comments: 37 pages, 19 figures, NeurIPS 2025
\\ ( https://arxiv.org/abs/2504.15364 ,  6480kb)
------------------------------------------------------------------------------
\\
arXiv:2505.08341
replaced with revised version Sun, 18 Jan 2026 09:39:51 GMT   (2277kb)

Title: Benchmarking AI scientists for omics data driven biological discovery
Authors: Erpai Luo, Jinmeng Jia, Yifan Xiong, Xiangyu Li, Xiaobo Guo, Baoqi Yu,
  Minsheng Hao, Lei Wei, Xuegong Zhang
Categories: cs.AI cs.MA q-bio.GN
\\ ( https://arxiv.org/abs/2505.08341 ,  2277kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18931
replaced with revised version Sat, 17 Jan 2026 00:21:45 GMT   (2222kb)

Title: Can Large Language Models Infer Causal Relationships from Real-World
  Text?
Authors: Ryan Saklad, Aman Chadha, Oleg Pavlov, Raha Moraffah
Categories: cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2505.18931 ,  2222kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21486
replaced with revised version Mon, 19 Jan 2026 06:50:06 GMT   (226kb)

Title: Hypothesis Generation via LLM-Automated Language Bias for ILP
Authors: Yang Yang, Jiemin Wu, Yutao Yue
Categories: cs.AI
Comments: accepted by AAAI 2026 Bridge LMReasoning
\\ ( https://arxiv.org/abs/2505.21486 ,  226kb)
------------------------------------------------------------------------------
\\
arXiv:2505.23473
replaced with revised version Tue, 20 Jan 2026 03:57:59 GMT   (5626kb)

Title: EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and
  Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions
Authors: Xiaorui Wu, Fei Li, Xiaofeng Mao, Xin Zhang, Li Zheng, Yuxiang Peng,
  Chong Teng, Donghong Ji, Zhuang Li
Categories: cs.AI
Comments: NeurIPS 2025
\\ ( https://arxiv.org/abs/2505.23473 ,  5626kb)
------------------------------------------------------------------------------
\\
arXiv:2506.13773
replaced with revised version Sat, 17 Jan 2026 21:19:08 GMT   (1488kb)

Title: Representing Time-Continuous Behavior of Cyber-Physical Systems in
  Knowledge Graphs
Authors: Milapji Singh Gill, Tom Jeleniewski, Felix Gehlhoff, Alexander Fay
Categories: cs.AI
Comments: \c{opyright} 2025 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
DOI: 10.1109/ETFA65518.2025.11205677
\\ ( https://arxiv.org/abs/2506.13773 ,  1488kb)
------------------------------------------------------------------------------
\\
arXiv:2506.14246
replaced with revised version Mon, 19 Jan 2026 16:44:25 GMT   (3106kb)

Title: Mxplainer: Explain and Learn Insights by Imitating Mahjong Agents
Authors: Lingfeng Li, Yunlong Lu, Yongyi Wang, Qifan Zheng and Wenxin Li
Categories: cs.AI
Comments: https://doi.org/10.3390/a18120738
Journal-ref: Algorithms 2025, 18(12), 738
DOI: 10.3390/a18120738
\\ ( https://arxiv.org/abs/2506.14246 ,  3106kb)
------------------------------------------------------------------------------
\\
arXiv:2507.12691
replaced with revised version Sat, 17 Jan 2026 22:28:46 GMT   (595kb)

Title: Benchmarking Deception Probes via Black-to-White Performance Boosts
Authors: Avi Parrack, Carlo Leonardo Attubato, Stefan Heimersheim
Categories: cs.AI cs.LG
Comments: Preprint. 39 pages, 11 figures, 7 tables
MSC-class: 68T01
ACM-class: I.2.7; K.4.1
\\ ( https://arxiv.org/abs/2507.12691 ,  595kb)
------------------------------------------------------------------------------
\\
arXiv:2507.21046
replaced with revised version Fri, 16 Jan 2026 20:59:08 GMT   (3766kb)

Title: A Survey of Self-Evolving Agents: What, When, How, and Where to Evolve
  on the Path to Artificial Super Intelligence
Authors: Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan,
  Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, Hongru Wang, Han
  Xiao, Yuhang Zhou, Shaokun Zhang, Jiayi Zhang, Jinyu Xiang, Yixiong Fang,
  Qiwen Zhao, Dongrui Liu, Qihan Ren, Cheng Qian, Zhenhailong Wang, Minda Hu,
  Huazheng Wang, Qingyun Wu, Heng Ji, Mengdi Wang
Categories: cs.AI
Comments: 77 pages, 9 figures, Transactions on Machine Learning Research
  (01/2026)
\\ ( https://arxiv.org/abs/2507.21046 ,  3766kb)
------------------------------------------------------------------------------
\\
arXiv:2508.01724
replaced with revised version Mon, 19 Jan 2026 09:57:24 GMT   (1133kb)

Title: ReflecSched: Solving Dynamic Flexible Job-Shop Scheduling via
  LLM-Powered Hierarchical Reflection
Authors: Shijie Cao, Yuan Yuan
Categories: cs.AI
\\ ( https://arxiv.org/abs/2508.01724 ,  1133kb)
------------------------------------------------------------------------------
\\
arXiv:2508.13634
replaced with revised version Tue, 20 Jan 2026 06:39:19 GMT   (5075kb)

Title: V2P: Visual Attention Calibration for GUI Grounding via Background
  Suppression and Center Peaking
Authors: Jikai Chen, Long Chen, Dong Wang, Qinglin Su, Zhixuan Chu, Bingguang
  Hao, Leilei Gan, Chenyi Zhuang, Jinjie Gu
Categories: cs.AI
\\ ( https://arxiv.org/abs/2508.13634 ,  5075kb)
------------------------------------------------------------------------------
\\
arXiv:2508.17825
replaced with revised version Mon, 19 Jan 2026 10:35:10 GMT   (404kb)

Title: FAIRGAMER: Evaluating Social Biases in LLM-Based Video Game NPCs
Authors: Bingkang Shi, Jen-tse Huang, Long Luo, Tianyu Zong, Hongzhu Yi,
  Yuanxiang Wang, Songlin Hu, Xiaodan Zhang, Zhongjiang Yao
Categories: cs.AI
\\ ( https://arxiv.org/abs/2508.17825 ,  404kb)
------------------------------------------------------------------------------
\\
arXiv:2508.18760
replaced with revised version Mon, 19 Jan 2026 04:26:30 GMT   (1119kb)

Title: Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating
  Abstention Failures in Large Reasoning Models
Authors: Yi Liu and Xiangyu Liu and Zequn Sun and Wei Hu
Categories: cs.AI cs.CL
Comments: Accepted in the 39th AAAI Conference on Artificial Intelligence (AAAI
  2026)
\\ ( https://arxiv.org/abs/2508.18760 ,  1119kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14750
replaced with revised version Mon, 19 Jan 2026 03:30:31 GMT   (0kb,I)

Title: Enhancing Retrieval Augmentation via Adversarial Collaboration
Authors: Letian Zhang, Guanghao Meng, Xudong Ren, Yiming Wang, Shu-Tao Xia
Categories: cs.AI
Comments: Due to some internal policies, we need to temporarily withdraw the
  paper. We will resubmit it after a further review
\\ ( https://arxiv.org/abs/2509.14750 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16444
replaced with revised version Tue, 20 Jan 2026 03:08:53 GMT   (702kb)

Title: Domain-Specific Constitutional AI: Enhancing Safety in LLM-Powered
  Mental Health Chatbots
Authors: Chenhan Lyu, Yutong Song, Pengfei Zhang, Amir M. Rahmani
Categories: cs.AI cs.LG
Comments: Accepted to 2025 IEEE 21st International Conference on Body Sensor
  Networks (BSN)
ACM-class: I.2.7; J.3; K.4.1
Journal-ref: 2025 IEEE 21st International Conference on Body Sensor Networks
  (BSN), pp. 1-4
DOI: 10.1109/BSN66969.2025.11337405
\\ ( https://arxiv.org/abs/2509.16444 ,  702kb)
------------------------------------------------------------------------------
\\
arXiv:2509.19464
replaced with revised version Sun, 18 Jan 2026 02:42:34 GMT   (1290kb)

Title: Evaluation-Aware Reinforcement Learning
Authors: Shripad Vilasrao Deshmukh, Will Schwarzer, Scott Niekum
Categories: cs.AI cs.LG
Comments: 9 pages
\\ ( https://arxiv.org/abs/2509.19464 ,  1290kb)
------------------------------------------------------------------------------
\\
arXiv:2509.24340
replaced with revised version Mon, 19 Jan 2026 12:27:39 GMT   (301kb)

Title: humancompatible.detect: a Python Toolkit for Detecting Bias in AI Models
Authors: German M. Matilla and Jiri Nemecek and Illia Kryvoviaz and Jakub
  Marecek
Categories: cs.AI
Comments: 6 pages, 5 figures
MSC-class: 68T01
\\ ( https://arxiv.org/abs/2509.24340 ,  301kb)
------------------------------------------------------------------------------
\\
arXiv:2509.25142
replaced with revised version Sat, 17 Jan 2026 21:40:52 GMT   (4693kb)

Title: Visual serial processing deficits explain divergences in human and VLM
  reasoning
Authors: Nicholas Budny, Kia Ghods, Declan Campbell, Raja Marjieh, Amogh Joshi,
  Sreejan Kumar, Jonathan D. Cohen, Taylor W. Webb, Thomas L. Griffiths
Categories: cs.AI
\\ ( https://arxiv.org/abs/2509.25142 ,  4693kb)
------------------------------------------------------------------------------
\\
arXiv:2509.25482
replaced with revised version Mon, 19 Jan 2026 14:46:24 GMT   (85kb)

Title: Message passing-based inference in an autoregressive active inference
  agent
Authors: Wouter M. Kouw, Tim N. Nisslbeck, Wouter L.N. Nuijten
Categories: cs.AI cs.LG cs.RO cs.SY eess.SY stat.ML
Comments: 14 pages, 4 figures, proceedings of the International Workshop on
  Active Inference 2025. Erratum v1: in Eq. (50), $p(y_t, \Theta, u_t \mid
  y_{*}, \mathcal{D}_k)$ should have been $p(y_t, \Theta \mid u_t, y_{*},
  \mathcal{D}_k)$
\\ ( https://arxiv.org/abs/2509.25482 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2510.00332
replaced with revised version Sat, 17 Jan 2026 06:27:25 GMT   (223kb)

Title: When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes
  Adversarial Financial Markets
Authors: Zeshi Dai, Zimo Peng, Zerui Cheng, Ryan Yihe Li
Categories: cs.AI cs.CE
Comments: 15 pages, 5 figures, 4 tables; Accepted to AAAI 2026 (AI-4-Finance
  Workshop - Oral, top 10%); In submission to ICML 2026
ACM-class: I.6.4; I.2.1
\\ ( https://arxiv.org/abs/2510.00332 ,  223kb)
------------------------------------------------------------------------------
\\
arXiv:2510.01114
replaced with revised version Mon, 19 Jan 2026 06:17:15 GMT   (1009kb)

Title: PRISM-Consult: A Panel-of-Experts Architecture for Clinician-Aligned
  Diagnosis
Authors: Lionel Levine, John Santerre, Alexander S. Young, T. Barry Levine,
  Francis Campion, Majid Sarrafzadeh
Categories: cs.AI
Comments: 8 pages, 6 figures
ACM-class: I.2.1; I.2.4; I.2.11
\\ ( https://arxiv.org/abs/2510.01114 ,  1009kb)
------------------------------------------------------------------------------
\\
arXiv:2510.05014
replaced with revised version Mon, 19 Jan 2026 16:13:58 GMT   (5507kb)

Title: Think Then Embed: Generative Context Improves Multimodal Embedding
Authors: Xuanming Cui, Jianpeng Cheng, Hong-you Chen, Satya Narayan Shukla,
  Abhijeet Awasthi, Xichen Pan, Chaitanya Ahuja, Shlok Kumar Mishra, Yonghuan
  Yang, Jun Xiao, Qi Guo, Ser-Nam Lim, Aashu Singh, Xiangjun Fan
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2510.05014 ,  5507kb)
------------------------------------------------------------------------------
\\
arXiv:2510.05134
replaced with revised version Tue, 20 Jan 2026 12:09:41 GMT   (9302kb)

Title: Structuring Reasoning for Complex Rules Beyond Flat Representations
Authors: Zhihao Yang, Ancheng Xu, Jingpeng Li, Liang Yan, Jiehui Zhou, Zhen
  Qin, Hengyu Chang, Yukun Chen, Longze Chen, Ahmadreza Argha, Hamid
  Alinejad-Rokny, Minghuan Tan, Yujun Cai, Min Yang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2510.05134 ,  9302kb)
------------------------------------------------------------------------------
\\
arXiv:2510.07593
replaced with revised version Mon, 19 Jan 2026 17:39:16 GMT   (8242kb)

Title: AgentAsk: Multi-Agent Systems Need to Ask
Authors: Bohan Lin, Kuo Yang, Zelin Tan, Yingchuan Lai, Chen Zhang, Guibin
  Zhang, Xinlei Yu, Miao Yu, Xu Wang, Yudong Zhang, Yang Wang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2510.07593 ,  8242kb)
------------------------------------------------------------------------------
\\
arXiv:2510.07772
replaced with revised version Sun, 18 Jan 2026 18:47:31 GMT   (502kb)

Title: An approach for systematic decomposition of complex llm tasks
Authors: Tianle Zhou, Jiakai Xu, Guanhong Liu, Jiaxiang Liu, Haonan Wang,
  Eugene Wu
Categories: cs.AI
\\ ( https://arxiv.org/abs/2510.07772 ,  502kb)
------------------------------------------------------------------------------
\\
arXiv:2510.13459
replaced with revised version Mon, 19 Jan 2026 15:14:38 GMT   (30002kb)

Title: Mobile Coverage Analysis using Crowdsourced Data
Authors: Timothy Wong, Tom Freeman, Joseph Feehily
Categories: cs.AI cs.CE cs.NI stat.AP
Comments: 8 pages
\\ ( https://arxiv.org/abs/2510.13459 ,  30002kb)
------------------------------------------------------------------------------
\\
arXiv:2510.16466
replaced with revised version Sat, 17 Jan 2026 05:27:00 GMT   (1627kb)

Title: ReviewSense: Transforming Customer Review Dynamics into Actionable
  Business Insights
Authors: Siddhartha Krothapalli, Kartikey Singh Bhandari, Tridib Kumar Das,
  Praveen Kumar, Naveen Suravarpu and Pratik Narang
Categories: cs.AI
Comments: 11 pages, 1 figure, 4 tables
\\ ( https://arxiv.org/abs/2510.16466 ,  1627kb)
------------------------------------------------------------------------------
\\
arXiv:2510.19314
replaced with revised version Tue, 20 Jan 2026 04:47:18 GMT   (513kb)

Title: Continual Knowledge Adaptation for Reinforcement Learning
Authors: Jinwu Hu, Zihao Lian, Zhiquan Wen, Chenghao Li, Guohao Chen, Xutao
  Wen, Bin Xiao, Mingkui Tan
Categories: cs.AI
Comments: NeurIPS 2025
\\ ( https://arxiv.org/abs/2510.19314 ,  513kb)
------------------------------------------------------------------------------
\\
arXiv:2510.19423
replaced with revised version Sun, 18 Jan 2026 10:40:06 GMT   (8496kb)

Title: ETOM: A Five-Level Benchmark for Evaluating Tool Orchestration within
  the MCP Ecosystem
Authors: Jia-Kai Dong, I-Wei Huang, Chun-Tin Wu, Yi-Tien Tsai
Categories: cs.AI
Comments: Accepted to the Findings of EACL 2026
ACM-class: I.2.0; I.2.1; I.2.4
\\ ( https://arxiv.org/abs/2510.19423 ,  8496kb)
------------------------------------------------------------------------------
\\
arXiv:2510.23045
replaced with revised version Sat, 17 Jan 2026 08:11:01 GMT   (752kb)

Title: A Survey of AI Scientists
Authors: Guiyao Tie, Pan Zhou, Lichao Sun
Categories: cs.AI
Comments: 28 pages, 9 figures, 1 table
\\ ( https://arxiv.org/abs/2510.23045 ,  752kb)
------------------------------------------------------------------------------
\\
arXiv:2510.25005
replaced with revised version Sat, 17 Jan 2026 06:48:45 GMT   (43kb)

Title: Cyclic Counterfactuals under Shift-Scale Interventions
Authors: Saptarshi Saha, Dhruv Vansraj Rathore, Utpal Garain
Categories: cs.AI cs.LG math.ST stat.ML stat.TH
Comments: Accepted at NeurIPS 2025
\\ ( https://arxiv.org/abs/2510.25005 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2510.26854
replaced with revised version Sat, 17 Jan 2026 07:26:05 GMT   (1280kb)

Title: Inverse Knowledge Search over Verifiable Reasoning: Synthesizing a
  Scientific Encyclopedia from a Long Chains-of-Thought Knowledge Base
Authors: Yu Li, Yuan Huang, Tao Wang, Caiyu Fan, Xiansheng Cai, Sihan Hu,
  Xinzijian Liu, Cheng Shi, Mingjun Xu, Zhen Wang, Yan Wang, Xiangqi Jin,
  Tianhan Zhang, Linfeng Zhang, Lei Wang, Youjin Deng, Pan Zhang, Weijie Sun,
  Xinyu Li, Weinan E, Linfeng Zhang, Zhiyuan Yao, Kun Chen
Categories: cs.AI cs.LG
Comments: 43 pages, 4 figures. This work is part of the SciencePedia project
  (sciencepedia.bohrium.com). Corrected author name spelling
\\ ( https://arxiv.org/abs/2510.26854 ,  1280kb)
------------------------------------------------------------------------------
\\
arXiv:2511.07260
replaced with revised version Mon, 19 Jan 2026 08:32:22 GMT   (1448kb)

Title: PADiff: Predictive and Adaptive Diffusion Policies for Ad Hoc Teamwork
Authors: Hohei Chan, Xinzhi Zhang, Antao Xiang, Weinan Zhang, Mengchen Zhao
Categories: cs.AI cs.LG
Comments: Accepted by AAAI 2026
\\ ( https://arxiv.org/abs/2511.07260 ,  1448kb)
------------------------------------------------------------------------------
\\
arXiv:2511.08581
replaced with revised version Mon, 19 Jan 2026 13:37:33 GMT   (146kb)

Title: DeepProofLog: Efficient Proving in Deep Stochastic Logic Programs
Authors: Ying Jiao, Rodrigo Castellano Ontiveros, Luc De Raedt, Marco Gori,
  Francesco Giannini, Michelangelo Diligenti, Giuseppe Marra
Categories: cs.AI
Comments: Accepted as an Oral at AAAI 2026
\\ ( https://arxiv.org/abs/2511.08581 ,  146kb)
------------------------------------------------------------------------------
\\
arXiv:2511.11914
replaced with revised version Sat, 17 Jan 2026 09:15:24 GMT   (8768kb)

Title: Forgetting-MarI: LLM Unlearning via Marginal Information Regularization
Authors: Shizhou Xu, Yuan Ni, Stefan Broecker, Thomas Strohmer
Categories: cs.AI cs.CL cs.CR cs.IT cs.LG math.IT
\\ ( https://arxiv.org/abs/2511.11914 ,  8768kb)
------------------------------------------------------------------------------
\\
arXiv:2511.12844
replaced with revised version Tue, 20 Jan 2026 02:54:22 GMT   (3188kb)

Title: Towards Reinforcement Learning from Neural Feedback: Mapping fNIRS
  Signals to Agent Performance
Authors: Julia Santaniello, Matthew Russell, Benson Jiang, Donatello Sassaroli,
  Robert Jacob, Jivko SInapov
Categories: cs.AI cs.LG
Comments: Accepted to the Association for the Advancement of Artificial
  Intelligence (AAAI) 2026. To appear in the AAAI 2026 Proceedings
\\ ( https://arxiv.org/abs/2511.12844 ,  3188kb)
------------------------------------------------------------------------------
\\
arXiv:2511.20321
replaced with revised version Sun, 18 Jan 2026 16:15:22 GMT   (41kb)

Title: Active Inference in Discrete State Spaces from First Principles
Authors: Patrick Kenny
Categories: cs.AI
Comments: 57 pages
\\ ( https://arxiv.org/abs/2511.20321 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2512.06705
replaced with revised version Tue, 20 Jan 2026 04:40:14 GMT   (2084kb)

Title: Academic journals' AI policies fail to curb the surge in AI-assisted
  academic writing
Authors: Yongyuan He, Yi Bu
Categories: cs.AI
Comments: 39 pages, 10 figures, and 9 tables
\\ ( https://arxiv.org/abs/2512.06705 ,  2084kb)
------------------------------------------------------------------------------
\\
arXiv:2512.15089
replaced with revised version Tue, 20 Jan 2026 08:55:05 GMT   (216kb)

Title: Beyond Fast and Slow: Cognitive-Inspired Elastic Reasoning for Large
  Language Models
Authors: Jinwu Hu, Dongjin Yang, Langyu Bian, Zhiquan Wen, Yufeng Wang, Yaofo
  Chen, Bin Xiao, Yuanqing Li, Mingkui Tan
Categories: cs.AI
Comments: under review
\\ ( https://arxiv.org/abs/2512.15089 ,  216kb)
------------------------------------------------------------------------------
\\
arXiv:2601.00814
replaced with revised version Tue, 20 Jan 2026 17:52:30 GMT   (18kb)

Title: Semantic Alignment of Multilingual Knowledge Graphs via Contextualized
  Vector Projections
Authors: Abhishek Kumar
Categories: cs.AI
\\ ( https://arxiv.org/abs/2601.00814 ,  18kb)
------------------------------------------------------------------------------
\\
arXiv:2601.01562
replaced with revised version Tue, 20 Jan 2026 04:18:13 GMT   (2752kb)

Title: Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training
  and Document Knowledge Enhancement
Authors: Mingyu Xu, Cheng Fang, Keyue Jiang, Yuqian Zheng, Yanghua Xiao,
  Baojian Zhou, Qifang Zhao, Suhang Zheng, Xiuwen Zhu, Jiyang Tang, Yongchi
  Zhao, Yijia Luo, Zhiqi Bai, Yuchi Xu, Wenbo Su, Wei Wang, Bing Zhao, Lin Qu,
  Xiaoxiao Xu
Categories: cs.AI
\\ ( https://arxiv.org/abs/2601.01562 ,  2752kb)
------------------------------------------------------------------------------
\\
arXiv:2601.05230
replaced with revised version Tue, 20 Jan 2026 14:03:19 GMT   (38656kb)

Title: Learning Latent Action World Models In The Wild
Authors: Quentin Garrido, Tushar Nagarajan, Basile Terver, Nicolas Ballas, Yann
  LeCun, Michael Rabbat
Categories: cs.AI cs.CV
Comments: 37 pages, 25 figures; updated references and experimental details
\\ ( https://arxiv.org/abs/2601.05230 ,  38656kb)
------------------------------------------------------------------------------
\\
arXiv:2601.05529
replaced with revised version Sun, 18 Jan 2026 11:03:44 GMT   (4548kb)

Title: Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision
  Making
Authors: Jua Han and Jaeyoon Seo and Jungbin Min and Jihie Kim and Jean Oh
Categories: cs.AI cs.RO
Comments: Corrected author order in metadata; manuscript unchanged
\\ ( https://arxiv.org/abs/2601.05529 ,  4548kb)
------------------------------------------------------------------------------
\\
arXiv:2601.06747
replaced with revised version Tue, 20 Jan 2026 04:03:53 GMT   (625kb)

Title: FinForge: Semi-Synthetic Financial Benchmark Generation
Authors: Glenn Matlin, Akhil Theerthala, Anant Gupta, Anirudh JM, Rayan
  Castilla, Yi Mei Ng, Sudheer Chava
Categories: cs.AI
\\ ( https://arxiv.org/abs/2601.06747 ,  625kb)
------------------------------------------------------------------------------
\\
arXiv:2601.06860
replaced with revised version Sun, 18 Jan 2026 02:13:27 GMT   (2499kb)

Title: ET-Agent: Incentivizing Effective Tool-Integrated Reasoning Agent via
  Behavior Calibration
Authors: Yifei Chen, Guanting Dong, Zhicheng Dou
Categories: cs.AI
\\ ( https://arxiv.org/abs/2601.06860 ,  2499kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08187
replaced with revised version Sat, 17 Jan 2026 13:01:51 GMT   (5163kb)

Title: Improving LLM Reasoning with Homophily-aware Structural and Semantic
  Text-Attributed Graph Compression
Authors: Zijun Di, Bin Lu, Huquan Kang, Luoyi Fu, Jiaxin Ding, Xiaoying Gan,
  Lei Zhou, Xinbing Wang, Chenghu Zhou
Categories: cs.AI
\\ ( https://arxiv.org/abs/2601.08187 ,  5163kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08382
replaced with revised version Mon, 19 Jan 2026 10:51:45 GMT   (38kb)

Title: A Qualitative Model to Reason about Object Rotations (QOR) applied to
  solve the Cube Comparison Test (CCT)
Authors: Zoe Falomir
Categories: cs.AI cs.SC
\\ ( https://arxiv.org/abs/2601.08382 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08545
replaced with revised version Sun, 18 Jan 2026 15:44:30 GMT   (911kb)

Title: Learner-Tailored Program Repair: A Solution Generator with Iterative
  Edit-Driven Retrieval Enhancement
Authors: Zhenlong Dai, Zhuoluo Zhao, Hengning Wang, Xiu Tang, Sai Wu, Chang
  Yao, Zhipeng Gao, Jingyuan Chen
Categories: cs.AI cs.CL cs.SE
Comments: Accepted by AAAI2026 main track
\\ ( https://arxiv.org/abs/2601.08545 ,  911kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08778
replaced with revised version Mon, 19 Jan 2026 20:11:33 GMT   (988kb)

Title: Pervasive Annotation Errors Break Text-to-SQL Benchmarks and
  Leaderboards
Authors: Tengjun Jin, Yoojin Choi, Yuxuan Zhu, Daniel Kang
Categories: cs.AI cs.DB
Comments: 18 pages, 14 figures, 9 tables
\\ ( https://arxiv.org/abs/2601.08778 ,  988kb)
------------------------------------------------------------------------------
\\
arXiv:2601.09269
replaced with revised version Mon, 19 Jan 2026 13:00:42 GMT   (3374kb)

Title: RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation
  Steering
Authors: Wencheng Ye, Xiaoyang Yuan, Yi Bin, Pengpeng Zeng, Hengyu Jin, Liang
  Peng and Heng Tao Shen
Categories: cs.AI
\\ ( https://arxiv.org/abs/2601.09269 ,  3374kb)
------------------------------------------------------------------------------
\\
arXiv:2601.10101
replaced with revised version Mon, 19 Jan 2026 09:34:40 GMT   (1268kb)

Title: Matrix as Plan: Structured Logical Reasoning with Feedback-Driven
  Replanning
Authors: Ke Chen, Jiandian Zeng, Zihao Peng, Guo Li, Guangxue Zhang and Tian
  Wang
Categories: cs.AI cs.CL
Comments: 12 pages, 5 figures, 2 tables. Accepted at The Web Conference (WWW)
  2026
\\ ( https://arxiv.org/abs/2601.10101 ,  1268kb)
------------------------------------------------------------------------------
\\
arXiv:2601.10462
replaced with revised version Mon, 19 Jan 2026 10:05:05 GMT   (754kb)

Title: ChartComplete: A Taxonomy-based Inclusive Chart Dataset
Authors: Ahmad Mustapha, Charbel Toumieh, and Mariette Awad
Categories: cs.AI cs.CV
Comments: 7 pages, 4 figures, 3 tables, 1 algorithm. Dataset and source code
  available at https://github.com/AI-DSCHubAUB/ChartComplete-Dataset
ACM-class: I.2.10; I.4.8
\\ ( https://arxiv.org/abs/2601.10462 ,  754kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11044
replaced with revised version Mon, 19 Jan 2026 13:21:07 GMT   (957kb)

Title: AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token
  Real-World Contexts
Authors: Keyu Li, Junhao Shi, Yang Xiao, Mohan Jiang, Jie Sun, Yunze Wu, Shijie
  Xia, Xiaojie Cai, Tianze Xu, Weiye Si, Wenjie Li, Dequan Wang, Pengfei Liu
Categories: cs.AI
\\ ( https://arxiv.org/abs/2601.11044 ,  957kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11089
replaced with revised version Mon, 19 Jan 2026 01:58:20 GMT   (2103kb)

Title: MiCA: A Mobility-Informed Causal Adapter for Lightweight Epidemic
  Forecasting
Authors: Suhan Guo, Jiahong Deng, Furao Shen
Categories: cs.AI
\\ ( https://arxiv.org/abs/2601.11089 ,  2103kb)
------------------------------------------------------------------------------
\\
arXiv:2105.02135
replaced with revised version Tue, 20 Jan 2026 15:57:19 GMT   (530kb)

Title: UVIP: Model-Free Approach to Evaluate Reinforcement Learning Algorithms
Authors: Denis Belomestny, Ilya Levin, Alexey Naumov, Sergey Samsonov
Categories: cs.LG math.OC
Comments: JOTA camera-ready version
\\ ( https://arxiv.org/abs/2105.02135 ,  530kb)
------------------------------------------------------------------------------
\\
arXiv:2208.00335
replaced with revised version Fri, 16 Jan 2026 23:37:50 GMT   (423kb)

Title: Functional Rule Extraction Method for Artificial Neural Networks
Authors: Caleb Princewill Nwokocha
Categories: cs.LG
Comments: arXiv admin note: text overlap with arXiv:1009.4984 by other authors
\\ ( https://arxiv.org/abs/2208.00335 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2301.10932
replaced with revised version Mon, 19 Jan 2026 10:32:41 GMT   (649kb)

Title: On the Global Convergence of Risk-Averse Natural Policy Gradient Methods
  with Expected Conditional Risk Measures
Authors: Xian Yu and Lei Ying
Categories: cs.LG math.OC stat.ML
\\ ( https://arxiv.org/abs/2301.10932 ,  649kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04919
replaced with revised version Sun, 18 Jan 2026 17:28:50 GMT   (31214kb)

Title: A Deep Probabilistic Flow-Based Framework for Unsupervised Cross-Domain
  Soft Sensing
Authors: Junn Yong Loo, Hwa Hui Tew, Fang Yu Leong, Ze Yang Ding, Vishnu Monn
  Baskaran, Chee-Ming Ting, Chee Pin Tan
Categories: cs.LG
Comments: Accepted at IEEE Transactions on Industrial Informatics
\\ ( https://arxiv.org/abs/2306.04919 ,  31214kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17797
replaced with revised version Tue, 20 Jan 2026 00:28:00 GMT   (1734kb)

Title: Learning to Simulate: Generative Metamodeling via Quantile Regression
Authors: L. Jeff Hong and Yanxi Hou and Qingkai Zhang and Xiaowei Zhang
Categories: cs.LG stat.ME
\\ ( https://arxiv.org/abs/2311.17797 ,  1734kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06578
replaced with revised version Tue, 20 Jan 2026 06:28:56 GMT   (2055kb)

Title: Multi-class Support Vector Machine with Maximizing Minimum Margin
Authors: Zhezheng Hao, Feiping Nie, Rong Wang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.06578 ,  2055kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10259
replaced with revised version Sun, 18 Jan 2026 22:03:03 GMT   (365kb)

Title: ComplicaCode: Enhancing Disease Complication Detection in Electronic
  Health Records through ICD Path Generation
Authors: Xiaofan Zhou
Categories: cs.LG cs.CL cs.NE
Comments: arXiv admin note: text overlap with arXiv:2305.13250
\\ ( https://arxiv.org/abs/2312.10259 ,  365kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16819
replaced with revised version Fri, 16 Jan 2026 20:20:34 GMT   (30kb)

Title: Hidden Minima in Two-Layer ReLU Networks
Authors: Yossi Arjevani
Categories: cs.LG math.OC stat.ML
\\ ( https://arxiv.org/abs/2312.16819 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01172
replaced with revised version Mon, 19 Jan 2026 12:02:42 GMT   (13674kb)

Title: Fusion of Quadratic Time-Frequency Analysis and Convolutional Neural
  Networks to Diagnose Bearing Faults Under Time-Varying Speeds
Authors: Mohammad Al-Sa'd, Tuomas Jalonen, Serkan Kiranyaz, and Moncef Gabbouj
Categories: cs.LG cs.AI cs.SY eess.SY
\\ ( https://arxiv.org/abs/2401.01172 ,  13674kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06122
replaced with revised version Tue, 20 Jan 2026 12:25:23 GMT   (7760kb)

Title: Manipulating Feature Visualizations with Gradient Slingshots
Authors: Dilyara Bareeva, Marina M.-C. H\"ohne, Alexander Warnecke, Lukas
  Pirch, Klaus-Robert M\"uller, Konrad Rieck, Sebastian Lapuschkin, Kirill
  Bykov
Categories: cs.LG cs.AI cs.CV
Comments: Accepted to NeurIPS 2025
\\ ( https://arxiv.org/abs/2401.06122 ,  7760kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00152
replaced with revised version Sun, 18 Jan 2026 02:53:12 GMT   (203kb)

Title: Deeper or Wider: A Perspective from Optimal Generalization Error with
  Sobolev Loss
Authors: Yahong Yang and Juncai He
Categories: cs.LG cs.NA math.NA stat.ML
Comments: arXiv admin note: text overlap with arXiv:2310.10766,
  arXiv:2305.08466
MSC-class: 68T05
\\ ( https://arxiv.org/abs/2402.00152 ,  203kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10818
replaced with revised version Sat, 17 Jan 2026 23:35:38 GMT   (758kb)

Title: Trading off Consistency and Dimensionality of Convex Surrogates for the
  Mode
Authors: Enrique Nueve, Bo Waggoner, Dhamma Kimpara, Jessie Finocchiaro
Categories: cs.LG stat.ML
Comments: Updated error with Bregman Losses to only Square Losses
\\ ( https://arxiv.org/abs/2402.10818 ,  758kb)
------------------------------------------------------------------------------
\\
arXiv:2405.15325
replaced with revised version Sun, 18 Jan 2026 21:32:15 GMT   (1320kb)

Title: On the Identification of Temporally Causal Representation with
  Instantaneous Dependence
Authors: Zijian Li, Yifan Shen, Kaitao Zheng, Ruichu Cai, Xiangchen Song,
  Mingming Gong, Guangyi Chen, Kun Zhang
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2405.15325 ,  1320kb)
------------------------------------------------------------------------------
\\
arXiv:2405.19650
replaced with revised version Sat, 17 Jan 2026 13:05:31 GMT   (1782kb)

Title: Few for Many: Tchebycheff Set Scalarization for Many-Objective
  Optimization
Authors: Xi Lin, Yilu Liu, Xiaoyuan Zhang, Fei Liu, Zhenkun Wang, Qingfu Zhang
Categories: cs.LG cs.AI cs.NE math.OC
Comments: Accepted by the 13th International Conference on Learning
  Representations (ICLR 2025)
\\ ( https://arxiv.org/abs/2405.19650 ,  1782kb)
------------------------------------------------------------------------------
\\
arXiv:2407.00449
replaced with revised version Sun, 18 Jan 2026 19:24:10 GMT   (16kb)

Title: Fully tensorial approach to hypercomplex neural networks
Authors: Agnieszka Niemczynowicz, Rados{\l}aw Antoni Kycia
Categories: cs.LG cs.AI cs.NE
Comments: 19 pages
MSC-class: 15A69, 15-04
\\ ( https://arxiv.org/abs/2407.00449 ,  16kb)
------------------------------------------------------------------------------
\\
arXiv:2408.14252
replaced with revised version Tue, 20 Jan 2026 09:36:50 GMT   (3071kb)

Title: An Evaluation of Explanation Methods for Black-Box Detectors of
  Machine-Generated Text
Authors: Loris Schoenegger, Yuxi Xia, Benjamin Roth
Categories: cs.LG cs.CL
Comments: 11 pages; added figures and discussion, improved writing
\\ ( https://arxiv.org/abs/2408.14252 ,  3071kb)
------------------------------------------------------------------------------
\\
arXiv:2409.00297
replaced with revised version Tue, 20 Jan 2026 05:41:37 GMT   (64kb)

Title: On Expressive Power of Quantized Neural Networks under Fixed-Point
  Arithmetic
Authors: Yeachan Park, Sejun Park, Geonho Hwang
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2409.00297 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2410.18164
replaced with revised version Sat, 17 Jan 2026 00:50:55 GMT   (287kb)

Title: TabDPT: Scaling Tabular Foundation Models on Real Data
Authors: Junwei Ma, Valentin Thomas, Rasa Hosseinzadeh, Alex Labach, Hamidreza
  Kamkari, Jesse C. Cresswell, Keyvan Golestan, Guangwei Yu, Anthony L.
  Caterini, Maksims Volkovs
Categories: cs.LG cs.AI stat.ML
Comments: Inference repo: github.com/layer6ai-labs/TabDPT-inference; Training
  repo: github.com/layer6ai-labs/TabDPT-training
Journal-ref: NeurIPS 2025 Proceedings
\\ ( https://arxiv.org/abs/2410.18164 ,  287kb)
------------------------------------------------------------------------------
\\
arXiv:2411.00515
replaced with revised version Mon, 19 Jan 2026 11:14:16 GMT   (105kb)

Title: Zero-shot Generalization in Inventory Management: Train, then Estimate
  and Decide
Authors: Tarkan Temiz\"oz, Christina Imdahl, Remco Dijkman, Douniel
  Lamghari-Idrissi, Willem van Jaarsveld
Categories: cs.LG
\\ ( https://arxiv.org/abs/2411.00515 ,  105kb)
------------------------------------------------------------------------------
\\
arXiv:2411.00839
replaced with revised version Sat, 17 Jan 2026 23:04:18 GMT   (1930kb)

Title: CausAdv: A Causal-based Framework for Detecting Adversarial Examples
Authors: Hichem Debbi
Categories: cs.LG cs.AI cs.CV stat.ME stat.ML
ACM-class: I.2.10
\\ ( https://arxiv.org/abs/2411.00839 ,  1930kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03941
replaced with revised version Mon, 19 Jan 2026 12:58:08 GMT   (189kb)

Title: Modular Deep Learning for Multivariate Time-Series: Decoupling
  Imputation and Downstream Tasks
Authors: Joseph Arul Raj, Linglong Qian and Zina Ibrahim
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2411.03941 ,  189kb)
------------------------------------------------------------------------------
\\
arXiv:2411.19769
replaced with revised version Mon, 19 Jan 2026 10:44:06 GMT   (9948kb)

Title: Riemannian Denoising Model for Molecular Structure Optimization with
  Chemical Accuracy
Authors: Jeheon Woo, Seonghwan Kim, Jun Hyeong Kim, Woo Youn Kim
Categories: cs.LG physics.chem-ph
\\ ( https://arxiv.org/abs/2411.19769 ,  9948kb)
------------------------------------------------------------------------------
\\
arXiv:2412.03938
replaced with revised version Sun, 18 Jan 2026 07:30:17 GMT   (917kb)

Title: JANUS: A Difference-Oriented Analyzer For Financial Centralization Risks
  in Smart Contracts
Authors: Wansen Wang, Pu Zhang, Renjie Ji, Wenchao Huang, Zhaoyi Meng, Yan
  Xiong
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2412.03938 ,  917kb)
------------------------------------------------------------------------------
\\
arXiv:2412.16765
replaced with revised version Mon, 19 Jan 2026 10:45:03 GMT   (79kb)

Title: Optimization Insights into Deep Diagonal Linear Networks
Authors: Hippolyte Labarri\`ere and Cesare Molinari and Lorenzo Rosasco and
  Cristian Vega and Silvia Villa
Categories: cs.LG math.OC stat.ML
\\ ( https://arxiv.org/abs/2412.16765 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2412.18134
replaced with revised version Sat, 17 Jan 2026 08:51:34 GMT   (1257kb)

Title: Learning Randomized Reductions
Authors: Ferhat Erata, Orr Paradise, Thanos Typaldos, Timos Antonopoulos,
  ThanhVu Nguyen, Shafi Goldwasser, Ruzica Piskac
Categories: cs.LG cs.CC cs.PL cs.SE
\\ ( https://arxiv.org/abs/2412.18134 ,  1257kb)
------------------------------------------------------------------------------
\\
arXiv:2501.06148
replaced with revised version Tue, 20 Jan 2026 09:07:11 GMT   (552kb)

Title: From discrete-time policies to continuous-time diffusion samplers:
  Asymptotic equivalences and faster training
Authors: Julius Berner, Lorenz Richter, Marcin Sendera, Jarrid Rector-Brooks,
  Nikolay Malkin
Categories: cs.LG stat.ML
Comments: TMLR; code: https://github.com/GFNOrg/gfn-diffusion/tree/stagger
\\ ( https://arxiv.org/abs/2501.06148 ,  552kb)
------------------------------------------------------------------------------
\\
arXiv:2501.18199
replaced with revised version Mon, 19 Jan 2026 19:58:02 GMT   (7165kb)

Title: HKAN: Hierarchical Kolmogorov-Arnold Network without Backpropagation
Authors: Grzegorz Dudek, Tomasz Rodak
Categories: cs.LG cs.AI
Comments: 16 pages, 9 figures
\\ ( https://arxiv.org/abs/2501.18199 ,  7165kb)
------------------------------------------------------------------------------
\\
arXiv:2502.04829
replaced with revised version Mon, 19 Jan 2026 20:14:58 GMT   (6051kb)

Title: Optimistic Gradient Learning with Hessian Corrections for
  High-Dimensional Black-Box Optimization
Authors: Yedidya Kfir, Elad Sarafian, Sarit Kraus, Yoram Louzoun
Categories: cs.LG cs.AI
Comments: We develop a black-box optimization algorithm that learns gradients
  with neural models and can be applied to solve non-convex high dimensional
  real-world problems
Journal-ref: AAAI2026
\\ ( https://arxiv.org/abs/2502.04829 ,  6051kb)
------------------------------------------------------------------------------
\\
arXiv:2502.08005
replaced with revised version Tue, 20 Jan 2026 17:24:34 GMT   (23685kb)

Title: DiffRatio: Training One-Step Diffusion Models Without Teacher
  Supervision
Authors: Wenlin Chen and Mingtian Zhang and Jiajun He and Zijing Ou and Jos\'e
  Miguel Hern\'andez-Lobato and Bernhard Sch\"olkopf and David Barber
Categories: cs.LG cs.CV
Comments: 21 pages, 8 figures, 5 tables, 2 algorithms
\\ ( https://arxiv.org/abs/2502.08005 ,  23685kb)
------------------------------------------------------------------------------
\\
arXiv:2502.08577
replaced with revised version Mon, 19 Jan 2026 14:25:52 GMT   (1636kb)

Title: FBFL: A Field-Based Coordination Approach for Data Heterogeneity in
  Federated Learning
Authors: Davide Domini, Gianluca Aguzzi, Lukas Esterle and Mirko Viroli
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2502.08577 ,  1636kb)
------------------------------------------------------------------------------
\\
arXiv:2502.11450
replaced with revised version Tue, 20 Jan 2026 09:36:37 GMT   (11339kb)

Title: What Scalable Second-Order Information Knows for Pruning at
  Initialization
Authors: Ivo Gollini Navarrete, Nicol\'as Mauricio Cuadrado \'Avila, Martin
  Tak\'a\v{c}, Samuel Horv\'ath
Categories: cs.LG cs.AI
Comments: 9 pages of main content (excluding references), 4 figures in main
  body, and 21 pages of appendix. Code available at
  https://github.com/Gollini/Scalable_Second_Order_PaI
MSC-class: 68T05
ACM-class: I.2.6; C.1.3
\\ ( https://arxiv.org/abs/2502.11450 ,  11339kb)
------------------------------------------------------------------------------
\\
arXiv:2502.12930
replaced with revised version Tue, 20 Jan 2026 11:33:56 GMT   (344kb)

Title: Universal Embedding Function for Traffic Classification via QUIC Domain
  Recognition Pretraining: A Transfer Learning Success
Authors: Jan Luxemburk, Karel Hynek, Richard Pln\'y, Tom\'a\v{s} \v{C}ejka
Categories: cs.LG cs.NI
Journal-ref: IEEE Transactions on Network and Service Management, vol. 23, pp.
  1647-1663, 2026
DOI: 10.1109/TNSM.2025.3642984
\\ ( https://arxiv.org/abs/2502.12930 ,  344kb)
------------------------------------------------------------------------------
\\
arXiv:2502.16049
replaced with revised version Sat, 17 Jan 2026 14:21:39 GMT   (393kb)

Title: Quasi Zigzag Persistence: A Topological Framework for Analyzing
  Time-Varying Data
Authors: Tamal K. Dey, Shreyas N. Samaga
Categories: cs.LG math.AT
\\ ( https://arxiv.org/abs/2502.16049 ,  393kb)
------------------------------------------------------------------------------
\\
arXiv:2502.18462
replaced with revised version Fri, 16 Jan 2026 22:16:33 GMT   (31620kb)

Title: Scalable Equilibrium Sampling with Sequential Boltzmann Generators
Authors: Charlie B. Tan, Avishek Joey Bose, Chen Lin, Leon Klein, Michael M.
  Bronstein, Alexander Tong
Categories: cs.LG cs.AI
Comments: Presented at ICML 2025
\\ ( https://arxiv.org/abs/2502.18462 ,  31620kb)
------------------------------------------------------------------------------
\\
arXiv:2503.05167
replaced with revised version Tue, 20 Jan 2026 13:19:48 GMT   (955kb)

Title: FMASH: Advancing Traditional Chinese Medicine Formula Recommendation
  with Efficient Fusion of Multiscale Associations of Symptoms and Herbs
Authors: Xinhan Zheng and Huyu Wu and Ruotai Li and Haopeng Jin and Xueting
  Wang and Yehan Yang and Guodong Shan
Categories: cs.LG
\\ ( https://arxiv.org/abs/2503.05167 ,  955kb)
------------------------------------------------------------------------------
\\
arXiv:2503.11824
replaced with revised version Mon, 19 Jan 2026 11:17:20 GMT   (660kb)

Title: Dual-Domain Fusion for Semi-Supervised Learning
Authors: Tuomas Jalonen, Mohammad Al-Sa'd, Serkan Kiranyaz, and Moncef Gabbouj
Categories: cs.LG cs.AI eess.SP
\\ ( https://arxiv.org/abs/2503.11824 ,  660kb)
------------------------------------------------------------------------------
\\
arXiv:2503.16219
replaced with revised version Tue, 20 Jan 2026 06:53:25 GMT   (1591kb)

Title: Reinforcement Learning for Reasoning in Small LLMs: What Works and What
  Doesn't
Authors: Quy-Anh Dang and Chris Ngo
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2503.16219 ,  1591kb)
------------------------------------------------------------------------------
\\
arXiv:2503.17393
replaced with revised version Tue, 20 Jan 2026 04:19:51 GMT   (1332kb)

Title: BPINN-EM-Post: Bayesian Physics-Informed Neural Network based Stochastic
  Electromigration Damage Analysis in the Post-void Phase
Authors: Subed Lamichhane, Haotian Lu, Sheldon X.-D. Tan
Categories: cs.LG
Comments: 8 pages, to appear in ISQED 2026
\\ ( https://arxiv.org/abs/2503.17393 ,  1332kb)
------------------------------------------------------------------------------
\\
arXiv:2504.08930
replaced with revised version Mon, 19 Jan 2026 15:08:57 GMT   (699kb)

Title: VectorLiteRAG: Latency-Aware and Fine-Grained Resource Partitioning for
  Efficient RAG
Authors: Junkyum Kim, Divya Mahajan
Categories: cs.LG
\\ ( https://arxiv.org/abs/2504.08930 ,  699kb)
------------------------------------------------------------------------------
\\
arXiv:2504.12501
replaced with revised version Sat, 17 Jan 2026 17:17:41 GMT   (8732kb)

Title: Reinforcement Learning from Human Feedback
Authors: Nathan Lambert
Categories: cs.LG
Comments: 201 pages. Web-native version at https://rlhfbook.com/ Continually
  improving, latest version at website
\\ ( https://arxiv.org/abs/2504.12501 ,  8732kb)
------------------------------------------------------------------------------
\\
arXiv:2504.14068
replaced with revised version Fri, 16 Jan 2026 23:27:01 GMT   (85kb)

Title: Contextual Embedding-based Clustering to Identify Topics for Healthcare
  Service Improvement
Authors: K M Sajjadul Islam, Ravi Teja Karri, Srujan Vegesna, Jiawei Wu,
  Praveen Madiraju
Categories: cs.LG cs.HC
Comments: The paper accepted at the 2025 IEEE COMPSAC, Toronto, Canada
DOI: 10.1109/COMPSAC65507.2025.00106
\\ ( https://arxiv.org/abs/2504.14068 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2504.15439
replaced with revised version Tue, 20 Jan 2026 07:09:12 GMT   (861kb)

Title: Combating Toxic Language: A Review of LLM-Based Strategies for Software
  Engineering
Authors: Hao Zhuo, Yicheng Yang, Kewen Peng
Categories: cs.LG cs.SE
\\ ( https://arxiv.org/abs/2504.15439 ,  861kb)
------------------------------------------------------------------------------
\\
arXiv:2504.15458
replaced with revised version Tue, 20 Jan 2026 13:49:21 GMT   (12491kb)

Title: Compton Form Factor Extraction using Quantum Deep Neural Networks
Authors: Brandon B. Le and Dustin Keller
Categories: cs.LG hep-ph nucl-th quant-ph
Comments: 36 pages, 17 figures. v3: major revisions
\\ ( https://arxiv.org/abs/2504.15458 ,  12491kb)
------------------------------------------------------------------------------
\\
arXiv:2505.01618
replaced with revised version Mon, 19 Jan 2026 19:50:08 GMT   (949kb)

Title: Don't be lazy: CompleteP enables compute-efficient deep transformers
Authors: Nolan Dey and Bin Claire Zhang and Lorenzo Noci and Mufan Li and Blake
  Bordelon and Shane Bergsma and Cengiz Pehlevan and Boris Hanin and Joel
  Hestness
Categories: cs.LG cs.AI
Comments: NeurIPS 2025. v4 fixes Table 1 typo to match AdamW eps to Equation 40
\\ ( https://arxiv.org/abs/2505.01618 ,  949kb)
------------------------------------------------------------------------------
\\
arXiv:2505.02880
replaced with revised version Sun, 18 Jan 2026 14:51:14 GMT   (2366kb)

Title: Beyond Fixed Patches: Enhancing GPTs for Financial Prediction with
  Adaptive Segmentation and Learnable Wavelets
Authors: Renjun Jia, Zian Liu, Peng Zhu, Dawei Cheng, Yuqi Liang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2505.02880 ,  2366kb)
------------------------------------------------------------------------------
\\
arXiv:2505.05527
replaced with revised version Sun, 18 Jan 2026 11:21:41 GMT   (148kb)

Title: ADMM-Based Training for Spiking Neural Networks
Authors: Giovanni Perin, Cesare Bidini, Riccardo Mazzieri, Michele Rossi
Categories: cs.LG cs.AI cs.NE eess.SP math.OC
Comments: \c{opyright} 2026 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
\\ ( https://arxiv.org/abs/2505.05527 ,  148kb)
------------------------------------------------------------------------------
\\
arXiv:2505.11574
replaced with revised version Tue, 20 Jan 2026 15:56:29 GMT   (321kb)

Title: Quantization Meets Reasoning: Exploring and Mitigating Degradation of
  Low-Bit LLMs in Mathematical Reasoning
Authors: Zhen Li, Yupeng Su, Songmiao Wang, Runming Yang, Congkai Xie, Aofan
  Liu, Ming Li, Jiannong Cao, Yuan Xie, Ngai Wong, Hongxia Yang
Categories: cs.LG cs.AI
Comments: 27pages
\\ ( https://arxiv.org/abs/2505.11574 ,  321kb)
------------------------------------------------------------------------------
\\
arXiv:2505.12509
replaced with revised version Tue, 20 Jan 2026 12:50:06 GMT   (3406kb)

Title: Revitalizing Black-Box Interpretability: Actionable Interpretability for
  LLMs via Proxy Models
Authors: Junhao Liu, Haonan Yu, Zhenyu Yan, Xin Zhang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2505.12509 ,  3406kb)
------------------------------------------------------------------------------
\\
arXiv:2505.13308
replaced with revised version Mon, 19 Jan 2026 14:21:52 GMT   (9682kb)

Title: Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient
  in Latent Space
Authors: Hengli Li, Chenxi Li, Tong Wu, Xuekai Zhu, Yuxuan Wang, Zhaoxin Yu,
  Eric Hanchen Jiang, Song-Chun Zhu, Zixia Jia, Ying Nian Wu, Zilong Zheng
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2505.13308 ,  9682kb)
------------------------------------------------------------------------------
\\
arXiv:2505.15647
replaced with revised version Sat, 17 Jan 2026 16:19:28 GMT   (177kb)

Title: Second-Order Convergence in Private Stochastic Non-Convex Optimization
Authors: Youming Tao, Zuyuan Zhang, Dongxiao Yu, Xiuzhen Cheng, Falko Dressler,
  Di Wang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2505.15647 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17272
replaced with revised version Tue, 20 Jan 2026 18:39:03 GMT   (3260kb)

Title: Zebra-Llama: Towards Extremely Efficient Hybrid Models
Authors: Mingyu Yang, Mehdi Rezagholizadeh, Guihong Li, Vikram Appia, Emad
  Barsoum
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2505.17272 ,  3260kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17344
replaced with revised version Tue, 20 Jan 2026 15:12:36 GMT   (1006kb)

Title: A Multi-Head Attention Soft Random Forest for Interpretable Patient
  No-Show Prediction
Authors: Ninda Nurseha Amalina and Heungjo An
Categories: cs.LG cs.AI cs.CY
Comments: 21 pages, 6 figures
\\ ( https://arxiv.org/abs/2505.17344 ,  1006kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18028
replaced with revised version Sun, 18 Jan 2026 19:17:38 GMT   (12756kb)

Title: Knot So Simple: A Minimalistic Environment for Spatial Reasoning
Authors: Zizhao Chen, Yoav Artzi
Categories: cs.LG cs.AI cs.CV cs.RO
Comments: Fix camera ready footer
\\ ( https://arxiv.org/abs/2505.18028 ,  12756kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18763
replaced with revised version Tue, 20 Jan 2026 12:33:48 GMT   (4454kb)

Title: GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning
Authors: Shutong Ding, Ke Hu, Shan Zhong, Haoyang Luo, Weinan Zhang, Jingya
  Wang, Jun Wang, Ye Shi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2505.18763 ,  4454kb)
------------------------------------------------------------------------------
\\
arXiv:2505.19825
replaced with revised version Sat, 17 Jan 2026 16:38:15 GMT   (672kb)

Title: Position: Foundation Models for Tabular Data within Systemic Contexts
  Need Grounding
Authors: Tassilo Klein and Johannes Hoffart
Categories: cs.LG cs.AI cs.DB
\\ ( https://arxiv.org/abs/2505.19825 ,  672kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21020
replaced with revised version Sun, 18 Jan 2026 16:02:00 GMT   (40872kb)

Title: NeuralOM: Neural Ocean Model for Subseasonal-to-Seasonal Simulation
Authors: Yuan Gao, Hao Wu, Fan Xu, Yanfei Xiang, Ruijian Gou, Ruiqi Shu,
  Qingsong Wen, Xian Wu, Kun Wang, Xiaomeng Huang
Categories: cs.LG physics.ao-ph
\\ ( https://arxiv.org/abs/2505.21020 ,  40872kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21184
replaced with revised version Sat, 17 Jan 2026 16:32:09 GMT   (7326kb)

Title: Universal Harmful Information Synthesis via Model Crowdsourcing
Authors: Yu Yan, Sheng Sun, Zhifei Zheng, Ziji Hao, Teli Liu, and Min Liu
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2505.21184 ,  7326kb)
------------------------------------------------------------------------------
\\
arXiv:2506.19023
replaced with revised version Sat, 17 Jan 2026 11:25:13 GMT   (2913kb)

Title: Automating Traffic Monitoring with SHM Sensor Networks via
  Vision-Supervised Deep Learning
Authors: Hanshuo Wu, Xudong Jian, Christos Lataniotis, Cyprien Hoelzl, Eleni
  Chatzi, Yves Reuland
Categories: cs.LG
\\ ( https://arxiv.org/abs/2506.19023 ,  2913kb)
------------------------------------------------------------------------------
\\
arXiv:2506.20537
replaced with revised version Tue, 20 Jan 2026 16:30:50 GMT   (0kb,I)

Title: Physics-Informed Machine Learning Regulated by Finite Element Analysis
  for Simulation Acceleration of Laser Powder Bed Fusion
Authors: R. Sharma, M. Raissi, Y.B. Guo
Categories: cs.LG
Comments: Further investigation revealed that the current version reflects an
  incomplete formulation and limited validation of the proposed method. We have
  since developed a substantially revised and extended study with updated
  assumptions and results, and therefore withdraw this version to prevent
  citation of superseded findings
\\ ( https://arxiv.org/abs/2506.20537 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2506.22621
replaced with revised version Sat, 17 Jan 2026 14:12:51 GMT   (2472kb)

Title: Modeling Hierarchical Spaces: A Review and Unified Framework for
  Surrogate-Based Architecture Design
Authors: Paul Saves and Edward Hall\'e-Hannan and Jasper Bussemaker and Youssef
  Diouane and Nathalie Bartoli
Categories: cs.LG math.OC stat.ML
Comments: Published in Structural and Multidisciplinary Optimization, Springer
  Nature (2026)
DOI: 10.1007/s00158-026-04249-2
\\ ( https://arxiv.org/abs/2506.22621 ,  2472kb)
------------------------------------------------------------------------------
\\
arXiv:2507.01389
replaced with revised version Mon, 19 Jan 2026 01:29:17 GMT   (1654kb)

Title: Surrogate Modeling via Factorization Machine and Ising Model with
  Enhanced Higher-Order Interaction Learning
Authors: Anbang Wang, Dunbo Cai, Yu Zhang, Yangqing Huang, Xiangyang Feng, and
  Zhihong Zhang
Categories: cs.LG quant-ph
DOI: 10.1103/knt1-yd9s
\\ ( https://arxiv.org/abs/2507.01389 ,  1654kb)
------------------------------------------------------------------------------
\\
arXiv:2507.05386
replaced with revised version Sun, 18 Jan 2026 13:38:52 GMT   (1812kb)

Title: Reinforcement Fine-Tuning Naturally Mitigates Forgetting in Continual
  Post-Training
Authors: Song Lai, Haohan Zhao, Rong Feng, Changyi Ma, Wenzhuo Liu, Hongbo
  Zhao, Xi Lin, Dong Yi, Qingfu Zhang, Hongbin Liu, Gaofeng Meng, Fei Zhu
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2507.05386 ,  1812kb)
------------------------------------------------------------------------------
\\
arXiv:2507.07222
replaced with revised version Sat, 17 Jan 2026 17:54:21 GMT   (313kb)

Title: Efficient Parametric SVD of Koopman Operator for Stochastic Dynamical
  Systems
Authors: Minchan Jeong, J. Jon Ryu, Se-Young Yun, Gregory W. Wornell
Categories: cs.LG cs.NA math.DS math.NA
Comments: Accepted for NeurIPS 2025. The first two authors contributed equally.
  30 pages, 5 figures, 4 tables. Code is available at
  https://github.com/MinchanJeong/NeuralKoopmanSVD
\\ ( https://arxiv.org/abs/2507.07222 ,  313kb)
------------------------------------------------------------------------------
\\
arXiv:2507.12969
replaced with revised version Tue, 20 Jan 2026 17:19:43 GMT   (15345kb)

Title: WaveletInception Networks for on-board Vibration-Based Infrastructure
  Health Monitoring
Authors: Reza Riahi Samani, Alfredo Nunez, Bart De Schutter
Categories: cs.LG cs.CV
Comments: Under reviewer for the Journal of Engineering Application of
  Artificial Intelligence
\\ ( https://arxiv.org/abs/2507.12969 ,  15345kb)
------------------------------------------------------------------------------
\\
arXiv:2507.20447
replaced with revised version Mon, 19 Jan 2026 23:46:19 GMT   (2989kb)

Title: WEEP: A Differentiable Nonconvex Sparse Regularizer via Weakly-Convex
  Envelope
Authors: Takanobu Furuhashi, Hidekata Hontani, Qibin Zhao, Tatsuya Yokota
Categories: cs.LG cs.CV
Comments: 5 pages, 5 figures, 1 tables. Accepted at ICASSP 2026
\\ ( https://arxiv.org/abs/2507.20447 ,  2989kb)
------------------------------------------------------------------------------
\\
arXiv:2507.21183
replaced with revised version Fri, 16 Jan 2026 22:47:57 GMT   (250kb)

Title: MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge
Authors: Guangchen Lan, Sipeng Zhang, Tianle Wang, Yuwei Zhang, Daoan Zhang,
  Xinpeng Wei, Xiaoman Pan, Hongming Zhang, Dong-Jun Han, Christopher G.
  Brinton
Categories: cs.LG cs.AI cs.CL
ACM-class: I.2.6; I.2.7
\\ ( https://arxiv.org/abs/2507.21183 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2508.02600
replaced with revised version Tue, 20 Jan 2026 16:23:35 GMT   (22806kb)

Title: Adaptive Riemannian Graph Neural Networks
Authors: Xudong Wang, Chris Ding, Tongxin Li, Jicong Fan
Categories: cs.LG
Comments: Accepted in The Fortieth AAAI Conference on Artificial Intelligence
  (AAAI-26), Main Technical Track
ACM-class: I.2; I.5.1; I.5.2
\\ ( https://arxiv.org/abs/2508.02600 ,  22806kb)
------------------------------------------------------------------------------
\\
arXiv:2508.02751
replaced with revised version Mon, 19 Jan 2026 09:51:39 GMT   (1115kb)

Title: SmallKV: Small Model Assisted Compensation of KV Cache Compression for
  Efficient LLM Inference
Authors: Yi Zhao, Yajuan Peng, Cam-Tu Nguyen, Zuchao Li, Xiaoliang Wang, Hai
  Zhao, Xiaoming Fu
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2508.02751 ,  1115kb)
------------------------------------------------------------------------------
\\
arXiv:2508.03222
replaced with revised version Fri, 16 Jan 2026 23:28:12 GMT   (4633kb)

Title: Revisiting Deep Information Propagation: Fractal Frontier and
  Finite-size Effects
Authors: Giuseppe Alessio D'Inverno, Zhiyuan Hu, Leo Davy, Michael Unser,
  Gianluigi Rozza, Jonathan Dong
Categories: cs.LG
Comments: 19 pages
\\ ( https://arxiv.org/abs/2508.03222 ,  4633kb)
------------------------------------------------------------------------------
\\
arXiv:2508.04444
replaced with revised version Sat, 17 Jan 2026 17:14:43 GMT   (889kb)

Title: Matrix-Free Two-to-Infinity and One-to-Two Norms Estimation
Authors: Askar Tsyganov, Evgeny Frolov, Sergey Samsonov, Maxim Rakhuba
Categories: cs.LG cs.NA math.NA stat.ML
Comments: AAAI-2026, camera-ready version
MSC-class: 65F35
\\ ( https://arxiv.org/abs/2508.04444 ,  889kb)
------------------------------------------------------------------------------
\\
arXiv:2508.04748
replaced with revised version Sun, 18 Jan 2026 12:43:56 GMT   (3631kb)

Title: AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular
  Property Prediction with Large Language Models
Authors: Xuan Lin, Long Chen, Yile Wang
Categories: cs.LG cs.AI cs.CL
Comments: 9 pages
\\ ( https://arxiv.org/abs/2508.04748 ,  3631kb)
------------------------------------------------------------------------------
\\
arXiv:2508.10646
replaced with revised version Mon, 19 Jan 2026 18:56:48 GMT   (6256kb)

Title: SPHENIC: Topology-Aware Multi-View Clustering for Spatial
  Transcriptomics
Authors: Chenkai Guo, Yikai Zhu, Renxiang Guan, Jinli Ma, Siwei Wang, Ke Liang,
  Guangdun Peng, Dayu Hu
Categories: cs.LG cs.AI
Comments: 9 pages, 5 figures, 2 tables
\\ ( https://arxiv.org/abs/2508.10646 ,  6256kb)
------------------------------------------------------------------------------
\\
arXiv:2508.15659
replaced with revised version Mon, 19 Jan 2026 23:05:27 GMT   (610kb)

Title: Amortized In-Context Mixed Effect Transformer Models: A Zero-Shot
  Approach for Pharmacokinetics
Authors: C\'esar Ali Ojeda Marin, Wilhelm Huisinga, Purity Kavwele, Rams\'es J.
  S\'anchez, Niklas Hartung
Categories: cs.LG
Comments: author actually help in data preprocessing, notified later from other
  author
\\ ( https://arxiv.org/abs/2508.15659 ,  610kb)
------------------------------------------------------------------------------
\\
arXiv:2508.16874
replaced with revised version Sun, 18 Jan 2026 14:15:08 GMT   (3812kb)

Title: UM3: Unsupervised Map to Map Matching
Authors: Chaolong Ying, Yinan Zhang, Lei Zhang, Jiazhuang Wang, Shujun Jia,
  Tianshu Yu
Categories: cs.LG cs.CV
Comments: 11 pages
\\ ( https://arxiv.org/abs/2508.16874 ,  3812kb)
------------------------------------------------------------------------------
\\
arXiv:2508.18175
replaced with revised version Fri, 16 Jan 2026 19:33:53 GMT   (2971kb)

Title: Amortized Sampling with Transferable Normalizing Flows
Authors: Charlie B. Tan, Majdi Hassan, Leon Klein, Saifuddin Syed, Dominique
  Beaini, Michael M. Bronstein, Alexander Tong, Kirill Neklyudov
Categories: cs.LG cs.AI
Comments: Presented at NeurIPS 2025
\\ ( https://arxiv.org/abs/2508.18175 ,  2971kb)
------------------------------------------------------------------------------
\\
arXiv:2508.20257
replaced with revised version Mon, 19 Jan 2026 18:18:06 GMT   (1406kb)

Title: Discovering equations from data: symbolic regression in dynamical
  systems
Authors: Beatriz R. Brum, Luiza Lober, Isolde Previdelli, Francisco A.
  Rodrigues
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2508.20257 ,  1406kb)
------------------------------------------------------------------------------
\\
arXiv:2509.04419
replaced with revised version Tue, 20 Jan 2026 05:05:15 GMT   (425kb)

Title: Towards a Unified View of Large Language Model Post-Training
Authors: Xingtai Lv, Yuxin Zuo, Youbang Sun, Hongyi Liu, Yuntian Wei, Zhekai
  Chen, Xuekai Zhu, Kaiyan Zhang, Bingning Wang, Ning Ding, Bowen Zhou
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2509.04419 ,  425kb)
------------------------------------------------------------------------------
\\
arXiv:2509.09219
replaced with revised version Mon, 19 Jan 2026 14:07:58 GMT   (153kb)

Title: Vejde: A Framework for Inductive Deep Reinforcement Learning Based on
  Factor Graph Color Refinement
Authors: Jakob Nyberg, Pontus Johnson
Categories: cs.LG cs.AI
Journal-ref: Transactions on Machine Learning Research, January 2026
\\ ( https://arxiv.org/abs/2509.09219 ,  153kb)
------------------------------------------------------------------------------
\\
arXiv:2509.12939
replaced with revised version Mon, 19 Jan 2026 21:21:11 GMT   (2394kb)

Title: Sy-FAR: Symmetry-based Fair Adversarial Robustness
Authors: Haneen Najjar, Eyal Ronen, Mahmood Sharif
Categories: cs.LG cs.AI cs.CR cs.CV
Comments: Accepted to USENIX Security 2026
\\ ( https://arxiv.org/abs/2509.12939 ,  2394kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14821
replaced with revised version Tue, 20 Jan 2026 10:48:51 GMT   (352kb)

Title: Precision Neural Networks: Joint Graph And Relational Learning
Authors: Andrea Cavallo and Samuel Rey and Antonio G. Marques and Elvin Isufi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2509.14821 ,  352kb)
------------------------------------------------------------------------------
\\
arXiv:2509.14968
replaced with revised version Mon, 19 Jan 2026 13:55:08 GMT   (17915kb)

Title: FAWN: A MultiEncoder Fusion-Attention Wave Network for Integrated
  Sensing and Communication Indoor Scene Inference
Authors: Carlos Barroso-Fern\'andez, Alejandro Calvillo-Fernandez, Antonio de
  la Oliva, Carlos J. Bernardos
Categories: cs.LG cs.NI
Comments: 7 pages, 6 figures and tables, less than 5500 words. Under revision
  at IEEE Communication Magazine
\\ ( https://arxiv.org/abs/2509.14968 ,  17915kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18751
replaced with revised version Mon, 19 Jan 2026 03:15:42 GMT   (1453kb)

Title: MOMEMTO: Patch-based Memory Gate Model in Time Series Foundation Model
Authors: Samuel Yoon, Jongwon Kim, Juyoung Ha, Young Myoung Ko
Categories: cs.LG
\\ ( https://arxiv.org/abs/2509.18751 ,  1453kb)
------------------------------------------------------------------------------
\\
arXiv:2509.20846
replaced with revised version Mon, 19 Jan 2026 07:53:59 GMT   (3153kb)

Title: Causal Time Series Generation via Diffusion Models
Authors: Yutong Xia, Chang Xu, Yuxuan Liang, Qingsong Wen, Roger Zimmermann,
  Jiang Bian
Categories: cs.LG
\\ ( https://arxiv.org/abs/2509.20846 ,  3153kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21498
replaced with revised version Tue, 20 Jan 2026 00:29:28 GMT   (27378kb)

Title: SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of
  Diffusion Models
Authors: Arani Roy, Shristi Das Biswas, Kaushik Roy
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2509.21498 ,  27378kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22279
replaced with revised version Tue, 20 Jan 2026 16:18:00 GMT   (999kb)

Title: Task-Aware Mixture-of-Experts for Time Series Analysis
Authors: Xingjian Wu, Zhengyu Li, Hanyin Cheng, Xiangfei Qiu, Jilin Hu,
  Chenjuan Guo, Bin Yang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2509.22279 ,  999kb)
------------------------------------------------------------------------------
\\
arXiv:2509.22592
replaced with revised version Sat, 17 Jan 2026 20:30:03 GMT   (9128kb)

Title: Transport Based Mean Flows for Generative Modeling
Authors: Elaheh Akbari, Ping He, Ahmadreza Moradipari, Yikun Bai, Soheil
  Kolouri
Categories: cs.LG
\\ ( https://arxiv.org/abs/2509.22592 ,  9128kb)
------------------------------------------------------------------------------
\\
arXiv:2509.24573
replaced with revised version Mon, 19 Jan 2026 15:44:28 GMT   (2871kb)

Title: Learning to Solve Optimization Problems Constrained with Partial
  Differential Equations
Authors: Yusuf Guven, Vincenzo Di Vito and Ferdinando Fioretto
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2509.24573 ,  2871kb)
------------------------------------------------------------------------------
\\
arXiv:2509.25778
replaced with revised version Sun, 18 Jan 2026 15:52:57 GMT   (70kb)

Title: A Hamiltonian driven Geometric Construction of Neural Networks on the
  Lognormal Statistical Manifold
Authors: Prosper Rosaire Mama Assandje, Teumsa Aboubakar, Thomas Bouetou
  Bouetou
Categories: cs.LG
\\ ( https://arxiv.org/abs/2509.25778 ,  70kb)
------------------------------------------------------------------------------
\\
arXiv:2510.01988
replaced with revised version Sat, 17 Jan 2026 10:45:35 GMT   (2794kb)

Title: PepCompass: Navigating peptide embedding spaces using Riemannian
  Geometry
Authors: Marcin Mo\.zejko, Adam Bielecki, Jurand Pr\k{a}dzy\'nski, Marcin
  Traskowski, Antoni Janowski, Hyun-Su Lee, Marcelo Der Torossian Torres,
  Micha{\l} Kmicikiewicz, Paulina Szymczak, Karol Jurasz, Micha{\l} Kucharczyk,
  Cesar de la Fuente-Nunez, Ewa Szczurek
Categories: cs.LG
\\ ( https://arxiv.org/abs/2510.01988 ,  2794kb)
------------------------------------------------------------------------------
\\
arXiv:2510.03129
replaced with revised version Sat, 17 Jan 2026 11:17:01 GMT   (902kb)

Title: Signature-Informed Transformer for Asset Allocation
Authors: Yoontae Hwang, Stefan Zohren
Categories: cs.LG cs.AI q-fin.PM
\\ ( https://arxiv.org/abs/2510.03129 ,  902kb)
------------------------------------------------------------------------------
\\
arXiv:2510.05178
replaced with revised version Sun, 18 Jan 2026 11:38:26 GMT   (4375kb)

Title: Auditable Unit-Aware Thresholds in Symbolic Regression via
  Logistic-Gated Operators
Authors: Ou Deng, Ruichen Cong, Jianting Xu, Shoji Nishimura, Atsushi Ogihara,
  and Qun Jin
Categories: cs.LG cs.AI cs.SC
\\ ( https://arxiv.org/abs/2510.05178 ,  4375kb)
------------------------------------------------------------------------------
\\
arXiv:2510.06092
replaced with revised version Sat, 17 Jan 2026 10:13:13 GMT   (682kb)

Title: Learning from Failures: Understanding LLM Alignment through
  Failure-Aware Inverse RL
Authors: Nyal Patel, Matthieu Bou, Arjun Jagota, Satyapriya Krishna, Sonali
  Parbhoo
Categories: cs.LG cs.CL
Comments: Preprint
\\ ( https://arxiv.org/abs/2510.06092 ,  682kb)
------------------------------------------------------------------------------
\\
arXiv:2510.09895
replaced with revised version Sun, 18 Jan 2026 17:09:15 GMT   (1477kb)

Title: Chain-of-Influence: Tracing Interdependencies Across Time and Features
  in Clinical Predictive Modelings
Authors: Yubo Li, Rema Padman
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2510.09895 ,  1477kb)
------------------------------------------------------------------------------
\\
arXiv:2510.10150
replaced with revised version Mon, 19 Jan 2026 15:00:58 GMT   (1166kb)

Title: Rethinking Entropy Interventions in RLVR: An Entropy Change Perspective
Authors: Zhezheng Hao, Hong Wang, Haoyang Liu, Jian Luo, Jiarui Yu, Hande Dong,
  Qiang Lin, Can Wang, Jiawei Chen
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2510.10150 ,  1166kb)
------------------------------------------------------------------------------
\\
arXiv:2510.10211
replaced with revised version Tue, 20 Jan 2026 16:35:22 GMT   (1616kb)

Title: Transport-Coupled Bayesian Flows for Molecular Graph Generation
Authors: Yida Xiong, Jiameng Chen, Kun Li, Hongzhi Zhang, Xiantao Cai, Jia Wu,
  Wenbin Hu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2510.10211 ,  1616kb)
------------------------------------------------------------------------------
\\
arXiv:2510.12618
replaced with revised version Tue, 20 Jan 2026 16:34:49 GMT   (214kb)

Title: Towards Fast Coarse-graining and Equation Discovery with Foundation
  Inference Models
Authors: Manuel Hinz and Maximilian Mauel and Patrick Seifner and David
  Berghaus and Kostadin Cvejoski and Ramses J. Sanchez
Categories: cs.LG
\\ ( https://arxiv.org/abs/2510.12618 ,  214kb)
------------------------------------------------------------------------------
\\
arXiv:2510.12640
replaced with revised version Tue, 20 Jan 2026 16:37:22 GMT   (622kb)

Title: On Foundation Models for Temporal Point Processes to Accelerate
  Scientific Discovery
Authors: David Berghaus and Patrick Seifner and Kostadin Cvejoski and Ramses J.
  Sanchez
Categories: cs.LG
\\ ( https://arxiv.org/abs/2510.12640 ,  622kb)
------------------------------------------------------------------------------
\\
arXiv:2510.13872
replaced with revised version Tue, 20 Jan 2026 15:22:16 GMT   (7992kb)

Title: Joint Discriminative-Generative Modeling via Dual Adversarial Training
Authors: Xuwang Yin, Claire Zhang, Julie Steele, Nir Shavit, Tony T. Wang
Categories: cs.LG cs.AI
Comments: Revised R1 regularization analysis using Roth et al. (2020) operator
  norm framework. Code: https://github.com/xuwangyin/DAT
\\ ( https://arxiv.org/abs/2510.13872 ,  7992kb)
------------------------------------------------------------------------------
\\
arXiv:2510.19514
replaced with revised version Mon, 19 Jan 2026 17:20:58 GMT   (16812kb)

Title: From Prototypes to Sparse ECG Explanations: SHAP-Driven Counterfactuals
  for Multivariate Time-Series Multi-class Classification
Authors: Maciej Mozolewski, Bet\"ul Bayrak, Kerstin Bach and Grzegorz J. Nalepa
Categories: cs.LG cs.AI cs.HC
\\ ( https://arxiv.org/abs/2510.19514 ,  16812kb)
------------------------------------------------------------------------------
\\
arXiv:2510.20627
replaced with revised version Mon, 19 Jan 2026 21:53:26 GMT   (5895kb)

Title: H-SPLID: HSIC-based Saliency Preserving Latent Information Decomposition
Authors: Lukas Miklautz, Chengzhi Shi, Andrii Shkabrii, Theodoros Thirimachos
  Davarakis, Prudence Lam, Claudia Plant, Jennifer Dy, Stratis Ioannidis
Categories: cs.LG
Comments: Accepted at NeurIPS 2025
\\ ( https://arxiv.org/abs/2510.20627 ,  5895kb)
------------------------------------------------------------------------------
\\
arXiv:2510.20671
replaced with revised version Sat, 17 Jan 2026 09:18:06 GMT   (9197kb)

Title: GRACE: Graph Neural Networks for Locus-of-Care Prediction under Extreme
  Class Imbalance
Authors: Subham Kumar, Lekhansh Shukla, Animesh Mukherjee, Koustav Rudra, and
  Prakrithi Shivaprakash
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2510.20671 ,  9197kb)
------------------------------------------------------------------------------
\\
arXiv:2510.21934
replaced with revised version Tue, 20 Jan 2026 18:20:51 GMT   (380kb)

Title: Joint Score-Threshold Optimization for Interpretable Risk Assessment
Authors: Fardin Gankhanloo, Emmett Springer, Erik H. Hoyer, Daniel L. Young,
  Kimia Ghobadi
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2510.21934 ,  380kb)
------------------------------------------------------------------------------
\\
arXiv:2510.22491
replaced with revised version Sun, 18 Jan 2026 03:11:00 GMT   (12464kb)

Title: LAMP: Data-Efficient Linear Affine Weight-Space Models for
  Parameter-Controlled 3D Shape Generation and Extrapolation
Authors: Ghadi Nehme, Yanxia Zhang, Dule Shu, Matt Klenk, Faez Ahmed
Categories: cs.LG cs.CE cs.CV
\\ ( https://arxiv.org/abs/2510.22491 ,  12464kb)
------------------------------------------------------------------------------
\\
arXiv:2511.05610
replaced with revised version Sun, 18 Jan 2026 13:57:41 GMT   (213kb)

Title: Conformal Prediction-Driven Adaptive Sampling for Digital Water Twins
Authors: Mohammadhossein Homaei, Mehran Tarif, Pablo Garcia Rodriguez, Andres
  Caro, Mar Avila
Categories: cs.LG cs.AI math.OC
Comments: 6 Pages, 7 tables, 1 Figure
\\ ( https://arxiv.org/abs/2511.05610 ,  213kb)
------------------------------------------------------------------------------
\\
arXiv:2511.06894
replaced with revised version Mon, 19 Jan 2026 09:52:23 GMT   (4515kb)

Title: COGNOS: Universal Enhancement for Time Series Anomaly Detection via
  Constrained Gaussian-Noise Optimization and Smoothing
Authors: Wenlong Shang and Shihao Tian and Xutong Wan and Peng Chang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2511.06894 ,  4515kb)
------------------------------------------------------------------------------
\\
arXiv:2511.07368
replaced with revised version Sat, 17 Jan 2026 17:52:54 GMT   (1295kb)

Title: Post-Training as Reweighting: A Stochastic View of Reasoning
  Trajectories in Language Models
Authors: Dake Bu, Wei Huang, Andi Han, Atsushi Nitanda, Bo Xue, Qingfu Zhang,
  Hau-San Wong, Taiji Suzuki
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2511.07368 ,  1295kb)
------------------------------------------------------------------------------
\\
arXiv:2511.09853
replaced with revised version Tue, 20 Jan 2026 04:07:06 GMT   (1456kb)

Title: ConSurv: Multimodal Continual Learning for Survival Analysis
Authors: Dianzhi Yu, Conghao Xiong, Yankai Chen, Wenqian Cui, Xinni Zhang,
  Yifei Zhang, Hao Chen, Joseph J.Y. Sung, Irwin King
Categories: cs.LG
Comments: 14 pages, 4 figures. This is the extended version of the paper
  accepted at AAAI 2026, which includes all technical appendices and additional
  experimental details
\\ ( https://arxiv.org/abs/2511.09853 ,  1456kb)
------------------------------------------------------------------------------
\\
arXiv:2511.14084
replaced with revised version Sat, 17 Jan 2026 18:12:45 GMT   (1945kb)

Title: Observational Auditing of Label Privacy
Authors: Iden Kalemaj, Luca Melis, Maxime Boucher, Ilya Mironov, Saeed
  Mahloujifar
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2511.14084 ,  1945kb)
------------------------------------------------------------------------------
\\
arXiv:2511.16073
replaced with revised version Tue, 20 Jan 2026 10:04:41 GMT   (467kb)

Title: Mathematical Framework for Custom Reward Functions in Job Application
  Evaluation using Reinforcement Learning
Authors: Shreyansh Jain, Madhav Singhvi, Shreya Rahul Jain, Pranav S, Dishaa
  Lokesh, Naren Chittibabu, Akash Anandhan
Categories: cs.LG cs.AI cs.MA
Comments: 13 pages, 4 figures, 2 equations, 3 Tables
Journal-ref: ICCCA 2025, pp. 1-6
DOI: 10.1109/ICCCA66364.2025.11325393
\\ ( https://arxiv.org/abs/2511.16073 ,  467kb)
------------------------------------------------------------------------------
\\
arXiv:2511.17100
replaced with revised version Sat, 17 Jan 2026 22:02:35 GMT   (889kb)

Title: Geometric-disentangelment Unlearning
Authors: Duo Zhou, Yuji Zhang, Tianxin Wei, Ruizhong Qiu, Ke Yang, Xiao Lin,
  Cheng Qian, Jingrui He, Hanghang Tong, Heng Ji, Huan Zhang
Categories: cs.LG cs.AI cs.CL
Comments: 27 Pages
\\ ( https://arxiv.org/abs/2511.17100 ,  889kb)
------------------------------------------------------------------------------
\\
arXiv:2512.03994
replaced with revised version Sun, 18 Jan 2026 07:49:40 GMT   (11080kb)

Title: Training-Free Policy Violation Detection via Activation-Space Whitening
  in LLMs
Authors: Oren Rachmil, Avishag Shapira, Roy Betser, Itay Gershon, Omer Hofman,
  Asaf Shabtai, Yuval Elovici, Roman Vainshtein
Categories: cs.LG
Comments: Accepted to the AAAI 2026 Deployable AI (DAI) Workshop
\\ ( https://arxiv.org/abs/2512.03994 ,  11080kb)
------------------------------------------------------------------------------
\\
arXiv:2512.04351
replaced with revised version Sun, 18 Jan 2026 22:35:38 GMT   (163kb)

Title: Distance Is All You Need: Radial Dispersion for Uncertainty Estimation
  in Large Language Models
Authors: Manh Nguyen, Sunil Gupta, Hung Le
Categories: cs.LG
\\ ( https://arxiv.org/abs/2512.04351 ,  163kb)
------------------------------------------------------------------------------
\\
arXiv:2512.04475
replaced with revised version Sun, 18 Jan 2026 13:50:47 GMT   (1201kb)

Title: GraphBench: Next-generation graph learning benchmarking
Authors: Timo Stoll, Chendi Qian, Ben Finkelshtein, Ali Parviz, Darius Weber,
  Fabrizio Frasca, Hadar Shavit, Antoine Siraudin, Arman Mielke, Marie
  Anastacio, Erik M\"uller, Maya Bechler-Speicher, Michael Bronstein, Mikhail
  Galkin, Holger Hoos, Mathias Niepert, Bryan Perozzi, Jan T\"onshoff, and
  Christopher Morris
Categories: cs.LG cs.AI cs.NE stat.ML
Comments: Fixed issue with two bibliography entries
\\ ( https://arxiv.org/abs/2512.04475 ,  1201kb)
------------------------------------------------------------------------------
\\
arXiv:2512.05620
replaced with revised version Sun, 18 Jan 2026 06:49:45 GMT   (715kb)

Title: Hyperparameter Transfer Enables Consistent Gains of
  Matrix-Preconditioned Optimizers Across Scales
Authors: Shikai Qiu, Zixi Chen, Hoang Phan, Qi Lei, Andrew Gordon Wilson
Categories: cs.LG
Comments: NeurIPS 2025. Code available at:
  https://github.com/charliezchen/scaling-matrix-preconditioning
\\ ( https://arxiv.org/abs/2512.05620 ,  715kb)
------------------------------------------------------------------------------
\\
arXiv:2512.08121
replaced with revised version Mon, 19 Jan 2026 17:30:49 GMT   (56kb)

Title: Balanced Accuracy: The Right Metric for Evaluating LLM Judges --
  Explained through Youden's J statistic
Authors: Stephane Collot, Colin Fraser, Justin Zhao, William F. Shen, Timon
  Willi, Ilias Leontiadis
Categories: cs.LG cs.AI cs.CL
Comments: 10 pages, 5 figures
\\ ( https://arxiv.org/abs/2512.08121 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2512.09972
replaced with revised version Sun, 18 Jan 2026 11:16:21 GMT   (10401kb)

Title: SIP-BMM: Constructing Capability-Efficiency Pareto Set of LLMs via
  Bayesian Model Merging with Structural Importance Prior
Authors: Kesheng Chen, Yamin Hu, Zhenqian Zhu, Yiya Diao, Wenjian Luo
Categories: cs.LG cs.CL cs.NE
\\ ( https://arxiv.org/abs/2512.09972 ,  10401kb)
------------------------------------------------------------------------------
\\
arXiv:2512.10032
replaced with revised version Mon, 19 Jan 2026 10:01:30 GMT   (108kb)

Title: Cluster-Dags as Powerful Background Knowledge For Causal Discovery
Authors: Jan Marco Ruiz de Vargas, Kirtan Padh, Niki Kilbertus
Categories: cs.LG cs.AI stat.ML
Comments: 23 pages, 5 figures
\\ ( https://arxiv.org/abs/2512.10032 ,  108kb)
------------------------------------------------------------------------------
\\
arXiv:2512.10350
replaced with revised version Tue, 20 Jan 2026 16:35:18 GMT   (3717kb)

Title: Dynamics of Agentic Loops in Large Language Models: A Geometric Theory
  of Trajectories
Authors: Nicolas Tacheny
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2512.10350 ,  3717kb)
------------------------------------------------------------------------------
\\
arXiv:2512.14241
replaced with revised version Tue, 20 Jan 2026 08:49:28 GMT   (3361kb)

Title: Beyond MMD: Evaluating Graph Generative Models with Geometric Deep
  Learning
Authors: Salvatore Romano, Marco Grassia, Giuseppe Mangioni
Categories: cs.LG cs.AI physics.soc-ph
Comments: 16 pages, 4 figures
\\ ( https://arxiv.org/abs/2512.14241 ,  3361kb)
------------------------------------------------------------------------------
\\
arXiv:2512.14338
replaced with revised version Mon, 19 Jan 2026 12:28:25 GMT   (1067kb)

Title: Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn
  Graph Orbits
Authors: Michael Murray, Tenzin Chan, Kedar Karhadker, Christopher J. Hillar
Categories: cs.LG
MSC-class: 68T07, 05C90
ACM-class: I.2.6; G.2.2
\\ ( https://arxiv.org/abs/2512.14338 ,  1067kb)
------------------------------------------------------------------------------
\\
arXiv:2512.14908
replaced with revised version Sun, 18 Jan 2026 13:25:01 GMT   (495kb)

Title: ATLAS: Adaptive Topology-based Learning at Scale for Homophilic and
  Heterophilic Graphs
Authors: Turja Kundu and Sanjukta Bhowmick
Categories: cs.LG
Comments: Preprint
\\ ( https://arxiv.org/abs/2512.14908 ,  495kb)
------------------------------------------------------------------------------
\\
arXiv:2512.19649
replaced with revised version Sun, 18 Jan 2026 18:17:33 GMT   (1337kb)

Title: Deep Legendre Transform
Authors: Aleksey Minabutdinov and Patrick Cheridito
Categories: cs.LG math.OC
Comments: Accepted at NeurIPS 2025 (poster). NeurIPS page:
  https://neurips.cc/virtual/2025/loc/san-diego/poster/120307
MSC-class: Primary 90C25, Secondary 68T07, 65K10, 35F21
ACM-class: I.2.6; G.1.6; I.5.1
\\ ( https://arxiv.org/abs/2512.19649 ,  1337kb)
------------------------------------------------------------------------------
\\
arXiv:2512.20232
replaced with revised version Sat, 17 Jan 2026 07:28:52 GMT   (2288kb)

Title: Adaptive Multi-task Learning for Probabilistic Load Forecasting
Authors: Onintze Zaballa, Ver\'onica \'Alvarez, Santiago Mazuelas
Categories: cs.LG stat.AP
\\ ( https://arxiv.org/abs/2512.20232 ,  2288kb)
------------------------------------------------------------------------------
\\
arXiv:2512.20291
replaced with revised version Mon, 19 Jan 2026 08:53:06 GMT   (2568kb)

Title: Mixture-of-Experts with Gradient Conflict-Driven Subspace Topology
  Pruning for Emergent Modularity
Authors: Yuxing Gan and Ziyu Lei
Categories: cs.LG
\\ ( https://arxiv.org/abs/2512.20291 ,  2568kb)
------------------------------------------------------------------------------
\\
arXiv:2512.20861
replaced with revised version Sat, 17 Jan 2026 17:29:18 GMT   (1527kb)

Title: Memory-Efficient Acceleration of Block Low-Rank Foundation Models on
  Resource Constrained GPUs
Authors: Pierre Abillama, Changwoo Lee, Juechu Dong, David Blaauw, Dennis
  Sylvester, Hun-Seok Kim
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2512.20861 ,  1527kb)
------------------------------------------------------------------------------
\\
arXiv:2512.20905
replaced with revised version Tue, 20 Jan 2026 09:14:23 GMT   (35499kb)

Title: DiEC: Diffusion Embedded Clustering
Authors: Haidong Hu, Xiaoyu Zheng, Jin Zhou, Yingxu Wang, Rui Wang, Pei Dong,
  Shiyuan Han, Lin Wang, C. L. Philip Chen, Tong Zhang, Yuehui Chen
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2512.20905 ,  35499kb)
------------------------------------------------------------------------------
\\
arXiv:2512.21650
replaced with revised version Tue, 20 Jan 2026 09:43:17 GMT   (587kb)

Title: Physic-HM: Restoring Physical Generative Logic in Multimodal Anomaly
  Detection via Hierarchical Modulation
Authors: Xiao Liu, Junchen Jin, Yanjie Zhao, Zhixuan Xing
Categories: cs.LG
Comments: Working in progress
\\ ( https://arxiv.org/abs/2512.21650 ,  587kb)
------------------------------------------------------------------------------
\\
arXiv:2512.22222
replaced with revised version Tue, 20 Jan 2026 09:31:37 GMT   (5967kb)

Title: M\"untz-Sz\'asz Networks: Neural Architectures with Learnable Power-Law
  Bases
Authors: Gnankan Landry Regis N'guessan
Categories: cs.LG cs.AI
Comments: V3: Corrected Full M\"untz Theorem (added constant function), fixed
  L2 projection error formula, clarified MLP bounds in terms of linear pieces.
  Acknowledgments added. Full code at
  https://github.com/ReFractals/muntz-szasz-networks
\\ ( https://arxiv.org/abs/2512.22222 ,  5967kb)
------------------------------------------------------------------------------
\\
arXiv:2512.22240
replaced with revised version Sun, 18 Jan 2026 16:38:58 GMT   (4938kb)

Title: EvoXplain: When Machine Learning Models Agree on Predictions but
  Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs
Authors: Chama Bensmail
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2512.22240 ,  4938kb)
------------------------------------------------------------------------------
\\
arXiv:2512.23192
replaced with revised version Tue, 20 Jan 2026 14:33:39 GMT   (42959kb)

Title: PGOT: A Physics-Geometry Operator Transformer for Complex PDEs
Authors: Zhuo Zhang, Xi Yang, Ying Miao, Xiaobin Hu, Yifu Gao, Yuan Zhao, Yong
  Yang, Canqun Yang, Boocheong Khoo
Categories: cs.LG
Comments: 24 pages, 17 figures
\\ ( https://arxiv.org/abs/2512.23192 ,  42959kb)
------------------------------------------------------------------------------
\\
arXiv:2512.23236
replaced with revised version Fri, 16 Jan 2026 22:31:23 GMT   (11383kb)

Title: KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI
  Accelerators at Meta
Authors: Gang Liao, Hongsen Qin, Ying Wang, Alicia Golden, Michael Kuchnik,
  Yavuz Yetim, Jia Jiunn Ang, Chunli Fu, Yihan He, Samuel Hsia, Zewei Jiang,
  Dianshi Li, Uladzimir Pashkevich, Varna Puvvada, Feng Shi, Matt Steiner,
  Ruichao Xiao, Nathan Yan, Xiayu Yu, Zhou Fang, Roman Levenstein, Kunming Ho,
  Haishan Zhu, Alec Hammond, Richard Li, Ajit Mathews, Kaustubh Gondkar, Abdul
  Zainul-Abedin, Ketan Singh, Hongtao Yu, Wenyuan Chi, Barney Huang, Sean
  Zhang, Noah Weller, Zach Marine, Wyatt Cook, Carole-Jean Wu, Gaoxiang Liu
Categories: cs.LG cs.AI cs.AR cs.MA cs.PF
\\ ( https://arxiv.org/abs/2512.23236 ,  11383kb)
------------------------------------------------------------------------------
\\
arXiv:2512.24075
replaced with revised version Tue, 20 Jan 2026 11:31:54 GMT   (1251kb)

Title: Multi-Scenario Highway Lane-Change Intention Prediction: A Temporal
  Physics-Informed Multi-Modal Framework
Authors: Jiazhao Shi, Ziyu Wang, Yichen Lin, Shoufeng Lu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2512.24075 ,  1251kb)
------------------------------------------------------------------------------
\\
arXiv:2512.24145
replaced with revised version Sat, 17 Jan 2026 12:16:35 GMT   (395kb)

Title: When Does Pairing Seeds Reduce Variance? Evidence from a Multi-Agent
  Economic Simulation
Authors: Udit Sharma
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2512.24145 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2512.24555
replaced with revised version Sat, 17 Jan 2026 15:21:04 GMT   (26173kb)

Title: From Perception to Punchline: Empowering VLM with the Art of In-the-wild
  Meme
Authors: Xueyan Li, Yingyi Xue, Mengjie Jiang, Qingzi Zhu, Yazhe Niu
Categories: cs.LG
Comments: 46 pages, 20 figures
\\ ( https://arxiv.org/abs/2512.24555 ,  26173kb)
------------------------------------------------------------------------------
\\
arXiv:2512.24713
replaced with revised version Tue, 20 Jan 2026 14:13:05 GMT   (0kb,I)

Title: FPGA Co-Design for Efficient N:M Sparse and Quantized Model Inference
Authors: Fen-Yu Hsieh and Yun-Chang Teng and Ding-Yong Hong and Jan-Jan Wu
Categories: cs.LG cs.AR
Comments: Withdrawn due to substantial inconsistencies between the
  machine-learning pipeline and the independently developed FPGA-based hardware
  accelerator. The manuscript does not reflect a coherent, jointly developed
  system and a clearly integrated methodology
\\ ( https://arxiv.org/abs/2512.24713 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2601.02094
replaced with revised version Sat, 17 Jan 2026 09:44:22 GMT   (668kb)

Title: Horizon Activation Mapping for Neural Networks in Time Series
  Forecasting
Authors: Krupakar Hans and V A Kandappan
Categories: cs.LG math.FA
\\ ( https://arxiv.org/abs/2601.02094 ,  668kb)
------------------------------------------------------------------------------
\\
arXiv:2601.02543
replaced with revised version Sun, 18 Jan 2026 22:25:07 GMT   (386kb)

Title: Normalized Conditional Mutual Information Surrogate Loss for Deep Neural
  Classifiers
Authors: Linfeng Ye and Zhixiang Chi and Konstantinos N. Plataniotis and En-hui
  Yang
Categories: cs.LG cs.AI cs.CV cs.IT math.IT
Comments: 8 pages, 4 figures
\\ ( https://arxiv.org/abs/2601.02543 ,  386kb)
------------------------------------------------------------------------------
\\
arXiv:2601.02888
replaced with revised version Sun, 18 Jan 2026 07:38:30 GMT   (6328kb)

Title: RPIQ: Residual-Projected Multi-Collaboration Closed-Loop and Single
  Instance Quantization for Visually Impaired Assistance
Authors: Xuanyu Wang, Haisen Su, Jingtao Zhang, Xiangxiang Wang, Yongbin Yu,
  Manping Fan, Jialing Xiao, Bo Gong, Siqi Chen, Mingsheng Cao, Liyong Ren and
  Zhenglin Yang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2601.02888 ,  6328kb)
------------------------------------------------------------------------------
\\
arXiv:2601.03385
replaced with revised version Mon, 19 Jan 2026 08:25:15 GMT   (120kb)

Title: SIGMA: Scalable Spectral Insights for LLM Model Collapse
Authors: Yi Gu, Lingyou Pang, Xiangkun Ye, Tianyu Wang, Jianyu Lin, Carey E.
  Priebe, Alexander Aue
Categories: cs.LG math.PR
MSC-class: 68T50(Primary), 60F05(Secondary)
ACM-class: I.2.7; G.3
\\ ( https://arxiv.org/abs/2601.03385 ,  120kb)
------------------------------------------------------------------------------
\\
arXiv:2601.03706
replaced with revised version Sun, 18 Jan 2026 09:18:24 GMT   (9kb)

Title: The Geometry of the Pivot: A Note on Lazy Pivoted Cholesky and Farthest
  Point Sampling
Authors: Gil Shabat
Categories: cs.LG cs.NA math.NA
\\ ( https://arxiv.org/abs/2601.03706 ,  9kb)
------------------------------------------------------------------------------
\\
arXiv:2601.07473
replaced with revised version Sat, 17 Jan 2026 22:32:43 GMT   (176kb)

Title: AntiPaSTO: Self-Supervised Steering of Moral Reasoning
Authors: Michael J. Clark
Categories: cs.LG
Comments: Code is available at https://github.com/wassname/AntiPaSTO
\\ ( https://arxiv.org/abs/2601.07473 ,  176kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08257
replaced with revised version Mon, 19 Jan 2026 11:59:45 GMT   (16kb)

Title: On Evaluation of Unsupervised Feature Selection for Pattern
  Classification
Authors: Gyu-Il Kim, Dae-Won Kim, Jaesung Lee
Categories: cs.LG cs.AI
Comments: EurIPS 2025 Workshop: AI for Tabular Data
\\ ( https://arxiv.org/abs/2601.08257 ,  16kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08334
replaced with revised version Tue, 20 Jan 2026 08:31:33 GMT   (2077kb)

Title: Automated Machine Learning in Radiomics: A Comparative Evaluation of
  Performance, Efficiency and Accessibility
Authors: Jose Lozano-Montoya, Emilio Soria-Olivas, Almudena Fuster-Matanzo,
  Angel Alberich-Bayarri, Ana Jimenez-Pastor
Categories: cs.LG
Comments: 27 pages, 4 figures, 3 tables, code available, see
  https://github.com/joselznom/AutoML-Comparison-in-Radiomics
\\ ( https://arxiv.org/abs/2601.08334 ,  2077kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08646
replaced with revised version Mon, 19 Jan 2026 11:54:59 GMT   (329kb)

Title: Provably Safe Reinforcement Learning for Stochastic Reach-Avoid Problems
  with Entropy Regularization
Authors: Abhijit Mazumdar, Rafal Wisniewski, Manuela L. Bujorianu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2601.08646 ,  329kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08760
replaced with revised version Sat, 17 Jan 2026 01:05:07 GMT   (344kb)

Title: Adaptive Requesting in Decentralized Edge Networks via Non-Stationary
  Bandits
Authors: Yi Zhuang and Kun Yang and Xingran Chen
Categories: cs.LG cs.MA
\\ ( https://arxiv.org/abs/2601.08760 ,  344kb)
------------------------------------------------------------------------------
\\
arXiv:2601.09173
replaced with revised version Mon, 19 Jan 2026 18:16:24 GMT   (853kb)

Title: Geometric Stability: The Missing Axis of Representations
Authors: Prashant C. Raju
Categories: cs.LG cs.CL q-bio.QM stat.ML
\\ ( https://arxiv.org/abs/2601.09173 ,  853kb)
------------------------------------------------------------------------------
\\
arXiv:2601.09469
replaced with revised version Sun, 18 Jan 2026 10:03:48 GMT   (162kb)

Title: FairGU: Fairness-aware Graph Unlearning in Social Networks
Authors: Renqiang Luo, Yongshuai Yang, Huafei Huang, Qing Qing, Mingliang Hou,
  Ziqi Xu, Yi Yu, Jingjing Zhou, Feng Xia
Categories: cs.LG cs.AI
Comments: 9 pages, 2 figs, WWW 2026 accepted
\\ ( https://arxiv.org/abs/2601.09469 ,  162kb)
------------------------------------------------------------------------------
\\
arXiv:2601.09841
replaced with revised version Sat, 17 Jan 2026 02:01:19 GMT   (627kb)

Title: A pipeline for enabling path-specific causal fairness in observational
  health data
Authors: Aparajita Kashyap, Sara Matijevic, No\'emie Elhadad, Steven A.
  Kushner, Shalmali Joshi
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2601.09841 ,  627kb)
------------------------------------------------------------------------------
\\
arXiv:2601.09949
replaced with revised version Sun, 18 Jan 2026 15:10:01 GMT   (3579kb)

Title: Kinematic Tokenization: Optimization-Based Continuous-Time Tokens for
  Learnable Decision Policies in Noisy Time Series
Authors: Griffin Kearney
Categories: cs.LG cs.AI math.OC
Comments: Pre-print, 19 pages
\\ ( https://arxiv.org/abs/2601.09949 ,  3579kb)
------------------------------------------------------------------------------
\\
arXiv:2601.10471
replaced with revised version Sat, 17 Jan 2026 06:21:38 GMT   (1785kb)

Title: DeFlow: Decoupling Manifold Modeling and Value Maximization for Offline
  Policy Extraction
Authors: Zhancun Mu
Categories: cs.LG
Comments: 14 pages, 3 figures
\\ ( https://arxiv.org/abs/2601.10471 ,  1785kb)
------------------------------------------------------------------------------
\\
arXiv:2601.10705
replaced with revised version Sun, 18 Jan 2026 14:35:19 GMT   (39kb)

Title: Distributed Perceptron under Bounded Staleness, Partial Participation,
  and Noisy Communication
Authors: Keval Jain, Anant Raj, Saurav Prakash, Girish Varma
Categories: cs.LG
\\ ( https://arxiv.org/abs/2601.10705 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11265
replaced with revised version Mon, 19 Jan 2026 12:56:49 GMT   (46kb)

Title: Sample-Near-Optimal Agnostic Boosting with Improved Running Time
Authors: Arthur da Cunha, Mikael M{\o}ller H{\o}gsgaard, Andrea Paudice
Categories: cs.LG
Comments: 28 pages, 0 figures. Accepted at the 37th International Conference on
  Algorithmic Learning Theory (ALT 2026)
\\ ( https://arxiv.org/abs/2601.11265 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11516
replaced with revised version Mon, 19 Jan 2026 16:05:05 GMT   (602kb)

Title: Building Production-Ready Probes For Gemini
Authors: J\'anos Kram\'ar, Joshua Engels, Zheng Wang, Bilal Chughtai, Rohin
  Shah, Neel Nanda, Arthur Conmy
Categories: cs.LG cs.AI cs.CL
Comments: v2 (minor typo fixes)
\\ ( https://arxiv.org/abs/2601.11516 ,  602kb)
------------------------------------------------------------------------------
\\
arXiv:2301.02206
replaced with revised version Mon, 19 Jan 2026 12:15:40 GMT   (3153kb)

Title: Lessons from Formally Verified Deployed Software Systems (Extended
  version)
Authors: Li Huang, Sophie Ebersold, Alexander Kogtenkov, Bertrand Meyer,
  Yinling Liu
Categories: cs.SE
Comments: 93 pages, 26 figures, 9 previous surveys, 32 systems descriptions
ACM-class: A.1; D.0; D.2; F.0; F.3; K.2; K.6
DOI: 10.1145/3785652
\\ ( https://arxiv.org/abs/2301.02206 ,  3153kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15547
replaced with revised version Sat, 17 Jan 2026 11:32:15 GMT   (4887kb)

Title: Quantum Approximate Optimization Algorithm for Test Case Optimization
Authors: Xinyi Wang, Shaukat Ali, Tao Yue, Paolo Arcaini
Categories: cs.SE
Journal-ref: in IEEE Transactions on Software Engineering, vol. 50, no. 12, pp.
  3249-3264, Dec. 2024
DOI: 10.1109/TSE.2024.3479421
\\ ( https://arxiv.org/abs/2312.15547 ,  4887kb)
------------------------------------------------------------------------------
\\
arXiv:2406.02034
replaced with revised version Sun, 18 Jan 2026 17:24:49 GMT   (338kb)

Title: Generator-Based Fuzzers with Type-Based Targeted Mutation
Authors: Soha Hussein, Stephen McCamant, Mike Whalen
Categories: cs.SE
Comments: Clarifying motivational example
\\ ( https://arxiv.org/abs/2406.02034 ,  338kb)
------------------------------------------------------------------------------
\\
arXiv:2502.08739
replaced with revised version Tue, 20 Jan 2026 08:44:31 GMT   (905kb)

Title: A Taxonomy of Real Faults in Hybrid Quantum-Classical Architectures
Authors: Avner Bensoussan, Gunel Jahangirova, Mohammad Reza Mousavi
Categories: cs.SE
DOI: 10.1145/3788677
\\ ( https://arxiv.org/abs/2502.08739 ,  905kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20139
replaced with revised version Mon, 19 Jan 2026 21:14:10 GMT   (1333kb)

Title: StructEval: Benchmarking LLMs' Capabilities to Generate Structural
  Outputs
Authors: Jialin Yang, Dongfu Jiang, Lipeng He, Sherman Siu, Yuxuan Zhang, Disen
  Liao, Zhuofeng Li, Huaye Zeng, Yiming Jia, Haozhe Wang, Benjamin Schneider,
  Chi Ruan, Wentao Ma, Zhiheng Lyu, Yifei Wang, Yi Lu, Quy Duc Do, Ziyan Jiang,
  Ping Nie, Wenhu Chen
Categories: cs.SE cs.AI cs.CL
Comments: 24 pages, 8 figures, 14 tables
\\ ( https://arxiv.org/abs/2505.20139 ,  1333kb)
------------------------------------------------------------------------------
\\
arXiv:2506.09550
replaced with revised version Tue, 20 Jan 2026 12:18:25 GMT   (5541kb)

Title: Integrating Symbolic Execution with LLMs for Automated Generation of
  Program Specifications
Authors: Fanpeng Yang, Xu Ma, Shuling Wang, Xiong Xu, Qinxiang Cao, Naijun
  Zhan, Xiaofeng Li, Bin Gu
Categories: cs.SE
\\ ( https://arxiv.org/abs/2506.09550 ,  5541kb)
------------------------------------------------------------------------------
\\
arXiv:2506.12084
replaced with revised version Tue, 20 Jan 2026 09:41:29 GMT   (474kb,D)

Title: The CAISAR Platform: Extending the Reach of Machine Learning
  Specification and Verification
Authors: Michele Alberti (LSL), Fran\c{c}ois Bobot (LSL), Julien Girard-Satabin
  (LSL), Alban Grastien (LSL), Aymeric Varasse, Zakaria Chihani (LSL)
Categories: cs.SE cs.AI cs.CL cs.FL cs.NE
\\ ( https://arxiv.org/abs/2506.12084 ,  474kb)
------------------------------------------------------------------------------
\\
arXiv:2506.17306
replaced with revised version Fri, 16 Jan 2026 19:37:26 GMT   (3074kb)

Title: Challenges and Practices in Quantum Software Testing and Debugging:
  Insights from Practitioners
Authors: Jake Zappin, Trevor Stalnaker, Oscar Chaparro and Denys Poshyvanyk
Categories: cs.SE
\\ ( https://arxiv.org/abs/2506.17306 ,  3074kb)
------------------------------------------------------------------------------
\\
arXiv:2506.17642
replaced with revised version Sat, 17 Jan 2026 07:37:13 GMT   (2378kb)

Title: May the Feedback Be with You! Unlocking the Power of Feedback-Driven
  Deep Learning Framework Fuzzing via LLMs
Authors: Shaoyu Yang, Chunrong Fang, Haifeng Lin, Xiang Chen, Jia Liu, Zhenyu
  Chen
Categories: cs.SE
\\ ( https://arxiv.org/abs/2506.17642 ,  2378kb)
------------------------------------------------------------------------------
\\
arXiv:2507.03160
replaced with revised version Sun, 18 Jan 2026 20:23:00 GMT   (687kb)

Title: Assessing Small Language Models for Code Generation: An Empirical Study
  with Benchmarks
Authors: Md Mahade Hasan and Muhammad Waseem and Kai-Kristian Kemell and Jussi
  Rasku and Juha Ala-Rantala and Pekka Abrahamsson
Categories: cs.SE
Comments: 17 pages, 10 Tables, 57 figures. Includes benchmarks and multilingual
  evaluation. Submitted to the Journal of Systems and Software
\\ ( https://arxiv.org/abs/2507.03160 ,  687kb)
------------------------------------------------------------------------------
\\
arXiv:2507.05269
replaced with revised version Sat, 17 Jan 2026 19:21:17 GMT   (908kb)

Title: CoRe: Benchmarking LLMs Code Reasoning Capabilities through Static
  Analysis Tasks
Authors: Danning Xie, Mingwei Zheng, Xuwei Liu, Jiannan Wang, Chengpeng Wang,
  Lin Tan, Xiangyu Zhang
Categories: cs.SE cs.AI
Comments: NeurIPS 2025 Datasets & Benchmarks Spotlight
\\ ( https://arxiv.org/abs/2507.05269 ,  908kb)
------------------------------------------------------------------------------
\\
arXiv:2509.05941
replaced with revised version Sat, 17 Jan 2026 14:57:55 GMT   (3420kb)

Title: Code2MCP: Transforming Code Repositories into MCP Services
Authors: Chaoqian Ouyang, Ling Yue, Shimin Di, Libin Zheng, Linan Yue, Shaowu
  Pan, Jian Yin, Min-Ling Zhang
Categories: cs.SE cs.LG cs.MA
\\ ( https://arxiv.org/abs/2509.05941 ,  3420kb)
------------------------------------------------------------------------------
\\
arXiv:2509.12087
replaced with revised version Sat, 17 Jan 2026 06:53:48 GMT   (2033kb)

Title: TransLibEval: Demystify Large Language Models' Capability in Third-party
  Library-targeted Code Translation
Authors: Pengyu Xue, Kunwu Zheng, Zhen Yang, Yifei Pei, Linhao Wu, Jiahui Dong,
  Xiapu Luo, Yan Xiao, Fei Liu, Yuxuan Zhang, Xiran Lyu, Xianhang Li, Xuanyu
  Zhu, Chengyi Wang
Categories: cs.SE
Comments: 24 pages, 5 figures, accepted by FSE 2026 (The ACM International
  Conference on the Foundations of Software Engineering)
\\ ( https://arxiv.org/abs/2509.12087 ,  2033kb)
------------------------------------------------------------------------------
\\
arXiv:2509.24032
replaced with revised version Sun, 18 Jan 2026 05:14:25 GMT   (2583kb)

Title: SandCell: Sandboxing Rust Beyond Unsafe Code
Authors: Jialun Zhang, Merve Gulmez, Thomas Nyman, Gang Tan
Categories: cs.SE cs.CR
\\ ( https://arxiv.org/abs/2509.24032 ,  2583kb)
------------------------------------------------------------------------------
\\
arXiv:2510.02854
replaced with revised version Tue, 20 Jan 2026 17:53:17 GMT   (626kb)

Title: C2|Q>: A Robust Framework for Bridging Classical and Quantum Software
  Development
Authors: Boshuai Ye, Arif Ali Khan, Teemu Pihkakoski, Peng Liang, Muhammad
  Azeem Akbar, Matti Silveri, Lauri Malmi
Categories: cs.SE
Comments: 55 pages, 11 images, 15 tables, Manuscript revision submitted to a
  journal (2026)
\\ ( https://arxiv.org/abs/2510.02854 ,  626kb)
------------------------------------------------------------------------------
\\
arXiv:2510.08996
replaced with revised version Sat, 17 Jan 2026 07:42:49 GMT   (3204kb)

Title: Saving SWE-Bench: A Benchmark Mutation Approach for Realistic Agent
  Evaluation
Authors: Spandan Garg, Benjamin Steenhoek, Yufan Huang
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2510.08996 ,  3204kb)
------------------------------------------------------------------------------
\\
arXiv:2510.18557
replaced with revised version Tue, 20 Jan 2026 13:17:19 GMT   (108kb)

Title: When Abstraction Breaks Physics: Rethinking Modular Design in Quantum
  Software
Authors: Jianjun Zhao
Categories: cs.SE quant-ph
Comments: Accepted at the NIER track of the 40th IEEE/ACM International
  Conference on Automated Software Engineering (ASE 2025)
\\ ( https://arxiv.org/abs/2510.18557 ,  108kb)
------------------------------------------------------------------------------
\\
arXiv:2510.26538
replaced with revised version Mon, 19 Jan 2026 13:23:45 GMT   (43kb)

Title: Empirical and Sustainability Aspects of Software Engineering Research in
  the Era of Large Language Models: A Reflection
Authors: David Williams, Max Hort, Maria Kechagia, Aldeida Aleti, Justyna
  Petke, Federica Sarro
Categories: cs.SE
Comments: 5 pages, Camera Ready Accepted at ICSE-NIER 2026
\\ ( https://arxiv.org/abs/2510.26538 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2510.26676
replaced with revised version Fri, 16 Jan 2026 22:38:33 GMT   (267kb)

Title: Process-based Indicators of Vulnerability Re-Introducing Code Changes:
  An Exploratory Case Study
Authors: Samiha Shimmi, Nicholas M. Synovic, Mona Rahimi, and George K.
  Thiruvathukal
Categories: cs.SE
Comments: 9 pages, 6 figures; Samiha Shimmi and Nicholas M. Synovic contributed
  equally to this work (co-first authors); Mona Rahimi and George K.
  Thiruvathukal contributed equally to this work (co-supervisors)
\\ ( https://arxiv.org/abs/2510.26676 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2511.23213
replaced with revised version Tue, 20 Jan 2026 10:39:37 GMT   (60kb)

Title: Mind the GAPS: Bridging the GAPS between Targeted Dynamic Analysis and
  Static Path Reconstruction in Android Apps
Authors: Samuele Doria, Eleonora Losiouk
Categories: cs.SE
\\ ( https://arxiv.org/abs/2511.23213 ,  60kb)
------------------------------------------------------------------------------
\\
arXiv:2512.16272
replaced with revised version Sun, 18 Jan 2026 14:05:07 GMT   (335kb)

Title: Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation
  Pitfalls
Authors: Ora Nova Fandina, Eitan Farchi, Shmulik Froimovich, Raviv Gal, Wesam
  Ibraheem, Rami Katan, Alice Podolsky
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2512.16272 ,  335kb)
------------------------------------------------------------------------------
\\
arXiv:2512.17363
replaced with revised version Tue, 20 Jan 2026 01:49:22 GMT   (151kb)

Title: What You Trust Is Insecure: Demystifying How Developers (Mis)Use Trusted
  Execution Environments in Practice
Authors: Yuqing Niu, Jieke Shi, Ruidong Han, Ye Liu, Chengyan Ma, Yunbo Lyu,
  and David Lo
Categories: cs.SE cs.CR
Comments: Accepted by the 33rd IEEE International Conference on Software
  Analysis, Evolution and Reengineering (SANER 2026), 12 Pages
\\ ( https://arxiv.org/abs/2512.17363 ,  151kb)
------------------------------------------------------------------------------
\\
arXiv:2601.02438
replaced with revised version Tue, 20 Jan 2026 07:00:06 GMT   (934kb)

Title: Focus on What Matters: Fisher-Guided Adaptive Multimodal Fusion for
  Vulnerability Detection
Authors: Yun Bian, Yi Chen, HaiQuan Wang, ShiHao Li, Zhe Cui
Categories: cs.SE cs.AI cs.CR
\\ ( https://arxiv.org/abs/2601.02438 ,  934kb)
------------------------------------------------------------------------------
\\
arXiv:2601.06266
replaced with revised version Tue, 20 Jan 2026 18:25:44 GMT   (1027kb)

Title: Self-Admitted Technical Debt in LLM Software: An Empirical Comparison
  with ML and Non-ML Software
Authors: Niruthiha Selvanayagam, Taher A. Ghaleb, Manel Abdellatif
Categories: cs.SE
Comments: Accepted to SANER 2026 (IEEE International Conference on Software
  Analysis, Evolution and Reengineering)
\\ ( https://arxiv.org/abs/2601.06266 ,  1027kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08856
replaced with revised version Mon, 19 Jan 2026 18:05:08 GMT   (984kb)

Title: LAUDE: LLM-Assisted Unit Test Generation and Debugging of Hardware
  DEsigns
Authors: Deeksha Nandal, Riccardo Revalor, Soham Dan, Debjit Pal
Categories: cs.SE cs.AI
Comments: 18 Pages, 21 Figures, Submitted to ARR Review
\\ ( https://arxiv.org/abs/2601.08856 ,  984kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08857
replaced with revised version Sun, 18 Jan 2026 15:48:31 GMT   (0kb,I)

Title: Revisiting Software Engineering Education in the Era of Large Language
  Models: A Curriculum Adaptation and Academic Integrity Framework
Authors: Mustafa Degerli
Categories: cs.SE cs.AI
Comments: The conclusion part is not complete. The correct and complete paper
  will not be uploaded
\\ ( https://arxiv.org/abs/2601.08857 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2601.09822
replaced with revised version Mon, 19 Jan 2026 19:10:53 GMT   (28kb)

Title: LLM-Based Agentic Systems for Software Engineering: Challenges and
  Opportunities
Authors: Yongjian Tang, Thomas Runkler
Categories: cs.SE cs.AI
Comments: Accepted to GenSE 2026 workshop
\\ ( https://arxiv.org/abs/2601.09822 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14080
replaced with revised version Mon, 19 Jan 2026 10:52:44 GMT   (10102kb)

Title: Eye-tracked Virtual Reality: A Comprehensive Survey on Methods and
  Privacy Challenges
Authors: Efe Bozkir and S\"uleyman \"Ozdel and Mengdi Wang and Brendan
  David-John and Hong Gao and Kevin Butler and Eakta Jain and Enkelejda Kasneci
Categories: cs.HC cs.AI cs.CR cs.GR cs.LG
Comments: Accepted for publication in the Proceedings of the IEEE
DOI: 10.1109/JPROC.2026.3653661
\\ ( https://arxiv.org/abs/2305.14080 ,  10102kb)
------------------------------------------------------------------------------
\\
arXiv:2308.00377
replaced with revised version Mon, 19 Jan 2026 17:20:52 GMT   (4846kb)

Title: Shape Completion with Prediction of Uncertain Regions
Authors: Matthias Humt, Dominik Winkelbauer, Ulrich Hillenbrand
Categories: cs.CV cs.AI cs.LG cs.RO
Comments: 7 pages, 5 figures, Published in IROS 2023. Project page:
  https://hummat.github.io/2023-iros-uncertain/
Journal-ref: in Proc. 2023 IEEE/RSJ Int. Conf. on Intelligent Robots and
  Systems (IROS), Detroit, MI, USA, Oct. 2023, pp. 1215-1221
DOI: 10.1109/IROS55552.2023.10342487
\\ ( https://arxiv.org/abs/2308.00377 ,  4846kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20350
replaced with revised version Mon, 19 Jan 2026 17:56:35 GMT   (4004kb)

Title: Combining Shape Completion and Grasp Prediction for Fast and Versatile
  Grasping with a Multi-Fingered Hand
Authors: Matthias Humt, Dominik Winkelbauer, Ulrich Hillenbrand and Berthold
  B\"auml
Categories: cs.RO cs.AI cs.CV cs.LG
Comments: 8 pages, 10 figures, 3 tables, 1 algorithm. Published in Humanoids
  2023. Project page: https://aidx-lab.org/grasping/humanoids23
Journal-ref: 2023 IEEE-RAS 22nd International Conference on Humanoid Robots
  (Humanoids), pp. 1-8, 2023
DOI: 10.1109/HUMANOIDS57100.2023.10375210
\\ ( https://arxiv.org/abs/2310.20350 ,  4004kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02317
replaced with revised version Sat, 17 Jan 2026 23:05:58 GMT   (1761kb)

Title: GNN2R: Weakly-Supervised Rationale-Providing Question Answering over
  Knowledge Graphs
Authors: Ruijie Wang, Luca Rossetto, Michael Cochez, Abraham Bernstein
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2312.02317 ,  1761kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10061
replaced with revised version Tue, 20 Jan 2026 18:02:51 GMT   (3661kb)

Title: DiffusionAgent: Navigating Expert Models for Agentic Image Generation
Authors: Jie Qin, Jie Wu, Weifeng Chen, Yueming Lyu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2401.10061 ,  3661kb)
------------------------------------------------------------------------------
\\
arXiv:2404.08113
replaced with revised version Sat, 17 Jan 2026 18:16:19 GMT   (2648kb)

Title: Predictive Handover Strategy in 6G and Beyond: A Deep and Transfer
  Learning Approach
Authors: Ioannis Panitsas, Akrit Mudvari, Ali Maatouk, Leandros Tassiulas
Categories: cs.NI cs.AI
\\ ( https://arxiv.org/abs/2404.08113 ,  2648kb)
------------------------------------------------------------------------------
\\
arXiv:2406.18944
replaced with revised version Sat, 17 Jan 2026 23:23:17 GMT   (18637kb)

Title: Rethinking and Red-Teaming Protective Perturbation in Personalized
  Diffusion Models
Authors: Yixin Liu, Ruoxi Chen, Xun Chen, Lichao Sun
Categories: cs.CV cs.AI cs.CR
Comments: Our code is available at
  https://github.com/liuyixin-louis/DiffShortcut
\\ ( https://arxiv.org/abs/2406.18944 ,  18637kb)
------------------------------------------------------------------------------
\\
arXiv:2409.07704 (*cross-listing*)
replaced with revised version Sat, 17 Jan 2026 22:13:51 GMT   (105kb)

Title: Super Monotonic Alignment Search
Authors: Junhyeok Lee, Hyeongju Kim
Categories: eess.AS cs.AI
Comments: Accepted to ICASSP 2026
\\ ( https://arxiv.org/abs/2409.07704 ,  105kb)
------------------------------------------------------------------------------
\\
arXiv:2409.10825
replaced with revised version Tue, 20 Jan 2026 01:38:54 GMT   (117kb)

Title: Unveiling and Mitigating Bias in Large Language Model Recommendations: A
  Path to Fairness
Authors: Anindya Bijoy Das and Shahnewaz Karim Sakib
Categories: cs.IR cs.AI cs.ET cs.LG
\\ ( https://arxiv.org/abs/2409.10825 ,  117kb)
------------------------------------------------------------------------------
\\
arXiv:2410.00475
replaced with revised version Sat, 17 Jan 2026 19:37:40 GMT   (71kb)

Title: Probabilistic Analysis of Copyright Disputes and Generative AI Safety
Authors: Hiroaki Chiba-Okabe
Categories: cs.CY cs.AI
Comments: 5 pages
Journal-ref: Proc. 20th Int. Conf. on Artificial Intelligence and Law (ICAIL
  '25), ACM, pp. 470-474 (2026)
\\ ( https://arxiv.org/abs/2410.00475 ,  71kb)
------------------------------------------------------------------------------
\\
arXiv:2410.02650
replaced with revised version Mon, 19 Jan 2026 16:52:09 GMT   (531kb)

Title: Undesirable Memorization in Large Language Models: A Survey
Authors: Ali Satvaty, Suzan Verberne, Fatih Turkmen
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2410.02650 ,  531kb)
------------------------------------------------------------------------------
\\
arXiv:2412.11500
replaced with revised version Sun, 18 Jan 2026 14:28:46 GMT   (207kb)

Title: Intention Knowledge Graph Construction for User Intention Relation
  Modeling
Authors: Jiaxin Bai, Zhaobo Wang, Junfei Cheng, Dan Yu, Zerui Huang, Weiqi
  Wang, Xin Liu, Chen Luo, Yanming Zhu, Bo Li, and Yangqiu Song
Categories: cs.CL cs.AI
Comments: Accepted by EACL'26
\\ ( https://arxiv.org/abs/2412.11500 ,  207kb)
------------------------------------------------------------------------------
\\
arXiv:2501.02267 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 17:45:41 GMT   (87kb)

Title: Towards a constructive framework for control theory
Authors: Pavel Osinenko
Categories: math.OC cs.AI cs.SY eess.SY
Comments: Published under: https://ieeexplore.ieee.org/document/9419858
Journal-ref: in IEEE Control Systems Letters, vol. 6, pp. 379-384, 2022
DOI: 10.1109/LCSYS.2021.3076972
\\ ( https://arxiv.org/abs/2501.02267 ,  87kb)
------------------------------------------------------------------------------
\\
arXiv:2501.16029
replaced with revised version Mon, 19 Jan 2026 15:24:33 GMT   (4267kb)

Title: FDLLM: A Dedicated Detector for Black-Box LLMs Fingerprinting
Authors: Zhiyuan Fu, Junfan Chen, Lan Zhang, Ting Yang, Jun Niu, Hongyu Sun,
  Ruidong Li, Peng Liu, Jice Wang, Fannv He, Qiuling Yue, Yuqing Zhang
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2501.16029 ,  4267kb)
------------------------------------------------------------------------------
\\
arXiv:2502.04345
replaced with revised version Tue, 20 Jan 2026 13:42:01 GMT   (9748kb)

Title: Jingfang: An LLM-Based Multi-Agent System for Precise Medical
  Consultation and Syndrome Differentiation in Traditional Chinese Medicine
Authors: Yehan Yang and Tianhao Ma and Ruotai Li and Xinhan Zheng and Guodong
  Shan
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2502.04345 ,  9748kb)
------------------------------------------------------------------------------
\\
arXiv:2502.06976
replaced with revised version Sat, 17 Jan 2026 16:01:49 GMT   (3188kb)

Title: Who is Helping Whom? Analyzing Inter-dependencies to Evaluate
  Cooperation in Human-AI Teaming
Authors: Upasana Biswas, Vardhan Palod, Siddhant Bhambri, Subbarao Kambhampati
Categories: cs.MA cs.AI
\\ ( https://arxiv.org/abs/2502.06976 ,  3188kb)
------------------------------------------------------------------------------
\\
arXiv:2502.09680
replaced with revised version Tue, 20 Jan 2026 08:56:31 GMT   (8060kb)

Title: Object-Centric Latent Action Learning
Authors: Albina Klepach, Alexander Nikulin, Ilya Zisman, Denis Tarasov,
  Alexander Derevyagin, Andrei Polubarov, Nikita Lyubaykin, Igor Kiselev,
  Vladislav Kurenkov
Categories: cs.CV cs.AI
Comments: Accepted by AAAI 2026 (Oral). Source code:
  https://github.com/dunnolab/object-centric-lapo
\\ ( https://arxiv.org/abs/2502.09680 ,  8060kb)
------------------------------------------------------------------------------
\\
arXiv:2502.12109
replaced with revised version Tue, 20 Jan 2026 06:47:31 GMT   (13579kb)

Title: Generative Personality Simulation via Theory-Informed Structured
  Interview
Authors: Pengda Wang, Huiqi Zou, Han Jiang, Hanjie Chen, Tianjun Sun, Xiaoyuan
  Yi, Ziang Xiao, Frederick L. Oswald
Categories: cs.CL cs.AI
Comments: Accepted at EACL 2026; 87 Pages, 68 Tables, 10 Figures
\\ ( https://arxiv.org/abs/2502.12109 ,  13579kb)
------------------------------------------------------------------------------
\\
arXiv:2502.17091
replaced with revised version Mon, 19 Jan 2026 16:39:11 GMT   (3290kb)

Title: Comparing the Framing Effect in Humans and LLMs on Naturally Occurring
  Texts
Authors: Gili Lior, Liron Nacchace, Gabriel Stanovsky
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2502.17091 ,  3290kb)
------------------------------------------------------------------------------
\\
arXiv:2503.04121
replaced with revised version Sat, 17 Jan 2026 05:37:23 GMT   (12244kb)

Title: Simple Self Organizing Map with Visual Transformer
Authors: Alan Luo, Kaiwen Yuan
Categories: cs.CV cs.AI cs.LG
Comments: 5 pages, 4 figures. Submitted to IEEE. All experiments and code work
  were performed by the first author, with the second author serving in a
  PI/mentor role, guiding the progression of the work
MSC-class: 65D19 (Primary)
ACM-class: I.2.6; I.2.10; I.5.1
Journal-ref: IEEE Signal Processing Letters, 2025, pp. 1-5
DOI: 10.1109/LSP.2025.3643388
\\ ( https://arxiv.org/abs/2503.04121 ,  12244kb)
------------------------------------------------------------------------------
\\
arXiv:2503.15550
replaced with revised version Tue, 20 Jan 2026 02:51:27 GMT   (392kb)

Title: Zero-Knowledge Federated Learning: A New Trustworthy and
  Privacy-Preserving Distributed Learning Paradigm
Authors: Taotao Wang, Yuxin Jin, Qing Yang, Yihan Xia, Long Shi, and Shengli
  Zhang
Categories: cs.CR cs.AI
Comments: Accepted by IEEE Communications Magazine. Copyright IEEE
\\ ( https://arxiv.org/abs/2503.15550 ,  392kb)
------------------------------------------------------------------------------
\\
arXiv:2503.19075
replaced with revised version Tue, 20 Jan 2026 14:57:23 GMT   (65kb)

Title: The Case for "Thick Evaluations" of Cultural Representation in AI
Authors: Rida Qadri, Mark Diaz, Ding Wang, Michael Madaio
Categories: cs.CY cs.AI cs.HC
Comments: 10 pages
Journal-ref: Qadri, R., Diaz, M., Wang, D., & Madaio, M. (2025). The Case for
  "Thick Evaluations" of Cultural Representation in AI. Proceedings of the
  AAAI/ACM Conference on AI, Ethics, and Society, 8(3), 2067-2080
DOI: 10.1609/aies.v8i3.36696
\\ ( https://arxiv.org/abs/2503.19075 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2504.13745
replaced with revised version Tue, 20 Jan 2026 15:03:24 GMT   (21281kb)

Title: ESPLoRA: Enhanced Spatial Precision with Low-Rank Adaption in
  Text-to-Image Diffusion Models for High-Definition Synthesis
Authors: Andrea Rigo, Luca Stornaiuolo, Mauro Martino, Bruno Lepri, Nicu Sebe
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2504.13745 ,  21281kb)
------------------------------------------------------------------------------
\\
arXiv:2504.16113
replaced with revised version Sat, 17 Jan 2026 16:06:00 GMT   (4080kb)

Title: AI-Based Vulnerability Analysis of NFT Smart Contracts
Authors: Xin Wang, Xiaoqi Li
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2504.16113 ,  4080kb)
------------------------------------------------------------------------------
\\
arXiv:2504.17901
replaced with revised version Sat, 17 Jan 2026 20:49:47 GMT   (23772kb)

Title: Beyond Task and Motion Planning: Hierarchical Robot Planning with
  General-Purpose Skills
Authors: Benned Hedegaard, Yichen Wei, Ahmed Jaafar, Stefanie Tellex, George
  Konidaris, and Naman Shah
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2504.17901 ,  23772kb)
------------------------------------------------------------------------------
\\
arXiv:2505.00295
replaced with revised version Mon, 19 Jan 2026 15:40:03 GMT   (414kb)

Title: Fine-grained spatial-temporal perception for gas leak segmentation
Authors: Xinlong Zhao, Shan Du
Categories: cs.CV cs.AI
Comments: Accepted at the 2025 IEEE International Conference on Image
  Processing (ICIP)
MSC-class: 68T45 (Primary), 68T07 (Secondary)
ACM-class: I.2.10; I.4.6
Journal-ref: IEEE International Conference on Image Processing (ICIP), pp.
  869-874, 2025
DOI: 10.1109/ICIP55913.2025.11084304
\\ ( https://arxiv.org/abs/2505.00295 ,  414kb)
------------------------------------------------------------------------------
\\
arXiv:2505.17654
replaced with revised version Mon, 19 Jan 2026 20:15:19 GMT   (18616kb)

Title: EVADE-Bench: Multimodal Benchmark for Evasive Content Detection in
  E-Commerce Applications
Authors: Ancheng Xu, Zhihao Yang, Jingpeng Li, Guanghu Yuan, Longze Chen, Liang
  Yan, Jiehui Zhou, Zhen Qin, Hengyu Chang, Hamid Alinejad-Rokny, Min Yang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2505.17654 ,  18616kb)
------------------------------------------------------------------------------
\\
arXiv:2505.20776
replaced with revised version Mon, 19 Jan 2026 09:43:38 GMT   (227kb)

Title: SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long
  Sequences
Authors: Jungyoub Cha, Hyunjong Kim, Sungzoon Cho
Categories: cs.CL cs.AI cs.LG
ACM-class: I.2.7; C.4
\\ ( https://arxiv.org/abs/2505.20776 ,  227kb)
------------------------------------------------------------------------------
\\
arXiv:2505.21092
replaced with revised version Mon, 19 Jan 2026 13:17:26 GMT   (2273kb)

Title: BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and
  Cultural Knowledge
Authors: Daeen Kabir, Minhajur Rahman Chowdhury Mahim, Sheikh Shafayat, Adnan
  Sadik, Arian Ahmed, Eunsu Kim, Alice Oh
Categories: cs.CL cs.AI
Comments: Accepted at BLP Workshop, IJCNLP-AACL 2025
\\ ( https://arxiv.org/abs/2505.21092 ,  2273kb)
------------------------------------------------------------------------------
\\
arXiv:2505.23292
replaced with revised version Tue, 20 Jan 2026 12:09:20 GMT   (11328kb)

Title: Federated Unsupervised Semantic Segmentation
Authors: Evangelos Charalampakis, Vasileios Mygdalis, Ioannis Pitas
Categories: cs.CV cs.AI
Comments: Accepted for publication in Neurocomputing
\\ ( https://arxiv.org/abs/2505.23292 ,  11328kb)
------------------------------------------------------------------------------
\\
arXiv:2505.23783 (*cross-listing*)
replaced with revised version Sat, 17 Jan 2026 16:59:14 GMT   (0kb,I)

Title: Boosting In-Context Learning in LLMs Through the Lens of Classical
  Supervised Learning
Authors: Korel Gundem, Juncheng Dong, Dennis Zhang, Vahid Tarokh, Zhengling Qi
Categories: stat.ML cs.AI cs.CL cs.LG
Comments: We are withdrawing this submission due to an issue discovered in our
  analysis/evaluation pipeline that impacts the reported experimental findings.
  Until the results have been fully revalidated, we do not believe the current
  version provides a reliable basis for the conclusions. We intend to release
  an updated manuscript after re-running and cross-checking the experiments
\\ ( https://arxiv.org/abs/2505.23783 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2506.00462
replaced with revised version Sun, 18 Jan 2026 13:54:01 GMT   (567kb)

Title: XMAD-Bench: Cross-Domain Multilingual Audio Deepfake Benchmark
Authors: Ioan-Paul Ciobanu, Andrei-Iulian Hiji, Nicolae-Catalin Ristea, Paul
  Irofti, Cristian Rusu, Radu Tudor Ionescu
Categories: cs.SD cs.AI cs.CL cs.LG eess.AS
Comments: Accepted at EACL 2026
\\ ( https://arxiv.org/abs/2506.00462 ,  567kb)
------------------------------------------------------------------------------
\\
arXiv:2506.05928
replaced with revised version Mon, 19 Jan 2026 07:51:01 GMT   (1094kb)

Title: MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient
  Fine-Tuning of Large Language Models
Authors: Jie Cao, Tianwei Lin, Bo Yuan, Rolan Yan, Hongyang He, Wenqiao Zhang,
  Juncheng Li, Dongping Zhang, Siliang Tang, Yueting Zhuang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2506.05928 ,  1094kb)
------------------------------------------------------------------------------
\\
arXiv:2506.08835
replaced with revised version Fri, 16 Jan 2026 19:25:15 GMT   (37887kb)

Title: CulturalFrames: Assessing Cultural Expectation Alignment in
  Text-to-Image Models and Evaluation Metrics
Authors: Shravan Nayak, Mehar Bhatia, Xiaofeng Zhang, Verena Rieser, Lisa Anne
  Hendricks, Sjoerd van Steenkiste, Yash Goyal, Karolina Sta\'nczak, Aishwarya
  Agrawal
Categories: cs.CV cs.AI cs.CL
Comments: Accepted to EMNLP 2025 Findings
\\ ( https://arxiv.org/abs/2506.08835 ,  37887kb)
------------------------------------------------------------------------------
\\
arXiv:2506.11164
replaced with revised version Mon, 19 Jan 2026 06:11:34 GMT   (5363kb)

Title: Synthetic Geology: Structural Geology Meets Deep Learning
Authors: Simon Ghyselincks, Valeriia Okhmak, Stefano Zampini, George Turkiyyah,
  David Keyes, Eldad Haber
Categories: cs.CV cs.AI cs.LG
Comments: Accepted for publication in Journal of Geophysical Research: Machine
  Learning and Computation (2026). 16 pages, 9 figures, geological simulation
  code at https://doi.org/10.5281/zenodo.15244035, generative AI code at
  https://github.com/chipnbits/flowtrain_stochastic_interpolation/releases/tag/v1.0.2
\\ ( https://arxiv.org/abs/2506.11164 ,  5363kb)
------------------------------------------------------------------------------
\\
arXiv:2507.01436
replaced with revised version Sun, 18 Jan 2026 07:48:08 GMT   (1600kb)

Title: Challenges & Opportunities with LLM-Assisted Visualization Retargeting
Authors: Luke S. Snyder, Chenglong Wang, Steven M. Drucker
Categories: cs.HC cs.AI
Comments: 5 pages, 3 figures, 1 table
\\ ( https://arxiv.org/abs/2507.01436 ,  1600kb)
------------------------------------------------------------------------------
\\
arXiv:2507.01607
replaced with revised version Tue, 20 Jan 2026 13:17:36 GMT   (2898kb)

Title: SoK: On the Survivability of Backdoor Attacks on Unconstrained Face
  Recognition Systems
Authors: Quentin Le Roux, Yannick Teglia, Teddy Furon, Philippe Loubet-Moundi,
  Eric Bourbao
Categories: cs.CV cs.AI cs.CR cs.LG
Comments: This work has been accepted for publication at the IEEE Conference on
  Secure and Trustworthy Machine Learning (SaTML). The final version will be
  available on IEEE Xplore
\\ ( https://arxiv.org/abs/2507.01607 ,  2898kb)
------------------------------------------------------------------------------
\\
arXiv:2507.03043
replaced with revised version Mon, 19 Jan 2026 18:03:51 GMT   (3550kb)

Title: K-Function: Joint Pronunciation Transcription and Feedback for
  Evaluating Kids Language Function
Authors: Shuhe Li, Chenxu Guo, Jiachen Lian, Cheol Jun Cho, Wenshuo Zhao, Xiner
  Xu, Ruiyu Jin, Xiaoyu Shi, Xuanru Zhou, Dingkun Zhou, Sam Wang, Grace Wang,
  Jingze Yang, Jingyi Xu, Ruohan Bao, Xingrui Chen, Elise Brenner, Brandon In,
  Francesca Pei, Maria Luisa Gorno-Tempini, Gopala Anumanchipalli
Categories: cs.CL cs.AI cs.SD eess.AS
\\ ( https://arxiv.org/abs/2507.03043 ,  3550kb)
------------------------------------------------------------------------------
\\
arXiv:2507.09929 (*cross-listing*)
replaced with revised version Sun, 18 Jan 2026 15:28:41 GMT   (1810kb)

Title: Aligning Generative Speech Enhancement with Perceptual Feedback
Authors: Haoyang Li, Nana Hou, Yuchen Hu, Jixun Yao, Sabato Marco Siniscalchi,
  Xuyi Zhuang, Deheng Ye, Wei Yang, Eng Siong Chng
Categories: eess.AS cs.AI cs.LG
Comments: Accepted to ICASSP 2026
\\ ( https://arxiv.org/abs/2507.09929 ,  1810kb)
------------------------------------------------------------------------------
\\
arXiv:2507.11408
replaced with revised version Mon, 19 Jan 2026 08:53:46 GMT   (6078kb)

Title: KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical
  Reasoning?
Authors: Soumadeep Saha, Akshay Chaturvedi, Saptarshi Saha, Utpal Garain,
  Nicholas Asher
Categories: cs.CL cs.AI
Comments: Pre-print; Accepted to TACL
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2507.11408 ,  6078kb)
------------------------------------------------------------------------------
\\
arXiv:2507.11935
replaced with revised version Sun, 18 Jan 2026 08:30:01 GMT   (1823kb)

Title: AI-Native Open RAN for Non-Terrestrial Networks: An Overview
Authors: Jikang Deng, S.Fizza Hassan, Hui Zhou, Saad Al-Ahmadi, Mohamed-Slim
  Alouini, Daniel B. Da Costa
Categories: cs.NI cs.AI cs.SY eess.SY
\\ ( https://arxiv.org/abs/2507.11935 ,  1823kb)
------------------------------------------------------------------------------
\\
arXiv:2507.12286
replaced with revised version Tue, 20 Jan 2026 15:29:27 GMT   (3596kb)

Title: SHACL Validation in the Presence of Ontologies: Semantics and Rewriting
  Techniques
Authors: Anouk Oudshoorn, Magdalena Ortiz, Mantas Simkus
Categories: cs.LO cs.AI
Comments: Published in AIJ
Journal-ref: Volume 352, 2026, 104483
DOI: 10.1016/j.artint.2026.104483
\\ ( https://arxiv.org/abs/2507.12286 ,  3596kb)
------------------------------------------------------------------------------
\\
arXiv:2507.13941 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 11:46:47 GMT   (10672kb)

Title: Shared representations in brains and models reveal a two-route cortical
  organization during scene perception
Authors: Pablo Marcos-Manch\'on, Llu\'is Fuentemilla
Categories: q-bio.NC cs.AI cs.CV eess.IV
Comments: for associate code, see
  https://github.com/memory-formation/convergent-transformations
ACM-class: I.2.10
\\ ( https://arxiv.org/abs/2507.13941 ,  10672kb)
------------------------------------------------------------------------------
\\
arXiv:2507.14833
replaced with revised version Tue, 20 Jan 2026 12:58:56 GMT   (465kb)

Title: Paired Image Generation with Diffusion-Guided Diffusion Models
Authors: Haoxuan Zhang, Wenju Cui, Yuzhu Cao, Tao Tan, Jie Liu, Yunsong Peng,
  Jian Zheng
Categories: cs.CV cs.AI
DOI: 10.1007/978-3-032-04965-0_35
\\ ( https://arxiv.org/abs/2507.14833 ,  465kb)
------------------------------------------------------------------------------
\\
arXiv:2507.19992 (*cross-listing*)
replaced with revised version Tue, 20 Jan 2026 07:57:50 GMT   (528kb)

Title: Development and Evaluation of a Standardized Ontology for Non-Invasive
  Respiratory Support to Improve Interoperability and Clinical Reasoning in
  Acute Care
Authors: Md Fantacher Islam, Jarrod Mosier, Vignesh Subbian
Categories: q-bio.OT cs.AI
\\ ( https://arxiv.org/abs/2507.19992 ,  528kb)
------------------------------------------------------------------------------
\\
arXiv:2507.20136
replaced with revised version Tue, 20 Jan 2026 01:13:34 GMT   (430kb)

Title: Multi-Stage Verification-Centric Framework for Mitigating Hallucination
  in Multi-Modal RAG
Authors: Baiyu Chen, Wilson Wongso, Xiaoqian Hu, Yue Tan, Flora Salim
Categories: cs.CL cs.AI cs.IR
Comments: KDD Cup 2025 Meta CRAG-MM Challenge: Third Prize in the Single-Source
  Augmentation Task
\\ ( https://arxiv.org/abs/2507.20136 ,  430kb)
------------------------------------------------------------------------------
\\
arXiv:2507.22936
replaced with revised version Mon, 19 Jan 2026 21:50:29 GMT   (1684kb)

Title: Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative
  Study on Financial Report Analysis
Authors: Md Talha Mohsin
Categories: cs.CL cs.AI cs.CE cs.HC q-fin.CP
Comments: 23 Pages
\\ ( https://arxiv.org/abs/2507.22936 ,  1684kb)
------------------------------------------------------------------------------
\\
arXiv:2508.00256
replaced with revised version Tue, 20 Jan 2026 08:53:47 GMT   (10538kb)

Title: Large AI Model-Enabled Secure Communications in Low-Altitude Wireless
  Networks: Concepts, Perspectives and Case Study
Authors: Chuang Zhang, Geng Sun, Yijing Lin, Weijie Yuan, Sinem Coleri, and
  Dusit Niyato
Categories: cs.NI cs.AI
Comments: This paper has been accepted to IEEE Communications Magazine
\\ ( https://arxiv.org/abs/2508.00256 ,  10538kb)
------------------------------------------------------------------------------
\\
arXiv:2508.00591
replaced with revised version Sun, 18 Jan 2026 08:14:17 GMT   (5544kb)

Title: Wukong Framework for Not Safe For Work Detection in Text-to-Image
  systems
Authors: Mingrui Liu, Sixiao Zhang, Cheng Long
Categories: cs.CV cs.AI cs.CR
Comments: Accepted by KDD'26 (round 1)
\\ ( https://arxiv.org/abs/2508.00591 ,  5544kb)
------------------------------------------------------------------------------
\\
arXiv:2508.01047
replaced with revised version Mon, 19 Jan 2026 00:27:29 GMT   (216kb)

Title: A Deep Reinforcement Learning-Based TCP Congestion Control Algorithm:
  Design, Simulation, and Evaluation
Authors: Efe A\u{g}lamazlar, Emirhan Eken, Harun Batur Ge\c{c}ici
Categories: cs.NI cs.AI
Comments: 11 pages, 4 figures. Presents a DRL agent that mitigates bufferbloat
  and achieves near-zero packet loss. Validated via NS-3 simulations under a
  strict training-testing protocol. Code:
  https://github.com/aglamazlarefe/DRL-TCP
MSC-class: 68M20, 68T05
ACM-class: C.2.2; I.2.6
\\ ( https://arxiv.org/abs/2508.01047 ,  216kb)
------------------------------------------------------------------------------
\\
arXiv:2508.02314
replaced with revised version Tue, 20 Jan 2026 03:09:51 GMT   (985kb)

Title: Large AI Models for Wireless Physical Layer
Authors: Jiajia Guo and Yiming Cui and Shi Jin and Jun Zhang
Categories: cs.IT cs.AI math.IT
Comments: A collection of paper on Large AI Models for wireless physical layer
  can be found at https://github.com/AI4Wireless/LAM4PHY_6G
\\ ( https://arxiv.org/abs/2508.02314 ,  985kb)
------------------------------------------------------------------------------
\\
arXiv:2508.03140
replaced with revised version Mon, 19 Jan 2026 07:25:37 GMT   (698kb)

Title: RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific
  Models by Considering Reasoning Capability as Prior
Authors: Junyao Yang, Jianwei Wang, Huiping Zhuang, Cen Chen, Ziqian Zeng
Categories: cs.CL cs.AI
Comments: 15 pages, 7 figures
\\ ( https://arxiv.org/abs/2508.03140 ,  698kb)
------------------------------------------------------------------------------
\\
arXiv:2508.04663
replaced with revised version Sat, 17 Jan 2026 17:17:04 GMT   (23527kb)

Title: HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion
  Models
Authors: Young D. Kwon, Rui Li, Sijia Li, Da Li, Sourav Bhattacharya, Stylianos
  I. Venieris
Categories: cs.CV cs.AI
Comments: Accepted at AAAI 2026 (Main Technical Track)
\\ ( https://arxiv.org/abs/2508.04663 ,  23527kb)
------------------------------------------------------------------------------
\\
arXiv:2508.08604
replaced with revised version Sat, 17 Jan 2026 15:36:14 GMT   (666kb)

Title: Transferable Model-agnostic Vision-Language Model Adaptation for
  Efficient Weak-to-Strong Generalization
Authors: Jihwan Park, Taehoon Song, Sanghyeok Lee, Miso Choi, Hyunwoo J. Kim
Categories: cs.CV cs.AI cs.LG
Comments: AAAI2026 Oral (camera ready version)
\\ ( https://arxiv.org/abs/2508.08604 ,  666kb)
------------------------------------------------------------------------------
\\
arXiv:2508.11281
replaced with revised version Mon, 19 Jan 2026 15:55:38 GMT   (1338kb)

Title: ToxiFrench: Benchmarking and Enhancing Language Models via CoT
  Fine-Tuning for French Toxicity Detection
Authors: Axel Delaval, Shujian Yang, Haicheng Wang, Han Qiu, Jialiang Lu
Categories: cs.CL cs.AI cs.CY
Comments: 22 pages, 5 figures, 11 tables. This paper introduces TOXIFRENCH, a
  benchmark of 53,622 comments for French toxicity detection. It proposes a
  Chain-of-Thought fine-tuning method with a dynamic weighted loss. The
  fine-tuned 4B model (Qwen3-4B) achieves state-of-the-art performance,
  outperforming larger models like GPT-4o and DeepSeek-R1
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2508.11281 ,  1338kb)
------------------------------------------------------------------------------
\\
arXiv:2508.14904
replaced with revised version Tue, 20 Jan 2026 13:15:53 GMT   (495kb)

Title: Efficient Switchable Safety Control in LLMs via Magic-Token-Guided
  Co-Training
Authors: Jianfeng Si, Lin Sun, Zhewen Tan, Xiangzheng Zhang
Categories: cs.CL cs.AI
Comments: 15 pages,3 figures,5 tables
\\ ( https://arxiv.org/abs/2508.14904 ,  495kb)
------------------------------------------------------------------------------
\\
arXiv:2508.15658
replaced with revised version Sun, 18 Jan 2026 08:42:57 GMT   (69kb)

Title: SurGE: A Benchmark and Evaluation Framework for Scientific Survey
  Generation
Authors: Weihang Su, Anzhe Xie, Qingyao Ai, Jianming Long, Xuanyi Chen, Jiaxin
  Mao, Ziyi Ye, Yiqun Liu
Categories: cs.CL cs.AI cs.IR
\\ ( https://arxiv.org/abs/2508.15658 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2508.21099
replaced with revised version Mon, 19 Jan 2026 07:34:03 GMT   (930kb)

Title: Beyond the Safety Tax: Mitigating Unsafe Text-to-Image Generation via
  External Safety Rectification
Authors: Xiangtao Meng, Yingkai Dong, Ning Yu, Li Wang, Zheng Li, Shanqing Guo
Categories: cs.CV cs.AI cs.CR
\\ ( https://arxiv.org/abs/2508.21099 ,  930kb)
------------------------------------------------------------------------------
\\
arXiv:2509.00052
replaced with revised version Mon, 19 Jan 2026 08:05:12 GMT   (6513kb)

Title: Lightning Fast Caching-based Parallel Denoising Prediction for
  Accelerating Talking Head Generation
Authors: Jianzhi Long, Wenhao Sun, Rongcheng Tu, Dacheng Tao
Categories: cs.GR cs.AI cs.CV
\\ ( https://arxiv.org/abs/2509.00052 ,  6513kb)
------------------------------------------------------------------------------
\\
arXiv:2509.03515
replaced with revised version Mon, 19 Jan 2026 23:50:26 GMT   (1669kb)

Title: Can the Waymo Open Motion Dataset Support Realistic Behavioral Modeling?
  A Validation Study with Naturalistic Trajectories
Authors: Yanlin Zhang, Sungyong Chung, Nachuan Li, Dana Monzer, Hani S.
  Mahmassani, Samer H. Hamdar, and Alireza Talebpour
Categories: cs.RO cs.AI cs.LG cs.SY eess.SY stat.AP
\\ ( https://arxiv.org/abs/2509.03515 ,  1669kb)
------------------------------------------------------------------------------
\\
arXiv:2509.04492
replaced with revised version Tue, 20 Jan 2026 15:36:40 GMT   (288kb)

Title: Learned Hallucination Detection in Black-Box LLMs using Token-level
  Entropy Production Rate
Authors: Charles Moslonka, Hicham Randrianarivo, Arthur Garnier and Emmanuel
  Malherbe
Categories: cs.CL cs.AI
Comments: 8 pages, 5 figures, 2 tables. pre-print version
\\ ( https://arxiv.org/abs/2509.04492 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2509.05309 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 04:25:12 GMT   (3852kb)

Title: ProtSAE: Disentangling and Interpreting Protein Language Models via
  Semantically-Guided Sparse Autoencoders
Authors: Xiangyu Liu and Haodi Lei and Yi Liu and Yang Liu and Wei Hu
Categories: q-bio.QM cs.AI cs.CL
Comments: Accepted in the 39th AAAI Conference on Artificial Intelligence (AAAI
  2026)
\\ ( https://arxiv.org/abs/2509.05309 ,  3852kb)
------------------------------------------------------------------------------
\\
arXiv:2509.05362
replaced with revised version Tue, 20 Jan 2026 03:48:38 GMT   (4213kb)

Title: AI-in-the-Loop: Privacy Preserving Real-Time Scam Detection and
  Conversational Scambaiting by Leveraging LLMs and Federated Learning
Authors: Ismail Hossain, Sai Puppala, Md Jahangir Alam, Sajedul Talukder
Categories: cs.CR cs.AI cs.LG cs.SI
Comments: This paper got accepted in 26th Privacy Enhancing Technologies
  Symposium (PETS 2026). We uploaded it into ArXiv as pre-print
\\ ( https://arxiv.org/abs/2509.05362 ,  4213kb)
------------------------------------------------------------------------------
\\
arXiv:2509.05890 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 06:46:47 GMT   (3673kb)

Title: Quantum spatial best-arm identification via quantum walks
Authors: Tomoki Yamagami, Etsuo Segawa, Takatomo Mihana, Andr\'e R\"ohm,
  Atsushi Uchida, and Ryoichi Horisaki
Categories: quant-ph cs.AI cs.LG math-ph math.MP
Comments: 15 pages, 8 figures
\\ ( https://arxiv.org/abs/2509.05890 ,  3673kb)
------------------------------------------------------------------------------
\\
arXiv:2509.07829
replaced with revised version Mon, 19 Jan 2026 09:02:37 GMT   (50kb)

Title: Building Large-Scale English-Romanian Literary Translation Resources
  with Open Models
Authors: Mihai Nadas, Laura Diosan, Andreea Tomescu, Andrei Piscoran
Categories: cs.CL cs.AI cs.LG
Comments: 25 pages, 8 figures, includes datasets and models released on Hugging
  Face
\\ ( https://arxiv.org/abs/2509.07829 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2509.10077
replaced with revised version Sat, 17 Jan 2026 16:59:36 GMT   (8880kb)

Title: Predictive Spike Timing Enables Distributed Shortest Path Computation in
  Spiking Neural Networks
Authors: Simen Storesund, Kristian Valset Aars, Robin Dietrich, Nicolai Waniek
Categories: cs.NE cs.AI cs.DS cs.LG
\\ ( https://arxiv.org/abs/2509.10077 ,  8880kb)
------------------------------------------------------------------------------
\\
arXiv:2509.18561 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 02:19:28 GMT   (3060kb)

Title: SoundCompass: Navigating Target Sound Extraction With Effective
  Directional Clue Integration In Complex Acoustic Scenes
Authors: Dayun Choi, Jung-Woo Choi
Categories: eess.AS cs.AI cs.SD
Comments: 5 pages, 4 figures, accepted to ICASSP 2026
\\ ( https://arxiv.org/abs/2509.18561 ,  3060kb)
------------------------------------------------------------------------------
\\
arXiv:2509.23038
replaced with revised version Tue, 20 Jan 2026 18:07:41 GMT   (3847kb)

Title: GeLoc3r: Enhancing Relative Camera Pose Regression with Geometric
  Consistency Regularization
Authors: Jingxing Li, Yongjae Lee, Deliang Fan
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2509.23038 ,  3847kb)
------------------------------------------------------------------------------
\\
arXiv:2510.00833
replaced with revised version Mon, 19 Jan 2026 15:56:28 GMT   (1153kb)

Title: Towards Verifiable Federated Unlearning: Framework, Challenges, and The
  Road Ahead
Authors: Thanh Linh Nguyen, Marcela Tuler de Oliveira, An Braeken, Aaron Yi
  Ding, Quoc-Viet Pham
Categories: cs.DC cs.AI
Comments: Accepted in IEEE Internet Computing
\\ ( https://arxiv.org/abs/2510.00833 ,  1153kb)
------------------------------------------------------------------------------
\\
arXiv:2510.06243
replaced with revised version Sat, 17 Jan 2026 09:14:14 GMT   (3955kb)

Title: CoT Referring: Improving Referring Expression Tasks with Grounded
  Reasoning
Authors: Qihua Dong, Luis Figueroa, Handong Zhao, Kushal Kafle, Jason Kuen,
  Zhihong Ding, Scott Cohen, Yun Fu
Categories: cs.CL cs.AI
Comments: MLLM, Referring Expression Segmentation
\\ ( https://arxiv.org/abs/2510.06243 ,  3955kb)
------------------------------------------------------------------------------
\\
arXiv:2510.08211
replaced with revised version Sun, 18 Jan 2026 07:03:40 GMT   (635kb)

Title: LLMs Deceive Unintentionally: Emergent Misalignment in Dishonesty from
  Misaligned Samples to Biased Human-AI Interactions
Authors: Xuhao Hu, Peng Wang, Xiaoya Lu, Dongrui Liu, Xuanjing Huang, Jing Shao
Categories: cs.CL cs.AI cs.CR
\\ ( https://arxiv.org/abs/2510.08211 ,  635kb)
------------------------------------------------------------------------------
\\
arXiv:2510.10671
replaced with revised version Mon, 19 Jan 2026 11:54:55 GMT   (1380kb)

Title: Image-to-Video Transfer Learning based on Image-Language Foundation
  Models: A Comprehensive Survey
Authors: Jinxuan Li, Chaolei Tan, Haoxuan Chen, Jianxin Ma, Jian-Fang Hu,
  Jianhuang Lai, Wei-Shi Zheng
Categories: cs.CV cs.AI
Comments: Updated version, github repository is available at
  https://github.com/YuriPreisdent/awesome-image-to-video-transfer
\\ ( https://arxiv.org/abs/2510.10671 ,  1380kb)
------------------------------------------------------------------------------
\\
arXiv:2510.11391
replaced with revised version Tue, 20 Jan 2026 13:19:13 GMT   (3366kb)

Title: DocReward: A Document Reward Model for Structuring and Stylizing
Authors: Junpeng Liu, Yuzhong Zhao, Bowen Cao, Jiayu Ding, Yilin Jia, Tengchao
  Lv, Yupan Huang, Shaohan Huang, Nan Yang, Li Dong, Lei Cui, Tao Ge, Xun Wang,
  Huitian Jiao, Sun Mao, FNU Kartik, Si-Qing Chen, Wai Lam, Furu Wei
Categories: cs.CV cs.AI cs.CL
\\ ( https://arxiv.org/abs/2510.11391 ,  3366kb)
------------------------------------------------------------------------------
\\
arXiv:2510.13855
replaced with revised version Mon, 19 Jan 2026 02:48:12 GMT   (2937kb)

Title: Harnessing Consistency for Robust Test-Time LLM Ensemble
Authors: Zhichen Zeng, Qi Yu, Xiao Lin, Ruizhong Qiu, Xuying Ning, Tianxin Wei,
  Yuchen Yan, Jingrui He, Hanghang Tong
Categories: cs.CL cs.AI
Comments: 18 pages, 15 figures
\\ ( https://arxiv.org/abs/2510.13855 ,  2937kb)
------------------------------------------------------------------------------
\\
arXiv:2510.20543
replaced with revised version Tue, 20 Jan 2026 17:46:36 GMT   (9527kb)

Title: The Dog the Cat Chased Stumped the Model: Measuring When Language Models
  Abandon Structure for Shortcuts
Authors: Sangmitra Madhusudan, Kaige Chen, and Ali Emami
Categories: cs.CL cs.AI
Comments: 9 pages (excluding references), accepted to EACL 2026 Main Conference
\\ ( https://arxiv.org/abs/2510.20543 ,  9527kb)
------------------------------------------------------------------------------
\\
arXiv:2510.24816
replaced with revised version Mon, 19 Jan 2026 10:51:42 GMT   (5530kb)

Title: Perception, Understanding and Reasoning, A Multimodal Benchmark for
  Video Fake News Detection
Authors: Cui Yakun, Peng Qi, Fushuo Huo, Hang Du, Weijie Shi, Juntao Dai,
  Zhenghao Zhu, Sirui Han, Yike Guo
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2510.24816 ,  5530kb)
------------------------------------------------------------------------------
\\
arXiv:2510.25522
replaced with revised version Tue, 20 Jan 2026 08:36:55 GMT   (15594kb)

Title: Comparative Study of UNet-based Architectures for Liver Tumor
  Segmentation in Multi-Phase Contrast-Enhanced Computed Tomography
Authors: Doan-Van-Anh Ly (1), Thanh-Hai Le (1), Thi-Thu-Hien Pham (2 and 3)
  ((1) The Saigon International University, (2) International University, (3)
  Vietnam National University HCMC)
Categories: cs.CV cs.AI
Comments: 18 pages, 11 figures
ACM-class: I.4.6
\\ ( https://arxiv.org/abs/2510.25522 ,  15594kb)
------------------------------------------------------------------------------
\\
arXiv:2511.06899
replaced with revised version Mon, 19 Jan 2026 12:53:44 GMT   (4058kb)

Title: RPTS: Tree-Structured Reasoning Process Scoring for Faithful Multimodal
  Evaluation
Authors: Haofeng Wang and Yu Zhang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2511.06899 ,  4058kb)
------------------------------------------------------------------------------
\\
arXiv:2511.07780
replaced with revised version Sat, 17 Jan 2026 08:35:44 GMT   (343kb)

Title: Semantic-Consistent Bidirectional Contrastive Hashing for Noisy
  Multi-Label Cross-Modal Retrieval
Authors: Likang Peng, Chao Su, Wenyuan Wu, Yuan Sun, Dezhong Peng, Xi Peng, Xu
  Wang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2511.07780 ,  343kb)
------------------------------------------------------------------------------
\\
arXiv:2511.08710
replaced with revised version Mon, 19 Jan 2026 21:20:21 GMT   (4498kb)

Title: Convergence dynamics of Agent-to-Agent Interactions with Misaligned
  objectives
Authors: Romain Cosentino, Sarath Shekkizhar, Adam Earle
Categories: cs.MA cs.AI
\\ ( https://arxiv.org/abs/2511.08710 ,  4498kb)
------------------------------------------------------------------------------
\\
arXiv:2511.14591
replaced with revised version Tue, 20 Jan 2026 10:39:37 GMT   (3164kb)

Title: Biased Minds Meet Biased AI: How Class Imbalance Shapes Appropriate
  Reliance and Interacts with Human Base Rate Neglect
Authors: Nick von Felten, Johannes Sch\"oning, Klaus Opwis, Nicolas Scharowski
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2511.14591 ,  3164kb)
------------------------------------------------------------------------------
\\
arXiv:2511.16544
replaced with revised version Mon, 19 Jan 2026 12:48:12 GMT   (3354kb)

Title: WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding
  in Patient Facing Dialogue
Authors: Zachary Ellis, Jared Joselowitz, Yash Deo, Yajie He, Anna Kalygina,
  Aisling Higham, Mana Rahimzadeh, Yan Jia, Ibrahim Habli, Ernest Lim
Categories: cs.CL cs.AI
Comments: Published as an Oral at IWSDS 2026
\\ ( https://arxiv.org/abs/2511.16544 ,  3354kb)
------------------------------------------------------------------------------
\\
arXiv:2511.17908
replaced with revised version Mon, 19 Jan 2026 00:27:10 GMT   (898kb)

Title: Principled Context Engineering for RAG: Statistical Guarantees via
  Conformal Prediction
Authors: Debashish Chakraborty, Eugene Yang, Daniel Khashabi, Dawn Lawrie, and
  Kevin Duh
Categories: cs.CL cs.AI cs.IR
Comments: Accepted at ECIR 2026
\\ ( https://arxiv.org/abs/2511.17908 ,  898kb)
------------------------------------------------------------------------------
\\
arXiv:2511.21755
replaced with revised version Sun, 18 Jan 2026 18:17:31 GMT   (440kb)

Title: Who Owns the Knowledge? Copyright, GenAI, and the Future of Academic
  Publishing
Authors: Dmitry Kochetkov
Categories: cs.DL cs.AI cs.CY
Comments: The second version version substantially revises the original
  preprint through expanded legal analysis, representation of the new technical
  standard (RSL 1.0), and removing substantial material lacking direct
  relevance to copyright and AI training
ACM-class: A.0
\\ ( https://arxiv.org/abs/2511.21755 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2512.07000
replaced with revised version Sat, 17 Jan 2026 12:17:52 GMT   (2702kb)

Title: Benchmarking Deep Neural Networks for Modern Recommendation Systems
Authors: Abderaouf Bahi and Inoussa Mouiche and Ibtissem Gasmi
Categories: cs.IR cs.AI
\\ ( https://arxiv.org/abs/2512.07000 ,  2702kb)
------------------------------------------------------------------------------
\\
arXiv:2512.09867
replaced with revised version Tue, 20 Jan 2026 03:41:34 GMT   (4224kb)

Title: Hierarchy-Aware Multimodal Unlearning for Medical AI
Authors: Fengli Wu, Vaidehi Patil, Jaehong Yoon, Yue Zhang, Mohit Bansal
Categories: cs.CV cs.AI cs.CL
Comments: Dataset and Code: https://github.com/fengli-wu/MedForget
\\ ( https://arxiv.org/abs/2512.09867 ,  4224kb)
------------------------------------------------------------------------------
\\
arXiv:2512.12802 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 18:31:40 GMT   (34kb)

Title: A Disproof of Large Language Model Consciousness: The Necessity of
  Continual Learning for Consciousness
Authors: Erik Hoel
Categories: q-bio.NC cs.AI
Comments: 31 pages, 3 figures. V3: Added new section (4.1), restructured
  section 5.1, and further expanded citations
\\ ( https://arxiv.org/abs/2512.12802 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2512.17853
replaced with revised version Tue, 20 Jan 2026 18:25:48 GMT   (35826kb)

Title: AnyTask: an Automated Task and Data Generation Framework for Advancing
  Sim-to-Real Policy Learning
Authors: Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti,
  Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh,
  Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper
Categories: cs.RO cs.AI
Comments: 28 pages, 25 figures. The first four authors contributed equally
\\ ( https://arxiv.org/abs/2512.17853 ,  35826kb)
------------------------------------------------------------------------------
\\
arXiv:2512.20156
replaced with revised version Tue, 20 Jan 2026 08:22:20 GMT   (2153kb)

Title: Fun-Audio-Chat Technical Report
Authors: Tongyi Fun Team, Qian Chen, Luyao Cheng, Chong Deng, Xiangang Li,
  Jiaqing Liu, Chao-Hong Tan, Wen Wang, Junhao Xu, Jieping Ye, Qinglin Zhang,
  Qiquan Zhang, Jingren Zhou
Categories: cs.CL cs.AI cs.SD eess.AS
Comments: Authors are listed in alphabetical order, 21 pages, open-source at
  https://github.com/FunAudioLLM/Fun-Audio-Chat
\\ ( https://arxiv.org/abs/2512.20156 ,  2153kb)
------------------------------------------------------------------------------
\\
arXiv:2512.20260
replaced with revised version Fri, 16 Jan 2026 17:37:51 GMT   (0kb,I)

Title: D^3ETOR: Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive
  Debiasing for Weakly-Supervised Camouflaged Object Detection with Scribble
  Annotations
Authors: Jiawei Ge, Jiuxin Cao, Xinyi Li, Xuelin Zhu, Chang Liu, Bo Liu, Chen
  Feng, Ioannis Patras
Categories: cs.CV cs.AI
Comments: We decided to withdraw the current arXiv version as the methodology
  and experimental conclusions are undergoing substantial revision, and the
  present version no longer accurately reflects our intended contribution
\\ ( https://arxiv.org/abs/2512.20260 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2512.21788
replaced with revised version Mon, 19 Jan 2026 06:32:10 GMT   (33396kb)

Title: InstructMoLE: Instruction-Guided Mixture of Low-rank Experts for
  Multi-Conditional Image Generation
Authors: Jinqi Xiao, Qing Yan, Liming Jiang, Zichuan Liu, Hao Kang, Shen Sang,
  Tiancheng Zhi, Jing Liu, Cheng Yang, Xin Lu, Bo Yuan
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2512.21788 ,  33396kb)
------------------------------------------------------------------------------
\\
arXiv:2512.23565
replaced with revised version Tue, 20 Jan 2026 09:46:33 GMT   (1565kb)

Title: RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on
  Chemical Reaction Understanding from Scientific Literature
Authors: Hanzheng Li and Xi Fang and Yixuan Li and Chaozheng Huang and Junjie
  Wang and Xi Wang and Hongzhe Bai and Bojun Hao and Shenyu Lin and Huiqi Liang
  and Linfeng Zhang and Guolin Ke
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2512.23565 ,  1565kb)
------------------------------------------------------------------------------
\\
arXiv:2512.23779
replaced with revised version Sat, 17 Jan 2026 02:07:08 GMT   (1917kb)

Title: Prompt-Induced Over-Generation as Denial-of-Service: A Black-Box
  Attack-Side Benchmark
Authors: Manu, Yi Guo, Kanchana Thilakarathna, Nirhoshan Sivaroopan, Jo
  Plested, Tim Lynar, Jack Yang, Wangli Yang
Categories: cs.CR cs.AI cs.LG
Comments: 17 pages, 5 figures
\\ ( https://arxiv.org/abs/2512.23779 ,  1917kb)
------------------------------------------------------------------------------
\\
arXiv:2512.24574
replaced with revised version Mon, 19 Jan 2026 21:17:51 GMT   (672kb)

Title: Understanding and Steering the Cognitive Behaviors of Reasoning Models
  at Test-Time
Authors: Zhenyu Zhang, Xiaoxia Wu, Zhongzhu Zhou, Qingyang Wu, Yineng Zhang,
  Pragaash Ponnusamy, Harikaran Subbaraj, Jue Wang, Shuaiwen Leon Song, Ben
  Athiwaratkun
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2512.24574 ,  672kb)
------------------------------------------------------------------------------
\\
arXiv:2601.01554
replaced with revised version Mon, 19 Jan 2026 04:45:09 GMT   (7055kb)

Title: MOSS Transcribe Diarize Technical Report
Authors: MOSI.AI: Donghua Yu, Zhengyuan Lin, Chen Yang, Yiyang Zhang, Hanfu
  Chen, Jingqi Chen, Ke Chen, Liwei Fan, Yi Jiang, Jie Zhu, Muchen Li, Wenxuan
  Wang, Yang Wang, Zhe Xu, Yitian Gong, Yuqian Zhang, Wenbo Zhang, Songlin
  Wang, Zhiyu Wu, Zhaoye Fei, Qinyuan Cheng, Shimin Li, Xipeng Qiu
Categories: cs.SD cs.AI eess.AS
\\ ( https://arxiv.org/abs/2601.01554 ,  7055kb)
------------------------------------------------------------------------------
\\
arXiv:2601.01576
replaced with revised version Mon, 19 Jan 2026 02:51:34 GMT   (2127kb)

Title: OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly
  Novelty Assessment
Authors: Ming Zhang, Kexin Tan, Yueyuan Huang, Yujiong Shen, Chunchun Ma, Li
  Ju, Xinran Zhang, Yuhui Wang, Wenqing Jing, Jingyi Deng, Huayu Sha, Binze Hu,
  Jingqi Tong, Changhao Jiang, Yage Geng, Yuankai Ying, Yue Zhang, Zhangyue
  Yin, Zhiheng Xi, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang
Categories: cs.IR cs.AI cs.CL
\\ ( https://arxiv.org/abs/2601.01576 ,  2127kb)
------------------------------------------------------------------------------
\\
arXiv:2601.01747
replaced with revised version Mon, 19 Jan 2026 13:14:16 GMT   (590kb)

Title: Crafting Adversarial Inputs for Large Vision-Language Models Using
  Black-Box Optimization
Authors: Jiwei Guan, Haibo Jin, Haohan Wang
Categories: cs.CR cs.AI cs.CV cs.LG
Comments: EACL
\\ ( https://arxiv.org/abs/2601.01747 ,  590kb)
------------------------------------------------------------------------------
\\
arXiv:2601.02076
replaced with revised version Tue, 20 Jan 2026 12:30:52 GMT   (538kb)

Title: Deferred Commitment Decoding for Diffusion Language Models
Authors: Yingte Shu, Yuchuan Tian, Chao Xu, Yunhe Wang, Hanting Chen
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2601.02076 ,  538kb)
------------------------------------------------------------------------------
\\
arXiv:2601.02123
replaced with revised version Tue, 20 Jan 2026 09:31:31 GMT   (95kb)

Title: DeCode: Decoupling Content and Delivery for Medical QA
Authors: Po-Jen Ko, Chen-Han Tsai, Yu-Shao Peng
Categories: cs.CL cs.AI
Comments: Preprint
\\ ( https://arxiv.org/abs/2601.02123 ,  95kb)
------------------------------------------------------------------------------
\\
arXiv:2601.03423
replaced with revised version Mon, 19 Jan 2026 20:30:45 GMT   (5646kb)

Title: Training-Free Adaptation of New-Generation LLMs using Legacy Clinical
  Models
Authors: Sasha Ronaghi, Chloe Stanwyck, Asad Aali, Amir Ronaghi, Miguel
  Fuentes, Tina Hernandez-Boussard, Emily Alsentzer
Categories: cs.CL cs.AI
Comments: 29 pages, 3 figures
\\ ( https://arxiv.org/abs/2601.03423 ,  5646kb)
------------------------------------------------------------------------------
\\
arXiv:2601.03597
replaced with revised version Tue, 20 Jan 2026 01:03:05 GMT   (1013kb)

Title: From Chains to Graphs: Self-Structured Reasoning for General-Domain LLMs
Authors: Yingjian Chen, Haoran Liu, Yinhong Liu, Sherry T. Tong, Aosong Feng,
  Jinghui Lu, Juntao Zhang, Yusuke Iwasawa, Yutaka Matsuo and Irene Li
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2601.03597 ,  1013kb)
------------------------------------------------------------------------------
\\
arXiv:2601.03785
replaced with revised version Tue, 20 Jan 2026 07:09:21 GMT   (406kb)

Title: Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents
Authors: Dehao Tao, Guoliang Ma, Yongfeng Huang, Minghu Jiang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2601.03785 ,  406kb)
------------------------------------------------------------------------------
\\
arXiv:2601.03824
replaced with revised version Tue, 20 Jan 2026 09:26:48 GMT   (23620kb)

Title: IDESplat: Iterative Depth Probability Estimation for Generalizable 3D
  Gaussian Splatting
Authors: Wei Long, Haifeng Wu, Shiyin Jiang, Jinhua Zhang, Xinchun Ji, Shuhang
  Gu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2601.03824 ,  23620kb)
------------------------------------------------------------------------------
\\
arXiv:2601.05654
replaced with revised version Sat, 17 Jan 2026 12:11:40 GMT   (4575kb)

Title: A Framework for Personalized Persuasiveness Prediction via Context-Aware
  User Profiling
Authors: Sejun Park, Yoonah Park, Jongwon Lim, Yohan Jo
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2601.05654 ,  4575kb)
------------------------------------------------------------------------------
\\
arXiv:2601.06474
replaced with revised version Sat, 17 Jan 2026 04:01:42 GMT   (2106kb)

Title: SparseOccVLA: Bridging Occupancy and Vision-Language Models via Sparse
  Queries for Unified 4D Scene Understanding and Planning
Authors: Chenxu Dang, Jie Wang, Guang Li, Zhiwen Hou, Zihan You, Hangjun Ye,
  Jie Ma, Long Chen, Yan Wang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2601.06474 ,  2106kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08146
replaced with revised version Mon, 19 Jan 2026 14:05:14 GMT   (2549kb)

Title: Mechanisms are Transferable: Data-Efficient Low-Resource Adaptation via
  Circuit-Targeted Supervised Fine-Tuning
Authors: Khumaisa Nur'aini, Ayu Purwarianti, Alham Fikri Aji, Derry Wijaya
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2601.08146 ,  2549kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08189
replaced with revised version Tue, 20 Jan 2026 06:49:45 GMT   (2008kb)

Title: ForgetMark: Stealthy Fingerprint Embedding via Targeted Unlearning in
  Language Models
Authors: Zhenhua Xu, Haobo Zhang, Zhebo Wang, Qichen Liu, Haitao Xu, Wenpeng
  Xing, Meng Han
Categories: cs.CR cs.AI
Comments: Accepted by ICASSP2026
\\ ( https://arxiv.org/abs/2601.08189 ,  2008kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08223
replaced with revised version Mon, 19 Jan 2026 12:33:13 GMT   (1461kb)

Title: DNF: Dual-Layer Nested Fingerprinting for Large Language Model
  Intellectual Property Protection
Authors: Zhenhua Xu and Yiran Zhao and Mengting Zhong and Dezhang Kong and
  Changting Lin and Tong Qiao and Meng Han
Categories: cs.CR cs.AI
Comments: Accepted by ICASSP2026
\\ ( https://arxiv.org/abs/2601.08223 ,  1461kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08434
replaced with revised version Sat, 17 Jan 2026 08:14:17 GMT   (3579kb)

Title: Large Multimodal Models for Embodied Intelligent Driving: The Next
  Frontier in Self-Driving?
Authors: Long Zhang and Yuchen Xia and Bingqing Wei and Zhen Liu and Shiwen Mao
  and Zhu Han and Mohsen Guizani
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2601.08434 ,  3579kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08510
replaced with revised version Sun, 18 Jan 2026 10:40:32 GMT   (5393kb)

Title: STAGE: A Benchmark for Knowledge Graph Construction, Question Answering,
  and In-Script Role-Playing over Movie Screenplays
Authors: Qiuyu Tian, Yiding Li, Fengyi Chen, Zequn Liu, Youyong Kong, Fan Guo,
  Yuyao Li, Jinjing Shen, Zhijing Xie, Yiyun Luo, Xin Zhang
Categories: cs.CL cs.AI
Comments: 66 pages, 9 figures
\\ ( https://arxiv.org/abs/2601.08510 ,  5393kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08697
replaced with revised version Fri, 16 Jan 2026 22:19:42 GMT   (117kb)

Title: Auditing Student-AI Collaboration: A Case Study of Online Graduate CS
  Students
Authors: Nifu Dan
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2601.08697 ,  117kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08816
replaced with revised version Sat, 17 Jan 2026 02:11:07 GMT   (340kb)

Title: MemRec: Collaborative Memory-Augmented Agentic Recommender System
Authors: Weixin Chen and Yuhan Zhao and Jingyuan Huang and Zihe Ye and Clark
  Mingxuan Ju and Tong Zhao and Neil Shah and Li Chen and Yongfeng Zhang
Categories: cs.IR cs.AI
\\ ( https://arxiv.org/abs/2601.08816 ,  340kb)
------------------------------------------------------------------------------
\\
arXiv:2601.08875
replaced with revised version Tue, 20 Jan 2026 13:01:19 GMT   (162kb)

Title: Learning Domain-Invariant Representations for Cross-Domain Image
  Registration via Scene-Appearance Disentanglement
Authors: Jiahao Qin, Yiwen Wang
Categories: cs.CV cs.AI cs.LG
Comments: 6 pages, 2 figures, 4 tables. Code available at
  https://github.com/D-ST-Sword/SAR-NET
MSC-class: 68T45, 68U10, 94A08
ACM-class: I.4.3; I.2.6; J.3
\\ ( https://arxiv.org/abs/2601.08875 ,  162kb)
------------------------------------------------------------------------------
\\
arXiv:2601.09012
replaced with revised version Mon, 19 Jan 2026 09:52:02 GMT   (1685kb)

Title: TranslateGemma Technical Report
Authors: Mara Finkelstein, Isaac Caswell, Tobias Domhan, Jan-Thorsten Peter,
  Juraj Juraska, Parker Riley, Daniel Deutsch, Geza Kovacs, Cole Dilanni, Colin
  Cherry, Eleftheria Briakou, Elizabeth Nielsen, Jiaming Luo, Kat Black, Ryan
  Mullins, Sweta Agrawal, Wenda Xu, Erin Kats, Stephane Jaskiewicz, Markus
  Freitag and David Vilar
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2601.09012 ,  1685kb)
------------------------------------------------------------------------------
\\
arXiv:2601.09147
replaced with revised version Mon, 19 Jan 2026 02:37:00 GMT   (5530kb)

Title: SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot
  Anomaly Detection
Authors: Chenhao Fu, Han Fang, Xiuzheng Zheng, Wenbo Wei, Yonghua Li, Hao Sun,
  Xuelong Li
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2601.09147 ,  5530kb)
------------------------------------------------------------------------------
\\
arXiv:2601.09208
replaced with revised version Mon, 19 Jan 2026 05:01:18 GMT   (2205kb)

Title: Mikasa: A Character-Driven Emotional AI Companion Inspired by Japanese
  Oshi Culture
Authors: Miki Ueno
Categories: cs.HC cs.AI
Comments: This version includes minor explanatory additions in Appendix B-D,
  Section 2.6, and selected footnotes. A short demonstration video link is also
  provided
\\ ( https://arxiv.org/abs/2601.09208 ,  2205kb)
------------------------------------------------------------------------------
\\
arXiv:2601.09394
replaced with revised version Sun, 18 Jan 2026 10:11:42 GMT   (6265kb)

Title: FairGE: Fairness-Aware Graph Encoding in Incomplete Social Networks
Authors: Renqiang Luo, Huafei Huang, Tao Tang, Jing Ren, Ziqi Xu, Mingliang
  Hou, Enyan Dai, Feng Xia
Categories: cs.SI cs.AI
Comments: 12 pages, WWW 2026
\\ ( https://arxiv.org/abs/2601.09394 ,  6265kb)
------------------------------------------------------------------------------
\\
arXiv:2601.09478
replaced with revised version Sun, 18 Jan 2026 10:01:59 GMT   (594kb)

Title: Bridging Semantic Understanding and Popularity Bias with LLMs
Authors: Renqiang Luo, Dong Zhang, Yupeng Gao, Wen Shi, Mingliang Hou, Jiaying
  Liu, Zhe Wang, Shuo Yu
Categories: cs.IR cs.AI
Comments: 10 pages, 4 figs, WWW 2026 accepted
\\ ( https://arxiv.org/abs/2601.09478 ,  594kb)
------------------------------------------------------------------------------
\\
arXiv:2601.10378
replaced with revised version Sat, 17 Jan 2026 02:11:12 GMT   (19407kb)

Title: Global Context Compression with Interleaved Vision-Text Transformation
Authors: Dian Jiao, Jiaxin Duan, Shuai Zhao, Jiabing Leng, Yiran Zhang, Feng
  Huang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2601.10378 ,  19407kb)
------------------------------------------------------------------------------
\\
arXiv:2601.10512
replaced with revised version Tue, 20 Jan 2026 12:44:42 GMT   (6094kb)

Title: SatMap: Revisiting Satellite Maps as Prior for Online HD Map
  Construction
Authors: Kanak Mazumder and Fabian B. Flohr
Categories: cs.CV cs.AI cs.LG
Comments: This work has been submitted to the IEEE for possible publication
\\ ( https://arxiv.org/abs/2601.10512 ,  6094kb)
------------------------------------------------------------------------------
\\
arXiv:2601.10700
replaced with revised version Sun, 18 Jan 2026 06:28:47 GMT   (497kb)

Title: LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations
  of LLMs with Structural Counterfactuals
Authors: Gilat Toker, Nitay Calderon, Ohad Amosy, Roi Reichart
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2601.10700 ,  497kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11144
replaced with revised version Mon, 19 Jan 2026 09:50:42 GMT   (2083kb)

Title: Deep GraphRAG: A Balanced Approach to Hierarchical Retrieval and
  Adaptive Integration
Authors: Yuejie Li, Ke Yang, Tao Wang, Bolin Chen, Bowen Li, Chengjun Mao
Categories: cs.IR cs.AI
\\ ( https://arxiv.org/abs/2601.11144 ,  2083kb)
------------------------------------------------------------------------------
\\
arXiv:2601.11369
replaced with revised version Tue, 20 Jan 2026 12:10:21 GMT   (1137kb)

Title: Institutional AI: Governing LLM Collusion in Multi-Agent Cournot Markets
  via Public Governance Graphs
Authors: Marcantonio Bracale Syrnikov, Federico Pierucci, Marcello Galisai,
  Matteo Prandi, Piercosma Bisconti, Francesco Giarrusso, Olga Sorokoletova,
  Vincenzo Suriani, Daniele Nardi
Categories: cs.GT cs.AI
\\ ( https://arxiv.org/abs/2601.11369 ,  1137kb)
------------------------------------------------------------------------------
\\
arXiv:2110.01950 (*cross-listing*)
replaced with revised version Sat, 17 Jan 2026 02:38:40 GMT   (90kb)

Title: Classification of high-dimensional data with spiked covariance matrix
  structure
Authors: Yin-Jen Chen, Minh Tang
Categories: stat.ML cs.LG
Comments: 40 pages, 2 figures
\\ ( https://arxiv.org/abs/2110.01950 ,  90kb)
------------------------------------------------------------------------------
\\
arXiv:2209.04036 (*cross-listing*)
replaced with revised version Sun, 18 Jan 2026 21:29:05 GMT   (65kb)

Title: Functional dimension of feedforward ReLU neural networks
Authors: J. Elisenda Grigsby, Kathryn Lindsey, Robert Meyerhoff, Chenxi Wu
Categories: math.MG cs.LG math.GT
Comments: 54 pages, 1 figure; Substantial changes in response to referee
  comments: related work section added to end of introduction, notation for
  parameters changed to "theta" from "s" to match literature conventions, text
  added throughout to improve clarity of exposition
MSC-class: 68T07, 51M20, 57Q99
Journal-ref: Advances in Mathematics, Volume 482, Part C, December 2025, 110636
DOI: 10.1016/j.aim.2025.110636
\\ ( https://arxiv.org/abs/2209.04036 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2209.07861
replaced with revised version Tue, 20 Jan 2026 10:13:33 GMT   (8807kb)

Title: Machine Learning Decoder for 5G NR PUCCH Format 0
Authors: Anil Kumar Yerrapragada, Jeeva Keshav S, Ankit Gautam, Radha Krishna
  Ganti
Categories: cs.NI cs.IT cs.LG math.IT
Comments: Accepted at 2023 National Conference on Communications (NCC 2023),
  IIT Guwahati
DOI: 10.1109/NCC56989.2023.10067950
\\ ( https://arxiv.org/abs/2209.07861 ,  8807kb)
------------------------------------------------------------------------------
\\
arXiv:2302.09049
replaced with revised version Mon, 19 Jan 2026 19:35:42 GMT   (34kb)

Title: Multiperiodic Processes: Ergodic Sources with a Sublinear Entropy
Authors: {\L}ukasz D\k{e}bowski
Categories: cs.IT cs.LG math.IT math.ST stat.TH
Comments: 30 pages; 1 figure
MSC-class: 60G10 (Primary) 62M20, 94A17 (Secondary)
\\ ( https://arxiv.org/abs/2302.09049 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2304.00634
replaced with revised version Mon, 19 Jan 2026 00:46:50 GMT   (6644kb)

Title: MMT: A Multilingual and Multi-Topic Indian Social Media Dataset
Authors: Dwip Dalal, Vivek Srivastava, Mayank Singh
Categories: cs.CL cs.LG cs.SI
Comments: Dataset link: https://huggingface.co/datasets/LingoIITGN/MMT
Journal-ref: EACL Workshop C3NLP 2023
\\ ( https://arxiv.org/abs/2304.00634 ,  6644kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03137 (*cross-listing*)
replaced with revised version Sat, 17 Jan 2026 15:09:36 GMT   (2635kb)

Title: Topology-Aware Loss for Aorta and Great Vessel Segmentation in Computed
  Tomography Images
Authors: Seher Ozcelik, Sinan Unver, Ilke Ali Gurses, Rustu Turkay, and Cigdem
  Gunduz-Demir
Categories: eess.IV cs.CV cs.LG
Journal-ref: Biomedical Signal Processing and Control, 117, 109512 (2026)
DOI: 10.1016/j.bspc.2026.109512
\\ ( https://arxiv.org/abs/2307.03137 ,  2635kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10634 (*cross-listing*)
replaced with revised version Tue, 20 Jan 2026 17:19:40 GMT   (418kb)

Title: Generative Language Models on Nucleotide Sequences of Human Genes
Authors: Musa Nuri Ihtiyar and Arzucan Ozgur
Categories: q-bio.GN cs.CL cs.LG
Journal-ref: Scientific Reports, 2024, 14.1: 22204
DOI: 10.1038/s41598-024-72512-x
\\ ( https://arxiv.org/abs/2307.10634 ,  418kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12162 (*cross-listing*)
replaced with revised version Fri, 16 Jan 2026 20:46:49 GMT   (70kb)

Title: Optimal Conditional Inference in Adaptive Experiments
Authors: Jiafeng Chen and Isaiah Andrews
Categories: stat.ME cs.LG econ.EM math.ST stat.TH
Comments: An extended abstract of this paper was presented at CODE@MIT 2021
\\ ( https://arxiv.org/abs/2309.12162 ,  70kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03227 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 02:09:21 GMT   (2019kb)

Title: Transferable Graphical MARL for Real-Time Estimation in Dynamic Wireless
  Networks
Authors: Xingran Chen, Navid NaderiAlizadeh, Alejandro Ribeiro, Shirin Saeedi
  Bidokhti
Categories: eess.SP cs.LG
\\ ( https://arxiv.org/abs/2404.03227 ,  2019kb)
------------------------------------------------------------------------------
\\
arXiv:2405.09660 (*cross-listing*)
replaced with revised version Sun, 18 Jan 2026 21:10:25 GMT   (193kb)

Title: Fast Two-Time-Scale Stochastic Gradient Method with Applications in
  Reinforcement Learning
Authors: Sihan Zeng, Thinh T. Doan
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2405.09660 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2406.03599
replaced with revised version Fri, 16 Jan 2026 21:07:53 GMT   (6462kb)

Title: Hi5: Synthetic Data for Inclusive, Robust, Hand Pose Estimation
Authors: Masum Hasan, Cengiz Ozel, Nina Long, Alexander Martin, Samuel Potter,
  Tariq Adnan, Sangwu Lee, Ehsan Hoque
Categories: cs.CV cs.GR cs.LG
\\ ( https://arxiv.org/abs/2406.03599 ,  6462kb)
------------------------------------------------------------------------------
\\
arXiv:2407.15301 (*cross-listing*)
replaced with revised version Sun, 18 Jan 2026 17:52:39 GMT   (4099kb)

Title: U-learning for Prediction Inference via Combinatory Multi-Subsampling:
  With Applications to LASSO and Neural Networks
Authors: Zhe Fei, Yi Li
Categories: stat.ML cs.LG math.ST q-bio.QM stat.TH
\\ ( https://arxiv.org/abs/2407.15301 ,  4099kb)
------------------------------------------------------------------------------
\\
arXiv:2407.18552
replaced with revised version Tue, 20 Jan 2026 13:25:43 GMT   (1539kb)

Title: Multimodal Emotion Recognition using Audio-Video Transformer Fusion with
  Cross Attention
Authors: Joe Dhanith P R, Shravan Venkatraman, Vigya Sharma, Santhosh
  Malarvannan
Categories: cs.MM cs.CL cs.CV cs.LG cs.SD eess.AS
ACM-class: F.2.2; I.2.7
\\ ( https://arxiv.org/abs/2407.18552 ,  1539kb)
------------------------------------------------------------------------------
\\
arXiv:2409.02684 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 23:11:54 GMT   (1759kb)

Title: Neural timescales from a computational perspective
Authors: Roxana Zeraati, Anna Levina, Jakob H. Macke, Richard Gao
Categories: q-bio.NC cs.LG stat.ML
Comments: 21 pages, 5 figures, 3 boxes, 1 table
\\ ( https://arxiv.org/abs/2409.02684 ,  1759kb)
------------------------------------------------------------------------------
\\
arXiv:2412.08763
replaced with revised version Mon, 19 Jan 2026 18:24:22 GMT   (7949kb)

Title: Beyond Knowledge Silos: Task Fingerprinting for Democratization of
  Medical Imaging AI
Authors: Patrick Godau and Akriti Srivastava and Constantin Ulrich and Tim
  Adler and Klaus Maier-Hein and Lena Maier-Hein
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2412.08763 ,  7949kb)
------------------------------------------------------------------------------
\\
arXiv:2412.09734 (*cross-listing*)
replaced with revised version Sun, 18 Jan 2026 01:35:29 GMT   (501kb)

Title: MPAX: Mathematical Programming in JAX
Authors: Haihao Lu, Zedong Peng, Jinwen Yang
Categories: math.OC cs.LG
MSC-class: 90-04
\\ ( https://arxiv.org/abs/2412.09734 ,  501kb)
------------------------------------------------------------------------------
\\
arXiv:2412.11483
replaced with revised version Tue, 20 Jan 2026 15:34:08 GMT   (984kb)

Title: "They've Stolen My GPL-Licensed Model!": Toward Standardized and
  Transparent Model Licensing
Authors: Moming Duan, Rui Zhao, Linshan Jiang, Nigel Shadbolt, Bingsheng He
Categories: cs.CY cs.LG
Comments: 12 pages, 8 figures. Accepted for publication in WWW2026 Web4Good
DOI: 10.1145/3774904.3792968
\\ ( https://arxiv.org/abs/2412.11483 ,  984kb)
------------------------------------------------------------------------------
\\
arXiv:2501.06300
replaced with revised version Tue, 20 Jan 2026 17:48:40 GMT   (1064kb)

Title: Tensorization of neural networks for improved privacy and
  interpretability
Authors: Jos\'e Ram\'on Pareja Monturiol, Alejandro Pozas-Kerstjens, David
  P\'erez-Garc\'ia
Categories: math.NA cs.LG cs.NA physics.comp-ph quant-ph
Comments: 44 pages, 9 figures, 3 tables. The code for the experiments is
  publicly available at https://github.com/joserapa98/tensorization-nns. V3:
  Published version
Journal-ref: SciPost Phys. Core 8, 095 (2025)
DOI: 10.21468/SciPostPhysCore.8.4.095
\\ ( https://arxiv.org/abs/2501.06300 ,  1064kb)
------------------------------------------------------------------------------
\\
arXiv:2501.14750
replaced with revised version Mon, 19 Jan 2026 06:25:06 GMT   (423kb)

Title: Engineering Carbon Credits Towards A Responsible FinTech Era: The
  Practices, Implications, and Future
Authors: Qingwen Zeng, Hanlin Xu, Nanjun Xu, Zhenghao Zhao, Joakim Westerholm,
  Flora Salim, Junbin Gao, Huaming Chen
Categories: cs.CY cs.LG
\\ ( https://arxiv.org/abs/2501.14750 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2501.17512 (*cross-listing*)
replaced with revised version Tue, 20 Jan 2026 09:43:22 GMT   (2292kb,D)

Title: A survey on Clustered Federated Learning: Taxonomy, Analysis and
  Applications
Authors: Michael Ben Ali (IRIT), Omar El-Rifai (CIS-ENSMSE), Imen Megdiche
  (IRIT-SIG, INUC), Andr\'e Peninou (IRIT-SIG, UT2J), Olivier Teste (IRIT-SIG)
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2501.17512 ,  2292kb)
------------------------------------------------------------------------------
\\
arXiv:2502.04162 (*cross-listing*)
replaced with revised version Tue, 20 Jan 2026 02:38:25 GMT   (24181kb)

Title: Network-Level Measures of Mobility from Aggregated Origin-Destination
  Data
Authors: Alisha Foster, David A. Meyer, Asif Shakeel
Categories: stat.AP cs.LG cs.SI stat.ML
Comments: 34 pages, 20 figures
MSC-class: 82C41, 05C81, 90B15
\\ ( https://arxiv.org/abs/2502.04162 ,  24181kb)
------------------------------------------------------------------------------
\\
arXiv:2502.11105 (*cross-listing*)
replaced with revised version Tue, 20 Jan 2026 07:44:51 GMT   (1239kb)

Title: Graceful forgetting: Memory as a process
Authors: Alain de Cheveign\'e
Categories: q-bio.NC cs.IR cs.LG
\\ ( https://arxiv.org/abs/2502.11105 ,  1239kb)
------------------------------------------------------------------------------
\\
arXiv:2502.16542 (*cross-listing*)
replaced with revised version Sun, 18 Jan 2026 06:30:08 GMT   (1278kb)

Title: Variable transformations in consistent loss functions
Authors: Hristos Tyralis, Georgia Papacharalampous
Categories: stat.ML cs.LG
Comments: 37 pages, 4 figures, 2 tables
Journal-ref: Knowledge-Based Systems 336 (2026) 115202
DOI: 10.1016/j.knosys.2025.115202
\\ ( https://arxiv.org/abs/2502.16542 ,  1278kb)
------------------------------------------------------------------------------
\\
arXiv:2503.06431 (*cross-listing*)
replaced with revised version Sun, 18 Jan 2026 02:09:29 GMT   (74kb)

Title: Fairness-aware kidney exchange and kidney paired donation
Authors: Mingrui Zhang, Xiaowu Dai, Lexin Li
Categories: stat.ME cs.LG
\\ ( https://arxiv.org/abs/2503.06431 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2503.12538
replaced with revised version Sun, 18 Jan 2026 23:29:33 GMT   (2523kb)

Title: EmoBipedNav: Emotion-aware Social Navigation for Bipedal Robots with
  Deep Reinforcement Learning
Authors: Wei Zhu, Abirath Raju, Abdulaziz Shamsah, Anqi Wu, Seth Hutchinson,
  and Ye Zhao
Categories: cs.RO cs.LG
Comments: 13 pages
\\ ( https://arxiv.org/abs/2503.12538 ,  2523kb)
------------------------------------------------------------------------------
\\
arXiv:2504.02134 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 03:01:59 GMT   (1081kb)

Title: Robust Channel Estimation for Optical Wireless Communications Using
  Neural Network
Authors: Dianxin Luan and John Thompson
Categories: eess.SP cs.LG
Comments: Accepted by WCL
\\ ( https://arxiv.org/abs/2504.02134 ,  1081kb)
------------------------------------------------------------------------------
\\
arXiv:2505.08464
replaced with revised version Mon, 19 Jan 2026 08:48:09 GMT   (6058kb)

Title: Large Language Models Meet Stance Detection: A Survey of Tasks, Methods,
  Applications, Challenges and Future Directions
Authors: Lata Pangtey, Anukriti Bhatnagar, Shubhi Bansal, Shahid Shafi Dar and
  Nagendra Kumar
Categories: cs.CL cs.LG cs.SI
\\ ( https://arxiv.org/abs/2505.08464 ,  6058kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16051 (*cross-listing*)
replaced with revised version Sat, 17 Jan 2026 20:05:11 GMT   (901kb)

Title: Flow-based Generative Modeling of Potential Outcomes and Counterfactuals
Authors: Dongze Wu, David I. Inouye, Yao Xie
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2505.16051 ,  901kb)
------------------------------------------------------------------------------
\\
arXiv:2505.16714 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 08:42:05 GMT   (10781kb)

Title: Experimental robustness benchmarking of quantum neural networks on a
  superconducting quantum processor
Authors: Hai-Feng Zhang, Zhao-Yun Chen, Peng Wang, Liang-Liang Guo, Tian-Le
  Wang, Xiao-Yan Yang, Ren-Ze Zhao, Ze-An Zhao, Sheng Zhang, Lei Du, Hao-Ran
  Tao, Zhi-Long Jia, Wei-Cheng Kong, Huan-Yu Liu, Athanasios V. Vasilakos, Yang
  Yang, Yu-Chun Wu, Ji Guan, Peng Duan, Guo-Ping Guo
Categories: quant-ph cs.LG
Comments: There are 8 pages with 5 figures in the main text
\\ ( https://arxiv.org/abs/2505.16714 ,  10781kb)
------------------------------------------------------------------------------
\\
arXiv:2505.18918 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 19:51:31 GMT   (3438kb)

Title: ALPCAHUS: Subspace Clustering for Heteroscedastic Data
Authors: Javier Salazar Cavazos, Jeffrey A Fessler, Laura Balzano
Categories: stat.ML cs.LG eess.SP
Comments: Manuscript submitted to IEEE Transactions on Signal Processing (TSP),
  revised, and pending acceptance
\\ ( https://arxiv.org/abs/2505.18918 ,  3438kb)
------------------------------------------------------------------------------
\\
arXiv:2505.23816
replaced with revised version Sat, 17 Jan 2026 23:31:26 GMT   (4364kb)

Title: A Course Correction in Steerability Evaluation: Revealing Miscalibration
  and Side Effects in LLMs
Authors: Trenton Chang, Tobias Schnabel, Adith Swaminathan, Jenna Wiens
Categories: cs.CL cs.LG
Comments: 8 pages, 6 figures. 26 pages of references and supplementary
  material, 22 additional figures. Association for the Advancement of
  Artificial Intelligence Conference (AAAI 2026)
\\ ( https://arxiv.org/abs/2505.23816 ,  4364kb)
------------------------------------------------------------------------------
\\
arXiv:2506.18344
replaced with revised version Sun, 18 Jan 2026 15:06:18 GMT   (688kb)

Title: Dynamic Hybrid Modeling: Incremental Identification and Model Predictive
  Control
Authors: Adrian Caspari, Thomas Bierweiler, Sarah Fadda, Daniel Labisch,
  Maarten Nauta, Franzisko Wagner, Merle Warmbold, Constantinos C. Pantelides
Categories: eess.SY cs.LG cs.SY math.OC
Comments: 21 pages, 11 Figures
MSC-class: 93A30, 37N35, 68T05
ACM-class: I.2.6; I.2.8; I.6.3; I.6.5; G.1.6; J.2
Journal-ref: Computers & Chemical Engineering, Volume 204, January 2026, 109413
DOI: 10.1016/j.compchemeng.2025.109413
\\ ( https://arxiv.org/abs/2506.18344 ,  688kb)
------------------------------------------------------------------------------
\\
arXiv:2506.18560
replaced with revised version Sat, 17 Jan 2026 10:54:34 GMT   (820kb)

Title: Efficient Beam Selection for ISAC in Cell-Free Massive MIMO via Digital
  Twin-Assisted Deep Reinforcement Learning
Authors: Jiexin Zhang, Shu Xu, Chunguo Li, Yongming Huang, and Luxi Yang
Categories: cs.ET cs.LG
Comments: Accepted for publication in IEEE Transactions on Wireless
  Communications. This version includes minor revisions
\\ ( https://arxiv.org/abs/2506.18560 ,  820kb)
------------------------------------------------------------------------------
\\
arXiv:2506.21686
replaced with revised version Sat, 17 Jan 2026 16:28:22 GMT   (1531kb)

Title: ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla
  Regional Languages
Authors: Swastika Kundu, Autoshi Ibrahim, Mithila Rahman, Tanvir Ahmed
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2506.21686 ,  1531kb)
------------------------------------------------------------------------------
\\
arXiv:2506.21894 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 08:55:11 GMT   (1966kb)

Title: Thompson Sampling in Function Spaces via Neural Operators
Authors: Rafael Oliveira, Xuesong Wang, Kian Ming A. Chai and Edwin V. Bonilla
Categories: stat.ML cs.LG
Comments: Final revision to appear at NeurIPS 2025 proceedings, expanded proof
  of Proposition 2, added Remark 2 on sublinear information gain, and revised
  discussion at the end of Appendix C.4
\\ ( https://arxiv.org/abs/2506.21894 ,  1966kb)
------------------------------------------------------------------------------
\\
arXiv:2507.05197
replaced with revised version Tue, 20 Jan 2026 11:54:49 GMT   (12991kb)

Title: Pre-Trained Policy Discriminators are General Reward Models
Authors: Shihan Dou, Shichun Liu, Yuming Yang, Yicheng Zou, Yunhua Zhou, Shuhao
  Xing, Chenhao Huang, Qiming Ge, Demin Song, Haijun Lv, Songyang Gao, Chengqi
  Lv, Enyu Zhou, Honglin Guo, Zhiheng Xi, Wenwei Zhang, Qipeng Guo, Qi Zhang,
  Xipeng Qiu, Xuanjing Huang, Tao Gui, Kai Chen
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2507.05197 ,  12991kb)
------------------------------------------------------------------------------
\\
arXiv:2507.20058 (*cross-listing*)
replaced with revised version Sat, 17 Jan 2026 03:13:06 GMT   (1658kb)

Title: Predicting Parkinson's Disease Progression Using Statistical and Neural
  Mixed Effects Models: Comparative Study on Longitudinal Biomarkers
Authors: Ran Tong, Lanruo Wang, Tong Wang, Wei Yan
Categories: stat.ML cs.LG stat.AP
Comments: 20pages,3 figures,currently under review
\\ ( https://arxiv.org/abs/2507.20058 ,  1658kb)
------------------------------------------------------------------------------
\\
arXiv:2508.09803 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 14:38:18 GMT   (197kb)

Title: Improving the Speaker Anonymization Evaluation's Robustness to Target
  Speakers with Adversarial Learning
Authors: Carlos Franzreb, Arnab Das, Tim Polzehl, Sebastian M\"oller
Categories: eess.AS cs.LG
Comments: Accepted to ICASSP 2026
\\ ( https://arxiv.org/abs/2508.09803 ,  197kb)
------------------------------------------------------------------------------
\\
arXiv:2508.18177
replaced with revised version Sat, 17 Jan 2026 14:22:02 GMT   (5642kb)

Title: Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal
  Differentiated Quantization VLMs for Visually Impaired Assistance
Authors: Xiangxiang Wang, Xuanyu Wang, YiJia Luo, Yongbin Yu, Manping Fan,
  Jingtao Zhang, Liyong Ren
Categories: cs.CV cs.LG cs.MA
Comments: 28 pages,9 figures
\\ ( https://arxiv.org/abs/2508.18177 ,  5642kb)
------------------------------------------------------------------------------
\\
arXiv:2509.00024 (*cross-listing*)
replaced with revised version Sun, 18 Jan 2026 00:19:46 GMT   (8839kb)

Title: Generalization vs. Memorization in Autoregressive Deep Learning: Or,
  Examining Temporal Decay of Gradient Coherence
Authors: James Amarel, Nicolas Hengartner, Robyn Miller, Kamaljeet Singh,
  Siddharth Mansingh, Arvind Mohan, Benjamin Migliori, Emily Casleton, Alexei
  Skurikhin, Earl Lawrence, Gerd J. Kunde
Categories: physics.comp-ph cs.LG
\\ ( https://arxiv.org/abs/2509.00024 ,  8839kb)
------------------------------------------------------------------------------
\\
arXiv:2509.04459
replaced with revised version Sun, 18 Jan 2026 11:54:08 GMT   (4867kb)

Title: Uncertainty-Aware Collaborative System of Large and Small Models for
  Multimodal Sentiment Analysis
Authors: Shiqin Han, Manning Gao, Menghua Jiang, Yuncheng Jiang, Haifeng Hu,
  Sijie Mai
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2509.04459 ,  4867kb)
------------------------------------------------------------------------------
\\
arXiv:2509.16715 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 15:49:09 GMT   (109kb)

Title: QASTAnet: A DNN-based Quality Metric for Spatial Audio
Authors: Adrien Llave, Emma Granier, Gr\'egory Pallone
Categories: eess.AS cs.LG
Comments: Minor typo corrections, figure updates for clarity, add two
  references, no modifications of the results and their interpretation
\\ ( https://arxiv.org/abs/2509.16715 ,  109kb)
------------------------------------------------------------------------------
\\
arXiv:2509.20485 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 23:49:31 GMT   (278kb)

Title: Objective Evaluation of Prosody and Intelligibility in Speech Synthesis
  via Conditional Prediction of Discrete Tokens
Authors: Ismail Rasim Ulgen, Zongyang Du, Junchen Lu, Philipp Koehn, Berrak
  Sisman
Categories: eess.AS cs.LG cs.SD
Comments: Accepted by IEEE OJSP
Journal-ref: I. R. Ulgen, Z. Du, J. Lu, P. Koehn and B. Sisman, "Objective
  Evaluation of Prosody and Intelligibility in Speech Synthesis via Conditional
  Prediction of Discrete Tokens," in IEEE Open Journal of Signal Processing,
  2026
DOI: 10.1109/OJSP.2026.3653666
\\ ( https://arxiv.org/abs/2509.20485 ,  278kb)
------------------------------------------------------------------------------
\\
arXiv:2509.21661 (*cross-listing*)
replaced with revised version Fri, 16 Jan 2026 21:06:24 GMT   (5042kb)

Title: Automating Sensor Characterization with Bayesian Optimization
Authors: J. Cuevas-Zepeda, C. Chavez, J. Estrada, J. Noonan, B. D. Nord, N.
  Saffold, M. Sofo-Haro, R. Spinola e Castro, S. Trivedi
Categories: physics.ins-det astro-ph.IM cs.LG
Report-no: FERMILAB-PUB-25-0671-CSAID-PPD
\\ ( https://arxiv.org/abs/2509.21661 ,  5042kb)
------------------------------------------------------------------------------
\\
arXiv:2510.00014
replaced with revised version Sun, 18 Jan 2026 05:22:53 GMT   (3493kb)

Title: FTSCommDetector: Discovering Behavioral Communities through Temporal
  Synchronization
Authors: Tianyang Luo, Xikun Zhang, Dongjin Song
Categories: cs.SI cs.LG
\\ ( https://arxiv.org/abs/2510.00014 ,  3493kb)
------------------------------------------------------------------------------
\\
arXiv:2510.00884
replaced with revised version Tue, 20 Jan 2026 14:34:17 GMT   (5936kb)

Title: COMMET: orders-of-magnitude speed-up in finite element method via
  batch-vectorized neural constitutive updates
Authors: Benjamin Alheit, Mathias Peirlinck, Siddhant Kumar
Categories: cs.CE cs.LG
Comments: 43 pages, 17 figures
Journal-ref: Benjamin Alheit, Mathias Peirlinck, Siddhant Kumar; COMMET:
  Orders-of-magnitude speed-up in finite element method via batch-vectorized
  neural constitutive updates; Computer Methods in Applied Mechanics and
  Engineering 452 (2026)
DOI: 10.1016/j.cma.2026.118728
\\ ( https://arxiv.org/abs/2510.00884 ,  5936kb)
------------------------------------------------------------------------------
\\
arXiv:2510.02050 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 12:38:16 GMT   (10250kb)

Title: Multidata Causal Discovery for Statistical Hurricane Intensity
  Forecasting
Authors: Saranya Ganesh S, Frederick Iat-Hin Tam, Milton S. Gomez, Marie
  McGraw, Mark DeMaria, Kate Musgrave, Jakob Runge and Tom Beucler
Categories: stat.AP cs.LG
Comments: 20 pages, 8 Figures, 1 Table, SI; Revised manuscript following peer
  review
\\ ( https://arxiv.org/abs/2510.02050 ,  10250kb)
------------------------------------------------------------------------------
\\
arXiv:2510.04460 (*cross-listing*)
replaced with revised version Sat, 17 Jan 2026 05:37:40 GMT   (42kb)

Title: Perspectives on Stochastic Localization
Authors: Bobby Shi, Kevin Tian, Matthew S. Zhang
Categories: math.PR cs.DS cs.LG math.ST stat.TH
\\ ( https://arxiv.org/abs/2510.04460 ,  42kb)
------------------------------------------------------------------------------
\\
arXiv:2510.06980
replaced with revised version Tue, 20 Jan 2026 13:44:08 GMT   (232kb)

Title: Relational Database Distillation: From Structured Tables to Condensed
  Graph Data
Authors: Xinyi Gao, Jingxi Zhang, Lijian Chen, Tong Chen, Lizhen Cui, Hongzhi
  Yin
Categories: cs.DB cs.LG
\\ ( https://arxiv.org/abs/2510.06980 ,  232kb)
------------------------------------------------------------------------------
\\
arXiv:2510.08335 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 16:56:18 GMT   (622kb)

Title: PAC Learnability in the Presence of Performativity
Authors: Ivan Kirev, Lyuben Baltadzhiev, Nikola Konstantinov
Categories: stat.ML cs.LG
Comments: 21 pages, 3 figures; Added another assumption on the RN derivative in
  Section 5, to fix an incorrect bounding argument in the proof of Theorem 5.1
  in the initial version; more details on page 6
\\ ( https://arxiv.org/abs/2510.08335 ,  622kb)
------------------------------------------------------------------------------
\\
arXiv:2510.10020 (*cross-listing*)
replaced with revised version Sat, 17 Jan 2026 01:24:40 GMT   (18925kb)

Title: Calibrating Generative Models to Distributional Constraints
Authors: Henry D. Smith, Nathaniel L. Diamant, Brian L. Trippe
Categories: stat.ML cs.LG q-bio.BM
Comments: Our codebase accompanying the paper is available at:
  https://github.com/smithhenryd/cgm
\\ ( https://arxiv.org/abs/2510.10020 ,  18925kb)
------------------------------------------------------------------------------
\\
arXiv:2510.12416 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 09:02:54 GMT   (5590kb)

Title: Geopolitics, Geoeconomics and Risk: A Machine Learning Approach
Authors: Alvaro Ortiz, Tomasa Rodrigo and Pablo Saborido
Categories: stat.ML cs.LG
Comments: This new version has an important contribution by Pablo Saborido who
  is now a co author of the paper
\\ ( https://arxiv.org/abs/2510.12416 ,  5590kb)
------------------------------------------------------------------------------
\\
arXiv:2510.13794
replaced with revised version Sun, 18 Jan 2026 17:46:05 GMT   (14868kb)

Title: MimicKit: A Reinforcement Learning Framework for Motion Imitation and
  Control
Authors: Xue Bin Peng
Categories: cs.GR cs.LG cs.RO
\\ ( https://arxiv.org/abs/2510.13794 ,  14868kb)
------------------------------------------------------------------------------
\\
arXiv:2510.18239
replaced with revised version Sat, 17 Jan 2026 19:53:33 GMT   (9038kb)

Title: LIME: Link-based user-item Interaction Modeling with decoupled xor
  attention for Efficient test time scaling
Authors: Yunjiang Jiang, Ayush Agarwal, Yang Liu, Bi Xue
Categories: cs.IR cs.LG
Comments: 19 pages
\\ ( https://arxiv.org/abs/2510.18239 ,  9038kb)
------------------------------------------------------------------------------
\\
arXiv:2510.19145
replaced with revised version Sun, 18 Jan 2026 18:05:20 GMT   (1901kb)

Title: HAMLOCK: HArdware-Model LOgically Combined attacK
Authors: Sanskar Amgain, Daniel Lobo, Atri Chatterjee, Swarup Bhunia, Fnu Suya
Categories: cs.CR cs.LG
Comments: Accepted to usenix security 2026
\\ ( https://arxiv.org/abs/2510.19145 ,  1901kb)
------------------------------------------------------------------------------
\\
arXiv:2510.21057
replaced with revised version Sat, 17 Jan 2026 09:50:35 GMT   (361kb)

Title: Soft Instruction De-escalation Defense
Authors: Nils Philipp Walter, Chawin Sitawarin, Jamie Hayes, David Stutz, Ilia
  Shumailov
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2510.21057 ,  361kb)
------------------------------------------------------------------------------
\\
arXiv:2510.24987 (*cross-listing*)
replaced with revised version Sat, 17 Jan 2026 05:05:58 GMT   (14437kb)

Title: scMRDR: A scalable and flexible framework for unpaired single-cell
  multi-omics data integration
Authors: Jianle Sun, Chaoqi Liang, Ran Wei, Peng Zheng, Lei Bai, Wanli Ouyang,
  Hongliang Yan, Peng Ye
Categories: q-bio.QM cs.LG q-bio.GN
Comments: Accepted at NeurIPS 2025 (Spotlight)
\\ ( https://arxiv.org/abs/2510.24987 ,  14437kb)
------------------------------------------------------------------------------
\\
arXiv:2511.06305
replaced with revised version Mon, 19 Jan 2026 14:43:39 GMT   (74kb)

Title: Setting $\varepsilon$ is not the Issue in Differential Privacy
Authors: Edwige Cyffers
Categories: cs.CR cs.LG
Comments: NeurIPS Position Paper track 2025
\\ ( https://arxiv.org/abs/2511.06305 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2511.07883
replaced with revised version Mon, 19 Jan 2026 02:52:14 GMT   (1195kb)

Title: SpikCommander: A High-performance Spiking Transformer with Multi-view
  Learning for Efficient Speech Command Recognition
Authors: Jiaqi Wang, Liutao Yu, Xiongri Shen, Sihang Guo, Chenlin Zhou, Leilei
  Zhao, Yi Zhong, Zhiguo Zhang, Zhengyu Ma
Categories: cs.SD cs.LG
Comments: Accepted by The Fortieth AAAI Conference on Artificial Intelligence
  (AAAI 2026)
\\ ( https://arxiv.org/abs/2511.07883 ,  1195kb)
------------------------------------------------------------------------------
\\
arXiv:2512.08601 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 06:33:34 GMT   (375kb)

Title: Heuristics for Combinatorial Optimization via Value-based Reinforcement
  Learning: A Unified Framework and Analysis
Authors: Orit Davidovich, Shimrit Shtern, Segev Wasserkrug, Nimrod Megiddo
Categories: stat.ML cs.LG math.OC
\\ ( https://arxiv.org/abs/2512.08601 ,  375kb)
------------------------------------------------------------------------------
\\
arXiv:2512.10825 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 15:57:45 GMT   (19kb)

Title: An Elementary Proof of the Near Optimality of LogSumExp Smoothing
Authors: Thabo Samakhoana, Benjamin Grimmer
Categories: math.ST cs.LG math.OC stat.TH
Comments: 11 pages
\\ ( https://arxiv.org/abs/2512.10825 ,  19kb)
------------------------------------------------------------------------------
\\
arXiv:2512.14308 (*cross-listing*)
replaced with revised version Tue, 20 Jan 2026 13:25:30 GMT   (2957kb)

Title: Improving the Accuracy of Amortized Model Comparison with
  Self-Consistency
Authors: \v{S}imon Kucharsk\'y and Aayush Mishra and Daniel Habermann and
  Stefan T. Radev and Paul-Christian B\"urkner
Categories: stat.ML cs.LG stat.CO
Comments: 17 pages, 9 figures
\\ ( https://arxiv.org/abs/2512.14308 ,  2957kb)
------------------------------------------------------------------------------
\\
arXiv:2512.18021 (*cross-listing*)
replaced with revised version Tue, 20 Jan 2026 12:52:08 GMT   (324kb)

Title: Shuttling Compiler for Trapped-Ion Quantum Computers Based on Large
  Language Models
Authors: Fabian Kreppel, Reza Salkhordeh, Ferdinand Schmidt-Kaler, Andr\'e
  Brinkmann
Categories: quant-ph cs.ET cs.LG
Comments: 17 pages, 5 figures, 2 tables
\\ ( https://arxiv.org/abs/2512.18021 ,  324kb)
------------------------------------------------------------------------------
\\
arXiv:2512.24116 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 12:27:02 GMT   (9686kb)

Title: Quantitative Understanding of PDF Fits and their Uncertainties
Authors: Amedeo Chiefa, Luigi Del Debbio and Richard Kenway
Categories: hep-ph cs.LG
Comments: 38 pages
\\ ( https://arxiv.org/abs/2512.24116 ,  9686kb)
------------------------------------------------------------------------------
\\
arXiv:2601.04083
replaced with revised version Mon, 19 Jan 2026 08:56:16 GMT   (362kb)

Title: Cells on Autopilot: Adaptive Cell (Re)Selection via Reinforcement
  Learning
Authors: Marvin Illian, Ramin Khalili, Antonio A. de A. Rocha and Lin Wang
Categories: cs.NI cs.LG
Comments: 11 pages, 13 figures, 3 tables, v3: Added analysis of heuristic
  tuning trade-offs (Config-A vs Config-B) across scenarios with corresponding
  reference-value table; corrected performance numbers in the conclusion; no
  change to methodology
\\ ( https://arxiv.org/abs/2601.04083 ,  362kb)
------------------------------------------------------------------------------
\\
arXiv:2601.07061 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 15:10:41 GMT   (2381kb)

Title: Local EGOP for Continuous Index Learning
Authors: Alex Kokot, Anand Hemmady, Vydhourie Thiyageswaran, Marina Meila
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2601.07061 ,  2381kb)
------------------------------------------------------------------------------
\\
arXiv:2601.07276
replaced with revised version Mon, 19 Jan 2026 11:48:59 GMT   (186kb)

Title: A High-Recall Cost-Sensitive Machine Learning Framework for Real-Time
  Online Banking Transaction Fraud Detection
Authors: Karthikeyan V. R., Premnath S., Kavinraaj S., J. Sangeetha
Categories: cs.CR cs.LG
Comments: 8 pages, 5 figures. Submitted to arXiv as a preprint
ACM-class: K.6.5; K.4.4; I.2.6
\\ ( https://arxiv.org/abs/2601.07276 ,  186kb)
------------------------------------------------------------------------------
\\
arXiv:2601.09258
replaced with revised version Tue, 20 Jan 2026 07:29:49 GMT   (844kb)

Title: LatencyPrism: Online Non-intrusive Latency Sculpting for SLO-Guaranteed
  LLM Inference
Authors: Yin Du, Jiayi Ren, Xiayu Sun, Tianyao Zhou, Haizhu Zhou, Ruiyan Ma,
  Danyang Zhang
Categories: cs.DC cs.LG cs.OS
Comments: 13 pages, 6 figures
\\ ( https://arxiv.org/abs/2601.09258 ,  844kb)
------------------------------------------------------------------------------
\\
arXiv:2601.10657
replaced with revised version Fri, 16 Jan 2026 22:31:40 GMT   (758kb)

Title: PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution
Authors: Minghao Yan, Bo Peng, Benjamin Coleman, Ziqi Chen, Zhouhang Xie, Shuo
  Chen, Zhankui He, Noveen Sachdeva, Isabella Ye, Weili Wang, Chi Wang, Ed H.
  Chi, Fernando Pereira, Wang-Cheng Kang, Derek Zhiyuan Cheng, Beidou Wang
Categories: cs.NE cs.LG
\\ ( https://arxiv.org/abs/2601.10657 ,  758kb)
------------------------------------------------------------------------------
\\
arXiv:2601.10993 (*cross-listing*)
replaced with revised version Mon, 19 Jan 2026 09:43:26 GMT   (398kb)

Title: Memorize Early, Then Query: Inlier-Memorization-Guided Active Outlier
  Detection
Authors: Minseo Kang, Seunghwan Park, Dongha Kim
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2601.10993 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2601.10999
replaced with revised version Tue, 20 Jan 2026 15:28:58 GMT   (0kb,I)

Title: Exact Constraint Enforcement in Physics-Informed Extreme Learning
  Machines using Null-Space Projection Framework
Authors: Rishi Mishra, Smriti, Balaji Srinivasan, Sundararajan Natarajan,
  Ganapathy Krishnamurthi
Categories: math.NA cs.LG cs.NA
Comments: The authors are withdrawing this manuscript in order to substantially
  revise the presentation and positioning of the work with respect to related
  literature. A revised version may be posted in the future
\\ ( https://arxiv.org/abs/2601.10999 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2502.13163
replaced with revised version Mon, 19 Jan 2026 18:59:22 GMT   (448kb)

Title: A Survey of Fuzzing Open-Source Operating Systems
Authors: Kun Hu, Qicai Chen, Wenzhuo Zhang, Zilong Lu, Bihuan Chen, You Lu,
  Haowen Jiang, Bingkun Sun, Xin Peng, Wenyun Zhao
Categories: cs.OS cs.CR cs.SE
Comments: 35 pages
\\ ( https://arxiv.org/abs/2502.13163 ,  448kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
